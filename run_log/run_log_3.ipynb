{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_log_@.ipynb","provenance":[],"collapsed_sections":["qNuNsfPbNnm1"],"authorship_tag":"ABX9TyOtg8SYvn8Pn261UV9zs7xk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"iqPo04N5w_1_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1593698727088,"user_tz":-480,"elapsed":26848,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"8da38a8a-67f3-4888-fdb9-7b012f31020a"},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VjRJqb6oNQVK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"status":"ok","timestamp":1593698730287,"user_tz":-480,"elapsed":5922,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"079d5456-915f-438d-8bbf-9300c7723fc1"},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Thu Jul  2 14:05:27 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   52C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bDo69TOpxbE5","colab_type":"code","colab":{}},"source":["!pip uninstall tensorflow\n","!pip install tensorflow-gpu==1.14.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qNuNsfPbNnm1","colab_type":"text"},"source":["# 数据处理"]},{"cell_type":"code","metadata":{"id":"MlWRib-2xo_J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1591080584328,"user_tz":-480,"elapsed":152157,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"20207cb7-5fcf-405e-c0c6-21767c57d660"},"source":["!pip install hanziconv\n","!pip install pyhanlp\n","from pyhanlp import *"],"execution_count":null,"outputs":[{"output_type":"stream","text":["下载 https://file.hankcs.com/bin/hanlp-1.7.7-release.zip 到 /usr/local/lib/python3.6/dist-packages/pyhanlp/static/hanlp-1.7.7-release.zip\n","100.00%, 1 MB, 698 KB/s, 还有 0 分  0 秒   \n","下载 https://file.hankcs.com/hanlp/data-for-1.7.5.zip 到 /usr/local/lib/python3.6/dist-packages/pyhanlp/static/data-for-1.7.7.zip\n","94.84%, 604 MB, 6579 KB/s, 还有 0 分  5 秒   "],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sAOSKcIayKhV","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jPwUwCiexeK0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1593699597010,"user_tz":-480,"elapsed":7442,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"ca09857c-0089-4c56-8ffc-0865b0766df4"},"source":["% cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/nlp_task/fine_gained/mycode\n","! pwd\n","! ls"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/mycode\n","/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/mycode\n","data\t\t elmo_no_all_drop.py  output\t       run_log.ipynb\n","data_not_aug\t elmo.py\t      preprocess.sh    weight_drop_lstm.py\n","data_process.py  elmo_run.py\t      __pycache__\n","dataset.py\t elmo_utils.py\t      run_log_@.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X5qWE0_6x4A8","colab_type":"code","colab":{}},"source":["!bash preprocess.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5SaET7_-lmQO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"status":"ok","timestamp":1591094716421,"user_tz":-480,"elapsed":622686,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"9abbf399-b0fb-4f44-cad1-f10474c0db8f"},"source":["# # 未增强的数据\n","# !python data_process.py \\\n","#     --data_file='../../data/trainingset/sentiment_analysis_trainingset.csv' \\\n","#     --output_file=data_not_aug/train.json \\\n","#     --vocab_file=data_not_aug/vocab.txt \\\n","# \t--is_trian_file=True \\\n","#     --vocab_size=50000"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# processed -- 10000 --\n","# processed -- 20000 --\n","# processed -- 30000 --\n","# processed -- 40000 --\n","# processed -- 50000 --\n","# processed -- 60000 --\n","# processed -- 70000 --\n","# processed -- 80000 --\n","# processed -- 90000 --\n","# processed -- 100000 --\n","# Start to create vocab ...\n","# Created vocab file data_not_aug/vocab.txt with vocab size 50000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9wdF1wHQpdf6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1591096177654,"user_tz":-480,"elapsed":213652,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"5ef4c68e-12cb-4323-b8c5-aa41069a0fa6"},"source":["# !python data_process.py \\\n","#     --data_file='../fsauor2018/wordvec/sgns.sogou.word' \\\n","#     --output_file=data_not_aug/embedding.txt \\\n","#     --vocab_file=data_not_aug/vocab.txt \\\n","#     --embedding=True\n","\n","# !python data_process.py \\\n","#     --data_file='../../data/validationset/sentiment_analysis_validationset.csv' \\\n","#     --output_file=data_not_aug/validation.json\n","\n","# !python data_process.py \\\n","#     --data_file='../../data/testa/sentiment_analysis_testa.csv' \\\n","#     --output_file=data_not_aug/testa.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# processed -- 10000 --\n","# processed -- 10000 --\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fw9m-5zmpxnH","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WneyaCLQkmLk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1591097204601,"user_tz":-480,"elapsed":11797,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"1236e508-2c0b-4554-aae9-b4fd1a620504"},"source":["# import json\n","\n","# max_len = 0\n","# count = 0\n","# with open('data/clip_trian.json', 'w',encoding='utf-8') as f:\n","#     for line in open('data/train.json'):\n","#         item = json.loads(line.strip())\n","#         content = item['content']\n","#         tokens = content.strip().split()\n","#         length = len(tokens)\n","#         if length > max_len:\n","#             max_len = length\n","#         if length > 1000:\n","#             count += 1\n","#             tokens = tokens[: 500] +  tokens[-500: ]\n","#         item['content'] = ' '.join(tokens)\n","#         abc = json.dumps(item, ensure_ascii=False) + '\\n'\n","#         f.write(abc)\n","        \n","# print(max_len)\n","# print(count)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1493\n","297\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MChEHA8Tx33Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1591097212879,"user_tz":-480,"elapsed":6210,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"b124f499-9a55-4549-9446-8948af0e9586"},"source":["# max_len = 0\n","# count = 0\n","# for line in open('data/clip_trian.json'):\n","#         item = json.loads(line.strip())\n","#         content = item['content']\n","#         tokens = content.strip().split()\n","#         length = len(tokens)\n","#         if length > max_len:\n","#             max_len = length\n","#         if length > 1000:\n","#             count += 1\n","# print(max_len)\n","# print(count)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1000\n","0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m15rpRR2NsNv","colab_type":"text"},"source":["# 模型训练"]},{"cell_type":"code","metadata":{"id":"32-ZAkWIODb7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1593698850586,"user_tz":-480,"elapsed":80588,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"68181dad-e7a0-4f08-cca7-2bc0e1687f9d"},"source":["% cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/nlp_task/fine_gained/mycode\n","! pwd\n","! ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/mycode\n","/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/mycode\n","data\t\t elmo_no_all_drop.py  output\t       run_log.ipynb\n","data_not_aug\t elmo.py\t      preprocess.sh    weight_drop_lstm.py\n","data_process.py  elmo_run.py\t      __pycache__\n","dataset.py\t elmo_utils.py\t      run_log_@.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9t_LMcfY1ieq","colab_type":"code","colab":{}},"source":["# 裁剪前后文字的数据，效果在长度减小后变差\n","!python elmo_run.py \\\n","--mode=train \\\n","--data_files=data/clip_trian.json \\\n","--eval_files=data/validation.json \\\n","--label_file=data/labels.txt \\\n","--vocab_file=data/vocab.txt \\\n","--embed_file=data/embedding.txt \\\n","--num_layers=3 \\\n","--batch_size=32 \\\n","--max_len=1000 \\\n","--rnn_cell_name=gru \\\n","--num_classes_each_label=4 \\\n","--num_labels=20 \\\n","--steps_per_eval=2000 \\\n","--optimizer='adam' \\\n","--learning_rate=0.001 \\\n","--focal_loss_gamma=0 \\\n","--label_smoothing=0.1 \\\n","--checkpoint_dir=output/elmo_ema_smoothing_aug"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzTXYfTZc7i4","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hk47S_XHc8Bd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591242184765,"user_tz":-480,"elapsed":169569,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"05113b40-44a3-4e81-c7af-9d26b4effd39"},"source":["# 未裁剪，只截断\n","!python elmo_run.py \\\n","--mode=train \\\n","--data_files=data/train.json \\\n","--eval_files=data/validation.json \\\n","--label_file=data/labels.txt \\\n","--vocab_file=data/vocab.txt \\\n","--embed_file=data/embedding.txt \\\n","--num_layers=3 \\\n","--batch_size=64 \\\n","--max_len=1200 \\\n","--rnn_cell_name='WEIGHT_LSTM' \\\n","--linear_dropout=0.5 \\\n","--num_classes_each_label=4 \\\n","--num_labels=20 \\\n","--steps_per_eval=1000 \\\n","--optimizer='rms' \\\n","--learning_rate=0.001 \\\n","--focal_loss_gamma=0 \\\n","--label_smoothing=0 \\\n","--checkpoint_dir=output/elmo_1200_more_drop_noSmooth"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/train.json ...\n","# Got 129956 data items with 2030 batches\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/validation.json ...\n","# Got 15000 data items with 117 batches\n","  saving config to output/elmo_1200_more_drop_noSmooth/config\n","mode=train,data_files=['data/train.json'],eval_files=['data/validation.json'],label_file=data/labels.txt,vocab_file=data/vocab.txt,embed_file=data/embedding.txt,out_file=None,split_word=True,reverse=False,weight_file=None,prob=False,max_len=1200,batch_size=64,num_layers=3,optimizer=rms,learning_rate=0.001,decay_schema=hand,decay_steps=10000,focal_loss_gamma=0.0,max_gradient_norm=2.0,l2_loss_ratio=0.0,label_smoothing=0.0,embedding_dropout=0.1,dropout_keep_prob=0.8,weight_keep_drop=0.8,linear_dropout=0.5,rnn_cell_name=WEIGHT_LSTM,embedding_size=300,num_units=300,num_classes_each_label=4,num_labels=20,fix_embedding=False,need_early_stop=True,patient=5,debug=False,num_train_epoch=50,steps_per_stats=20,steps_per_summary=50,steps_per_eval=1000,checkpoint_dir=output/elmo_1200_more_drop_noSmooth,checkpoint_load_step=None,vocab_size=50000\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","variable <tf.Variable 'embedding/embedding:0' shape=(50000, 300) dtype=float32_ref> with parameter number 15000000\n","variable <tf.Variable 'embedding/label_embedding:0' shape=(20, 300) dtype=float32_ref> with parameter number 6000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/weight:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","variable <tf.Variable 'elmo_encoder/scalar:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/memory_layer/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/attention_op/dense/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/dense_1/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense_1/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/drop_connect_layer/kernel:0' shape=(300, 1200) dtype=float32_ref> with parameter number 360000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/luong_attention/attention_g:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/attention_layer/kernel:0' shape=(900, 300) dtype=float32_ref> with parameter number 270000\n","variable <tf.Variable 'classifier/predict_clf/dense/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/predict_clf/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/predict_clf/dense_1/kernel:0' shape=(300, 4) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/predict_clf/dense_1/bias:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","# total parameter number 23566510\n","loading config from output/elmo_1200_more_drop_noSmooth/config\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","!!! Unable to restore model, train from scratch !!!\n","=>> Start to train with learning rate 0.001\n","=>> Global step 0\n","=>> Epoch 1  global step 20 loss 21.85718 batch 20/2030 lr 0.001 accuracy 67.70508 wps 17586.26 step time 0.77s\n","=>> Epoch 1  global step 40 loss 20.95148 batch 40/2030 lr 0.001 accuracy 79.07031 wps 31583.70 step time 0.47s\n","=>> Epoch 1  global step 60 loss 18.63008 batch 60/2030 lr 0.001 accuracy 80.66992 wps 31512.31 step time 0.47s\n","=>> Epoch 1  global step 80 loss 15.73575 batch 80/2030 lr 0.001 accuracy 81.27734 wps 29874.12 step time 0.54s\n","=>> Epoch 1  global step 100 loss 15.12157 batch 100/2030 lr 0.001 accuracy 82.89844 wps 30337.48 step time 0.50s\n","=>> Epoch 1  global step 120 loss 14.40001 batch 120/2030 lr 0.001 accuracy 83.73633 wps 29606.87 step time 0.42s\n","=>> Epoch 1  global step 140 loss 14.26866 batch 140/2030 lr 0.001 accuracy 83.63672 wps 30568.45 step time 0.45s\n","=>> Epoch 1  global step 160 loss 13.89607 batch 160/2030 lr 0.001 accuracy 84.21484 wps 29258.75 step time 0.40s\n","=>> Epoch 1  global step 180 loss 14.50921 batch 180/2030 lr 0.001 accuracy 83.10742 wps 31234.97 step time 0.52s\n","=>> Epoch 1  global step 200 loss 13.83053 batch 200/2030 lr 0.001 accuracy 83.99023 wps 29992.03 step time 0.42s\n","=>> Epoch 1  global step 220 loss 14.15481 batch 220/2030 lr 0.001 accuracy 83.39062 wps 29645.69 step time 0.53s\n","=>> Epoch 1  global step 240 loss 14.40396 batch 240/2030 lr 0.001 accuracy 82.39648 wps 33037.12 step time 0.51s\n","=>> Epoch 1  global step 260 loss 13.80026 batch 260/2030 lr 0.001 accuracy 83.76367 wps 30045.92 step time 0.43s\n","=>> Epoch 1  global step 280 loss 13.65132 batch 280/2030 lr 0.001 accuracy 83.92969 wps 29759.53 step time 0.40s\n","=>> Epoch 1  global step 300 loss 13.88730 batch 300/2030 lr 0.001 accuracy 83.15430 wps 32010.47 step time 0.48s\n","=>> Epoch 1  global step 320 loss 13.78824 batch 320/2030 lr 0.001 accuracy 83.06445 wps 33157.54 step time 0.52s\n","=>> Epoch 1  global step 340 loss 13.35584 batch 340/2030 lr 0.001 accuracy 83.81445 wps 30083.01 step time 0.48s\n","=>> Epoch 1  global step 360 loss 13.45398 batch 360/2030 lr 0.001 accuracy 83.56641 wps 30684.05 step time 0.44s\n","=>> Epoch 1  global step 380 loss 13.11494 batch 380/2030 lr 0.001 accuracy 84.40234 wps 29762.75 step time 0.42s\n","=>> Epoch 1  global step 400 loss 13.45612 batch 400/2030 lr 0.001 accuracy 83.71484 wps 31511.19 step time 0.53s\n","=>> Epoch 1  global step 420 loss 13.02490 batch 420/2030 lr 0.001 accuracy 84.47461 wps 30276.26 step time 0.42s\n","=>> Epoch 1  global step 440 loss 13.23168 batch 440/2030 lr 0.001 accuracy 84.04102 wps 30776.86 step time 0.43s\n","=>> Epoch 1  global step 460 loss 13.17412 batch 460/2030 lr 0.001 accuracy 84.01562 wps 30874.37 step time 0.45s\n","=>> Epoch 1  global step 480 loss 13.34846 batch 480/2030 lr 0.001 accuracy 83.53125 wps 33234.25 step time 0.53s\n","=>> Epoch 1  global step 500 loss 12.95316 batch 500/2030 lr 0.001 accuracy 84.14844 wps 31527.49 step time 0.46s\n","=>> Epoch 1  global step 520 loss 12.87755 batch 520/2030 lr 0.001 accuracy 84.25977 wps 31479.83 step time 0.46s\n","=>> Epoch 1  global step 540 loss 12.79790 batch 540/2030 lr 0.001 accuracy 84.56055 wps 30942.23 step time 0.44s\n","=>> Epoch 1  global step 560 loss 12.99682 batch 560/2030 lr 0.001 accuracy 84.01953 wps 32259.49 step time 0.55s\n","=>> Epoch 1  global step 580 loss 12.90249 batch 580/2030 lr 0.001 accuracy 84.36133 wps 31251.83 step time 0.45s\n","=>> Epoch 1  global step 600 loss 12.45389 batch 600/2030 lr 0.001 accuracy 85.09570 wps 30913.76 step time 0.44s\n","=>> Epoch 1  global step 620 loss 12.60093 batch 620/2030 lr 0.001 accuracy 84.97461 wps 30383.16 step time 0.49s\n","=>> Epoch 1  global step 640 loss 12.15971 batch 640/2030 lr 0.001 accuracy 85.52344 wps 29573.93 step time 0.41s\n","=>> Epoch 1  global step 660 loss 12.31889 batch 660/2030 lr 0.001 accuracy 85.14844 wps 30043.87 step time 0.42s\n","=>> Epoch 1  global step 680 loss 12.36799 batch 680/2030 lr 0.001 accuracy 85.35156 wps 30543.72 step time 0.43s\n","=>> Epoch 1  global step 700 loss 12.23337 batch 700/2030 lr 0.001 accuracy 85.31250 wps 31840.46 step time 0.47s\n","=>> Epoch 1  global step 720 loss 11.87009 batch 720/2030 lr 0.001 accuracy 85.82422 wps 30654.02 step time 0.43s\n","=>> Epoch 1  global step 740 loss 11.96788 batch 740/2030 lr 0.001 accuracy 85.83594 wps 29856.82 step time 0.41s\n","=>> Epoch 1  global step 760 loss 12.86962 batch 760/2030 lr 0.001 accuracy 83.93750 wps 32359.81 step time 0.63s\n","=>> Epoch 1  global step 780 loss 11.94810 batch 780/2030 lr 0.001 accuracy 85.68555 wps 30929.42 step time 0.44s\n","=>> Epoch 1  global step 800 loss 11.84665 batch 800/2030 lr 0.001 accuracy 85.72656 wps 31144.62 step time 0.45s\n","=>> Epoch 1  global step 820 loss 11.88421 batch 820/2030 lr 0.001 accuracy 85.79883 wps 31769.57 step time 0.47s\n","=>> Epoch 1  global step 840 loss 11.60912 batch 840/2030 lr 0.001 accuracy 86.08398 wps 31855.61 step time 0.47s\n","=>> Epoch 1  global step 860 loss 11.23175 batch 860/2030 lr 0.001 accuracy 86.80078 wps 30152.35 step time 0.42s\n","=>> Epoch 1  global step 880 loss 11.37820 batch 880/2030 lr 0.001 accuracy 86.53516 wps 30968.99 step time 0.44s\n","=>> Epoch 1  global step 900 loss 10.94241 batch 900/2030 lr 0.001 accuracy 87.09375 wps 30864.76 step time 0.44s\n","=>> Epoch 1  global step 920 loss 11.82138 batch 920/2030 lr 0.001 accuracy 85.58984 wps 31889.32 step time 0.62s\n","=>> Epoch 1  global step 940 loss 11.08723 batch 940/2030 lr 0.001 accuracy 86.62109 wps 31024.07 step time 0.51s\n","=>> Epoch 1  global step 960 loss 11.32212 batch 960/2030 lr 0.001 accuracy 86.44727 wps 30893.62 step time 0.58s\n","=>> Epoch 1  global step 980 loss 11.34392 batch 980/2030 lr 0.001 accuracy 86.37305 wps 31505.04 step time 0.59s\n","=>> Epoch 1  global step 1000 loss 11.16851 batch 1000/2030 lr 0.001 accuracy 86.83203 wps 32996.24 step time 0.51s\n","=>> global step 1000, eval result: \n","=>> location_traffic_convenience - 0.2822513588703147\n","=>> location_distance_from_business_district - 0.23340094841201867\n","=>> location_easy_to_find - 0.21715190828179212\n","=>> service_wait_time - 0.2343910472075645\n","=>> service_waiters_attitude - 0.19545346199805047\n","=>> service_parking_convenience - 0.241788886593679\n","=>> service_serving_speed - 0.22901687321602773\n","=>> price_level - 0.16662221629550608\n","=>> price_cost_effective - 0.21621008021795066\n","=>> price_discount - 0.19087461874536316\n","=>> environment_decoration - 0.18124178357567342\n","=>> environment_noise - 0.20612436816739158\n","=>> environment_space - 0.19376505655138623\n","=>> environment_cleaness - 0.19507236949097415\n","=>> dish_portion - 0.17549325025960538\n","=>> dish_taste - 0.2796926982320389\n","=>> dish_look - 0.2088283251805264\n","=>> dish_recommendation - 0.22307351475095077\n","=>> others_overall_experience - 0.20090532025205393\n","=>> others_willing_to_consume_again - 0.19204237496920423\n","=>> Eval loss 90.07907, f1 0.21317\n","=>> current result -0.21317002306340357, previous best result 1000000000\n","=>> Epoch 1  global step 1020 loss 11.10284 batch 1020/2030 lr 0.001 accuracy 87.00195 wps 31585.37 step time 0.50s\n","=>> Epoch 1  global step 1040 loss 10.57579 batch 1040/2030 lr 0.001 accuracy 87.63867 wps 33408.54 step time 0.52s\n","=>> Epoch 1  global step 1060 loss 10.22007 batch 1060/2030 lr 0.001 accuracy 88.14844 wps 30959.08 step time 0.46s\n","=>> Epoch 1  global step 1080 loss 10.14516 batch 1080/2030 lr 0.001 accuracy 88.28711 wps 27982.64 step time 0.52s\n","=>> Epoch 1  global step 1100 loss 10.24952 batch 1100/2030 lr 0.001 accuracy 88.04883 wps 28482.02 step time 0.58s\n","=>> Epoch 1  global step 1120 loss 10.20425 batch 1120/2030 lr 0.001 accuracy 88.19727 wps 27991.87 step time 0.56s\n","=>> Epoch 1  global step 1140 loss 9.60213 batch 1140/2030 lr 0.001 accuracy 88.96094 wps 27271.67 step time 0.44s\n","=>> Epoch 1  global step 1160 loss 9.72768 batch 1160/2030 lr 0.001 accuracy 89.01172 wps 26022.10 step time 0.57s\n","=>> Epoch 1  global step 1180 loss 9.73036 batch 1180/2030 lr 0.001 accuracy 88.78320 wps 24806.73 step time 0.61s\n","=>> Epoch 1  global step 1200 loss 9.93280 batch 1200/2030 lr 0.001 accuracy 88.46680 wps 27313.17 step time 0.64s\n","=>> Epoch 1  global step 1220 loss 9.68791 batch 1220/2030 lr 0.001 accuracy 88.97070 wps 28141.49 step time 0.57s\n","=>> Epoch 1  global step 1240 loss 9.60702 batch 1240/2030 lr 0.001 accuracy 88.86523 wps 26586.83 step time 0.65s\n","=>> Epoch 1  global step 1260 loss 9.63227 batch 1260/2030 lr 0.001 accuracy 88.94531 wps 27112.50 step time 0.63s\n","=>> Epoch 1  global step 1280 loss 9.59421 batch 1280/2030 lr 0.001 accuracy 89.02539 wps 27329.93 step time 0.57s\n","=>> Epoch 1  global step 1300 loss 9.32382 batch 1300/2030 lr 0.001 accuracy 89.74219 wps 27666.66 step time 0.50s\n","=>> Epoch 1  global step 1320 loss 9.13381 batch 1320/2030 lr 0.001 accuracy 89.81445 wps 26584.34 step time 0.58s\n","=>> Epoch 1  global step 1340 loss 8.75289 batch 1340/2030 lr 0.001 accuracy 90.35938 wps 27323.31 step time 0.48s\n","=>> Epoch 1  global step 1360 loss 9.03879 batch 1360/2030 lr 0.001 accuracy 89.77734 wps 28116.44 step time 0.46s\n","=>> Epoch 1  global step 1380 loss 9.04184 batch 1380/2030 lr 0.001 accuracy 89.90625 wps 28206.19 step time 0.53s\n","=>> Epoch 1  global step 1400 loss 9.07082 batch 1400/2030 lr 0.001 accuracy 89.79687 wps 28295.05 step time 0.55s\n","=>> Epoch 1  global step 1420 loss 9.28012 batch 1420/2030 lr 0.001 accuracy 89.57422 wps 27606.90 step time 0.54s\n","=>> Epoch 1  global step 1440 loss 8.89169 batch 1440/2030 lr 0.001 accuracy 90.08789 wps 26866.28 step time 0.48s\n","=>> Epoch 1  global step 1460 loss 8.94300 batch 1460/2030 lr 0.001 accuracy 89.94141 wps 27148.18 step time 0.51s\n","=>> Epoch 1  global step 1480 loss 8.53186 batch 1480/2030 lr 0.001 accuracy 90.66602 wps 27605.91 step time 0.47s\n","=>> Epoch 1  global step 1500 loss 8.56005 batch 1500/2030 lr 0.001 accuracy 90.27930 wps 25761.21 step time 0.58s\n","=>> Epoch 1  global step 1520 loss 8.78398 batch 1520/2030 lr 0.001 accuracy 90.06250 wps 27864.86 step time 0.50s\n","=>> Epoch 1  global step 1540 loss 8.61300 batch 1540/2030 lr 0.001 accuracy 90.31055 wps 27790.52 step time 0.48s\n","=>> Epoch 1  global step 1560 loss 8.43315 batch 1560/2030 lr 0.001 accuracy 90.42578 wps 26751.37 step time 0.57s\n","=>> Epoch 1  global step 1580 loss 8.69399 batch 1580/2030 lr 0.001 accuracy 90.30273 wps 26758.50 step time 0.56s\n","=>> Epoch 1  global step 1600 loss 8.57874 batch 1600/2030 lr 0.001 accuracy 90.43750 wps 26119.85 step time 0.55s\n","=>> Epoch 1  global step 1620 loss 8.44034 batch 1620/2030 lr 0.001 accuracy 90.44922 wps 25271.13 step time 0.64s\n","=>> Epoch 1  global step 1640 loss 8.76561 batch 1640/2030 lr 0.001 accuracy 89.98633 wps 26428.84 step time 0.62s\n","=>> Epoch 1  global step 1660 loss 8.23994 batch 1660/2030 lr 0.001 accuracy 90.72070 wps 27840.10 step time 0.53s\n","=>> Epoch 1  global step 1680 loss 8.29640 batch 1680/2030 lr 0.001 accuracy 90.62305 wps 28031.40 step time 0.56s\n","=>> Epoch 1  global step 1700 loss 8.21827 batch 1700/2030 lr 0.001 accuracy 90.82617 wps 27787.46 step time 0.49s\n","=>> Epoch 1  global step 1720 loss 7.95032 batch 1720/2030 lr 0.001 accuracy 91.06641 wps 27781.18 step time 0.48s\n","=>> Epoch 1  global step 1740 loss 8.39638 batch 1740/2030 lr 0.001 accuracy 90.47266 wps 26861.87 step time 0.61s\n","=>> Epoch 1  global step 1760 loss 8.53968 batch 1760/2030 lr 0.001 accuracy 90.31836 wps 25522.65 step time 0.69s\n","=>> Epoch 1  global step 1780 loss 7.88405 batch 1780/2030 lr 0.001 accuracy 91.17773 wps 27579.91 step time 0.46s\n","=>> Epoch 1  global step 1800 loss 8.35359 batch 1800/2030 lr 0.001 accuracy 90.44922 wps 27306.42 step time 0.61s\n","=>> Epoch 1  global step 1820 loss 7.87410 batch 1820/2030 lr 0.001 accuracy 91.16992 wps 25195.15 step time 0.63s\n","=>> Epoch 1  global step 1840 loss 8.19633 batch 1840/2030 lr 0.001 accuracy 90.77148 wps 28150.04 step time 0.57s\n","=>> Epoch 1  global step 1860 loss 7.89478 batch 1860/2030 lr 0.001 accuracy 91.17578 wps 25363.26 step time 0.60s\n","=>> Epoch 1  global step 1880 loss 7.84369 batch 1880/2030 lr 0.001 accuracy 91.16016 wps 25857.72 step time 0.58s\n","=>> Epoch 1  global step 1900 loss 7.89549 batch 1900/2030 lr 0.001 accuracy 91.24023 wps 27883.57 step time 0.47s\n","=>> Epoch 1  global step 1920 loss 7.78249 batch 1920/2030 lr 0.001 accuracy 91.18555 wps 27112.14 step time 0.50s\n","=>> Epoch 1  global step 1940 loss 8.23385 batch 1940/2030 lr 0.001 accuracy 90.58984 wps 28911.74 step time 0.59s\n","=>> Epoch 1  global step 1960 loss 7.40920 batch 1960/2030 lr 0.001 accuracy 91.85742 wps 27525.20 step time 0.44s\n","=>> Epoch 1  global step 1980 loss 7.87093 batch 1980/2030 lr 0.001 accuracy 91.23437 wps 28376.20 step time 0.48s\n","=>> Epoch 1  global step 2000 loss 8.08527 batch 2000/2030 lr 0.001 accuracy 90.82031 wps 28468.64 step time 0.56s\n","=>> global step 2000, eval result: \n","=>> location_traffic_convenience - 0.45945971526727847\n","=>> location_distance_from_business_district - 0.37445375658196905\n","=>> location_easy_to_find - 0.44181692116152194\n","=>> service_wait_time - 0.2343910472075645\n","=>> service_waiters_attitude - 0.2528681274385714\n","=>> service_parking_convenience - 0.241788886593679\n","=>> service_serving_speed - 0.22901687321602773\n","=>> price_level - 0.16914216125372966\n","=>> price_cost_effective - 0.21621008021795066\n","=>> price_discount - 0.26499074233751463\n","=>> environment_decoration - 0.20163730869459923\n","=>> environment_noise - 0.20612436816739158\n","=>> environment_space - 0.19391891134902148\n","=>> environment_cleaness - 0.19520069576941787\n","=>> dish_portion - 0.17549325025960538\n","=>> dish_taste - 0.39991475185325137\n","=>> dish_look - 0.2088283251805264\n","=>> dish_recommendation - 0.22307351475095077\n","=>> others_overall_experience - 0.5074802792514788\n","=>> others_willing_to_consume_again - 0.440813694839834\n","=>> Eval loss 71.05041, f1 0.28183\n","=>> current result -0.2818311705695942, previous best result -0.21317002306340357\n","=>> Epoch 1  global step 2020 loss 8.02319 batch 2020/2030 lr 0.001 accuracy 90.83008 wps 27171.51 step time 0.57s\n","=>> Finsh epoch 1, global step 2031\n","=>> Epoch 2  global step 2040 loss 3.32714 batch 9/2030 lr 0.001 accuracy 41.29297 wps 29565.61 step time 0.25s\n","=>> Epoch 2  global step 2060 loss 7.37523 batch 29/2030 lr 0.001 accuracy 91.75195 wps 30740.60 step time 0.44s\n","=>> Epoch 2  global step 2080 loss 7.38575 batch 49/2030 lr 0.001 accuracy 91.71094 wps 31433.06 step time 0.46s\n","=>> Epoch 2  global step 2100 loss 7.61210 batch 69/2030 lr 0.001 accuracy 91.38086 wps 31018.10 step time 0.51s\n","=>> Epoch 2  global step 2120 loss 7.34348 batch 89/2030 lr 0.001 accuracy 91.79688 wps 30726.70 step time 0.44s\n","=>> Epoch 2  global step 2140 loss 7.62561 batch 109/2030 lr 0.001 accuracy 91.51953 wps 31355.94 step time 0.46s\n","=>> Epoch 2  global step 2160 loss 7.51554 batch 129/2030 lr 0.001 accuracy 91.55664 wps 31819.48 step time 0.46s\n","=>> Epoch 2  global step 2180 loss 7.66107 batch 149/2030 lr 0.001 accuracy 91.38477 wps 30824.32 step time 0.52s\n","=>> Epoch 2  global step 2200 loss 7.37187 batch 169/2030 lr 0.001 accuracy 91.66797 wps 29837.51 step time 0.48s\n","=>> Epoch 2  global step 2220 loss 7.43395 batch 189/2030 lr 0.001 accuracy 91.75977 wps 30327.98 step time 0.49s\n","=>> Epoch 2  global step 2240 loss 7.72745 batch 209/2030 lr 0.001 accuracy 91.33398 wps 31775.60 step time 0.47s\n","=>> Epoch 2  global step 2260 loss 7.35662 batch 229/2030 lr 0.001 accuracy 91.68359 wps 30700.21 step time 0.44s\n","=>> Epoch 2  global step 2280 loss 7.25881 batch 249/2030 lr 0.001 accuracy 91.79492 wps 30865.62 step time 0.44s\n","=>> Epoch 2  global step 2300 loss 7.39324 batch 269/2030 lr 0.001 accuracy 91.57812 wps 31316.78 step time 0.53s\n","=>> Epoch 2  global step 2320 loss 7.45725 batch 289/2030 lr 0.001 accuracy 91.64844 wps 30778.32 step time 0.44s\n","=>> Epoch 2  global step 2340 loss 7.24328 batch 309/2030 lr 0.001 accuracy 91.78125 wps 30789.18 step time 0.44s\n","=>> Epoch 2  global step 2360 loss 7.28264 batch 329/2030 lr 0.001 accuracy 91.84766 wps 31455.53 step time 0.46s\n","=>> Epoch 2  global step 2380 loss 7.17448 batch 349/2030 lr 0.001 accuracy 91.83203 wps 31270.31 step time 0.46s\n","=>> Epoch 2  global step 2400 loss 7.02622 batch 369/2030 lr 0.001 accuracy 92.11523 wps 28296.34 step time 0.38s\n","=>> Epoch 2  global step 2420 loss 7.53112 batch 389/2030 lr 0.001 accuracy 91.48633 wps 31367.28 step time 0.45s\n","=>> Epoch 2  global step 2440 loss 7.13821 batch 409/2030 lr 0.001 accuracy 92.02930 wps 30539.55 step time 0.43s\n","=>> Epoch 2  global step 2460 loss 7.70521 batch 429/2030 lr 0.001 accuracy 91.46484 wps 32464.16 step time 0.48s\n","=>> Epoch 2  global step 2480 loss 7.19130 batch 449/2030 lr 0.001 accuracy 91.96289 wps 30764.12 step time 0.49s\n","=>> Epoch 2  global step 2500 loss 7.01366 batch 469/2030 lr 0.001 accuracy 92.16992 wps 30689.19 step time 0.49s\n","=>> Epoch 2  global step 2520 loss 7.27432 batch 489/2030 lr 0.001 accuracy 91.74414 wps 31699.03 step time 0.46s\n","=>> Epoch 2  global step 2540 loss 7.61634 batch 509/2030 lr 0.001 accuracy 91.33008 wps 33568.00 step time 0.54s\n","=>> Epoch 2  global step 2560 loss 7.43714 batch 529/2030 lr 0.001 accuracy 91.49805 wps 31849.47 step time 0.47s\n","=>> Epoch 2  global step 2580 loss 7.26876 batch 549/2030 lr 0.001 accuracy 91.77930 wps 30519.19 step time 0.51s\n","=>> Epoch 2  global step 2600 loss 7.68070 batch 569/2030 lr 0.001 accuracy 91.15625 wps 32184.77 step time 0.55s\n","=>> Epoch 2  global step 2620 loss 7.16469 batch 589/2030 lr 0.001 accuracy 91.97852 wps 30014.66 step time 0.48s\n","=>> Epoch 2  global step 2640 loss 7.33641 batch 609/2030 lr 0.001 accuracy 91.57617 wps 31424.10 step time 0.45s\n","=>> Epoch 2  global step 2660 loss 7.55755 batch 629/2030 lr 0.001 accuracy 91.32031 wps 33869.93 step time 0.54s\n","=>> Epoch 2  global step 2680 loss 7.40529 batch 649/2030 lr 0.001 accuracy 91.65820 wps 31796.22 step time 0.52s\n","=>> Epoch 2  global step 2700 loss 7.08265 batch 669/2030 lr 0.001 accuracy 91.95703 wps 29991.52 step time 0.42s\n","=>> Epoch 2  global step 2720 loss 7.60961 batch 689/2030 lr 0.001 accuracy 91.24023 wps 31125.48 step time 0.52s\n","=>> Epoch 2  global step 2740 loss 7.07535 batch 709/2030 lr 0.001 accuracy 91.97656 wps 30842.35 step time 0.44s\n","=>> Epoch 2  global step 2760 loss 7.27490 batch 729/2030 lr 0.001 accuracy 91.76758 wps 30673.43 step time 0.48s\n","=>> Epoch 2  global step 2780 loss 7.44506 batch 749/2030 lr 0.001 accuracy 91.38672 wps 31480.16 step time 0.53s\n","=>> Epoch 2  global step 2800 loss 7.47183 batch 769/2030 lr 0.001 accuracy 91.43945 wps 31800.74 step time 0.56s\n","=>> Epoch 2  global step 2820 loss 7.06680 batch 789/2030 lr 0.001 accuracy 92.05664 wps 31336.91 step time 0.46s\n","=>> Epoch 2  global step 2840 loss 7.12198 batch 809/2030 lr 0.001 accuracy 91.87891 wps 29555.08 step time 0.41s\n","=>> Epoch 2  global step 2860 loss 7.12532 batch 829/2030 lr 0.001 accuracy 92.02344 wps 30799.98 step time 0.50s\n","=>> Epoch 2  global step 2880 loss 7.10535 batch 849/2030 lr 0.001 accuracy 91.91016 wps 31958.51 step time 0.47s\n","=>> Epoch 2  global step 2900 loss 6.86138 batch 869/2030 lr 0.001 accuracy 92.34180 wps 29681.19 step time 0.41s\n","=>> Epoch 2  global step 2920 loss 7.41345 batch 889/2030 lr 0.001 accuracy 91.67383 wps 32054.07 step time 0.48s\n","=>> Epoch 2  global step 2940 loss 7.08042 batch 909/2030 lr 0.001 accuracy 91.99414 wps 31166.73 step time 0.53s\n","=>> Epoch 2  global step 2960 loss 7.08275 batch 929/2030 lr 0.001 accuracy 91.89063 wps 31493.01 step time 0.52s\n","=>> Epoch 2  global step 2980 loss 7.47843 batch 949/2030 lr 0.001 accuracy 91.43164 wps 30720.81 step time 0.63s\n","=>> Epoch 2  global step 3000 loss 6.97949 batch 969/2030 lr 0.001 accuracy 92.25000 wps 29185.86 step time 0.45s\n","=>> global step 3000, eval result: \n","=>> location_traffic_convenience - 0.5621888494059415\n","=>> location_distance_from_business_district - 0.4693261501709437\n","=>> location_easy_to_find - 0.6055128871942829\n","=>> service_wait_time - 0.2499037828530916\n","=>> service_waiters_attitude - 0.7136712833917092\n","=>> service_parking_convenience - 0.2794416284527453\n","=>> service_serving_speed - 0.46959090524243524\n","=>> price_level - 0.5994995359204589\n","=>> price_cost_effective - 0.5418926401812358\n","=>> price_discount - 0.5196126133085617\n","=>> environment_decoration - 0.5710840745776171\n","=>> environment_noise - 0.5814678188915577\n","=>> environment_space - 0.5648816919719328\n","=>> environment_cleaness - 0.6459981413102683\n","=>> dish_portion - 0.48308675172148496\n","=>> dish_taste - 0.6383294222030077\n","=>> dish_look - 0.279326169919068\n","=>> dish_recommendation - 0.2434333551656476\n","=>> others_overall_experience - 0.5595944986996674\n","=>> others_willing_to_consume_again - 0.6131033754034212\n","=>> Eval loss 46.95306, f1 0.50955\n","=>> current result -0.509547278799254, previous best result -0.2818311705695942\n","=>> Epoch 2  global step 3020 loss 7.00684 batch 989/2030 lr 0.001 accuracy 92.20313 wps 29511.81 step time 0.45s\n","=>> Epoch 2  global step 3040 loss 6.87637 batch 1009/2030 lr 0.001 accuracy 92.19531 wps 31542.46 step time 0.46s\n","=>> Epoch 2  global step 3060 loss 7.04883 batch 1029/2030 lr 0.001 accuracy 92.04883 wps 31070.81 step time 0.44s\n","=>> Epoch 2  global step 3080 loss 6.92041 batch 1049/2030 lr 0.001 accuracy 92.20508 wps 30934.78 step time 0.44s\n","=>> Epoch 2  global step 3100 loss 6.68377 batch 1069/2030 lr 0.001 accuracy 92.54883 wps 31061.37 step time 0.44s\n","=>> Epoch 2  global step 3120 loss 7.43877 batch 1089/2030 lr 0.001 accuracy 91.52148 wps 32030.02 step time 0.61s\n","=>> Epoch 2  global step 3140 loss 7.03014 batch 1109/2030 lr 0.001 accuracy 92.04883 wps 30430.84 step time 0.46s\n","=>> Epoch 2  global step 3160 loss 7.11951 batch 1129/2030 lr 0.001 accuracy 91.93555 wps 27777.21 step time 0.51s\n","=>> Epoch 2  global step 3180 loss 6.80541 batch 1149/2030 lr 0.001 accuracy 92.29688 wps 27553.94 step time 0.46s\n","=>> Epoch 2  global step 3200 loss 6.75046 batch 1169/2030 lr 0.001 accuracy 92.46680 wps 27224.05 step time 0.48s\n","=>> Epoch 2  global step 3220 loss 6.45848 batch 1189/2030 lr 0.001 accuracy 92.79102 wps 26415.53 step time 0.41s\n","=>> Epoch 2  global step 3240 loss 7.26862 batch 1209/2030 lr 0.001 accuracy 91.77539 wps 26027.68 step time 0.69s\n","=>> Epoch 2  global step 3260 loss 7.17477 batch 1229/2030 lr 0.001 accuracy 91.78125 wps 26794.69 step time 0.62s\n","=>> Epoch 2  global step 3280 loss 7.17914 batch 1249/2030 lr 0.001 accuracy 91.72852 wps 29605.63 step time 0.55s\n","=>> Epoch 2  global step 3300 loss 7.05221 batch 1269/2030 lr 0.001 accuracy 92.08594 wps 27535.21 step time 0.52s\n","=>> Epoch 2  global step 3320 loss 7.19701 batch 1289/2030 lr 0.001 accuracy 91.87891 wps 27088.16 step time 0.58s\n","=>> Epoch 2  global step 3340 loss 6.85082 batch 1309/2030 lr 0.001 accuracy 92.18555 wps 27996.40 step time 0.51s\n","=>> Epoch 2  global step 3360 loss 7.02476 batch 1329/2030 lr 0.001 accuracy 92.11914 wps 27260.29 step time 0.53s\n","=>> Epoch 2  global step 3380 loss 7.08374 batch 1349/2030 lr 0.001 accuracy 91.88867 wps 28373.28 step time 0.52s\n","=>> Epoch 2  global step 3400 loss 6.83416 batch 1369/2030 lr 0.001 accuracy 92.33984 wps 28054.92 step time 0.47s\n","=>> Epoch 2  global step 3420 loss 6.98560 batch 1389/2030 lr 0.001 accuracy 92.22461 wps 27270.35 step time 0.50s\n","=>> Epoch 2  global step 3440 loss 6.86774 batch 1409/2030 lr 0.001 accuracy 92.18945 wps 26412.16 step time 0.56s\n","=>> Epoch 2  global step 3460 loss 7.50066 batch 1429/2030 lr 0.001 accuracy 91.40820 wps 27396.51 step time 0.66s\n","=>> Epoch 2  global step 3480 loss 7.06534 batch 1449/2030 lr 0.001 accuracy 91.98047 wps 27901.64 step time 0.52s\n","=>> Epoch 2  global step 3500 loss 6.84153 batch 1469/2030 lr 0.001 accuracy 92.28320 wps 26811.81 step time 0.52s\n","=>> Epoch 2  global step 3520 loss 6.75438 batch 1489/2030 lr 0.001 accuracy 92.31641 wps 26761.05 step time 0.57s\n","=>> Epoch 2  global step 3540 loss 6.80319 batch 1509/2030 lr 0.001 accuracy 92.34961 wps 28309.38 step time 0.50s\n","=>> Epoch 2  global step 3560 loss 6.67670 batch 1529/2030 lr 0.001 accuracy 92.55078 wps 28250.83 step time 0.47s\n","=>> Epoch 2  global step 3580 loss 7.13975 batch 1549/2030 lr 0.001 accuracy 91.89453 wps 29467.03 step time 0.57s\n","=>> Epoch 2  global step 3600 loss 7.02565 batch 1569/2030 lr 0.001 accuracy 92.09375 wps 28909.77 step time 0.59s\n","=>> Epoch 2  global step 3620 loss 6.69215 batch 1589/2030 lr 0.001 accuracy 92.47656 wps 28144.31 step time 0.47s\n","=>> Epoch 2  global step 3640 loss 7.09897 batch 1609/2030 lr 0.001 accuracy 92.00586 wps 26487.75 step time 0.63s\n","=>> Epoch 2  global step 3660 loss 6.91819 batch 1629/2030 lr 0.001 accuracy 92.06836 wps 24785.94 step time 0.64s\n","=>> Epoch 2  global step 3680 loss 6.77340 batch 1649/2030 lr 0.001 accuracy 92.31250 wps 27811.82 step time 0.51s\n","=>> Epoch 2  global step 3700 loss 7.13845 batch 1669/2030 lr 0.001 accuracy 91.86328 wps 28107.81 step time 0.54s\n","=>> Epoch 2  global step 3720 loss 6.55398 batch 1689/2030 lr 0.001 accuracy 92.67773 wps 27528.40 step time 0.44s\n","=>> Epoch 2  global step 3740 loss 6.60642 batch 1709/2030 lr 0.001 accuracy 92.56250 wps 28929.46 step time 0.50s\n","=>> Epoch 2  global step 3760 loss 6.90809 batch 1729/2030 lr 0.001 accuracy 92.07617 wps 25707.97 step time 0.67s\n","=>> Epoch 2  global step 3780 loss 6.79813 batch 1749/2030 lr 0.001 accuracy 92.26562 wps 28130.08 step time 0.53s\n","=>> Epoch 2  global step 3800 loss 6.80674 batch 1769/2030 lr 0.001 accuracy 92.17969 wps 26648.57 step time 0.48s\n","=>> Epoch 2  global step 3820 loss 6.52556 batch 1789/2030 lr 0.001 accuracy 92.68164 wps 26698.95 step time 0.49s\n","=>> Epoch 2  global step 3840 loss 6.48723 batch 1809/2030 lr 0.001 accuracy 92.54297 wps 26901.00 step time 0.47s\n","=>> Epoch 2  global step 3860 loss 6.99003 batch 1829/2030 lr 0.001 accuracy 91.92773 wps 26998.40 step time 0.58s\n","=>> Epoch 2  global step 3880 loss 6.74416 batch 1849/2030 lr 0.001 accuracy 92.37695 wps 26948.09 step time 0.60s\n","=>> Epoch 2  global step 3900 loss 6.60631 batch 1869/2030 lr 0.001 accuracy 92.51953 wps 28373.99 step time 0.46s\n","=>> Epoch 2  global step 3920 loss 6.97606 batch 1889/2030 lr 0.001 accuracy 92.15430 wps 26669.32 step time 0.62s\n","=>> Epoch 2  global step 3940 loss 7.02438 batch 1909/2030 lr 0.001 accuracy 92.03320 wps 27395.49 step time 0.53s\n","=>> Epoch 2  global step 3960 loss 7.01476 batch 1929/2030 lr 0.001 accuracy 92.08789 wps 27981.46 step time 0.58s\n","=>> Epoch 2  global step 3980 loss 6.70306 batch 1949/2030 lr 0.001 accuracy 92.46484 wps 26280.59 step time 0.52s\n","=>> Epoch 2  global step 4000 loss 6.99687 batch 1969/2030 lr 0.001 accuracy 92.08594 wps 27552.80 step time 0.56s\n","=>> global step 4000, eval result: \n","=>> location_traffic_convenience - 0.6193314990767482\n","=>> location_distance_from_business_district - 0.502517201686393\n","=>> location_easy_to_find - 0.6613988500391954\n","=>> service_wait_time - 0.5084584767997138\n","=>> service_waiters_attitude - 0.7632406762276409\n","=>> service_parking_convenience - 0.6531978787517746\n","=>> service_serving_speed - 0.6746675268440367\n","=>> price_level - 0.7429727165099913\n","=>> price_cost_effective - 0.6837146296510965\n","=>> price_discount - 0.5873046431734346\n","=>> environment_decoration - 0.6779080941235335\n","=>> environment_noise - 0.7338796461319929\n","=>> environment_space - 0.7417691379440938\n","=>> environment_cleaness - 0.7191733274691701\n","=>> dish_portion - 0.6650944356318192\n","=>> dish_taste - 0.6869346081568797\n","=>> dish_look - 0.4911580982537065\n","=>> dish_recommendation - 0.6395223518063252\n","=>> others_overall_experience - 0.5694860636069379\n","=>> others_willing_to_consume_again - 0.6322032695826161\n","=>> Eval loss 38.20307, f1 0.64770\n","=>> current result -0.647696656573355, previous best result -0.509547278799254\n","=>> Epoch 2  global step 4020 loss 6.87640 batch 1989/2030 lr 0.001 accuracy 92.27148 wps 26648.35 step time 0.65s\n","=>> Epoch 2  global step 4040 loss 6.83711 batch 2009/2030 lr 0.001 accuracy 92.31055 wps 25872.25 step time 0.55s\n","=>> Epoch 2  global step 4060 loss 6.57439 batch 2029/2030 lr 0.001 accuracy 92.50000 wps 27121.45 step time 0.53s\n","=>> Finsh epoch 2, global step 4062\n","=>> Epoch 3  global step 4080 loss 6.24684 batch 18/2030 lr 0.001 accuracy 83.02148 wps 31053.31 step time 0.47s\n","=>> Epoch 3  global step 4100 loss 6.57323 batch 38/2030 lr 0.001 accuracy 92.44141 wps 32098.22 step time 0.48s\n","=>> Epoch 3  global step 4120 loss 6.41907 batch 58/2030 lr 0.001 accuracy 92.77148 wps 30942.97 step time 0.44s\n","=>> Epoch 3  global step 4140 loss 6.91188 batch 78/2030 lr 0.001 accuracy 92.16016 wps 30788.64 step time 0.57s\n","=>> Epoch 3  global step 4160 loss 6.65997 batch 98/2030 lr 0.001 accuracy 92.31836 wps 32466.12 step time 0.56s\n","=>> Epoch 3  global step 4180 loss 6.61777 batch 118/2030 lr 0.001 accuracy 92.56836 wps 30756.29 step time 0.45s\n","=>> Epoch 3  global step 4200 loss 6.51617 batch 138/2030 lr 0.001 accuracy 92.49023 wps 27541.73 step time 0.52s\n","=>> Epoch 3  global step 4220 loss 6.70659 batch 158/2030 lr 0.001 accuracy 92.49609 wps 27781.28 step time 0.53s\n","=>> Epoch 3  global step 4240 loss 6.86507 batch 178/2030 lr 0.001 accuracy 92.02539 wps 27943.55 step time 0.63s\n","=>> Epoch 3  global step 4260 loss 6.64277 batch 198/2030 lr 0.001 accuracy 92.41211 wps 28400.47 step time 0.59s\n","=>> Epoch 3  global step 4280 loss 6.66411 batch 218/2030 lr 0.001 accuracy 92.55273 wps 27774.64 step time 0.50s\n","=>> Epoch 3  global step 4300 loss 6.82000 batch 238/2030 lr 0.001 accuracy 92.28320 wps 25942.10 step time 0.73s\n","=>> Epoch 3  global step 4320 loss 6.74698 batch 258/2030 lr 0.001 accuracy 92.43750 wps 26616.66 step time 0.59s\n","=>> Epoch 3  global step 4340 loss 6.60274 batch 278/2030 lr 0.001 accuracy 92.57422 wps 27729.14 step time 0.58s\n","=>> Epoch 3  global step 4360 loss 6.59915 batch 298/2030 lr 0.001 accuracy 92.60156 wps 27627.22 step time 0.49s\n","Traceback (most recent call last):\n","  File \"elmo_run.py\", line 343, in <module>\n","    train_clf(flags)\n","  File \"elmo_run.py\", line 203, in train_clf\n","    run_info=add_summary and flags.debug\n","  File \"/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/mycode/elmo.py\", line 487, in train_clf_one_step\n","    feed_dict=feed_dict)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n","    run_metadata_ptr)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n","    run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n","    return fn(*args)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n","    options, feed_dict, fetch_list, target_list, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zN63ESyzbO6q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591247215772,"user_tz":-480,"elapsed":4938299,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"29a1b7cf-2366-4f13-ba6c-3bb57155825c"},"source":["# 原始数据\n","!python elmo_run.py \\\n","--mode=train \\\n","--data_files=data/train_not_aug.json \\\n","--eval_files=data/validation.json \\\n","--label_file=data/labels.txt \\\n","--vocab_file=data/vocab.txt \\\n","--embed_file=data/embedding.txt \\\n","--num_layers=3 \\\n","--batch_size=64 \\\n","--max_len=1200 \\\n","--rnn_cell_name='WEIGHT_LSTM' \\\n","--linear_dropout=0.5 \\\n","--num_classes_each_label=4 \\\n","--num_labels=20 \\\n","--steps_per_eval=1000 \\\n","--optimizer='rms' \\\n","--learning_rate=0.001 \\\n","--focal_loss_gamma=0 \\\n","--label_smoothing=0 \\\n","--checkpoint_dir=output/elmo_1200_more_drop_noSmooth_notAug \\\n","--checkpoint_load_step=4000"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/train_not_aug.json ...\n","# Got 105000 data items with 1640 batches\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/validation.json ...\n","# Got 15000 data items with 117 batches\n","  saving config to output/elmo_1200_more_drop_noSmooth_notAug/config\n","mode=train,data_files=['data/train_not_aug.json'],eval_files=['data/validation.json'],label_file=data/labels.txt,vocab_file=data/vocab.txt,embed_file=data/embedding.txt,out_file=None,split_word=True,reverse=False,weight_file=None,prob=False,max_len=1200,batch_size=64,num_layers=3,optimizer=rms,learning_rate=0.001,decay_schema=hand,decay_steps=10000,focal_loss_gamma=0.0,max_gradient_norm=2.0,l2_loss_ratio=0.0,label_smoothing=0.0,embedding_dropout=0.1,dropout_keep_prob=0.8,weight_keep_drop=0.8,linear_dropout=0.5,rnn_cell_name=WEIGHT_LSTM,embedding_size=300,num_units=300,num_classes_each_label=4,num_labels=20,fix_embedding=False,need_early_stop=True,patient=5,debug=False,num_train_epoch=50,steps_per_stats=20,steps_per_summary=50,steps_per_eval=1000,checkpoint_dir=output/elmo_1200_more_drop_noSmooth_notAug,checkpoint_load_step=4000,vocab_size=50000\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","variable <tf.Variable 'embedding/embedding:0' shape=(50000, 300) dtype=float32_ref> with parameter number 15000000\n","variable <tf.Variable 'embedding/label_embedding:0' shape=(20, 300) dtype=float32_ref> with parameter number 6000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/weight:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","variable <tf.Variable 'elmo_encoder/scalar:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/memory_layer/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/attention_op/dense/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/dense_1/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense_1/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/drop_connect_layer/kernel:0' shape=(300, 1200) dtype=float32_ref> with parameter number 360000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/luong_attention/attention_g:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/attention_layer/kernel:0' shape=(900, 300) dtype=float32_ref> with parameter number 270000\n","variable <tf.Variable 'classifier/predict_clf/dense/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/predict_clf/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/predict_clf/dense_1/kernel:0' shape=(300, 4) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/predict_clf/dense_1/bias:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","# total parameter number 23566510\n","loading config from output/elmo_1200_more_drop_noSmooth_notAug/config\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","output/elmo_1200_more_drop_noSmooth_notAug/model.ckpt-4000\n","\n","!!! Restored model\n","=>> Start to train with learning rate 0.001\n","=>> Global step 4000\n","=>> Epoch 1  global step 4020 loss 6.45717 batch 20/1640 lr 0.001 accuracy 92.58594 wps 21403.07 step time 0.81s\n","=>> Epoch 1  global step 4040 loss 6.38716 batch 40/1640 lr 0.001 accuracy 92.76953 wps 30986.07 step time 0.44s\n","=>> Epoch 1  global step 4060 loss 6.29110 batch 60/1640 lr 0.001 accuracy 92.93750 wps 30173.02 step time 0.43s\n","=>> Epoch 1  global step 4080 loss 6.19491 batch 80/1640 lr 0.001 accuracy 92.93164 wps 30589.41 step time 0.50s\n","=>> Epoch 1  global step 4100 loss 6.20619 batch 100/1640 lr 0.001 accuracy 93.12500 wps 30177.44 step time 0.42s\n","=>> Epoch 1  global step 4120 loss 6.52429 batch 120/1640 lr 0.001 accuracy 92.58203 wps 30609.66 step time 0.56s\n","=>> Epoch 1  global step 4140 loss 6.29824 batch 140/1640 lr 0.001 accuracy 92.92383 wps 31114.71 step time 0.46s\n","=>> Epoch 1  global step 4160 loss 6.49617 batch 160/1640 lr 0.001 accuracy 92.61328 wps 31042.66 step time 0.46s\n","=>> Epoch 1  global step 4180 loss 6.58138 batch 180/1640 lr 0.001 accuracy 92.41602 wps 32087.45 step time 0.48s\n","=>> Epoch 1  global step 4200 loss 6.36738 batch 200/1640 lr 0.001 accuracy 92.99609 wps 30774.83 step time 0.44s\n","=>> Epoch 1  global step 4220 loss 6.22017 batch 220/1640 lr 0.001 accuracy 92.89844 wps 31796.42 step time 0.47s\n","=>> Epoch 1  global step 4240 loss 6.11724 batch 240/1640 lr 0.001 accuracy 93.08594 wps 30566.12 step time 0.44s\n","=>> Epoch 1  global step 4260 loss 6.16827 batch 260/1640 lr 0.001 accuracy 93.08203 wps 30084.17 step time 0.48s\n","=>> Epoch 1  global step 4280 loss 6.09354 batch 280/1640 lr 0.001 accuracy 93.21875 wps 30204.21 step time 0.42s\n","=>> Epoch 1  global step 4300 loss 6.66535 batch 300/1640 lr 0.001 accuracy 92.45313 wps 30167.39 step time 0.55s\n","=>> Epoch 1  global step 4320 loss 6.19411 batch 320/1640 lr 0.001 accuracy 92.97070 wps 31438.84 step time 0.45s\n","=>> Epoch 1  global step 4340 loss 6.55253 batch 340/1640 lr 0.001 accuracy 92.58008 wps 31246.62 step time 0.52s\n","=>> Epoch 1  global step 4360 loss 6.44192 batch 360/1640 lr 0.001 accuracy 92.62500 wps 32065.85 step time 0.49s\n","=>> Epoch 1  global step 4380 loss 6.27883 batch 380/1640 lr 0.001 accuracy 92.98242 wps 30239.44 step time 0.42s\n","=>> Epoch 1  global step 4400 loss 6.39386 batch 400/1640 lr 0.001 accuracy 92.76758 wps 30576.49 step time 0.51s\n","=>> Epoch 1  global step 4420 loss 6.19563 batch 420/1640 lr 0.001 accuracy 93.00977 wps 29406.02 step time 0.41s\n","=>> Epoch 1  global step 4440 loss 6.31888 batch 440/1640 lr 0.001 accuracy 92.75195 wps 31357.28 step time 0.46s\n","=>> Epoch 1  global step 4460 loss 5.97016 batch 460/1640 lr 0.001 accuracy 93.32422 wps 29323.04 step time 0.40s\n","=>> Epoch 1  global step 4480 loss 5.99682 batch 480/1640 lr 0.001 accuracy 93.15820 wps 30800.79 step time 0.44s\n","=>> Epoch 1  global step 4500 loss 6.31410 batch 500/1640 lr 0.001 accuracy 92.94922 wps 32071.92 step time 0.48s\n","=>> Epoch 1  global step 4520 loss 6.27507 batch 520/1640 lr 0.001 accuracy 92.96094 wps 31403.35 step time 0.46s\n","=>> Epoch 1  global step 4540 loss 6.55498 batch 540/1640 lr 0.001 accuracy 92.46875 wps 30616.81 step time 0.57s\n","=>> Epoch 1  global step 4560 loss 6.01548 batch 560/1640 lr 0.001 accuracy 93.34570 wps 30265.49 step time 0.42s\n","=>> Epoch 1  global step 4580 loss 6.66634 batch 580/1640 lr 0.001 accuracy 92.27344 wps 32652.96 step time 0.56s\n","=>> Epoch 1  global step 4600 loss 6.25202 batch 600/1640 lr 0.001 accuracy 92.94727 wps 31828.97 step time 0.46s\n","=>> Epoch 1  global step 4620 loss 6.37116 batch 620/1640 lr 0.001 accuracy 92.63281 wps 30970.53 step time 0.52s\n","=>> Epoch 1  global step 4640 loss 6.63511 batch 640/1640 lr 0.001 accuracy 92.36719 wps 31638.11 step time 0.62s\n","=>> Epoch 1  global step 4660 loss 6.23531 batch 660/1640 lr 0.001 accuracy 93.05664 wps 29065.74 step time 0.46s\n","=>> Epoch 1  global step 4680 loss 6.07537 batch 680/1640 lr 0.001 accuracy 93.38672 wps 30846.02 step time 0.44s\n","=>> Epoch 1  global step 4700 loss 6.14300 batch 700/1640 lr 0.001 accuracy 92.98047 wps 31808.93 step time 0.47s\n","=>> Epoch 1  global step 4720 loss 6.43481 batch 720/1640 lr 0.001 accuracy 92.63477 wps 31557.47 step time 0.46s\n","=>> Epoch 1  global step 4740 loss 6.06774 batch 740/1640 lr 0.001 accuracy 93.17773 wps 30531.88 step time 0.42s\n","=>> Epoch 1  global step 4760 loss 6.41399 batch 760/1640 lr 0.001 accuracy 92.64844 wps 32798.67 step time 0.50s\n","=>> Epoch 1  global step 4780 loss 6.12770 batch 780/1640 lr 0.001 accuracy 93.12695 wps 30739.54 step time 0.44s\n","=>> Epoch 1  global step 4800 loss 6.31359 batch 800/1640 lr 0.001 accuracy 92.92383 wps 31167.97 step time 0.45s\n","=>> Epoch 1  global step 4820 loss 6.12183 batch 820/1640 lr 0.001 accuracy 92.98633 wps 31070.78 step time 0.50s\n","=>> Epoch 1  global step 4840 loss 6.24337 batch 840/1640 lr 0.001 accuracy 92.99414 wps 31992.60 step time 0.48s\n","=>> Epoch 1  global step 4860 loss 6.08078 batch 860/1640 lr 0.001 accuracy 93.19531 wps 29379.14 step time 0.40s\n","=>> Epoch 1  global step 4880 loss 6.21113 batch 880/1640 lr 0.001 accuracy 93.03320 wps 31256.19 step time 0.45s\n","=>> Epoch 1  global step 4900 loss 6.08707 batch 900/1640 lr 0.001 accuracy 93.08984 wps 31443.47 step time 0.46s\n","=>> Epoch 1  global step 4920 loss 6.03934 batch 920/1640 lr 0.001 accuracy 93.11914 wps 30159.92 step time 0.49s\n","=>> Epoch 1  global step 4940 loss 6.26775 batch 940/1640 lr 0.001 accuracy 92.90234 wps 31526.18 step time 0.45s\n","=>> Epoch 1  global step 4960 loss 6.80476 batch 960/1640 lr 0.001 accuracy 92.25000 wps 31875.21 step time 0.61s\n","=>> Epoch 1  global step 4980 loss 6.55476 batch 980/1640 lr 0.001 accuracy 92.50195 wps 33089.00 step time 0.52s\n","=>> Epoch 1  global step 5000 loss 6.45871 batch 1000/1640 lr 0.001 accuracy 92.66406 wps 30925.32 step time 0.52s\n","=>> global step 5000, eval result: \n","=>> location_traffic_convenience - 0.6214071630306555\n","=>> location_distance_from_business_district - 0.4893641741342424\n","=>> location_easy_to_find - 0.6799816466332519\n","=>> service_wait_time - 0.6043275978669759\n","=>> service_waiters_attitude - 0.7772829427093165\n","=>> service_parking_convenience - 0.685972769714533\n","=>> service_serving_speed - 0.7320290019722503\n","=>> price_level - 0.7588308714580263\n","=>> price_cost_effective - 0.6970715708669379\n","=>> price_discount - 0.6093687485863478\n","=>> environment_decoration - 0.7009515197650306\n","=>> environment_noise - 0.7494950204195993\n","=>> environment_space - 0.7558748856782949\n","=>> environment_cleaness - 0.7383388691255095\n","=>> dish_portion - 0.6982568707592305\n","=>> dish_taste - 0.69709365031343\n","=>> dish_look - 0.5354364812957164\n","=>> dish_recommendation - 0.6896521390488457\n","=>> others_overall_experience - 0.5775943998285142\n","=>> others_willing_to_consume_again - 0.6437716621104193\n","=>> Eval loss 36.19195, f1 0.67211\n","=>> current result -0.6721050992658565, previous best result 1000000000\n","=>> Epoch 1  global step 5020 loss 5.81460 batch 1020/1640 lr 0.001 accuracy 93.48633 wps 29684.68 step time 0.41s\n","=>> Epoch 1  global step 5040 loss 6.21833 batch 1040/1640 lr 0.001 accuracy 92.91211 wps 29149.55 step time 0.49s\n","=>> Epoch 1  global step 5060 loss 5.85799 batch 1060/1640 lr 0.001 accuracy 93.43164 wps 29543.57 step time 0.41s\n","=>> Epoch 1  global step 5080 loss 6.26066 batch 1080/1640 lr 0.001 accuracy 92.94531 wps 31147.14 step time 0.51s\n","=>> Epoch 1  global step 5100 loss 6.06086 batch 1100/1640 lr 0.001 accuracy 93.28125 wps 29733.67 step time 0.41s\n","=>> Epoch 1  global step 5120 loss 5.90032 batch 1120/1640 lr 0.001 accuracy 93.35547 wps 29608.50 step time 0.41s\n","=>> Epoch 1  global step 5140 loss 6.09735 batch 1140/1640 lr 0.001 accuracy 93.24805 wps 30733.69 step time 0.43s\n","=>> Epoch 1  global step 5160 loss 6.00685 batch 1160/1640 lr 0.001 accuracy 93.13086 wps 30862.06 step time 0.44s\n","=>> Epoch 1  global step 5180 loss 6.06635 batch 1180/1640 lr 0.001 accuracy 93.20898 wps 30180.09 step time 0.42s\n","=>> Epoch 1  global step 5200 loss 6.39825 batch 1200/1640 lr 0.001 accuracy 92.80469 wps 31018.74 step time 0.45s\n","=>> Epoch 1  global step 5220 loss 6.03445 batch 1220/1640 lr 0.001 accuracy 93.20312 wps 30012.74 step time 0.42s\n","=>> Epoch 1  global step 5240 loss 6.55741 batch 1240/1640 lr 0.001 accuracy 92.60352 wps 32637.15 step time 0.50s\n","=>> Epoch 1  global step 5260 loss 5.99469 batch 1260/1640 lr 0.001 accuracy 93.26367 wps 29375.99 step time 0.40s\n","=>> Epoch 1  global step 5280 loss 6.44226 batch 1280/1640 lr 0.001 accuracy 92.63086 wps 32164.61 step time 0.47s\n","=>> Epoch 1  global step 5300 loss 6.37814 batch 1300/1640 lr 0.001 accuracy 92.65234 wps 28471.20 step time 0.53s\n","=>> Epoch 1  global step 5320 loss 6.28221 batch 1320/1640 lr 0.001 accuracy 92.83008 wps 28501.19 step time 0.51s\n","=>> Epoch 1  global step 5340 loss 6.22181 batch 1340/1640 lr 0.001 accuracy 92.98633 wps 26434.07 step time 0.63s\n","=>> Epoch 1  global step 5360 loss 6.37017 batch 1360/1640 lr 0.001 accuracy 92.85937 wps 28108.30 step time 0.54s\n","=>> Epoch 1  global step 5380 loss 6.18592 batch 1380/1640 lr 0.001 accuracy 92.87891 wps 26398.79 step time 0.63s\n","=>> Epoch 1  global step 5400 loss 6.33468 batch 1400/1640 lr 0.001 accuracy 92.85547 wps 26717.17 step time 0.50s\n","=>> Epoch 1  global step 5420 loss 6.06985 batch 1420/1640 lr 0.001 accuracy 93.10547 wps 26059.73 step time 0.54s\n","=>> Epoch 1  global step 5440 loss 6.12416 batch 1440/1640 lr 0.001 accuracy 93.08203 wps 25981.49 step time 0.56s\n","=>> Epoch 1  global step 5460 loss 6.02419 batch 1460/1640 lr 0.001 accuracy 93.14062 wps 27029.37 step time 0.50s\n","=>> Epoch 1  global step 5480 loss 6.36649 batch 1480/1640 lr 0.001 accuracy 92.88477 wps 29011.79 step time 0.57s\n","=>> Epoch 1  global step 5500 loss 6.32138 batch 1500/1640 lr 0.001 accuracy 92.91602 wps 26709.78 step time 0.52s\n","=>> Epoch 1  global step 5520 loss 6.74356 batch 1520/1640 lr 0.001 accuracy 92.27734 wps 26596.61 step time 0.75s\n","=>> Epoch 1  global step 5540 loss 6.39396 batch 1540/1640 lr 0.001 accuracy 92.66016 wps 27141.70 step time 0.63s\n","=>> Epoch 1  global step 5560 loss 6.11506 batch 1560/1640 lr 0.001 accuracy 93.07617 wps 27033.26 step time 0.55s\n","=>> Epoch 1  global step 5580 loss 6.05673 batch 1580/1640 lr 0.001 accuracy 93.14453 wps 27209.12 step time 0.49s\n","=>> Epoch 1  global step 5600 loss 6.33216 batch 1600/1640 lr 0.001 accuracy 92.93164 wps 28438.39 step time 0.52s\n","=>> Epoch 1  global step 5620 loss 6.09156 batch 1620/1640 lr 0.001 accuracy 93.04883 wps 27227.51 step time 0.56s\n","=>> Epoch 1  global step 5640 loss 6.28574 batch 1640/1640 lr 0.001 accuracy 92.81641 wps 27351.54 step time 0.54s\n","=>> Finsh epoch 1, global step 5641\n","=>> Epoch 2  global step 5660 loss 5.97162 batch 19/1640 lr 0.001 accuracy 88.04297 wps 31333.30 step time 0.56s\n","=>> Epoch 2  global step 5680 loss 5.85750 batch 39/1640 lr 0.001 accuracy 93.40820 wps 30623.38 step time 0.43s\n","=>> Epoch 2  global step 5700 loss 6.40506 batch 59/1640 lr 0.001 accuracy 92.65430 wps 33321.76 step time 0.53s\n","=>> Epoch 2  global step 5720 loss 6.24940 batch 79/1640 lr 0.001 accuracy 92.83789 wps 30979.84 step time 0.52s\n","=>> Epoch 2  global step 5740 loss 5.88046 batch 99/1640 lr 0.001 accuracy 93.31250 wps 29761.65 step time 0.55s\n","=>> Epoch 2  global step 5760 loss 5.77580 batch 119/1640 lr 0.001 accuracy 93.56055 wps 29291.05 step time 0.39s\n","=>> Epoch 2  global step 5780 loss 5.98615 batch 139/1640 lr 0.001 accuracy 93.31836 wps 31564.94 step time 0.47s\n","=>> Epoch 2  global step 5800 loss 5.92301 batch 159/1640 lr 0.001 accuracy 93.33203 wps 27702.43 step time 0.48s\n","=>> Epoch 2  global step 5820 loss 5.85810 batch 179/1640 lr 0.001 accuracy 93.27930 wps 28139.97 step time 0.46s\n","=>> Epoch 2  global step 5840 loss 6.21803 batch 199/1640 lr 0.001 accuracy 92.94922 wps 27127.50 step time 0.58s\n","=>> Epoch 2  global step 5860 loss 6.00097 batch 219/1640 lr 0.001 accuracy 93.28125 wps 26602.55 step time 0.54s\n","=>> Epoch 2  global step 5880 loss 5.70477 batch 239/1640 lr 0.001 accuracy 93.58008 wps 26625.77 step time 0.49s\n","=>> Epoch 2  global step 5900 loss 5.93522 batch 259/1640 lr 0.001 accuracy 93.26758 wps 26404.10 step time 0.54s\n","=>> Epoch 2  global step 5920 loss 6.18377 batch 279/1640 lr 0.001 accuracy 92.86914 wps 25438.07 step time 0.66s\n","=>> Epoch 2  global step 5940 loss 5.69382 batch 299/1640 lr 0.001 accuracy 93.60352 wps 27305.85 step time 0.47s\n","=>> Epoch 2  global step 5960 loss 5.80374 batch 319/1640 lr 0.001 accuracy 93.46289 wps 26907.20 step time 0.51s\n","=>> Epoch 2  global step 5980 loss 5.86836 batch 339/1640 lr 0.001 accuracy 93.36914 wps 27141.87 step time 0.48s\n","=>> Epoch 2  global step 6000 loss 5.84738 batch 359/1640 lr 0.001 accuracy 93.37500 wps 27962.89 step time 0.47s\n","=>> global step 6000, eval result: \n","=>> location_traffic_convenience - 0.6356804279402393\n","=>> location_distance_from_business_district - 0.5054372442106768\n","=>> location_easy_to_find - 0.6897400727032013\n","=>> service_wait_time - 0.63090913564134\n","=>> service_waiters_attitude - 0.783299605320929\n","=>> service_parking_convenience - 0.7049964817312633\n","=>> service_serving_speed - 0.7441058039063443\n","=>> price_level - 0.7715123744813035\n","=>> price_cost_effective - 0.7124987772582374\n","=>> price_discount - 0.6305901098659072\n","=>> environment_decoration - 0.7140294396544029\n","=>> environment_noise - 0.7562736971337045\n","=>> environment_space - 0.7601385554077265\n","=>> environment_cleaness - 0.7382520745499817\n","=>> dish_portion - 0.7111595212282958\n","=>> dish_taste - 0.7051893756541484\n","=>> dish_look - 0.5547429943911462\n","=>> dish_recommendation - 0.7169948409711665\n","=>> others_overall_experience - 0.5811074595371027\n","=>> others_willing_to_consume_again - 0.6604262127607058\n","=>> Eval loss 35.33131, f1 0.68535\n","=>> current result -0.6853542102173911, previous best result -0.6721050992658565\n","=>> Epoch 2  global step 6020 loss 5.90948 batch 379/1640 lr 0.001 accuracy 93.33594 wps 24377.23 step time 0.57s\n","=>> Epoch 2  global step 6040 loss 5.68978 batch 399/1640 lr 0.001 accuracy 93.48047 wps 27416.10 step time 0.46s\n","=>> Epoch 2  global step 6060 loss 6.23887 batch 419/1640 lr 0.001 accuracy 92.90820 wps 27874.50 step time 0.51s\n","=>> Epoch 2  global step 6080 loss 6.16078 batch 439/1640 lr 0.001 accuracy 92.96289 wps 25366.89 step time 0.66s\n","=>> Epoch 2  global step 6100 loss 6.12481 batch 459/1640 lr 0.001 accuracy 93.08984 wps 28069.18 step time 0.48s\n","=>> Epoch 2  global step 6120 loss 5.82806 batch 479/1640 lr 0.001 accuracy 93.40234 wps 26147.35 step time 0.54s\n","=>> Epoch 2  global step 6140 loss 5.77589 batch 499/1640 lr 0.001 accuracy 93.39062 wps 27784.93 step time 0.48s\n","=>> Epoch 2  global step 6160 loss 5.92248 batch 519/1640 lr 0.001 accuracy 93.30469 wps 26147.71 step time 0.65s\n","=>> Epoch 2  global step 6180 loss 6.12365 batch 539/1640 lr 0.001 accuracy 93.06836 wps 25633.82 step time 0.65s\n","=>> Epoch 2  global step 6200 loss 5.97990 batch 559/1640 lr 0.001 accuracy 93.28906 wps 26202.73 step time 0.44s\n","=>> Epoch 2  global step 6220 loss 6.12453 batch 579/1640 lr 0.001 accuracy 92.97656 wps 26863.97 step time 0.65s\n","=>> Epoch 2  global step 6240 loss 6.37296 batch 599/1640 lr 0.001 accuracy 92.85742 wps 28046.20 step time 0.66s\n","=>> Epoch 2  global step 6260 loss 5.97138 batch 619/1640 lr 0.001 accuracy 93.30469 wps 27981.39 step time 0.55s\n","=>> Epoch 2  global step 6280 loss 6.14979 batch 639/1640 lr 0.001 accuracy 93.07031 wps 26488.63 step time 0.57s\n","=>> Epoch 2  global step 6300 loss 5.84124 batch 659/1640 lr 0.001 accuracy 93.41602 wps 26531.33 step time 0.52s\n","=>> Epoch 2  global step 6320 loss 5.91785 batch 679/1640 lr 0.001 accuracy 93.19141 wps 27844.54 step time 0.51s\n","=>> Epoch 2  global step 6340 loss 6.11967 batch 699/1640 lr 0.001 accuracy 92.95313 wps 27112.39 step time 0.62s\n","=>> Epoch 2  global step 6360 loss 6.27832 batch 719/1640 lr 0.001 accuracy 92.96875 wps 28666.22 step time 0.59s\n","=>> Epoch 2  global step 6380 loss 6.19661 batch 739/1640 lr 0.001 accuracy 92.93164 wps 26207.69 step time 0.68s\n","=>> Epoch 2  global step 6400 loss 6.05217 batch 759/1640 lr 0.001 accuracy 93.06836 wps 27570.71 step time 0.54s\n","=>> Epoch 2  global step 6420 loss 5.83969 batch 779/1640 lr 0.001 accuracy 93.53125 wps 27290.18 step time 0.46s\n","=>> Epoch 2  global step 6440 loss 5.86035 batch 799/1640 lr 0.001 accuracy 93.37109 wps 28088.51 step time 0.52s\n","=>> Epoch 2  global step 6460 loss 6.17784 batch 819/1640 lr 0.001 accuracy 93.11133 wps 28054.93 step time 0.53s\n","=>> Epoch 2  global step 6480 loss 5.74417 batch 839/1640 lr 0.001 accuracy 93.60937 wps 27083.09 step time 0.44s\n","=>> Epoch 2  global step 6500 loss 6.00665 batch 859/1640 lr 0.001 accuracy 93.21094 wps 25919.50 step time 0.55s\n","=>> Epoch 2  global step 6520 loss 6.05393 batch 879/1640 lr 0.001 accuracy 93.15820 wps 28680.46 step time 0.49s\n","=>> Epoch 2  global step 6540 loss 6.07627 batch 899/1640 lr 0.001 accuracy 93.19336 wps 26390.89 step time 0.58s\n","=>> Epoch 2  global step 6560 loss 6.10173 batch 919/1640 lr 0.001 accuracy 92.93164 wps 28654.39 step time 0.59s\n","=>> Epoch 2  global step 6580 loss 5.71743 batch 939/1640 lr 0.001 accuracy 93.48047 wps 26065.91 step time 0.49s\n","=>> Epoch 2  global step 6600 loss 5.95084 batch 959/1640 lr 0.001 accuracy 93.17187 wps 27467.41 step time 0.49s\n","=>> Epoch 2  global step 6620 loss 5.98519 batch 979/1640 lr 0.001 accuracy 93.31641 wps 28465.00 step time 0.50s\n","=>> Epoch 2  global step 6640 loss 5.95977 batch 999/1640 lr 0.001 accuracy 93.13672 wps 26055.61 step time 0.56s\n","=>> Epoch 2  global step 6660 loss 6.20942 batch 1019/1640 lr 0.001 accuracy 92.97852 wps 27930.62 step time 0.62s\n","=>> Epoch 2  global step 6680 loss 5.69182 batch 1039/1640 lr 0.001 accuracy 93.49805 wps 27088.83 step time 0.51s\n","=>> Epoch 2  global step 6700 loss 6.36172 batch 1059/1640 lr 0.001 accuracy 92.77734 wps 26719.84 step time 0.61s\n","=>> Epoch 2  global step 6720 loss 6.12804 batch 1079/1640 lr 0.001 accuracy 93.12891 wps 29130.54 step time 0.50s\n","=>> Epoch 2  global step 6740 loss 5.79165 batch 1099/1640 lr 0.001 accuracy 93.37500 wps 27362.78 step time 0.48s\n","=>> Epoch 2  global step 6760 loss 6.06649 batch 1119/1640 lr 0.001 accuracy 93.07812 wps 27719.71 step time 0.53s\n","=>> Epoch 2  global step 6780 loss 6.04260 batch 1139/1640 lr 0.001 accuracy 93.24609 wps 27163.03 step time 0.50s\n","=>> Epoch 2  global step 6800 loss 5.95257 batch 1159/1640 lr 0.001 accuracy 93.31836 wps 26731.44 step time 0.51s\n","=>> Epoch 2  global step 6820 loss 5.88442 batch 1179/1640 lr 0.001 accuracy 93.38672 wps 26526.59 step time 0.56s\n","=>> Epoch 2  global step 6840 loss 6.04413 batch 1199/1640 lr 0.001 accuracy 93.15039 wps 25945.89 step time 0.56s\n","=>> Epoch 2  global step 6860 loss 5.59458 batch 1219/1640 lr 0.001 accuracy 93.54492 wps 26668.00 step time 0.47s\n","=>> Epoch 2  global step 6880 loss 6.14499 batch 1239/1640 lr 0.001 accuracy 93.08789 wps 26327.60 step time 0.63s\n","=>> Epoch 2  global step 6900 loss 5.84778 batch 1259/1640 lr 0.001 accuracy 93.32422 wps 26585.75 step time 0.46s\n","=>> Epoch 2  global step 6920 loss 5.88863 batch 1279/1640 lr 0.001 accuracy 93.35156 wps 25459.58 step time 0.56s\n","=>> Epoch 2  global step 6940 loss 5.89890 batch 1299/1640 lr 0.001 accuracy 93.53516 wps 26865.31 step time 0.46s\n","=>> Epoch 2  global step 6960 loss 6.38160 batch 1319/1640 lr 0.001 accuracy 92.78125 wps 28008.25 step time 0.58s\n","=>> Epoch 2  global step 6980 loss 5.86779 batch 1339/1640 lr 0.001 accuracy 93.30664 wps 27436.42 step time 0.54s\n","=>> Epoch 2  global step 7000 loss 6.14973 batch 1359/1640 lr 0.001 accuracy 93.01172 wps 25661.55 step time 0.55s\n","=>> global step 7000, eval result: \n","=>> location_traffic_convenience - 0.6504014715738886\n","=>> location_distance_from_business_district - 0.5189926898500193\n","=>> location_easy_to_find - 0.6910457295525726\n","=>> service_wait_time - 0.642354751816072\n","=>> service_waiters_attitude - 0.7888339995995901\n","=>> service_parking_convenience - 0.7163147511415122\n","=>> service_serving_speed - 0.7448260978146858\n","=>> price_level - 0.7744010238261791\n","=>> price_cost_effective - 0.7051854267423113\n","=>> price_discount - 0.6384468026478084\n","=>> environment_decoration - 0.7157014206218196\n","=>> environment_noise - 0.7544534358286522\n","=>> environment_space - 0.7604779299326768\n","=>> environment_cleaness - 0.7436723374339621\n","=>> dish_portion - 0.7181114257848324\n","=>> dish_taste - 0.7148077522537162\n","=>> dish_look - 0.5576442322584054\n","=>> dish_recommendation - 0.7210192659949068\n","=>> others_overall_experience - 0.5839124010006701\n","=>> others_willing_to_consume_again - 0.668957662959655\n","=>> Eval loss 34.93649, f1 0.69048\n","=>> current result -0.6904780304316966, previous best result -0.6853542102173911\n","=>> Epoch 2  global step 7020 loss 5.96139 batch 1379/1640 lr 0.001 accuracy 93.28125 wps 27342.48 step time 0.51s\n","=>> Epoch 2  global step 7040 loss 5.90996 batch 1399/1640 lr 0.001 accuracy 93.32031 wps 27636.62 step time 0.45s\n","=>> Epoch 2  global step 7060 loss 5.98390 batch 1419/1640 lr 0.001 accuracy 93.18945 wps 26848.66 step time 0.53s\n","=>> Epoch 2  global step 7080 loss 5.80689 batch 1439/1640 lr 0.001 accuracy 93.41992 wps 25395.06 step time 0.57s\n","=>> Epoch 2  global step 7100 loss 5.83948 batch 1459/1640 lr 0.001 accuracy 93.47852 wps 27112.96 step time 0.51s\n","=>> Epoch 2  global step 7120 loss 6.09743 batch 1479/1640 lr 0.001 accuracy 93.11719 wps 26868.43 step time 0.51s\n","=>> Epoch 2  global step 7140 loss 6.16244 batch 1499/1640 lr 0.001 accuracy 92.98242 wps 27939.13 step time 0.56s\n","=>> Epoch 2  global step 7160 loss 6.39579 batch 1519/1640 lr 0.001 accuracy 92.64258 wps 26951.80 step time 0.66s\n","=>> Epoch 2  global step 7180 loss 6.54429 batch 1539/1640 lr 0.001 accuracy 92.48242 wps 27483.78 step time 0.63s\n","=>> Epoch 2  global step 7200 loss 5.70521 batch 1559/1640 lr 0.001 accuracy 93.60547 wps 27557.16 step time 0.46s\n","=>> Epoch 2  global step 7220 loss 6.72389 batch 1579/1640 lr 0.001 accuracy 92.21094 wps 27067.84 step time 0.76s\n","=>> Epoch 2  global step 7240 loss 5.75181 batch 1599/1640 lr 0.001 accuracy 93.58789 wps 27235.94 step time 0.45s\n","=>> Epoch 2  global step 7260 loss 6.00135 batch 1619/1640 lr 0.001 accuracy 93.07813 wps 27347.10 step time 0.51s\n","=>> Epoch 2  global step 7280 loss 5.94788 batch 1639/1640 lr 0.001 accuracy 93.21680 wps 28606.71 step time 0.49s\n","=>> Finsh epoch 2, global step 7282\n","=>> Epoch 3  global step 7300 loss 5.20060 batch 18/1640 lr 0.001 accuracy 84.10547 wps 31187.84 step time 0.47s\n","=>> Epoch 3  global step 7320 loss 5.55650 batch 38/1640 lr 0.001 accuracy 93.71094 wps 30070.26 step time 0.48s\n","=>> Epoch 3  global step 7340 loss 5.74502 batch 58/1640 lr 0.001 accuracy 93.46875 wps 29703.46 step time 0.41s\n","=>> Epoch 3  global step 7360 loss 5.79657 batch 78/1640 lr 0.001 accuracy 93.34766 wps 30548.24 step time 0.50s\n","=>> Epoch 3  global step 7380 loss 5.66594 batch 98/1640 lr 0.001 accuracy 93.65430 wps 29467.70 step time 0.42s\n","=>> Epoch 3  global step 7400 loss 6.03327 batch 118/1640 lr 0.001 accuracy 93.12891 wps 31756.14 step time 0.55s\n","=>> Epoch 3  global step 7420 loss 5.83292 batch 138/1640 lr 0.001 accuracy 93.48437 wps 30567.49 step time 0.50s\n","=>> Epoch 3  global step 7440 loss 6.04545 batch 158/1640 lr 0.001 accuracy 93.11523 wps 31327.90 step time 0.52s\n","=>> Epoch 3  global step 7460 loss 6.06210 batch 178/1640 lr 0.001 accuracy 93.19922 wps 32862.30 step time 0.50s\n","=>> Epoch 3  global step 7480 loss 5.79098 batch 198/1640 lr 0.001 accuracy 93.54102 wps 30462.14 step time 0.42s\n","=>> Epoch 3  global step 7500 loss 5.87716 batch 218/1640 lr 0.001 accuracy 93.36523 wps 32038.44 step time 0.48s\n","=>> Epoch 3  global step 7520 loss 5.58851 batch 238/1640 lr 0.001 accuracy 93.76953 wps 29752.32 step time 0.41s\n","=>> Epoch 3  global step 7540 loss 5.79064 batch 258/1640 lr 0.001 accuracy 93.50391 wps 31266.25 step time 0.46s\n","=>> Epoch 3  global step 7560 loss 5.98555 batch 278/1640 lr 0.001 accuracy 93.25977 wps 31801.69 step time 0.46s\n","=>> Epoch 3  global step 7580 loss 5.66381 batch 298/1640 lr 0.001 accuracy 93.51563 wps 29979.66 step time 0.42s\n","=>> Epoch 3  global step 7600 loss 5.59643 batch 318/1640 lr 0.001 accuracy 93.62891 wps 30249.94 step time 0.42s\n","=>> Epoch 3  global step 7620 loss 6.09595 batch 338/1640 lr 0.001 accuracy 93.21484 wps 31926.75 step time 0.54s\n","=>> Epoch 3  global step 7640 loss 5.74752 batch 358/1640 lr 0.001 accuracy 93.49023 wps 30858.99 step time 0.44s\n","=>> Epoch 3  global step 7660 loss 5.75728 batch 378/1640 lr 0.001 accuracy 93.56445 wps 30456.16 step time 0.42s\n","=>> Epoch 3  global step 7680 loss 5.65793 batch 398/1640 lr 0.001 accuracy 93.55859 wps 31304.85 step time 0.45s\n","=>> Epoch 3  global step 7700 loss 5.73471 batch 418/1640 lr 0.001 accuracy 93.49023 wps 31284.67 step time 0.45s\n","=>> Epoch 3  global step 7720 loss 5.45804 batch 438/1640 lr 0.001 accuracy 93.75781 wps 29646.67 step time 0.41s\n","=>> Epoch 3  global step 7740 loss 5.83042 batch 458/1640 lr 0.001 accuracy 93.34961 wps 30949.69 step time 0.51s\n","=>> Epoch 3  global step 7760 loss 6.04594 batch 478/1640 lr 0.001 accuracy 93.09570 wps 31047.64 step time 0.52s\n","=>> Epoch 3  global step 7780 loss 5.48095 batch 498/1640 lr 0.001 accuracy 93.81445 wps 29142.33 step time 0.40s\n","=>> Epoch 3  global step 7800 loss 6.00531 batch 518/1640 lr 0.001 accuracy 93.21094 wps 31364.82 step time 0.44s\n","=>> Epoch 3  global step 7820 loss 5.64813 batch 538/1640 lr 0.001 accuracy 93.59766 wps 31377.33 step time 0.45s\n","=>> Epoch 3  global step 7840 loss 5.75634 batch 558/1640 lr 0.001 accuracy 93.62695 wps 30845.00 step time 0.50s\n","=>> Epoch 3  global step 7860 loss 5.81120 batch 578/1640 lr 0.001 accuracy 93.38477 wps 32074.09 step time 0.54s\n","=>> Epoch 3  global step 7880 loss 5.70830 batch 598/1640 lr 0.001 accuracy 93.56250 wps 30840.67 step time 0.44s\n","=>> Epoch 3  global step 7900 loss 5.97184 batch 618/1640 lr 0.001 accuracy 93.27539 wps 30564.94 step time 0.51s\n","=>> Epoch 3  global step 7920 loss 5.79254 batch 638/1640 lr 0.001 accuracy 93.34766 wps 30984.65 step time 0.51s\n","=>> Epoch 3  global step 7940 loss 5.73494 batch 658/1640 lr 0.001 accuracy 93.56641 wps 30402.59 step time 0.42s\n","=>> Epoch 3  global step 7960 loss 5.79932 batch 678/1640 lr 0.001 accuracy 93.47461 wps 30573.16 step time 0.43s\n","=>> Epoch 3  global step 7980 loss 6.21800 batch 698/1640 lr 0.001 accuracy 92.87695 wps 31697.97 step time 0.53s\n","=>> Epoch 3  global step 8000 loss 5.60817 batch 718/1640 lr 0.001 accuracy 93.53906 wps 31485.23 step time 0.47s\n","=>> global step 8000, eval result: \n","=>> location_traffic_convenience - 0.6542135441476324\n","=>> location_distance_from_business_district - 0.5167185056735262\n","=>> location_easy_to_find - 0.7013208975454582\n","=>> service_wait_time - 0.6439141436223554\n","=>> service_waiters_attitude - 0.7876011208958847\n","=>> service_parking_convenience - 0.721302923944499\n","=>> service_serving_speed - 0.7483843862788604\n","=>> price_level - 0.7772319301379608\n","=>> price_cost_effective - 0.7063934338630369\n","=>> price_discount - 0.6455672266347445\n","=>> environment_decoration - 0.7128994854501206\n","=>> environment_noise - 0.7577732229012775\n","=>> environment_space - 0.763015261741596\n","=>> environment_cleaness - 0.7481379988578428\n","=>> dish_portion - 0.7172753921022637\n","=>> dish_taste - 0.7178038996109399\n","=>> dish_look - 0.5637096100091155\n","=>> dish_recommendation - 0.7363810709775592\n","=>> others_overall_experience - 0.5833230182529339\n","=>> others_willing_to_consume_again - 0.6801761070201648\n","=>> Eval loss 34.69023, f1 0.69416\n","=>> current result -0.6941571589833886, previous best result -0.6904780304316966\n","=>> Epoch 3  global step 8020 loss 5.70710 batch 738/1640 lr 0.001 accuracy 93.55273 wps 29680.66 step time 0.45s\n","=>> Epoch 3  global step 8040 loss 5.87573 batch 758/1640 lr 0.001 accuracy 93.39063 wps 31609.81 step time 0.47s\n","=>> Epoch 3  global step 8060 loss 5.99923 batch 778/1640 lr 0.001 accuracy 93.30274 wps 31778.05 step time 0.46s\n","=>> Epoch 3  global step 8080 loss 5.95323 batch 798/1640 lr 0.001 accuracy 93.20312 wps 32078.56 step time 0.55s\n","=>> Epoch 3  global step 8100 loss 5.92867 batch 818/1640 lr 0.001 accuracy 93.36914 wps 25974.21 step time 0.58s\n","=>> Epoch 3  global step 8120 loss 6.10698 batch 838/1640 lr 0.001 accuracy 92.90234 wps 26618.36 step time 0.59s\n","=>> Epoch 3  global step 8140 loss 5.68560 batch 858/1640 lr 0.001 accuracy 93.52148 wps 27314.92 step time 0.50s\n","=>> Epoch 3  global step 8160 loss 5.97892 batch 878/1640 lr 0.001 accuracy 93.11133 wps 26100.43 step time 0.61s\n","=>> Epoch 3  global step 8180 loss 6.09700 batch 898/1640 lr 0.001 accuracy 92.96094 wps 27530.92 step time 0.65s\n","=>> Epoch 3  global step 8200 loss 5.72306 batch 918/1640 lr 0.001 accuracy 93.43945 wps 26369.63 step time 0.53s\n","=>> Epoch 3  global step 8220 loss 5.84073 batch 938/1640 lr 0.001 accuracy 93.33008 wps 27706.27 step time 0.56s\n","=>> Epoch 3  global step 8240 loss 5.55793 batch 958/1640 lr 0.001 accuracy 93.82227 wps 27487.97 step time 0.43s\n","=>> Epoch 3  global step 8260 loss 5.76622 batch 978/1640 lr 0.001 accuracy 93.33594 wps 27958.15 step time 0.52s\n","=>> Epoch 3  global step 8280 loss 5.73543 batch 998/1640 lr 0.001 accuracy 93.48242 wps 27205.72 step time 0.52s\n","=>> Epoch 3  global step 8300 loss 5.81244 batch 1018/1640 lr 0.001 accuracy 93.45313 wps 26094.08 step time 0.59s\n","=>> Epoch 3  global step 8320 loss 5.70366 batch 1038/1640 lr 0.001 accuracy 93.68164 wps 25788.76 step time 0.57s\n","=>> Epoch 3  global step 8340 loss 6.34161 batch 1058/1640 lr 0.001 accuracy 92.79883 wps 28683.14 step time 0.61s\n","=>> Epoch 3  global step 8360 loss 5.45525 batch 1078/1640 lr 0.001 accuracy 93.80273 wps 26016.55 step time 0.42s\n","=>> Epoch 3  global step 8380 loss 5.82818 batch 1098/1640 lr 0.001 accuracy 93.44922 wps 26821.89 step time 0.54s\n","=>> Epoch 3  global step 8400 loss 6.13808 batch 1118/1640 lr 0.001 accuracy 92.97852 wps 28308.00 step time 0.53s\n","=>> Epoch 3  global step 8420 loss 6.06407 batch 1138/1640 lr 0.001 accuracy 93.01953 wps 27709.37 step time 0.56s\n","=>> Epoch 3  global step 8440 loss 5.72292 batch 1158/1640 lr 0.001 accuracy 93.51758 wps 26530.08 step time 0.58s\n","=>> Epoch 3  global step 8460 loss 6.03170 batch 1178/1640 lr 0.001 accuracy 93.17773 wps 24570.30 step time 0.64s\n","=>> Epoch 3  global step 8480 loss 5.81313 batch 1198/1640 lr 0.001 accuracy 93.35547 wps 27917.53 step time 0.54s\n","=>> Epoch 3  global step 8500 loss 5.85957 batch 1218/1640 lr 0.001 accuracy 93.41211 wps 28032.31 step time 0.54s\n","=>> Epoch 3  global step 8520 loss 5.79217 batch 1238/1640 lr 0.001 accuracy 93.35547 wps 29130.92 step time 0.49s\n","=>> Epoch 3  global step 8540 loss 5.65309 batch 1258/1640 lr 0.001 accuracy 93.52539 wps 26414.13 step time 0.49s\n","=>> Epoch 3  global step 8560 loss 5.55444 batch 1278/1640 lr 0.001 accuracy 93.68945 wps 26662.12 step time 0.48s\n","=>> Epoch 3  global step 8580 loss 5.83001 batch 1298/1640 lr 0.001 accuracy 93.42187 wps 26456.66 step time 0.54s\n","=>> Epoch 3  global step 8600 loss 5.50090 batch 1318/1640 lr 0.001 accuracy 93.90234 wps 26722.63 step time 0.46s\n","=>> Epoch 3  global step 8620 loss 5.79073 batch 1338/1640 lr 0.001 accuracy 93.44531 wps 27170.74 step time 0.47s\n","=>> Epoch 3  global step 8640 loss 5.76086 batch 1358/1640 lr 0.001 accuracy 93.38672 wps 26777.44 step time 0.52s\n","=>> Epoch 3  global step 8660 loss 6.11295 batch 1378/1640 lr 0.001 accuracy 93.02344 wps 26902.54 step time 0.60s\n","=>> Epoch 3  global step 8680 loss 6.04722 batch 1398/1640 lr 0.001 accuracy 93.21875 wps 27907.96 step time 0.55s\n","=>> Epoch 3  global step 8700 loss 5.76001 batch 1418/1640 lr 0.001 accuracy 93.57227 wps 27711.55 step time 0.45s\n","=>> Epoch 3  global step 8720 loss 5.94963 batch 1438/1640 lr 0.001 accuracy 93.19727 wps 27905.89 step time 0.54s\n","=>> Epoch 3  global step 8740 loss 6.33128 batch 1458/1640 lr 0.001 accuracy 92.82812 wps 28605.65 step time 0.58s\n","=>> Epoch 3  global step 8760 loss 6.00087 batch 1478/1640 lr 0.001 accuracy 93.19336 wps 26096.80 step time 0.60s\n","=>> Epoch 3  global step 8780 loss 6.23354 batch 1498/1640 lr 0.001 accuracy 92.77344 wps 25702.86 step time 0.85s\n","=>> Epoch 3  global step 8800 loss 6.11111 batch 1518/1640 lr 0.001 accuracy 92.92188 wps 25398.72 step time 0.67s\n","=>> Epoch 3  global step 8820 loss 5.91582 batch 1538/1640 lr 0.001 accuracy 93.26367 wps 26702.38 step time 0.60s\n","=>> Epoch 3  global step 8840 loss 5.64613 batch 1558/1640 lr 0.001 accuracy 93.59961 wps 27689.44 step time 0.48s\n","=>> Epoch 3  global step 8860 loss 5.68918 batch 1578/1640 lr 0.001 accuracy 93.65820 wps 27173.08 step time 0.45s\n","=>> Epoch 3  global step 8880 loss 5.75570 batch 1598/1640 lr 0.001 accuracy 93.43164 wps 26532.36 step time 0.47s\n","=>> Epoch 3  global step 8900 loss 6.11335 batch 1618/1640 lr 0.001 accuracy 93.01562 wps 27436.53 step time 0.56s\n","=>> Epoch 3  global step 8920 loss 5.75937 batch 1638/1640 lr 0.001 accuracy 93.53711 wps 26938.73 step time 0.49s\n","=>> Finsh epoch 3, global step 8923\n","=>> Epoch 4  global step 8940 loss 5.07613 batch 17/1640 lr 0.001 accuracy 79.28906 wps 31585.98 step time 0.48s\n","=>> Epoch 4  global step 8960 loss 5.48602 batch 37/1640 lr 0.001 accuracy 93.81250 wps 30272.82 step time 0.43s\n","=>> Epoch 4  global step 8980 loss 5.87392 batch 57/1640 lr 0.001 accuracy 93.21289 wps 31440.75 step time 0.53s\n","=>> Epoch 4  global step 9000 loss 5.69034 batch 77/1640 lr 0.001 accuracy 93.50977 wps 31394.75 step time 0.46s\n","=>> global step 9000, eval result: \n","=>> location_traffic_convenience - 0.6605548369667554\n","=>> location_distance_from_business_district - 0.5411419172180529\n","=>> location_easy_to_find - 0.7070180288229703\n","=>> service_wait_time - 0.6562396008553515\n","=>> service_waiters_attitude - 0.7894945925695385\n","=>> service_parking_convenience - 0.7279355476700976\n","=>> service_serving_speed - 0.7590805756869772\n","=>> price_level - 0.7784743740495844\n","=>> price_cost_effective - 0.7077065348681147\n","=>> price_discount - 0.6454845448181721\n","=>> environment_decoration - 0.720171116147281\n","=>> environment_noise - 0.757669044018569\n","=>> environment_space - 0.7674771100026082\n","=>> environment_cleaness - 0.7482886949976967\n","=>> dish_portion - 0.7209222587218392\n","=>> dish_taste - 0.7212565979829962\n","=>> dish_look - 0.5655482233165874\n","=>> dish_recommendation - 0.7315381356114694\n","=>> others_overall_experience - 0.587499829512018\n","=>> others_willing_to_consume_again - 0.6848119034765937\n","=>> Eval loss 34.49365, f1 0.69892\n","=>> current result -0.6989156733656635, previous best result -0.6941571589833886\n","=>> Epoch 4  global step 9020 loss 5.77235 batch 97/1640 lr 0.001 accuracy 93.51367 wps 31145.23 step time 0.56s\n","=>> Epoch 4  global step 9040 loss 5.92446 batch 117/1640 lr 0.001 accuracy 93.30078 wps 32613.72 step time 0.51s\n","=>> Epoch 4  global step 9060 loss 5.43997 batch 137/1640 lr 0.001 accuracy 93.82227 wps 29506.46 step time 0.41s\n","=>> Epoch 4  global step 9080 loss 5.73302 batch 157/1640 lr 0.001 accuracy 93.48633 wps 31765.93 step time 0.48s\n","=>> Epoch 4  global step 9100 loss 5.24399 batch 177/1640 lr 0.001 accuracy 94.04492 wps 28871.89 step time 0.46s\n","=>> Epoch 4  global step 9120 loss 5.69480 batch 197/1640 lr 0.001 accuracy 93.56445 wps 31192.19 step time 0.52s\n","=>> Epoch 4  global step 9140 loss 6.14214 batch 217/1640 lr 0.001 accuracy 92.98438 wps 32581.53 step time 0.58s\n","=>> Epoch 4  global step 9160 loss 5.39552 batch 237/1640 lr 0.001 accuracy 94.00000 wps 29220.79 step time 0.40s\n","=>> Epoch 4  global step 9180 loss 5.82674 batch 257/1640 lr 0.001 accuracy 93.42187 wps 29107.22 step time 0.50s\n","=>> Epoch 4  global step 9200 loss 5.68070 batch 277/1640 lr 0.001 accuracy 93.53516 wps 25857.67 step time 0.66s\n","=>> Epoch 4  global step 9220 loss 5.62047 batch 297/1640 lr 0.001 accuracy 93.73633 wps 26498.72 step time 0.50s\n","=>> Epoch 4  global step 9240 loss 5.43816 batch 317/1640 lr 0.001 accuracy 93.81055 wps 26993.94 step time 0.49s\n","=>> Epoch 4  global step 9260 loss 5.32251 batch 337/1640 lr 0.001 accuracy 93.95312 wps 26447.25 step time 0.46s\n","=>> Epoch 4  global step 9280 loss 5.68099 batch 357/1640 lr 0.001 accuracy 93.65820 wps 27370.20 step time 0.52s\n","=>> Epoch 4  global step 9300 loss 6.05890 batch 377/1640 lr 0.001 accuracy 93.02930 wps 26116.91 step time 0.66s\n","=>> Epoch 4  global step 9320 loss 5.36051 batch 397/1640 lr 0.001 accuracy 93.95898 wps 26251.88 step time 0.46s\n","=>> Epoch 4  global step 9340 loss 5.62828 batch 417/1640 lr 0.001 accuracy 93.57812 wps 25629.09 step time 0.55s\n","=>> Epoch 4  global step 9360 loss 5.71715 batch 437/1640 lr 0.001 accuracy 93.38086 wps 26197.15 step time 0.69s\n","=>> Epoch 4  global step 9380 loss 5.64328 batch 457/1640 lr 0.001 accuracy 93.60352 wps 28241.74 step time 0.52s\n","=>> Epoch 4  global step 9400 loss 5.67343 batch 477/1640 lr 0.001 accuracy 93.50000 wps 27360.85 step time 0.52s\n","=>> Epoch 4  global step 9420 loss 5.59127 batch 497/1640 lr 0.001 accuracy 93.70898 wps 27602.33 step time 0.51s\n","=>> Epoch 4  global step 9440 loss 5.51660 batch 517/1640 lr 0.001 accuracy 93.80664 wps 26297.26 step time 0.58s\n","=>> Epoch 4  global step 9460 loss 5.57482 batch 537/1640 lr 0.001 accuracy 93.69336 wps 27349.53 step time 0.53s\n","=>> Epoch 4  global step 9480 loss 5.82348 batch 557/1640 lr 0.001 accuracy 93.26367 wps 27368.43 step time 0.57s\n","=>> Epoch 4  global step 9500 loss 5.44366 batch 577/1640 lr 0.001 accuracy 93.79883 wps 26929.34 step time 0.46s\n","=>> Epoch 4  global step 9520 loss 5.46672 batch 597/1640 lr 0.001 accuracy 93.75586 wps 27775.61 step time 0.50s\n","=>> Epoch 4  global step 9540 loss 5.87112 batch 617/1640 lr 0.001 accuracy 93.33008 wps 27492.26 step time 0.55s\n","=>> Epoch 4  global step 9560 loss 5.70253 batch 637/1640 lr 0.001 accuracy 93.62109 wps 27075.39 step time 0.59s\n","=>> Epoch 4  global step 9580 loss 5.65789 batch 657/1640 lr 0.001 accuracy 93.56445 wps 27445.66 step time 0.54s\n","=>> Epoch 4  global step 9600 loss 5.88919 batch 677/1640 lr 0.001 accuracy 93.29492 wps 26744.07 step time 0.66s\n","=>> Epoch 4  global step 9620 loss 5.97096 batch 697/1640 lr 0.001 accuracy 93.35352 wps 26051.84 step time 0.64s\n","=>> Epoch 4  global step 9640 loss 5.55308 batch 717/1640 lr 0.001 accuracy 93.68945 wps 28115.44 step time 0.49s\n","=>> Epoch 4  global step 9660 loss 5.53112 batch 737/1640 lr 0.001 accuracy 93.76172 wps 25789.86 step time 0.55s\n","=>> Epoch 4  global step 9680 loss 5.73793 batch 757/1640 lr 0.001 accuracy 93.46680 wps 28195.75 step time 0.56s\n","=>> Epoch 4  global step 9700 loss 5.53161 batch 777/1640 lr 0.001 accuracy 93.66406 wps 27030.97 step time 0.50s\n","=>> Epoch 4  global step 9720 loss 5.65652 batch 797/1640 lr 0.001 accuracy 93.58398 wps 28966.42 step time 0.48s\n","=>> Epoch 4  global step 9740 loss 5.59858 batch 817/1640 lr 0.001 accuracy 93.68359 wps 26666.02 step time 0.47s\n","=>> Epoch 4  global step 9760 loss 5.37913 batch 837/1640 lr 0.001 accuracy 94.02344 wps 26548.83 step time 0.43s\n","=>> Epoch 4  global step 9780 loss 5.74377 batch 857/1640 lr 0.001 accuracy 93.50391 wps 27353.20 step time 0.51s\n","=>> Epoch 4  global step 9800 loss 5.80580 batch 877/1640 lr 0.001 accuracy 93.36914 wps 26570.95 step time 0.59s\n","=>> Epoch 4  global step 9820 loss 5.72601 batch 897/1640 lr 0.001 accuracy 93.49609 wps 27975.87 step time 0.54s\n","=>> Epoch 4  global step 9840 loss 5.75841 batch 917/1640 lr 0.001 accuracy 93.61523 wps 27019.27 step time 0.60s\n","=>> Epoch 4  global step 9860 loss 5.62082 batch 937/1640 lr 0.001 accuracy 93.76172 wps 27183.53 step time 0.50s\n","=>> Epoch 4  global step 9880 loss 5.76487 batch 957/1640 lr 0.001 accuracy 93.49023 wps 25610.41 step time 0.63s\n","=>> Epoch 4  global step 9900 loss 5.68058 batch 977/1640 lr 0.001 accuracy 93.50000 wps 28197.68 step time 0.48s\n","=>> Epoch 4  global step 9920 loss 5.99262 batch 997/1640 lr 0.001 accuracy 93.35547 wps 28155.15 step time 0.61s\n","=>> Epoch 4  global step 9940 loss 5.61964 batch 1017/1640 lr 0.001 accuracy 93.65234 wps 26750.34 step time 0.58s\n","=>> Epoch 4  global step 9960 loss 5.81769 batch 1037/1640 lr 0.001 accuracy 93.30078 wps 26773.33 step time 0.56s\n","=>> Epoch 4  global step 9980 loss 5.36052 batch 1057/1640 lr 0.001 accuracy 94.06836 wps 27306.21 step time 0.45s\n","=>> Epoch 4  global step 10000 loss 5.90460 batch 1077/1640 lr 0.001 accuracy 93.26367 wps 26671.25 step time 0.67s\n","=>> global step 10000, eval result: \n","=>> location_traffic_convenience - 0.6589599632724087\n","=>> location_distance_from_business_district - 0.5334731576337286\n","=>> location_easy_to_find - 0.7060937181111532\n","=>> service_wait_time - 0.6538310031611293\n","=>> service_waiters_attitude - 0.791054676534327\n","=>> service_parking_convenience - 0.7339048164271623\n","=>> service_serving_speed - 0.7528065203885668\n","=>> price_level - 0.780290141726749\n","=>> price_cost_effective - 0.7082685090897236\n","=>> price_discount - 0.6552778275104971\n","=>> environment_decoration - 0.7226242739635977\n","=>> environment_noise - 0.7602417787314961\n","=>> environment_space - 0.7643286095273232\n","=>> environment_cleaness - 0.7494671102194299\n","=>> dish_portion - 0.72403976324796\n","=>> dish_taste - 0.7195462658679936\n","=>> dish_look - 0.5619536341297677\n","=>> dish_recommendation - 0.7297419936911682\n","=>> others_overall_experience - 0.5893941944442127\n","=>> others_willing_to_consume_again - 0.6898013450714519\n","=>> Eval loss 34.43728, f1 0.69925\n","=>> current result -0.6992549651374923, previous best result -0.6989156733656635\n","=>> Epoch 4  global step 10020 loss 5.35190 batch 1097/1640 lr 0.001 accuracy 93.94336 wps 25829.99 step time 0.52s\n","=>> Epoch 4  global step 10040 loss 5.56588 batch 1117/1640 lr 0.001 accuracy 93.69727 wps 27130.87 step time 0.46s\n","=>> Epoch 4  global step 10060 loss 6.15294 batch 1137/1640 lr 0.001 accuracy 92.98242 wps 24851.78 step time 0.71s\n","=>> Epoch 4  global step 10080 loss 5.75979 batch 1157/1640 lr 0.001 accuracy 93.44531 wps 27874.55 step time 0.48s\n","=>> Epoch 4  global step 10100 loss 5.47966 batch 1177/1640 lr 0.001 accuracy 93.85742 wps 26992.07 step time 0.47s\n","=>> Epoch 4  global step 10120 loss 5.78350 batch 1197/1640 lr 0.001 accuracy 93.47656 wps 27032.32 step time 0.63s\n","=>> Epoch 4  global step 10140 loss 5.76108 batch 1217/1640 lr 0.001 accuracy 93.35938 wps 27880.67 step time 0.58s\n","=>> Epoch 4  global step 10160 loss 5.87213 batch 1237/1640 lr 0.001 accuracy 93.21680 wps 26485.82 step time 0.60s\n","=>> Epoch 4  global step 10180 loss 5.61460 batch 1257/1640 lr 0.001 accuracy 93.72656 wps 25887.08 step time 0.55s\n","=>> Epoch 4  global step 10200 loss 5.52879 batch 1277/1640 lr 0.001 accuracy 93.75391 wps 27936.19 step time 0.47s\n","=>> Epoch 4  global step 10220 loss 5.87429 batch 1297/1640 lr 0.001 accuracy 93.45508 wps 26134.42 step time 0.64s\n","=>> Epoch 4  global step 10240 loss 6.05846 batch 1317/1640 lr 0.001 accuracy 93.07422 wps 26513.66 step time 0.64s\n","=>> Epoch 4  global step 10260 loss 5.65651 batch 1337/1640 lr 0.001 accuracy 93.66016 wps 26271.23 step time 0.48s\n","=>> Epoch 4  global step 10280 loss 5.66181 batch 1357/1640 lr 0.001 accuracy 93.61133 wps 27138.80 step time 0.51s\n","=>> Epoch 4  global step 10300 loss 5.67186 batch 1377/1640 lr 0.001 accuracy 93.58398 wps 25651.61 step time 0.54s\n","=>> Epoch 4  global step 10320 loss 5.68070 batch 1397/1640 lr 0.001 accuracy 93.57813 wps 28436.45 step time 0.48s\n","=>> Epoch 4  global step 10340 loss 5.47432 batch 1417/1640 lr 0.001 accuracy 93.85547 wps 27076.53 step time 0.48s\n","=>> Epoch 4  global step 10360 loss 5.56852 batch 1437/1640 lr 0.001 accuracy 93.61328 wps 27942.54 step time 0.49s\n","=>> Epoch 4  global step 10380 loss 5.93043 batch 1457/1640 lr 0.001 accuracy 93.23828 wps 27476.58 step time 0.58s\n","=>> Epoch 4  global step 10400 loss 5.84779 batch 1477/1640 lr 0.001 accuracy 93.42383 wps 25176.42 step time 0.65s\n","=>> Epoch 4  global step 10420 loss 5.55595 batch 1497/1640 lr 0.001 accuracy 93.70898 wps 26333.46 step time 0.50s\n","=>> Epoch 4  global step 10440 loss 5.62015 batch 1517/1640 lr 0.001 accuracy 93.51367 wps 27392.05 step time 0.50s\n","=>> Epoch 4  global step 10460 loss 5.46652 batch 1537/1640 lr 0.001 accuracy 93.76953 wps 26343.56 step time 0.52s\n","=>> Epoch 4  global step 10480 loss 5.53320 batch 1557/1640 lr 0.001 accuracy 93.60547 wps 27323.72 step time 0.45s\n","=>> Epoch 4  global step 10500 loss 5.87465 batch 1577/1640 lr 0.001 accuracy 93.25977 wps 26189.45 step time 0.63s\n","=>> Epoch 4  global step 10520 loss 5.89465 batch 1597/1640 lr 0.001 accuracy 93.25977 wps 28485.74 step time 0.50s\n","=>> Epoch 4  global step 10540 loss 5.45709 batch 1617/1640 lr 0.001 accuracy 93.85547 wps 27882.96 step time 0.46s\n","=>> Epoch 4  global step 10560 loss 5.23281 batch 1637/1640 lr 0.001 accuracy 94.16016 wps 26490.48 step time 0.45s\n","=>> Finsh epoch 4, global step 10564\n","=>> Epoch 5  global step 10580 loss 4.46058 batch 16/1640 lr 0.001 accuracy 74.90820 wps 30452.27 step time 0.47s\n","=>> Epoch 5  global step 10600 loss 5.35292 batch 36/1640 lr 0.001 accuracy 93.83008 wps 30275.34 step time 0.42s\n","=>> Epoch 5  global step 10620 loss 5.22873 batch 56/1640 lr 0.001 accuracy 94.16602 wps 28996.66 step time 0.46s\n","=>> Epoch 5  global step 10640 loss 5.46307 batch 76/1640 lr 0.001 accuracy 93.75977 wps 31067.22 step time 0.45s\n","=>> Epoch 5  global step 10660 loss 5.26660 batch 96/1640 lr 0.001 accuracy 94.05273 wps 30131.17 step time 0.41s\n","=>> Epoch 5  global step 10680 loss 5.34199 batch 116/1640 lr 0.001 accuracy 94.07031 wps 30692.58 step time 0.43s\n","=>> Epoch 5  global step 10700 loss 5.41928 batch 136/1640 lr 0.001 accuracy 93.91602 wps 30232.29 step time 0.42s\n","=>> Epoch 5  global step 10720 loss 5.85428 batch 156/1640 lr 0.001 accuracy 93.32422 wps 31823.92 step time 0.61s\n","=>> Epoch 5  global step 10740 loss 5.31226 batch 176/1640 lr 0.001 accuracy 93.95703 wps 30753.04 step time 0.44s\n","=>> Epoch 5  global step 10760 loss 5.67270 batch 196/1640 lr 0.001 accuracy 93.59180 wps 31501.59 step time 0.52s\n","=>> Epoch 5  global step 10780 loss 5.41094 batch 216/1640 lr 0.001 accuracy 93.88672 wps 30106.88 step time 0.49s\n","=>> Epoch 5  global step 10800 loss 5.67133 batch 236/1640 lr 0.001 accuracy 93.48047 wps 31873.27 step time 0.47s\n","=>> Epoch 5  global step 10820 loss 5.51905 batch 256/1640 lr 0.001 accuracy 93.78711 wps 30743.43 step time 0.50s\n","=>> Epoch 5  global step 10840 loss 5.48978 batch 276/1640 lr 0.001 accuracy 93.72461 wps 29061.50 step time 0.47s\n","=>> Epoch 5  global step 10860 loss 5.26554 batch 296/1640 lr 0.001 accuracy 94.12500 wps 26733.69 step time 0.46s\n","=>> Epoch 5  global step 10880 loss 5.72163 batch 316/1640 lr 0.001 accuracy 93.45898 wps 26079.59 step time 0.62s\n","=>> Epoch 5  global step 10900 loss 5.49785 batch 336/1640 lr 0.001 accuracy 93.71289 wps 27175.85 step time 0.55s\n","=>> Epoch 5  global step 10920 loss 5.43369 batch 356/1640 lr 0.001 accuracy 93.71484 wps 26800.06 step time 0.53s\n","=>> Epoch 5  global step 10940 loss 5.07439 batch 376/1640 lr 0.001 accuracy 94.35742 wps 26512.12 step time 0.44s\n","=>> Epoch 5  global step 10960 loss 5.27747 batch 396/1640 lr 0.001 accuracy 93.98633 wps 25787.21 step time 0.54s\n","=>> Epoch 5  global step 10980 loss 5.65324 batch 416/1640 lr 0.001 accuracy 93.57812 wps 29340.74 step time 0.58s\n","=>> Epoch 5  global step 11000 loss 5.64523 batch 436/1640 lr 0.001 accuracy 93.64453 wps 27166.94 step time 0.53s\n","=>> global step 11000, eval result: \n","=>> location_traffic_convenience - 0.6588439962550793\n","=>> location_distance_from_business_district - 0.5298929453206944\n","=>> location_easy_to_find - 0.7129398980467633\n","=>> service_wait_time - 0.6598810409737054\n","=>> service_waiters_attitude - 0.7909913576863258\n","=>> service_parking_convenience - 0.7334952804430909\n","=>> service_serving_speed - 0.7590522145380454\n","=>> price_level - 0.7804911930363783\n","=>> price_cost_effective - 0.7144822345108259\n","=>> price_discount - 0.6614896848743327\n","=>> environment_decoration - 0.7303846865699399\n","=>> environment_noise - 0.7625531685052084\n","=>> environment_space - 0.7661207719427929\n","=>> environment_cleaness - 0.7525174362681177\n","=>> dish_portion - 0.7262863430931805\n","=>> dish_taste - 0.7234216083697795\n","=>> dish_look - 0.5746488062583561\n","=>> dish_recommendation - 0.7343817255407911\n","=>> others_overall_experience - 0.5892954657815239\n","=>> others_willing_to_consume_again - 0.6881660670423861\n","=>> Eval loss 34.44263, f1 0.70247\n","=>> current result -0.7024667962528659, previous best result -0.6992549651374923\n","=>> Epoch 5  global step 11020 loss 5.47453 batch 456/1640 lr 0.001 accuracy 93.74023 wps 25320.18 step time 0.66s\n","=>> Epoch 5  global step 11040 loss 4.99148 batch 476/1640 lr 0.001 accuracy 94.31055 wps 26179.87 step time 0.48s\n","=>> Epoch 5  global step 11060 loss 5.36948 batch 496/1640 lr 0.001 accuracy 94.00976 wps 25951.49 step time 0.59s\n","=>> Epoch 5  global step 11080 loss 5.74736 batch 516/1640 lr 0.001 accuracy 93.57031 wps 26570.26 step time 0.62s\n","=>> Epoch 5  global step 11100 loss 5.58999 batch 536/1640 lr 0.001 accuracy 93.61719 wps 28658.33 step time 0.57s\n","=>> Epoch 5  global step 11120 loss 5.26892 batch 556/1640 lr 0.001 accuracy 94.16797 wps 27380.44 step time 0.43s\n","=>> Epoch 5  global step 11140 loss 5.63307 batch 576/1640 lr 0.001 accuracy 93.58984 wps 28043.87 step time 0.51s\n","=>> Epoch 5  global step 11160 loss 5.41168 batch 596/1640 lr 0.001 accuracy 93.84766 wps 26669.44 step time 0.51s\n","=>> Epoch 5  global step 11180 loss 5.51851 batch 616/1640 lr 0.001 accuracy 93.80469 wps 27820.75 step time 0.54s\n","=>> Epoch 5  global step 11200 loss 5.26883 batch 636/1640 lr 0.001 accuracy 94.01758 wps 26687.40 step time 0.46s\n","=>> Epoch 5  global step 11220 loss 5.69250 batch 656/1640 lr 0.001 accuracy 93.48438 wps 25947.75 step time 0.62s\n","=>> Epoch 5  global step 11240 loss 5.55164 batch 676/1640 lr 0.001 accuracy 93.51562 wps 28260.39 step time 0.54s\n","=>> Epoch 5  global step 11260 loss 5.59530 batch 696/1640 lr 0.001 accuracy 93.60156 wps 27765.19 step time 0.50s\n","=>> Epoch 5  global step 11280 loss 5.64515 batch 716/1640 lr 0.001 accuracy 93.65820 wps 28287.46 step time 0.54s\n","=>> Epoch 5  global step 11300 loss 5.54274 batch 736/1640 lr 0.001 accuracy 93.80664 wps 26884.46 step time 0.52s\n","=>> Epoch 5  global step 11320 loss 5.42453 batch 756/1640 lr 0.001 accuracy 93.88867 wps 27339.67 step time 0.49s\n","=>> Epoch 5  global step 11340 loss 5.43197 batch 776/1640 lr 0.001 accuracy 93.75781 wps 26806.32 step time 0.49s\n","=>> Epoch 5  global step 11360 loss 5.71603 batch 796/1640 lr 0.001 accuracy 93.38477 wps 28103.99 step time 0.53s\n","=>> Epoch 5  global step 11380 loss 5.45575 batch 816/1640 lr 0.001 accuracy 93.87109 wps 27006.45 step time 0.47s\n","=>> Epoch 5  global step 11400 loss 5.56884 batch 836/1640 lr 0.001 accuracy 93.60547 wps 26921.95 step time 0.58s\n","=>> Epoch 5  global step 11420 loss 5.50652 batch 856/1640 lr 0.001 accuracy 93.73242 wps 26889.07 step time 0.54s\n","=>> Epoch 5  global step 11440 loss 5.77279 batch 876/1640 lr 0.001 accuracy 93.46484 wps 28222.30 step time 0.54s\n","=>> Epoch 5  global step 11460 loss 5.37368 batch 896/1640 lr 0.001 accuracy 93.94727 wps 27283.35 step time 0.50s\n","=>> Epoch 5  global step 11480 loss 5.66652 batch 916/1640 lr 0.001 accuracy 93.53516 wps 28613.51 step time 0.54s\n","=>> Epoch 5  global step 11500 loss 5.46511 batch 936/1640 lr 0.001 accuracy 93.77930 wps 27606.17 step time 0.50s\n","=>> Epoch 5  global step 11520 loss 5.51614 batch 956/1640 lr 0.001 accuracy 93.83398 wps 27868.64 step time 0.49s\n","=>> Epoch 5  global step 11540 loss 5.53806 batch 976/1640 lr 0.001 accuracy 93.69531 wps 25248.38 step time 0.62s\n","=>> Epoch 5  global step 11560 loss 5.12487 batch 996/1640 lr 0.001 accuracy 94.19336 wps 27337.20 step time 0.48s\n","=>> Epoch 5  global step 11580 loss 5.67154 batch 1016/1640 lr 0.001 accuracy 93.60547 wps 26469.05 step time 0.64s\n","=>> Epoch 5  global step 11600 loss 5.31841 batch 1036/1640 lr 0.001 accuracy 94.06445 wps 28463.89 step time 0.43s\n","=>> Epoch 5  global step 11620 loss 5.48189 batch 1056/1640 lr 0.001 accuracy 93.72266 wps 26430.93 step time 0.57s\n","=>> Epoch 5  global step 11640 loss 5.56293 batch 1076/1640 lr 0.001 accuracy 93.75977 wps 24479.65 step time 0.64s\n","=>> Epoch 5  global step 11660 loss 5.57422 batch 1096/1640 lr 0.001 accuracy 93.65234 wps 27934.05 step time 0.55s\n","=>> Epoch 5  global step 11680 loss 5.50780 batch 1116/1640 lr 0.001 accuracy 93.72656 wps 28234.37 step time 0.54s\n","=>> Epoch 5  global step 11700 loss 5.72857 batch 1136/1640 lr 0.001 accuracy 93.44727 wps 27412.25 step time 0.68s\n","=>> Epoch 5  global step 11720 loss 5.45523 batch 1156/1640 lr 0.001 accuracy 93.92773 wps 27001.47 step time 0.46s\n","=>> Epoch 5  global step 11740 loss 5.81972 batch 1176/1640 lr 0.001 accuracy 93.35742 wps 27220.55 step time 0.71s\n","=>> Epoch 5  global step 11760 loss 5.53005 batch 1196/1640 lr 0.001 accuracy 93.73242 wps 26240.09 step time 0.55s\n","=>> Epoch 5  global step 11780 loss 5.48815 batch 1216/1640 lr 0.001 accuracy 93.80469 wps 27814.25 step time 0.49s\n","=>> Epoch 5  global step 11800 loss 5.74841 batch 1236/1640 lr 0.001 accuracy 93.46094 wps 28106.59 step time 0.52s\n","=>> Epoch 5  global step 11820 loss 5.82072 batch 1256/1640 lr 0.001 accuracy 93.32227 wps 27078.34 step time 0.70s\n","=>> Epoch 5  global step 11840 loss 5.72208 batch 1276/1640 lr 0.001 accuracy 93.53906 wps 25399.02 step time 0.61s\n","=>> Epoch 5  global step 11860 loss 5.72598 batch 1296/1640 lr 0.001 accuracy 93.58984 wps 28255.82 step time 0.50s\n","=>> Epoch 5  global step 11880 loss 5.59610 batch 1316/1640 lr 0.001 accuracy 93.64648 wps 27745.14 step time 0.48s\n","=>> Epoch 5  global step 11900 loss 5.57234 batch 1336/1640 lr 0.001 accuracy 93.69727 wps 27543.13 step time 0.49s\n","=>> Epoch 5  global step 11920 loss 5.56841 batch 1356/1640 lr 0.001 accuracy 93.60547 wps 26361.99 step time 0.64s\n","=>> Epoch 5  global step 11940 loss 5.77270 batch 1376/1640 lr 0.001 accuracy 93.49023 wps 27530.98 step time 0.53s\n","=>> Epoch 5  global step 11960 loss 5.56463 batch 1396/1640 lr 0.001 accuracy 93.57617 wps 25822.29 step time 0.58s\n","=>> Epoch 5  global step 11980 loss 5.36020 batch 1416/1640 lr 0.001 accuracy 93.81445 wps 26164.21 step time 0.57s\n","=>> Epoch 5  global step 12000 loss 5.42605 batch 1436/1640 lr 0.001 accuracy 93.96875 wps 27277.31 step time 0.55s\n","=>> global step 12000, eval result: \n","=>> location_traffic_convenience - 0.6551075920784586\n","=>> location_distance_from_business_district - 0.5190735569906555\n","=>> location_easy_to_find - 0.7136668105216577\n","=>> service_wait_time - 0.6608187456126126\n","=>> service_waiters_attitude - 0.7942401548295811\n","=>> service_parking_convenience - 0.7340073715691047\n","=>> service_serving_speed - 0.7573898442757268\n","=>> price_level - 0.7816364791446997\n","=>> price_cost_effective - 0.7111150956436414\n","=>> price_discount - 0.6615838480347138\n","=>> environment_decoration - 0.7290089804797336\n","=>> environment_noise - 0.7615425749158028\n","=>> environment_space - 0.7648251786343713\n","=>> environment_cleaness - 0.7541260537326817\n","=>> dish_portion - 0.7226699077733955\n","=>> dish_taste - 0.7212383137100556\n","=>> dish_look - 0.5712959244170679\n","=>> dish_recommendation - 0.7278818627576816\n","=>> others_overall_experience - 0.5912734543954699\n","=>> others_willing_to_consume_again - 0.6929371481080351\n","=>> Eval loss 34.50497, f1 0.70127\n","=>> current result -0.7012719448812573, previous best result -0.7024667962528659\n","=>> Epoch 5  global step 12020 loss 5.64649 batch 1456/1640 lr 0.001 accuracy 93.61719 wps 27335.79 step time 0.49s\n","=>> Epoch 5  global step 12040 loss 5.77090 batch 1476/1640 lr 0.001 accuracy 93.44336 wps 25757.72 step time 0.65s\n","=>> Epoch 5  global step 12060 loss 5.63726 batch 1496/1640 lr 0.001 accuracy 93.48047 wps 28098.11 step time 0.48s\n","=>> Epoch 5  global step 12080 loss 5.49913 batch 1516/1640 lr 0.001 accuracy 93.71484 wps 27247.88 step time 0.46s\n","=>> Epoch 5  global step 12100 loss 5.48565 batch 1536/1640 lr 0.001 accuracy 93.78906 wps 27916.46 step time 0.53s\n","=>> Epoch 5  global step 12120 loss 5.71540 batch 1556/1640 lr 0.001 accuracy 93.43945 wps 27919.56 step time 0.56s\n","=>> Epoch 5  global step 12140 loss 5.73814 batch 1576/1640 lr 0.001 accuracy 93.41797 wps 28453.10 step time 0.52s\n","=>> Epoch 5  global step 12160 loss 5.54396 batch 1596/1640 lr 0.001 accuracy 93.60547 wps 28149.27 step time 0.51s\n","=>> Epoch 5  global step 12180 loss 5.55535 batch 1616/1640 lr 0.001 accuracy 93.61914 wps 26436.71 step time 0.56s\n","=>> Epoch 5  global step 12200 loss 5.47839 batch 1636/1640 lr 0.001 accuracy 93.89258 wps 27655.69 step time 0.49s\n","=>> Finsh epoch 5, global step 12205\n","=>> Epoch 6  global step 12220 loss 4.20867 batch 15/1640 lr 0.001 accuracy 70.18750 wps 31805.31 step time 0.42s\n","=>> Epoch 6  global step 12240 loss 5.12450 batch 35/1640 lr 0.001 accuracy 94.18555 wps 31444.54 step time 0.45s\n","Traceback (most recent call last):\n","  File \"elmo_run.py\", line 343, in <module>\n","    train_clf(flags)\n","  File \"elmo_run.py\", line 203, in train_clf\n","    run_info=add_summary and flags.debug\n","  File \"/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/mycode/elmo.py\", line 487, in train_clf_one_step\n","    feed_dict=feed_dict)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n","    run_metadata_ptr)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n","    run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n","    return fn(*args)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n","    options, feed_dict, fetch_list, target_list, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OZvzylmPc7PE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591250374253,"user_tz":-480,"elapsed":1677647,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"f328e74b-16e9-48b0-ae01-76889e794304"},"source":["# fix embedding + 原始数据\n","!python elmo_run.py \\\n","--mode=train \\\n","--data_files=data/train_not_aug.json \\\n","--eval_files=data/validation.json \\\n","--label_file=data/labels.txt \\\n","--vocab_file=data/vocab.txt \\\n","--embed_file=data/embedding.txt \\\n","--num_layers=3 \\\n","--batch_size=64 \\\n","--max_len=1400 \\\n","--rnn_cell_name='WEIGHT_LSTM' \\\n","--linear_dropout=0.5 \\\n","--fix_embedding=True \\\n","--num_classes_each_label=4 \\\n","--num_labels=20 \\\n","--steps_per_eval=1000 \\\n","--optimizer='rms' \\\n","--learning_rate=0.001 \\\n","--focal_loss_gamma=0 \\\n","--label_smoothing=0 \\\n","--checkpoint_dir=output/elmo_1200_more_drop_noSmooth_notAug_fix \\\n","--checkpoint_load_step=11000 \\\n","--previous_best_eval=-0.7024667962528659"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/train_not_aug.json ...\n","# Got 105000 data items with 1640 batches\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/validation.json ...\n","# Got 15000 data items with 117 batches\n","  saving config to output/elmo_1200_more_drop_noSmooth_notAug_fix/config\n","mode=train,data_files=['data/train_not_aug.json'],eval_files=['data/validation.json'],label_file=data/labels.txt,vocab_file=data/vocab.txt,embed_file=data/embedding.txt,out_file=None,split_word=True,reverse=False,weight_file=None,prob=False,max_len=1400,batch_size=64,num_layers=3,optimizer=rms,learning_rate=0.001,decay_schema=hand,decay_steps=10000,focal_loss_gamma=0.0,max_gradient_norm=2.0,l2_loss_ratio=0.0,label_smoothing=0.0,embedding_dropout=0.1,dropout_keep_prob=0.8,weight_keep_drop=0.8,linear_dropout=0.5,rnn_cell_name=WEIGHT_LSTM,embedding_size=300,num_units=300,num_classes_each_label=4,num_labels=20,fix_embedding=True,need_early_stop=True,patient=5,debug=False,num_train_epoch=50,steps_per_stats=20,steps_per_summary=50,steps_per_eval=1000,checkpoint_dir=output/elmo_1200_more_drop_noSmooth_notAug_fix,checkpoint_load_step=11000,previous_best_eval=-0.7024667962528659,vocab_size=50000\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/weight:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","variable <tf.Variable 'elmo_encoder/scalar:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/memory_layer/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/attention_op/dense/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/dense_1/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense_1/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/drop_connect_layer/kernel:0' shape=(300, 1200) dtype=float32_ref> with parameter number 360000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/luong_attention/attention_g:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/attention_layer/kernel:0' shape=(900, 300) dtype=float32_ref> with parameter number 270000\n","variable <tf.Variable 'classifier/predict_clf/dense/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/predict_clf/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/predict_clf/dense_1/kernel:0' shape=(300, 4) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/predict_clf/dense_1/bias:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","# total parameter number 8560510\n","loading config from output/elmo_1200_more_drop_noSmooth_notAug_fix/config\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","output/elmo_1200_more_drop_noSmooth_notAug_fix/model.ckpt-11000\n","\n","!!! Restored model\n","=>> Start to train with learning rate 0.001\n","=>> Global step 11000\n","=>> Epoch 1  global step 11020 loss 5.82905 batch 20/1640 lr 0.001 accuracy 93.21875 wps 25071.73 step time 0.78s\n","=>> Epoch 1  global step 11040 loss 5.14796 batch 40/1640 lr 0.001 accuracy 94.13281 wps 35315.75 step time 0.37s\n","=>> Epoch 1  global step 11060 loss 5.88309 batch 60/1640 lr 0.001 accuracy 93.34766 wps 37947.97 step time 0.49s\n","=>> Epoch 1  global step 11080 loss 5.41524 batch 80/1640 lr 0.001 accuracy 93.90234 wps 35978.18 step time 0.40s\n","=>> Epoch 1  global step 11100 loss 5.55759 batch 100/1640 lr 0.001 accuracy 93.79297 wps 34721.46 step time 0.47s\n","=>> Epoch 1  global step 11120 loss 5.27517 batch 120/1640 lr 0.001 accuracy 94.08789 wps 34859.21 step time 0.36s\n","=>> Epoch 1  global step 11140 loss 5.56671 batch 140/1640 lr 0.001 accuracy 93.69922 wps 35056.79 step time 0.50s\n","=>> Epoch 1  global step 11160 loss 5.17672 batch 160/1640 lr 0.001 accuracy 94.10547 wps 34038.37 step time 0.42s\n","=>> Epoch 1  global step 11180 loss 5.53714 batch 180/1640 lr 0.001 accuracy 93.80469 wps 36018.49 step time 0.40s\n","=>> Epoch 1  global step 11200 loss 5.53775 batch 200/1640 lr 0.001 accuracy 93.73828 wps 35764.63 step time 0.38s\n","=>> Epoch 1  global step 11220 loss 5.09310 batch 220/1640 lr 0.001 accuracy 94.28906 wps 33987.65 step time 0.34s\n","=>> Epoch 1  global step 11240 loss 5.29490 batch 240/1640 lr 0.001 accuracy 93.92383 wps 34213.76 step time 0.44s\n","=>> Epoch 1  global step 11260 loss 5.52022 batch 260/1640 lr 0.001 accuracy 93.49805 wps 36709.12 step time 0.43s\n","=>> Epoch 1  global step 11280 loss 5.74142 batch 280/1640 lr 0.001 accuracy 93.60156 wps 33530.87 step time 0.54s\n","=>> Epoch 1  global step 11300 loss 5.18944 batch 300/1640 lr 0.001 accuracy 94.14453 wps 33086.65 step time 0.42s\n","=>> Epoch 1  global step 11320 loss 5.41463 batch 320/1640 lr 0.001 accuracy 93.99023 wps 36177.79 step time 0.41s\n","=>> Epoch 1  global step 11340 loss 5.77040 batch 340/1640 lr 0.001 accuracy 93.43359 wps 33123.92 step time 0.55s\n","=>> Epoch 1  global step 11360 loss 5.28644 batch 360/1640 lr 0.001 accuracy 93.93359 wps 35769.33 step time 0.39s\n","=>> Epoch 1  global step 11380 loss 5.25618 batch 380/1640 lr 0.001 accuracy 94.01953 wps 36585.24 step time 0.41s\n","=>> Epoch 1  global step 11400 loss 5.38425 batch 400/1640 lr 0.001 accuracy 93.96875 wps 34946.67 step time 0.37s\n","=>> Epoch 1  global step 11420 loss 5.33221 batch 420/1640 lr 0.001 accuracy 93.93164 wps 33795.05 step time 0.44s\n","=>> Epoch 1  global step 11440 loss 5.48999 batch 440/1640 lr 0.001 accuracy 93.91992 wps 36642.18 step time 0.41s\n","=>> Epoch 1  global step 11460 loss 5.44747 batch 460/1640 lr 0.001 accuracy 93.87109 wps 35466.77 step time 0.39s\n","=>> Epoch 1  global step 11480 loss 5.23265 batch 480/1640 lr 0.001 accuracy 93.89844 wps 32350.48 step time 0.40s\n","=>> Epoch 1  global step 11500 loss 5.03407 batch 500/1640 lr 0.001 accuracy 94.35156 wps 30700.02 step time 0.37s\n","=>> Epoch 1  global step 11520 loss 5.24996 batch 520/1640 lr 0.001 accuracy 94.08789 wps 28007.80 step time 0.49s\n","=>> Epoch 1  global step 11540 loss 5.48085 batch 540/1640 lr 0.001 accuracy 93.88867 wps 28707.76 step time 0.51s\n","=>> Epoch 1  global step 11560 loss 5.62375 batch 560/1640 lr 0.001 accuracy 93.55859 wps 27144.56 step time 0.64s\n","=>> Epoch 1  global step 11580 loss 5.52824 batch 580/1640 lr 0.001 accuracy 93.79883 wps 28419.29 step time 0.56s\n","=>> Epoch 1  global step 11600 loss 5.31768 batch 600/1640 lr 0.001 accuracy 93.79883 wps 28800.48 step time 0.53s\n","=>> Epoch 1  global step 11620 loss 5.32915 batch 620/1640 lr 0.001 accuracy 93.92188 wps 30771.58 step time 0.46s\n","=>> Epoch 1  global step 11640 loss 5.36909 batch 640/1640 lr 0.001 accuracy 94.00000 wps 30806.52 step time 0.42s\n","=>> Epoch 1  global step 11660 loss 5.67047 batch 660/1640 lr 0.001 accuracy 93.42969 wps 31796.99 step time 0.51s\n","=>> Epoch 1  global step 11680 loss 5.59657 batch 680/1640 lr 0.001 accuracy 93.56641 wps 30262.25 step time 0.48s\n","=>> Epoch 1  global step 11700 loss 5.33122 batch 700/1640 lr 0.001 accuracy 93.99805 wps 33353.08 step time 0.40s\n","=>> Epoch 1  global step 11720 loss 5.51840 batch 720/1640 lr 0.001 accuracy 93.84570 wps 31836.33 step time 0.47s\n","=>> Epoch 1  global step 11740 loss 5.74289 batch 740/1640 lr 0.001 accuracy 93.41016 wps 31297.89 step time 0.50s\n","=>> Epoch 1  global step 11760 loss 5.14990 batch 760/1640 lr 0.001 accuracy 94.10352 wps 31650.82 step time 0.39s\n","=>> Epoch 1  global step 11780 loss 5.18685 batch 780/1640 lr 0.001 accuracy 94.23242 wps 31582.98 step time 0.36s\n","=>> Epoch 1  global step 11800 loss 5.38989 batch 800/1640 lr 0.001 accuracy 93.86914 wps 31348.35 step time 0.44s\n","=>> Epoch 1  global step 11820 loss 5.52968 batch 820/1640 lr 0.001 accuracy 93.76172 wps 30528.16 step time 0.46s\n","=>> Epoch 1  global step 11840 loss 5.68563 batch 840/1640 lr 0.001 accuracy 93.55078 wps 29948.09 step time 0.49s\n","=>> Epoch 1  global step 11860 loss 5.02090 batch 860/1640 lr 0.001 accuracy 94.33984 wps 30713.79 step time 0.39s\n","=>> Epoch 1  global step 11880 loss 5.51786 batch 880/1640 lr 0.001 accuracy 93.68164 wps 30957.12 step time 0.44s\n","=>> Epoch 1  global step 11900 loss 5.44986 batch 900/1640 lr 0.001 accuracy 93.88867 wps 31476.00 step time 0.42s\n","=>> Epoch 1  global step 11920 loss 5.32758 batch 920/1640 lr 0.001 accuracy 93.91211 wps 30934.80 step time 0.42s\n","=>> Epoch 1  global step 11940 loss 5.39165 batch 940/1640 lr 0.001 accuracy 93.90234 wps 31942.16 step time 0.45s\n","=>> Epoch 1  global step 11960 loss 5.57767 batch 960/1640 lr 0.001 accuracy 93.73242 wps 32095.04 step time 0.47s\n","=>> Epoch 1  global step 11980 loss 5.24602 batch 980/1640 lr 0.001 accuracy 94.09570 wps 31802.80 step time 0.41s\n","=>> Epoch 1  global step 12000 loss 5.21162 batch 1000/1640 lr 0.001 accuracy 94.12109 wps 30916.23 step time 0.42s\n","=>> global step 12000, eval result: \n","=>> location_traffic_convenience - 0.6611033598683482\n","=>> location_distance_from_business_district - 0.5245618584768063\n","=>> location_easy_to_find - 0.7108940248985371\n","=>> service_wait_time - 0.6653555916462117\n","=>> service_waiters_attitude - 0.7956335813164748\n","=>> service_parking_convenience - 0.741924485076446\n","=>> service_serving_speed - 0.7584808774194389\n","=>> price_level - 0.7821810704724885\n","=>> price_cost_effective - 0.7183904403759153\n","=>> price_discount - 0.6637997639492842\n","=>> environment_decoration - 0.7198911920777081\n","=>> environment_noise - 0.7630818306965488\n","=>> environment_space - 0.7653642179931774\n","=>> environment_cleaness - 0.7504079140287743\n","=>> dish_portion - 0.7212308710925954\n","=>> dish_taste - 0.7246157286681214\n","=>> dish_look - 0.5739161633860168\n","=>> dish_recommendation - 0.7314303229389988\n","=>> others_overall_experience - 0.5893639985565583\n","=>> others_willing_to_consume_again - 0.6969445834043673\n","=>> Eval loss 34.48093, f1 0.70293\n","=>> current result -0.7029285938171409, previous best result -0.7024667962528659\n","=>> Epoch 1  global step 12020 loss 5.75647 batch 1020/1640 lr 0.001 accuracy 93.39062 wps 27557.92 step time 0.66s\n","=>> Epoch 1  global step 12040 loss 5.39246 batch 1040/1640 lr 0.001 accuracy 93.91992 wps 30976.73 step time 0.43s\n","=>> Epoch 1  global step 12060 loss 5.43843 batch 1060/1640 lr 0.001 accuracy 93.87695 wps 30123.80 step time 0.42s\n","=>> Epoch 1  global step 12080 loss 5.54795 batch 1080/1640 lr 0.001 accuracy 93.67383 wps 31583.35 step time 0.46s\n","=>> Epoch 1  global step 12100 loss 5.23186 batch 1100/1640 lr 0.001 accuracy 94.12109 wps 31446.82 step time 0.40s\n","=>> Epoch 1  global step 12120 loss 5.71237 batch 1120/1640 lr 0.001 accuracy 93.58398 wps 29543.24 step time 0.52s\n","=>> Epoch 1  global step 12140 loss 5.64449 batch 1140/1640 lr 0.001 accuracy 93.60547 wps 31426.29 step time 0.48s\n","=>> Epoch 1  global step 12160 loss 5.49857 batch 1160/1640 lr 0.001 accuracy 93.81055 wps 31569.13 step time 0.47s\n","=>> Epoch 1  global step 12180 loss 5.26782 batch 1180/1640 lr 0.001 accuracy 93.90625 wps 32301.22 step time 0.40s\n","=>> Epoch 1  global step 12200 loss 5.37509 batch 1200/1640 lr 0.001 accuracy 94.00586 wps 31676.96 step time 0.45s\n","=>> Epoch 1  global step 12220 loss 5.25322 batch 1220/1640 lr 0.001 accuracy 94.06445 wps 29222.45 step time 0.49s\n","=>> Epoch 1  global step 12240 loss 5.57099 batch 1240/1640 lr 0.001 accuracy 93.75000 wps 28986.05 step time 0.57s\n","=>> Epoch 1  global step 12260 loss 5.35891 batch 1260/1640 lr 0.001 accuracy 93.90820 wps 31772.75 step time 0.39s\n","=>> Epoch 1  global step 12280 loss 5.54335 batch 1280/1640 lr 0.001 accuracy 93.84961 wps 28949.93 step time 0.52s\n","=>> Epoch 1  global step 12300 loss 5.25580 batch 1300/1640 lr 0.001 accuracy 94.10742 wps 30884.30 step time 0.40s\n","=>> Epoch 1  global step 12320 loss 5.57762 batch 1320/1640 lr 0.001 accuracy 93.69922 wps 29976.51 step time 0.50s\n","=>> Epoch 1  global step 12340 loss 5.43854 batch 1340/1640 lr 0.001 accuracy 93.84180 wps 30265.42 step time 0.48s\n","=>> Epoch 1  global step 12360 loss 5.42484 batch 1360/1640 lr 0.001 accuracy 93.74219 wps 31925.48 step time 0.44s\n","=>> Epoch 1  global step 12380 loss 5.68059 batch 1380/1640 lr 0.001 accuracy 93.52539 wps 30546.76 step time 0.44s\n","=>> Epoch 1  global step 12400 loss 5.45832 batch 1400/1640 lr 0.001 accuracy 93.82422 wps 32570.35 step time 0.41s\n","=>> Epoch 1  global step 12420 loss 5.56850 batch 1420/1640 lr 0.001 accuracy 93.68750 wps 31240.85 step time 0.49s\n","=>> Epoch 1  global step 12440 loss 5.42981 batch 1440/1640 lr 0.001 accuracy 93.87891 wps 28643.82 step time 0.56s\n","=>> Epoch 1  global step 12460 loss 5.69587 batch 1460/1640 lr 0.001 accuracy 93.49609 wps 26757.86 step time 0.64s\n","=>> Epoch 1  global step 12480 loss 5.77940 batch 1480/1640 lr 0.001 accuracy 93.34375 wps 27691.33 step time 0.68s\n","=>> Epoch 1  global step 12500 loss 5.70208 batch 1500/1640 lr 0.001 accuracy 93.53320 wps 31542.33 step time 0.48s\n","=>> Epoch 1  global step 12520 loss 5.87070 batch 1520/1640 lr 0.001 accuracy 93.26367 wps 27222.74 step time 0.65s\n","=>> Epoch 1  global step 12540 loss 5.59422 batch 1540/1640 lr 0.001 accuracy 93.63867 wps 30504.29 step time 0.54s\n","=>> Epoch 1  global step 12560 loss 5.43686 batch 1560/1640 lr 0.001 accuracy 93.84570 wps 28878.33 step time 0.48s\n","=>> Epoch 1  global step 12580 loss 5.69639 batch 1580/1640 lr 0.001 accuracy 93.55664 wps 28627.75 step time 0.59s\n","=>> Epoch 1  global step 12600 loss 5.54934 batch 1600/1640 lr 0.001 accuracy 93.71289 wps 30628.02 step time 0.46s\n","=>> Epoch 1  global step 12620 loss 5.48597 batch 1620/1640 lr 0.001 accuracy 93.79883 wps 32901.66 step time 0.44s\n","=>> Epoch 1  global step 12640 loss 5.83351 batch 1640/1640 lr 0.001 accuracy 93.46484 wps 29051.24 step time 0.57s\n","=>> Finsh epoch 1, global step 12641\n","=>> Epoch 2  global step 12660 loss 5.15935 batch 19/1640 lr 0.001 accuracy 89.16211 wps 34303.84 step time 0.44s\n","=>> Epoch 2  global step 12680 loss 5.30445 batch 39/1640 lr 0.001 accuracy 93.97461 wps 34302.81 step time 0.44s\n","=>> Epoch 2  global step 12700 loss 5.15907 batch 59/1640 lr 0.001 accuracy 94.09961 wps 36333.62 step time 0.41s\n","=>> Epoch 2  global step 12720 loss 5.47760 batch 79/1640 lr 0.001 accuracy 93.76172 wps 35305.48 step time 0.46s\n","=>> Epoch 2  global step 12740 loss 4.99356 batch 99/1640 lr 0.001 accuracy 94.31250 wps 33792.80 step time 0.32s\n","=>> Epoch 2  global step 12760 loss 4.95427 batch 119/1640 lr 0.001 accuracy 94.31055 wps 33842.90 step time 0.32s\n","=>> Epoch 2  global step 12780 loss 5.35606 batch 139/1640 lr 0.001 accuracy 94.01562 wps 33675.81 step time 0.44s\n","=>> Epoch 2  global step 12800 loss 5.22266 batch 159/1640 lr 0.001 accuracy 94.10938 wps 35384.67 step time 0.36s\n","=>> Epoch 2  global step 12820 loss 5.15169 batch 179/1640 lr 0.001 accuracy 94.12695 wps 33971.10 step time 0.43s\n","=>> Epoch 2  global step 12840 loss 5.38083 batch 199/1640 lr 0.001 accuracy 93.91992 wps 33760.10 step time 0.43s\n","=>> Epoch 2  global step 12860 loss 5.36113 batch 219/1640 lr 0.001 accuracy 93.85156 wps 35214.27 step time 0.50s\n","=>> Epoch 2  global step 12880 loss 5.06176 batch 239/1640 lr 0.001 accuracy 94.31445 wps 34216.93 step time 0.44s\n","=>> Epoch 2  global step 12900 loss 5.38139 batch 259/1640 lr 0.001 accuracy 93.84180 wps 34524.19 step time 0.46s\n","=>> Epoch 2  global step 12920 loss 5.29078 batch 279/1640 lr 0.001 accuracy 94.03125 wps 33113.08 step time 0.51s\n","=>> Epoch 2  global step 12940 loss 5.26733 batch 299/1640 lr 0.001 accuracy 94.05078 wps 34086.51 step time 0.45s\n","=>> Epoch 2  global step 12960 loss 5.60147 batch 319/1640 lr 0.001 accuracy 93.68945 wps 35767.03 step time 0.48s\n","=>> Epoch 2  global step 12980 loss 5.69138 batch 339/1640 lr 0.001 accuracy 93.57617 wps 36995.02 step time 0.44s\n","=>> Epoch 2  global step 13000 loss 5.28354 batch 359/1640 lr 0.001 accuracy 94.04883 wps 34775.22 step time 0.45s\n","=>> global step 13000, eval result: \n","=>> location_traffic_convenience - 0.6562557666203592\n","=>> location_distance_from_business_district - 0.5143446191720613\n","=>> location_easy_to_find - 0.7020532437994732\n","=>> service_wait_time - 0.6617413611092338\n","=>> service_waiters_attitude - 0.7942057956885684\n","=>> service_parking_convenience - 0.7425606104091367\n","=>> service_serving_speed - 0.7575511339395768\n","=>> price_level - 0.7849984455884245\n","=>> price_cost_effective - 0.7179275237176556\n","=>> price_discount - 0.6679856625826415\n","=>> environment_decoration - 0.7275592955217424\n","=>> environment_noise - 0.7597177979155656\n","=>> environment_space - 0.7653943863892871\n","=>> environment_cleaness - 0.7485050453389587\n","=>> dish_portion - 0.7228294892673872\n","=>> dish_taste - 0.7277113723090782\n","=>> dish_look - 0.5773912977362095\n","=>> dish_recommendation - 0.7254688650311547\n","=>> others_overall_experience - 0.5893056336906857\n","=>> others_willing_to_consume_again - 0.6978358843132633\n","=>> Eval loss 34.57691, f1 0.70207\n","=>> current result -0.7020671615070233, previous best result -0.7029285938171409\n","=>> Epoch 2  global step 13020 loss 4.98664 batch 379/1640 lr 0.001 accuracy 94.46484 wps 34948.22 step time 0.35s\n","=>> Epoch 2  global step 13040 loss 5.39691 batch 399/1640 lr 0.001 accuracy 93.84570 wps 35053.11 step time 0.48s\n","=>> Epoch 2  global step 13060 loss 5.36102 batch 419/1640 lr 0.001 accuracy 93.88477 wps 36299.77 step time 0.41s\n","=>> Epoch 2  global step 13080 loss 5.19444 batch 439/1640 lr 0.001 accuracy 94.08398 wps 35362.45 step time 0.37s\n","=>> Epoch 2  global step 13100 loss 5.27542 batch 459/1640 lr 0.001 accuracy 94.01367 wps 36152.55 step time 0.39s\n","=>> Epoch 2  global step 13120 loss 5.50361 batch 479/1640 lr 0.001 accuracy 93.72266 wps 36306.56 step time 0.42s\n","=>> Epoch 2  global step 13140 loss 5.54467 batch 499/1640 lr 0.001 accuracy 93.67969 wps 35288.62 step time 0.50s\n","=>> Epoch 2  global step 13160 loss 5.17938 batch 519/1640 lr 0.001 accuracy 94.20898 wps 34382.93 step time 0.34s\n","=>> Epoch 2  global step 13180 loss 5.15391 batch 539/1640 lr 0.001 accuracy 94.18555 wps 35431.00 step time 0.36s\n","=>> Epoch 2  global step 13200 loss 5.25866 batch 559/1640 lr 0.001 accuracy 93.90234 wps 36237.84 step time 0.39s\n","=>> Epoch 2  global step 13220 loss 5.64861 batch 579/1640 lr 0.001 accuracy 93.58008 wps 33451.97 step time 0.52s\n","=>> Epoch 2  global step 13240 loss 5.45138 batch 599/1640 lr 0.001 accuracy 93.84180 wps 36622.93 step time 0.40s\n","=>> Epoch 2  global step 13260 loss 5.26467 batch 619/1640 lr 0.001 accuracy 94.07812 wps 35158.40 step time 0.36s\n","=>> Epoch 2  global step 13280 loss 5.19375 batch 639/1640 lr 0.001 accuracy 94.21484 wps 34650.76 step time 0.44s\n","=>> Epoch 2  global step 13300 loss 5.25121 batch 659/1640 lr 0.001 accuracy 93.96094 wps 35767.69 step time 0.38s\n","=>> Epoch 2  global step 13320 loss 5.19997 batch 679/1640 lr 0.001 accuracy 94.15820 wps 35878.93 step time 0.38s\n","=>> Epoch 2  global step 13340 loss 5.11829 batch 699/1640 lr 0.001 accuracy 94.24219 wps 35478.49 step time 0.36s\n","=>> Epoch 2  global step 13360 loss 5.50536 batch 719/1640 lr 0.001 accuracy 93.81250 wps 36919.03 step time 0.41s\n","=>> Epoch 2  global step 13380 loss 5.28216 batch 739/1640 lr 0.001 accuracy 94.01562 wps 35534.93 step time 0.36s\n","=>> Epoch 2  global step 13400 loss 5.41395 batch 759/1640 lr 0.001 accuracy 93.90039 wps 36322.10 step time 0.39s\n","=>> Epoch 2  global step 13420 loss 5.79056 batch 779/1640 lr 0.001 accuracy 93.31445 wps 33752.43 step time 0.55s\n","=>> Epoch 2  global step 13440 loss 5.36015 batch 799/1640 lr 0.001 accuracy 93.91406 wps 36668.50 step time 0.41s\n","=>> Epoch 2  global step 13460 loss 5.32212 batch 819/1640 lr 0.001 accuracy 94.08789 wps 35311.98 step time 0.36s\n","=>> Epoch 2  global step 13480 loss 5.91481 batch 839/1640 lr 0.001 accuracy 93.17578 wps 35019.67 step time 0.61s\n","=>> Epoch 2  global step 13500 loss 5.02849 batch 859/1640 lr 0.001 accuracy 94.32422 wps 35515.53 step time 0.36s\n","=>> Epoch 2  global step 13520 loss 5.49585 batch 879/1640 lr 0.001 accuracy 93.75977 wps 37071.84 step time 0.42s\n","=>> Epoch 2  global step 13540 loss 5.24106 batch 899/1640 lr 0.001 accuracy 94.02539 wps 35568.94 step time 0.37s\n","=>> Epoch 2  global step 13560 loss 5.30722 batch 919/1640 lr 0.001 accuracy 94.05469 wps 35656.15 step time 0.38s\n","=>> Epoch 2  global step 13580 loss 5.43187 batch 939/1640 lr 0.001 accuracy 93.85937 wps 35369.19 step time 0.38s\n","=>> Epoch 2  global step 13600 loss 5.21339 batch 959/1640 lr 0.001 accuracy 94.08203 wps 30148.06 step time 0.48s\n","=>> Epoch 2  global step 13620 loss 5.38147 batch 979/1640 lr 0.001 accuracy 93.92578 wps 30621.82 step time 0.44s\n","=>> Epoch 2  global step 13640 loss 5.55887 batch 999/1640 lr 0.001 accuracy 93.68359 wps 31408.97 step time 0.51s\n","=>> Epoch 2  global step 13660 loss 5.07831 batch 1019/1640 lr 0.001 accuracy 94.12500 wps 31975.38 step time 0.37s\n","=>> Epoch 2  global step 13680 loss 5.60261 batch 1039/1640 lr 0.001 accuracy 93.80273 wps 31665.20 step time 0.46s\n","=>> Epoch 2  global step 13700 loss 5.10652 batch 1059/1640 lr 0.001 accuracy 94.35937 wps 31570.46 step time 0.37s\n","=>> Epoch 2  global step 13720 loss 5.42846 batch 1079/1640 lr 0.001 accuracy 93.83984 wps 29782.47 step time 0.50s\n","=>> Epoch 2  global step 13740 loss 5.44352 batch 1099/1640 lr 0.001 accuracy 93.88086 wps 31910.20 step time 0.45s\n","=>> Epoch 2  global step 13760 loss 5.30542 batch 1119/1640 lr 0.001 accuracy 93.95508 wps 28996.15 step time 0.49s\n","=>> Epoch 2  global step 13780 loss 5.28066 batch 1139/1640 lr 0.001 accuracy 93.91211 wps 31534.36 step time 0.41s\n","=>> Epoch 2  global step 13800 loss 5.18961 batch 1159/1640 lr 0.001 accuracy 94.00391 wps 31615.54 step time 0.41s\n","=>> Epoch 2  global step 13820 loss 5.26708 batch 1179/1640 lr 0.001 accuracy 94.11719 wps 30241.71 step time 0.46s\n","=>> Epoch 2  global step 13840 loss 5.03001 batch 1199/1640 lr 0.001 accuracy 94.36914 wps 31360.36 step time 0.38s\n","=>> Epoch 2  global step 13860 loss 5.29166 batch 1219/1640 lr 0.001 accuracy 94.01953 wps 31224.85 step time 0.46s\n","=>> Epoch 2  global step 13880 loss 5.44013 batch 1239/1640 lr 0.001 accuracy 93.71875 wps 31936.45 step time 0.49s\n","=>> Epoch 2  global step 13900 loss 5.41559 batch 1259/1640 lr 0.001 accuracy 93.82812 wps 31624.17 step time 0.54s\n","=>> Epoch 2  global step 13920 loss 5.49282 batch 1279/1640 lr 0.001 accuracy 93.74023 wps 31969.57 step time 0.47s\n","=>> Epoch 2  global step 13940 loss 5.49647 batch 1299/1640 lr 0.001 accuracy 93.62695 wps 28675.25 step time 0.54s\n","=>> Epoch 2  global step 13960 loss 5.27825 batch 1319/1640 lr 0.001 accuracy 94.02734 wps 31375.30 step time 0.41s\n","=>> Epoch 2  global step 13980 loss 5.29898 batch 1339/1640 lr 0.001 accuracy 93.95898 wps 30927.93 step time 0.44s\n","=>> Epoch 2  global step 14000 loss 5.45298 batch 1359/1640 lr 0.001 accuracy 93.80859 wps 31968.69 step time 0.49s\n","=>> global step 14000, eval result: \n","=>> location_traffic_convenience - 0.6422280147242315\n","=>> location_distance_from_business_district - 0.5100090499116513\n","=>> location_easy_to_find - 0.7056474515127761\n","=>> service_wait_time - 0.6604341972462954\n","=>> service_waiters_attitude - 0.7939662623133774\n","=>> service_parking_convenience - 0.7388437091156299\n","=>> service_serving_speed - 0.7589223441973096\n","=>> price_level - 0.7825771040644891\n","=>> price_cost_effective - 0.7186844203829944\n","=>> price_discount - 0.6670150924948404\n","=>> environment_decoration - 0.7211544789897302\n","=>> environment_noise - 0.7625085978663553\n","=>> environment_space - 0.7632307524124478\n","=>> environment_cleaness - 0.7491900063188869\n","=>> dish_portion - 0.7245076959355834\n","=>> dish_taste - 0.727808521977707\n","=>> dish_look - 0.5752018843754837\n","=>> dish_recommendation - 0.7274845048728117\n","=>> others_overall_experience - 0.586527350095244\n","=>> others_willing_to_consume_again - 0.6971057423090941\n","=>> Eval loss 34.63932, f1 0.70065\n","=>> current result -0.700652359055847, previous best result -0.7029285938171409\n","=>> Epoch 2  global step 14020 loss 5.17470 batch 1379/1640 lr 0.001 accuracy 94.16992 wps 31437.08 step time 0.40s\n","=>> Epoch 2  global step 14040 loss 5.40199 batch 1399/1640 lr 0.001 accuracy 93.92578 wps 29031.50 step time 0.51s\n","Traceback (most recent call last):\n","  File \"elmo_run.py\", line 349, in <module>\n","    train_clf(flags)\n","  File \"elmo_run.py\", line 204, in train_clf\n","    run_info=add_summary and flags.debug\n","  File \"/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/mycode/elmo.py\", line 487, in train_clf_one_step\n","    feed_dict=feed_dict)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n","    run_metadata_ptr)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n","    run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n","    return fn(*args)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n","    options, feed_dict, fetch_list, target_list, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CMRkF-BiJO6L","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEO9mWXeJOva","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62dB6brGJOf4","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1GK7ZuDpyVi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591260806644,"user_tz":-480,"elapsed":8127414,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"dd8c5eed-46df-4b97-914f-3875f5dfe95b"},"source":["# softmax global loss\n","!python elmo_run.py \\\n","--mode=train \\\n","--data_files=data/train_not_aug.json \\\n","--eval_files=data/validation.json \\\n","--label_file=data/labels.txt \\\n","--vocab_file=data/vocab.txt \\\n","--embed_file=data/embedding.txt \\\n","--num_layers=3 \\\n","--batch_size=64 \\\n","--max_len=1200 \\\n","--rnn_cell_name='WEIGHT_LSTM' \\\n","--linear_dropout=0.5 \\\n","--num_classes_each_label=4 \\\n","--num_labels=20 \\\n","--steps_per_eval=1000 \\\n","--optimizer='rms' \\\n","--learning_rate=0.001 \\\n","--focal_loss_gamma=0 \\\n","--label_smoothing=0 \\\n","--checkpoint_dir=output/elmo_origin_loss"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/train_not_aug.json ...\n","# Got 105000 data items with 1640 batches\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/validation.json ...\n","# Got 15000 data items with 117 batches\n","  saving config to output/elmo_origin_loss/config\n","mode=train,data_files=['data/train_not_aug.json'],eval_files=['data/validation.json'],label_file=data/labels.txt,vocab_file=data/vocab.txt,embed_file=data/embedding.txt,out_file=None,split_word=True,reverse=False,weight_file=None,prob=False,max_len=1200,batch_size=64,num_layers=3,optimizer=rms,learning_rate=0.001,decay_schema=hand,decay_steps=10000,loss_name=softmax,focal_loss_gamma=0.0,max_gradient_norm=2.0,l2_loss_ratio=0.0,label_smoothing=0.0,embedding_dropout=0.1,dropout_keep_prob=0.8,weight_keep_drop=0.8,linear_dropout=0.5,rnn_cell_name=WEIGHT_LSTM,embedding_size=300,num_units=300,num_classes_each_label=4,num_labels=20,fix_embedding=False,need_early_stop=True,patient=5,debug=False,num_train_epoch=50,steps_per_stats=20,steps_per_summary=50,steps_per_eval=1000,checkpoint_dir=output/elmo_origin_loss,checkpoint_load_step=None,previous_best_eval=10000.0,vocab_size=50000\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","variable <tf.Variable 'embedding/embedding:0' shape=(50000, 300) dtype=float32_ref> with parameter number 15000000\n","variable <tf.Variable 'embedding/label_embedding:0' shape=(20, 300) dtype=float32_ref> with parameter number 6000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/weight:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","variable <tf.Variable 'elmo_encoder/scalar:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/memory_layer/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/attention_op/dense/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/dense_1/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense_1/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/drop_connect_layer/kernel:0' shape=(300, 1200) dtype=float32_ref> with parameter number 360000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/luong_attention/attention_g:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/attention_layer/kernel:0' shape=(900, 300) dtype=float32_ref> with parameter number 270000\n","variable <tf.Variable 'classifier/predict_clf/dense/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/predict_clf/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/predict_clf/dense_1/kernel:0' shape=(300, 4) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/predict_clf/dense_1/bias:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","# total parameter number 23566510\n","loading config from output/elmo_origin_loss/config\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","\n","!!! Restored model\n","=>> Start to train with learning rate 0.001\n","=>> Global step 2000\n","=>> Epoch 1  global step 2020 loss 0.47350 batch 20/1640 lr 0.001 accuracy 91.38867 wps 21227.64 step time 0.69s\n","=>> Epoch 1  global step 2040 loss 0.49292 batch 40/1640 lr 0.001 accuracy 91.13672 wps 33657.24 step time 0.53s\n","=>> Epoch 1  global step 2060 loss 0.46232 batch 60/1640 lr 0.001 accuracy 91.74805 wps 31189.29 step time 0.44s\n","=>> Epoch 1  global step 2080 loss 0.47426 batch 80/1640 lr 0.001 accuracy 91.40039 wps 32502.73 step time 0.48s\n","=>> Epoch 1  global step 2100 loss 0.45245 batch 100/1640 lr 0.001 accuracy 91.76953 wps 30899.35 step time 0.43s\n","=>> Epoch 1  global step 2120 loss 0.47848 batch 120/1640 lr 0.001 accuracy 91.17383 wps 30372.40 step time 0.57s\n","=>> Epoch 1  global step 2140 loss 0.45674 batch 140/1640 lr 0.001 accuracy 91.80469 wps 30949.12 step time 0.43s\n","=>> Epoch 1  global step 2160 loss 0.45449 batch 160/1640 lr 0.001 accuracy 91.83594 wps 31527.65 step time 0.45s\n","=>> Epoch 1  global step 2180 loss 0.45846 batch 180/1640 lr 0.001 accuracy 91.70703 wps 30326.85 step time 0.49s\n","=>> Epoch 1  global step 2200 loss 0.45854 batch 200/1640 lr 0.001 accuracy 91.77734 wps 31107.97 step time 0.43s\n","=>> Epoch 1  global step 2220 loss 0.47009 batch 220/1640 lr 0.001 accuracy 91.53320 wps 31071.71 step time 0.55s\n","=>> Epoch 1  global step 2240 loss 0.44702 batch 240/1640 lr 0.001 accuracy 91.96680 wps 31549.08 step time 0.45s\n","=>> Epoch 1  global step 2260 loss 0.46207 batch 260/1640 lr 0.001 accuracy 91.69922 wps 30941.92 step time 0.49s\n","=>> Epoch 1  global step 2280 loss 0.43566 batch 280/1640 lr 0.001 accuracy 92.12109 wps 29311.26 step time 0.51s\n","=>> Epoch 1  global step 2300 loss 0.46931 batch 300/1640 lr 0.001 accuracy 91.50781 wps 32777.09 step time 0.48s\n","=>> Epoch 1  global step 2320 loss 0.46767 batch 320/1640 lr 0.001 accuracy 91.46875 wps 31213.83 step time 0.50s\n","=>> Epoch 1  global step 2340 loss 0.47220 batch 340/1640 lr 0.001 accuracy 91.34766 wps 33309.50 step time 0.51s\n","=>> Epoch 1  global step 2360 loss 0.43232 batch 360/1640 lr 0.001 accuracy 92.22461 wps 31804.36 step time 0.45s\n","=>> Epoch 1  global step 2380 loss 0.45883 batch 380/1640 lr 0.001 accuracy 91.67773 wps 31148.12 step time 0.50s\n","=>> Epoch 1  global step 2400 loss 0.44040 batch 400/1640 lr 0.001 accuracy 92.03125 wps 31159.65 step time 0.44s\n","=>> Epoch 1  global step 2420 loss 0.43579 batch 420/1640 lr 0.001 accuracy 92.33984 wps 30488.61 step time 0.41s\n","=>> Epoch 1  global step 2440 loss 0.44314 batch 440/1640 lr 0.001 accuracy 92.06250 wps 32090.71 step time 0.48s\n","=>> Epoch 1  global step 2460 loss 0.46556 batch 460/1640 lr 0.001 accuracy 91.63867 wps 31335.15 step time 0.52s\n","=>> Epoch 1  global step 2480 loss 0.43966 batch 480/1640 lr 0.001 accuracy 92.13867 wps 30712.64 step time 0.42s\n","=>> Epoch 1  global step 2500 loss 0.43688 batch 500/1640 lr 0.001 accuracy 92.14648 wps 30900.30 step time 0.51s\n","=>> Epoch 1  global step 2520 loss 0.45318 batch 520/1640 lr 0.001 accuracy 91.73633 wps 32413.30 step time 0.47s\n","=>> Epoch 1  global step 2540 loss 0.42254 batch 540/1640 lr 0.001 accuracy 92.39453 wps 30815.93 step time 0.43s\n","=>> Epoch 1  global step 2560 loss 0.44267 batch 560/1640 lr 0.001 accuracy 92.03320 wps 31375.08 step time 0.52s\n","=>> Epoch 1  global step 2580 loss 0.43969 batch 580/1640 lr 0.001 accuracy 92.05273 wps 32210.16 step time 0.46s\n","=>> Epoch 1  global step 2600 loss 0.45504 batch 600/1640 lr 0.001 accuracy 91.69727 wps 32261.06 step time 0.55s\n","=>> Epoch 1  global step 2620 loss 0.42336 batch 620/1640 lr 0.001 accuracy 92.50586 wps 30721.11 step time 0.43s\n","=>> Epoch 1  global step 2640 loss 0.41100 batch 640/1640 lr 0.001 accuracy 92.68359 wps 30385.34 step time 0.41s\n","=>> Epoch 1  global step 2660 loss 0.44438 batch 660/1640 lr 0.001 accuracy 91.97461 wps 30914.94 step time 0.50s\n","=>> Epoch 1  global step 2680 loss 0.43761 batch 680/1640 lr 0.001 accuracy 92.07031 wps 31954.77 step time 0.46s\n","=>> Epoch 1  global step 2700 loss 0.44460 batch 700/1640 lr 0.001 accuracy 92.03516 wps 31976.91 step time 0.46s\n","=>> Epoch 1  global step 2720 loss 0.43822 batch 720/1640 lr 0.001 accuracy 92.11328 wps 31843.50 step time 0.45s\n","=>> Epoch 1  global step 2740 loss 0.42789 batch 740/1640 lr 0.001 accuracy 92.27734 wps 30951.17 step time 0.50s\n","=>> Epoch 1  global step 2760 loss 0.43123 batch 760/1640 lr 0.001 accuracy 92.23047 wps 30860.17 step time 0.50s\n","=>> Epoch 1  global step 2780 loss 0.43534 batch 780/1640 lr 0.001 accuracy 92.21680 wps 32197.58 step time 0.46s\n","=>> Epoch 1  global step 2800 loss 0.43419 batch 800/1640 lr 0.001 accuracy 92.18555 wps 31371.46 step time 0.44s\n","=>> Epoch 1  global step 2820 loss 0.42898 batch 820/1640 lr 0.001 accuracy 92.20117 wps 31453.81 step time 0.45s\n","=>> Epoch 1  global step 2840 loss 0.44288 batch 840/1640 lr 0.001 accuracy 91.93164 wps 31658.94 step time 0.46s\n","=>> Epoch 1  global step 2860 loss 0.43354 batch 860/1640 lr 0.001 accuracy 92.00781 wps 31088.19 step time 0.58s\n","=>> Epoch 1  global step 2880 loss 0.44001 batch 880/1640 lr 0.001 accuracy 92.04687 wps 31513.07 step time 0.52s\n","=>> Epoch 1  global step 2900 loss 0.41590 batch 900/1640 lr 0.001 accuracy 92.36719 wps 31561.73 step time 0.44s\n","=>> Epoch 1  global step 2920 loss 0.42550 batch 920/1640 lr 0.001 accuracy 92.28906 wps 32283.73 step time 0.46s\n","=>> Epoch 1  global step 2940 loss 0.42557 batch 940/1640 lr 0.001 accuracy 92.35156 wps 30901.14 step time 0.42s\n","=>> Epoch 1  global step 2960 loss 0.41713 batch 960/1640 lr 0.001 accuracy 92.58789 wps 29799.34 step time 0.40s\n","=>> Epoch 1  global step 2980 loss 0.42801 batch 980/1640 lr 0.001 accuracy 92.19727 wps 32005.39 step time 0.46s\n","=>> Epoch 1  global step 3000 loss 0.43691 batch 1000/1640 lr 0.001 accuracy 92.10742 wps 31183.54 step time 0.43s\n","=>> global step 3000, eval result: \n","=>> location_traffic_convenience - 0.45772527509756444\n","=>> location_distance_from_business_district - 0.38578787038561324\n","=>> location_easy_to_find - 0.5436707200253299\n","=>> service_wait_time - 0.2343910472075645\n","=>> service_waiters_attitude - 0.6890710559098525\n","=>> service_parking_convenience - 0.25405826138207643\n","=>> service_serving_speed - 0.27561073599438785\n","=>> price_level - 0.44874229158228074\n","=>> price_cost_effective - 0.40733806539392825\n","=>> price_discount - 0.43496881628553985\n","=>> environment_decoration - 0.4970230654722573\n","=>> environment_noise - 0.458412179196799\n","=>> environment_space - 0.4305950663039815\n","=>> environment_cleaness - 0.5421202082778036\n","=>> dish_portion - 0.3273661670621882\n","=>> dish_taste - 0.5977601761431458\n","=>> dish_look - 0.2088283251805264\n","=>> dish_recommendation - 0.22307351475095077\n","=>> others_overall_experience - 0.5469186415292899\n","=>> others_willing_to_consume_again - 0.5726244548179854\n","=>> Eval loss 2.60594, f1 0.42680\n","=>> current result -0.42680429689995325, previous best result 10000.0\n","=>> Epoch 1  global step 3020 loss 0.40238 batch 1020/1640 lr 0.001 accuracy 92.60156 wps 30119.24 step time 0.41s\n","=>> Epoch 1  global step 3040 loss 0.43907 batch 1040/1640 lr 0.001 accuracy 92.13672 wps 30453.03 step time 0.52s\n","=>> Epoch 1  global step 3060 loss 0.40273 batch 1060/1640 lr 0.001 accuracy 92.88477 wps 29745.29 step time 0.40s\n","=>> Epoch 1  global step 3080 loss 0.42903 batch 1080/1640 lr 0.001 accuracy 92.18750 wps 31535.63 step time 0.45s\n","=>> Epoch 1  global step 3100 loss 0.42914 batch 1100/1640 lr 0.001 accuracy 92.26367 wps 31209.98 step time 0.44s\n","=>> Epoch 1  global step 3120 loss 0.42293 batch 1120/1640 lr 0.001 accuracy 92.31055 wps 31375.42 step time 0.44s\n","=>> Epoch 1  global step 3140 loss 0.43463 batch 1140/1640 lr 0.001 accuracy 92.22070 wps 31682.83 step time 0.53s\n","=>> Epoch 1  global step 3160 loss 0.41744 batch 1160/1640 lr 0.001 accuracy 92.46484 wps 31959.01 step time 0.47s\n","=>> Epoch 1  global step 3180 loss 0.39249 batch 1180/1640 lr 0.001 accuracy 92.98242 wps 30172.57 step time 0.41s\n","=>> Epoch 1  global step 3200 loss 0.39665 batch 1200/1640 lr 0.001 accuracy 92.74414 wps 30347.53 step time 0.42s\n","=>> Epoch 1  global step 3220 loss 0.42332 batch 1220/1640 lr 0.001 accuracy 92.38477 wps 30966.01 step time 0.49s\n","=>> Epoch 1  global step 3240 loss 0.43015 batch 1240/1640 lr 0.001 accuracy 92.11914 wps 30912.06 step time 0.56s\n","=>> Epoch 1  global step 3260 loss 0.39382 batch 1260/1640 lr 0.001 accuracy 92.83398 wps 27778.51 step time 0.45s\n","=>> Epoch 1  global step 3280 loss 0.40902 batch 1280/1640 lr 0.001 accuracy 92.66602 wps 27792.66 step time 0.47s\n","=>> Epoch 1  global step 3300 loss 0.44250 batch 1300/1640 lr 0.001 accuracy 91.92773 wps 26495.66 step time 0.67s\n","=>> Epoch 1  global step 3320 loss 0.40695 batch 1320/1640 lr 0.001 accuracy 92.62695 wps 28242.14 step time 0.47s\n","=>> Epoch 1  global step 3340 loss 0.43580 batch 1340/1640 lr 0.001 accuracy 92.14648 wps 29706.54 step time 0.53s\n","=>> Epoch 1  global step 3360 loss 0.39639 batch 1360/1640 lr 0.001 accuracy 92.88672 wps 27873.02 step time 0.47s\n","=>> Epoch 1  global step 3380 loss 0.42201 batch 1380/1640 lr 0.001 accuracy 92.33203 wps 27989.68 step time 0.51s\n","=>> Epoch 1  global step 3400 loss 0.40825 batch 1400/1640 lr 0.001 accuracy 92.42383 wps 27104.10 step time 0.48s\n","=>> Epoch 1  global step 3420 loss 0.42248 batch 1420/1640 lr 0.001 accuracy 92.34375 wps 26912.64 step time 0.54s\n","=>> Epoch 1  global step 3440 loss 0.42069 batch 1440/1640 lr 0.001 accuracy 92.26367 wps 28490.78 step time 0.57s\n","=>> Epoch 1  global step 3460 loss 0.41894 batch 1460/1640 lr 0.001 accuracy 92.42383 wps 26474.00 step time 0.61s\n","=>> Epoch 1  global step 3480 loss 0.43090 batch 1480/1640 lr 0.001 accuracy 92.16797 wps 24092.02 step time 0.77s\n","=>> Epoch 1  global step 3500 loss 0.40768 batch 1500/1640 lr 0.001 accuracy 92.66016 wps 26930.47 step time 0.49s\n","=>> Epoch 1  global step 3520 loss 0.39342 batch 1520/1640 lr 0.001 accuracy 93.02148 wps 26993.44 step time 0.45s\n","=>> Epoch 1  global step 3540 loss 0.39951 batch 1540/1640 lr 0.001 accuracy 92.80273 wps 28173.19 step time 0.49s\n","=>> Epoch 1  global step 3560 loss 0.39343 batch 1560/1640 lr 0.001 accuracy 92.97461 wps 27865.99 step time 0.45s\n","=>> Epoch 1  global step 3580 loss 0.43050 batch 1580/1640 lr 0.001 accuracy 92.18164 wps 27228.05 step time 0.66s\n","=>> Epoch 1  global step 3600 loss 0.40849 batch 1600/1640 lr 0.001 accuracy 92.56836 wps 26797.56 step time 0.48s\n","=>> Epoch 1  global step 3620 loss 0.41370 batch 1620/1640 lr 0.001 accuracy 92.52930 wps 28343.67 step time 0.50s\n","=>> Epoch 1  global step 3640 loss 0.40933 batch 1640/1640 lr 0.001 accuracy 92.58594 wps 29000.92 step time 0.47s\n","=>> Finsh epoch 1, global step 3641\n","=>> Epoch 2  global step 3660 loss 0.39929 batch 19/1640 lr 0.001 accuracy 87.72070 wps 32778.58 step time 0.54s\n","=>> Epoch 2  global step 3680 loss 0.39483 batch 39/1640 lr 0.001 accuracy 92.80469 wps 31033.44 step time 0.43s\n","=>> Epoch 2  global step 3700 loss 0.38609 batch 59/1640 lr 0.001 accuracy 93.01172 wps 32306.90 step time 0.47s\n","=>> Epoch 2  global step 3720 loss 0.38832 batch 79/1640 lr 0.001 accuracy 93.05664 wps 30430.67 step time 0.42s\n","=>> Epoch 2  global step 3740 loss 0.39545 batch 99/1640 lr 0.001 accuracy 92.90234 wps 30275.61 step time 0.41s\n","=>> Epoch 2  global step 3760 loss 0.41372 batch 119/1640 lr 0.001 accuracy 92.34375 wps 31749.35 step time 0.52s\n","=>> Epoch 2  global step 3780 loss 0.39737 batch 139/1640 lr 0.001 accuracy 92.78320 wps 31127.82 step time 0.50s\n","=>> Epoch 2  global step 3800 loss 0.39623 batch 159/1640 lr 0.001 accuracy 92.87500 wps 30628.80 step time 0.42s\n","=>> Epoch 2  global step 3820 loss 0.40948 batch 179/1640 lr 0.001 accuracy 92.62305 wps 31803.64 step time 0.52s\n","=>> Epoch 2  global step 3840 loss 0.41277 batch 199/1640 lr 0.001 accuracy 92.44141 wps 31460.89 step time 0.51s\n","=>> Epoch 2  global step 3860 loss 0.38751 batch 219/1640 lr 0.001 accuracy 92.98828 wps 31300.78 step time 0.43s\n","=>> Epoch 2  global step 3880 loss 0.41453 batch 239/1640 lr 0.001 accuracy 92.46094 wps 32286.10 step time 0.55s\n","=>> Epoch 2  global step 3900 loss 0.40236 batch 259/1640 lr 0.001 accuracy 92.76563 wps 31462.36 step time 0.44s\n","=>> Epoch 2  global step 3920 loss 0.39275 batch 279/1640 lr 0.001 accuracy 92.98242 wps 31562.90 step time 0.44s\n","=>> Epoch 2  global step 3940 loss 0.40209 batch 299/1640 lr 0.001 accuracy 92.70313 wps 32561.08 step time 0.48s\n","=>> Epoch 2  global step 3960 loss 0.41010 batch 319/1640 lr 0.001 accuracy 92.44141 wps 30366.35 step time 0.48s\n","=>> Epoch 2  global step 3980 loss 0.40207 batch 339/1640 lr 0.001 accuracy 92.73047 wps 31788.47 step time 0.47s\n","=>> Epoch 2  global step 4000 loss 0.38309 batch 359/1640 lr 0.001 accuracy 93.09766 wps 28534.61 step time 0.47s\n","=>> global step 4000, eval result: \n","=>> location_traffic_convenience - 0.5731331911095063\n","=>> location_distance_from_business_district - 0.4046241143501885\n","=>> location_easy_to_find - 0.6498178294674674\n","=>> service_wait_time - 0.5079854451830432\n","=>> service_waiters_attitude - 0.7697658917421187\n","=>> service_parking_convenience - 0.6864280595189283\n","=>> service_serving_speed - 0.6613627628692855\n","=>> price_level - 0.7373882279840709\n","=>> price_cost_effective - 0.6944361388152341\n","=>> price_discount - 0.5854903212275349\n","=>> environment_decoration - 0.6507534503935488\n","=>> environment_noise - 0.6987047738750631\n","=>> environment_space - 0.7273627252347549\n","=>> environment_cleaness - 0.7038293621913109\n","=>> dish_portion - 0.6547695708224599\n","=>> dish_taste - 0.6819016618673455\n","=>> dish_look - 0.41336445763248664\n","=>> dish_recommendation - 0.6283826338069417\n","=>> others_overall_experience - 0.5671439230267293\n","=>> others_willing_to_consume_again - 0.6350265375366042\n","=>> Eval loss 1.94569, f1 0.63158\n","=>> current result -0.6315835539327309, previous best result -0.42680429689995325\n","=>> Epoch 2  global step 4020 loss 0.40348 batch 379/1640 lr 0.001 accuracy 92.69727 wps 27644.32 step time 0.51s\n","=>> Epoch 2  global step 4040 loss 0.40538 batch 399/1640 lr 0.001 accuracy 92.52539 wps 26435.51 step time 0.58s\n","=>> Epoch 2  global step 4060 loss 0.39788 batch 419/1640 lr 0.001 accuracy 92.79883 wps 25211.86 step time 0.57s\n","=>> Epoch 2  global step 4080 loss 0.39052 batch 439/1640 lr 0.001 accuracy 92.99219 wps 27532.46 step time 0.53s\n","=>> Epoch 2  global step 4100 loss 0.41088 batch 459/1640 lr 0.001 accuracy 92.51172 wps 29178.17 step time 0.61s\n","=>> Epoch 2  global step 4120 loss 0.39374 batch 479/1640 lr 0.001 accuracy 92.81250 wps 27304.97 step time 0.54s\n","=>> Epoch 2  global step 4140 loss 0.40919 batch 499/1640 lr 0.001 accuracy 92.63867 wps 28284.33 step time 0.48s\n","=>> Epoch 2  global step 4160 loss 0.39709 batch 519/1640 lr 0.001 accuracy 92.96289 wps 26853.42 step time 0.51s\n","=>> Epoch 2  global step 4180 loss 0.40045 batch 539/1640 lr 0.001 accuracy 92.65820 wps 26395.98 step time 0.58s\n","=>> Epoch 2  global step 4200 loss 0.38610 batch 559/1640 lr 0.001 accuracy 92.95703 wps 26621.19 step time 0.46s\n","=>> Epoch 2  global step 4220 loss 0.41043 batch 579/1640 lr 0.001 accuracy 92.56445 wps 27319.52 step time 0.62s\n","=>> Epoch 2  global step 4240 loss 0.38710 batch 599/1640 lr 0.001 accuracy 92.98438 wps 27133.89 step time 0.47s\n","=>> Epoch 2  global step 4260 loss 0.39822 batch 619/1640 lr 0.001 accuracy 92.65820 wps 26640.29 step time 0.56s\n","=>> Epoch 2  global step 4280 loss 0.38867 batch 639/1640 lr 0.001 accuracy 92.87695 wps 27720.88 step time 0.47s\n","=>> Epoch 2  global step 4300 loss 0.41160 batch 659/1640 lr 0.001 accuracy 92.40039 wps 26348.94 step time 0.67s\n","=>> Epoch 2  global step 4320 loss 0.40900 batch 679/1640 lr 0.001 accuracy 92.53516 wps 25847.10 step time 0.62s\n","=>> Epoch 2  global step 4340 loss 0.39186 batch 699/1640 lr 0.001 accuracy 93.01367 wps 28680.74 step time 0.47s\n","=>> Epoch 2  global step 4360 loss 0.39633 batch 719/1640 lr 0.001 accuracy 92.76758 wps 27184.11 step time 0.55s\n","=>> Epoch 2  global step 4380 loss 0.41354 batch 739/1640 lr 0.001 accuracy 92.48437 wps 27417.66 step time 0.58s\n","=>> Epoch 2  global step 4400 loss 0.39505 batch 759/1640 lr 0.001 accuracy 92.72852 wps 27979.31 step time 0.49s\n","=>> Epoch 2  global step 4420 loss 0.40242 batch 779/1640 lr 0.001 accuracy 92.65234 wps 27855.98 step time 0.62s\n","=>> Epoch 2  global step 4440 loss 0.38788 batch 799/1640 lr 0.001 accuracy 93.01953 wps 26468.87 step time 0.58s\n","=>> Epoch 2  global step 4460 loss 0.38448 batch 819/1640 lr 0.001 accuracy 93.02148 wps 26783.69 step time 0.46s\n","=>> Epoch 2  global step 4480 loss 0.38792 batch 839/1640 lr 0.001 accuracy 93.01563 wps 27649.84 step time 0.48s\n","=>> Epoch 2  global step 4500 loss 0.37865 batch 859/1640 lr 0.001 accuracy 93.02539 wps 27635.26 step time 0.52s\n","=>> Epoch 2  global step 4520 loss 0.39501 batch 879/1640 lr 0.001 accuracy 92.92773 wps 27293.79 step time 0.50s\n","=>> Epoch 2  global step 4540 loss 0.39397 batch 899/1640 lr 0.001 accuracy 93.01367 wps 27429.55 step time 0.53s\n","=>> Epoch 2  global step 4560 loss 0.39914 batch 919/1640 lr 0.001 accuracy 92.63477 wps 28676.86 step time 0.61s\n","=>> Epoch 2  global step 4580 loss 0.39676 batch 939/1640 lr 0.001 accuracy 92.74805 wps 28776.54 step time 0.45s\n","=>> Epoch 2  global step 4600 loss 0.40671 batch 959/1640 lr 0.001 accuracy 92.50781 wps 26874.28 step time 0.60s\n","=>> Epoch 2  global step 4620 loss 0.38259 batch 979/1640 lr 0.001 accuracy 92.93750 wps 26444.78 step time 0.58s\n","=>> Epoch 2  global step 4640 loss 0.41795 batch 999/1640 lr 0.001 accuracy 92.40234 wps 28309.76 step time 0.59s\n","=>> Epoch 2  global step 4660 loss 0.41100 batch 1019/1640 lr 0.001 accuracy 92.50781 wps 25834.13 step time 0.66s\n","=>> Epoch 2  global step 4680 loss 0.38484 batch 1039/1640 lr 0.001 accuracy 93.02930 wps 25984.48 step time 0.55s\n","=>> Epoch 2  global step 4700 loss 0.39392 batch 1059/1640 lr 0.001 accuracy 92.76758 wps 28729.38 step time 0.52s\n","=>> Epoch 2  global step 4720 loss 0.39290 batch 1079/1640 lr 0.001 accuracy 92.75391 wps 26275.30 step time 0.57s\n","=>> Epoch 2  global step 4740 loss 0.38686 batch 1099/1640 lr 0.001 accuracy 92.96289 wps 27124.92 step time 0.49s\n","=>> Epoch 2  global step 4760 loss 0.39133 batch 1119/1640 lr 0.001 accuracy 92.80273 wps 27677.98 step time 0.53s\n","=>> Epoch 2  global step 4780 loss 0.39280 batch 1139/1640 lr 0.001 accuracy 92.81836 wps 27445.49 step time 0.50s\n","=>> Epoch 2  global step 4800 loss 0.37821 batch 1159/1640 lr 0.001 accuracy 93.23242 wps 27593.95 step time 0.47s\n","=>> Epoch 2  global step 4820 loss 0.39624 batch 1179/1640 lr 0.001 accuracy 92.67774 wps 26239.32 step time 0.62s\n","=>> Epoch 2  global step 4840 loss 0.39011 batch 1199/1640 lr 0.001 accuracy 93.03906 wps 25704.16 step time 0.54s\n","=>> Epoch 2  global step 4860 loss 0.39310 batch 1219/1640 lr 0.001 accuracy 92.94141 wps 26173.13 step time 0.55s\n","=>> Epoch 2  global step 4880 loss 0.38116 batch 1239/1640 lr 0.001 accuracy 93.15234 wps 28229.96 step time 0.45s\n","=>> Epoch 2  global step 4900 loss 0.39315 batch 1259/1640 lr 0.001 accuracy 92.86523 wps 27203.90 step time 0.50s\n","=>> Epoch 2  global step 4920 loss 0.37208 batch 1279/1640 lr 0.001 accuracy 93.18555 wps 27460.77 step time 0.45s\n","=>> Epoch 2  global step 4940 loss 0.38235 batch 1299/1640 lr 0.001 accuracy 93.09375 wps 27919.14 step time 0.45s\n","=>> Epoch 2  global step 4960 loss 0.37144 batch 1319/1640 lr 0.001 accuracy 93.33984 wps 27281.93 step time 0.48s\n","=>> Epoch 2  global step 4980 loss 0.37920 batch 1339/1640 lr 0.001 accuracy 93.05273 wps 27345.72 step time 0.47s\n","=>> Epoch 2  global step 5000 loss 0.37717 batch 1359/1640 lr 0.001 accuracy 93.30469 wps 26542.75 step time 0.52s\n","=>> global step 5000, eval result: \n","=>> location_traffic_convenience - 0.6196382888867833\n","=>> location_distance_from_business_district - 0.47917866034753925\n","=>> location_easy_to_find - 0.6710052940149095\n","=>> service_wait_time - 0.6198321637242319\n","=>> service_waiters_attitude - 0.7801333801190637\n","=>> service_parking_convenience - 0.6926528823796867\n","=>> service_serving_speed - 0.7235701332719033\n","=>> price_level - 0.7622591181694328\n","=>> price_cost_effective - 0.7029076479874463\n","=>> price_discount - 0.6229691589255368\n","=>> environment_decoration - 0.706910888086081\n","=>> environment_noise - 0.7469982140777156\n","=>> environment_space - 0.7521349687957091\n","=>> environment_cleaness - 0.7342639491493388\n","=>> dish_portion - 0.6970578954708063\n","=>> dish_taste - 0.7043216805497485\n","=>> dish_look - 0.5030653055338173\n","=>> dish_recommendation - 0.6904266782058187\n","=>> others_overall_experience - 0.5765382177361665\n","=>> others_willing_to_consume_again - 0.6612545910038743\n","=>> Eval loss 1.80075, f1 0.67236\n","=>> current result -0.6723559558217805, previous best result -0.6315835539327309\n","=>> Epoch 2  global step 5020 loss 0.39178 batch 1379/1640 lr 0.001 accuracy 92.88477 wps 25893.71 step time 0.59s\n","=>> Epoch 2  global step 5040 loss 0.38939 batch 1399/1640 lr 0.001 accuracy 92.88477 wps 28135.40 step time 0.51s\n","=>> Epoch 2  global step 5060 loss 0.36961 batch 1419/1640 lr 0.001 accuracy 93.32422 wps 27600.87 step time 0.42s\n","=>> Epoch 2  global step 5080 loss 0.41037 batch 1439/1640 lr 0.001 accuracy 92.45313 wps 27758.91 step time 0.67s\n","=>> Epoch 2  global step 5100 loss 0.39359 batch 1459/1640 lr 0.001 accuracy 92.83398 wps 28299.70 step time 0.50s\n","=>> Epoch 2  global step 5120 loss 0.39359 batch 1479/1640 lr 0.001 accuracy 92.80273 wps 26798.48 step time 0.62s\n","=>> Epoch 2  global step 5140 loss 0.38808 batch 1499/1640 lr 0.001 accuracy 92.98047 wps 27045.06 step time 0.48s\n","=>> Epoch 2  global step 5160 loss 0.39463 batch 1519/1640 lr 0.001 accuracy 92.71289 wps 27833.34 step time 0.51s\n","=>> Epoch 2  global step 5180 loss 0.39214 batch 1539/1640 lr 0.001 accuracy 92.87305 wps 27465.68 step time 0.53s\n","=>> Epoch 2  global step 5200 loss 0.38812 batch 1559/1640 lr 0.001 accuracy 92.88477 wps 26285.54 step time 0.59s\n","=>> Epoch 2  global step 5220 loss 0.38399 batch 1579/1640 lr 0.001 accuracy 92.99023 wps 27561.69 step time 0.63s\n","=>> Epoch 2  global step 5240 loss 0.39902 batch 1599/1640 lr 0.001 accuracy 92.83984 wps 26588.49 step time 0.60s\n","=>> Epoch 2  global step 5260 loss 0.39528 batch 1619/1640 lr 0.001 accuracy 92.78320 wps 27557.99 step time 0.51s\n","=>> Epoch 2  global step 5280 loss 0.39828 batch 1639/1640 lr 0.001 accuracy 92.77344 wps 28555.24 step time 0.50s\n","=>> Finsh epoch 2, global step 5282\n","=>> Epoch 3  global step 5300 loss 0.34391 batch 18/1640 lr 0.001 accuracy 83.75781 wps 31534.17 step time 0.47s\n","=>> Epoch 3  global step 5320 loss 0.38829 batch 38/1640 lr 0.001 accuracy 92.86133 wps 32148.73 step time 0.47s\n","=>> Epoch 3  global step 5340 loss 0.39012 batch 58/1640 lr 0.001 accuracy 93.03125 wps 31395.77 step time 0.44s\n","=>> Epoch 3  global step 5360 loss 0.38743 batch 78/1640 lr 0.001 accuracy 92.97070 wps 31465.50 step time 0.52s\n","=>> Epoch 3  global step 5380 loss 0.37314 batch 98/1640 lr 0.001 accuracy 93.19336 wps 30347.23 step time 0.48s\n","=>> Epoch 3  global step 5400 loss 0.38427 batch 118/1640 lr 0.001 accuracy 92.92773 wps 32180.40 step time 0.46s\n","=>> Epoch 3  global step 5420 loss 0.38200 batch 138/1640 lr 0.001 accuracy 93.05859 wps 30617.41 step time 0.42s\n","=>> Epoch 3  global step 5440 loss 0.38694 batch 158/1640 lr 0.001 accuracy 92.98437 wps 32952.45 step time 0.48s\n","=>> Epoch 3  global step 5460 loss 0.38748 batch 178/1640 lr 0.001 accuracy 92.96094 wps 31622.99 step time 0.45s\n","=>> Epoch 3  global step 5480 loss 0.36413 batch 198/1640 lr 0.001 accuracy 93.39258 wps 30745.05 step time 0.42s\n","=>> Epoch 3  global step 5500 loss 0.37808 batch 218/1640 lr 0.001 accuracy 93.10938 wps 31944.76 step time 0.46s\n","=>> Epoch 3  global step 5520 loss 0.39315 batch 238/1640 lr 0.001 accuracy 92.79102 wps 32886.13 step time 0.48s\n","=>> Epoch 3  global step 5540 loss 0.37655 batch 258/1640 lr 0.001 accuracy 93.03320 wps 31244.42 step time 0.52s\n","=>> Epoch 3  global step 5560 loss 0.37417 batch 278/1640 lr 0.001 accuracy 93.17187 wps 29651.48 step time 0.52s\n","=>> Epoch 3  global step 5580 loss 0.37600 batch 298/1640 lr 0.001 accuracy 93.09961 wps 31742.66 step time 0.46s\n","=>> Epoch 3  global step 5600 loss 0.39543 batch 318/1640 lr 0.001 accuracy 92.72461 wps 32061.86 step time 0.61s\n","=>> Epoch 3  global step 5620 loss 0.37990 batch 338/1640 lr 0.001 accuracy 93.00000 wps 31130.82 step time 0.44s\n","=>> Epoch 3  global step 5640 loss 0.37063 batch 358/1640 lr 0.001 accuracy 93.34375 wps 30161.27 step time 0.41s\n","=>> Epoch 3  global step 5660 loss 0.35859 batch 378/1640 lr 0.001 accuracy 93.50976 wps 30909.98 step time 0.43s\n","=>> Epoch 3  global step 5680 loss 0.37092 batch 398/1640 lr 0.001 accuracy 93.22656 wps 31693.12 step time 0.45s\n","=>> Epoch 3  global step 5700 loss 0.40198 batch 418/1640 lr 0.001 accuracy 92.57422 wps 32046.04 step time 0.55s\n","=>> Epoch 3  global step 5720 loss 0.38343 batch 438/1640 lr 0.001 accuracy 92.90430 wps 30315.34 step time 0.55s\n","=>> Epoch 3  global step 5740 loss 0.37774 batch 458/1640 lr 0.001 accuracy 93.13281 wps 31457.02 step time 0.44s\n","=>> Epoch 3  global step 5760 loss 0.39247 batch 478/1640 lr 0.001 accuracy 92.84180 wps 32434.11 step time 0.56s\n","=>> Epoch 3  global step 5780 loss 0.38911 batch 498/1640 lr 0.001 accuracy 92.95703 wps 32412.52 step time 0.47s\n","=>> Epoch 3  global step 5800 loss 0.37462 batch 518/1640 lr 0.001 accuracy 93.19922 wps 32061.22 step time 0.47s\n","=>> Epoch 3  global step 5820 loss 0.38468 batch 538/1640 lr 0.001 accuracy 93.05469 wps 30405.39 step time 0.57s\n","=>> Epoch 3  global step 5840 loss 0.35604 batch 558/1640 lr 0.001 accuracy 93.49023 wps 30760.26 step time 0.42s\n","=>> Epoch 3  global step 5860 loss 0.36238 batch 578/1640 lr 0.001 accuracy 93.52344 wps 29780.95 step time 0.40s\n","=>> Epoch 3  global step 5880 loss 0.37689 batch 598/1640 lr 0.001 accuracy 93.20117 wps 31852.00 step time 0.45s\n","=>> Epoch 3  global step 5900 loss 0.37837 batch 618/1640 lr 0.001 accuracy 93.12695 wps 31541.98 step time 0.45s\n","=>> Epoch 3  global step 5920 loss 0.36469 batch 638/1640 lr 0.001 accuracy 93.42969 wps 31110.12 step time 0.42s\n","=>> Epoch 3  global step 5940 loss 0.38764 batch 658/1640 lr 0.001 accuracy 92.92383 wps 32253.21 step time 0.52s\n","=>> Epoch 3  global step 5960 loss 0.36177 batch 678/1640 lr 0.001 accuracy 93.52930 wps 29117.64 step time 0.38s\n","=>> Epoch 3  global step 5980 loss 0.37015 batch 698/1640 lr 0.001 accuracy 93.32227 wps 31632.45 step time 0.44s\n","=>> Epoch 3  global step 6000 loss 0.38330 batch 718/1640 lr 0.001 accuracy 93.07227 wps 32089.86 step time 0.46s\n","=>> global step 6000, eval result: \n","=>> location_traffic_convenience - 0.6282614546867283\n","=>> location_distance_from_business_district - 0.5099401713863199\n","=>> location_easy_to_find - 0.6831424455800734\n","=>> service_wait_time - 0.6429994792541955\n","=>> service_waiters_attitude - 0.7874808045800357\n","=>> service_parking_convenience - 0.6920126455220795\n","=>> service_serving_speed - 0.7429399371079017\n","=>> price_level - 0.7695511735375056\n","=>> price_cost_effective - 0.7095919985629587\n","=>> price_discount - 0.6443524925175447\n","=>> environment_decoration - 0.7125886302341231\n","=>> environment_noise - 0.7552711595149914\n","=>> environment_space - 0.7596193842233757\n","=>> environment_cleaness - 0.7418890692589186\n","=>> dish_portion - 0.7104478173518866\n","=>> dish_taste - 0.7144062029440507\n","=>> dish_look - 0.5419522377723083\n","=>> dish_recommendation - 0.7136010692764327\n","=>> others_overall_experience - 0.5813621279568776\n","=>> others_willing_to_consume_again - 0.6671770166415399\n","=>> Eval loss 1.75406, f1 0.68543\n","=>> current result -0.6854293658954924, previous best result -0.6723559558217805\n","=>> Epoch 3  global step 6020 loss 0.37921 batch 738/1640 lr 0.001 accuracy 93.19726 wps 31168.58 step time 0.47s\n","=>> Epoch 3  global step 6040 loss 0.38240 batch 758/1640 lr 0.001 accuracy 92.94531 wps 30318.26 step time 0.48s\n","=>> Epoch 3  global step 6060 loss 0.38667 batch 778/1640 lr 0.001 accuracy 92.90820 wps 31429.68 step time 0.51s\n","=>> Epoch 3  global step 6080 loss 0.37260 batch 798/1640 lr 0.001 accuracy 93.22852 wps 31676.39 step time 0.45s\n","=>> Epoch 3  global step 6100 loss 0.37242 batch 818/1640 lr 0.001 accuracy 93.29883 wps 31536.32 step time 0.44s\n","=>> Epoch 3  global step 6120 loss 0.38737 batch 838/1640 lr 0.001 accuracy 92.94531 wps 30965.66 step time 0.49s\n","=>> Epoch 3  global step 6140 loss 0.37776 batch 858/1640 lr 0.001 accuracy 92.92969 wps 31947.27 step time 0.46s\n","=>> Epoch 3  global step 6160 loss 0.39822 batch 878/1640 lr 0.001 accuracy 92.75000 wps 31144.08 step time 0.58s\n","=>> Epoch 3  global step 6180 loss 0.38403 batch 898/1640 lr 0.001 accuracy 92.97070 wps 31383.62 step time 0.43s\n","=>> Epoch 3  global step 6200 loss 0.37349 batch 918/1640 lr 0.001 accuracy 93.24023 wps 31877.88 step time 0.45s\n","=>> Epoch 3  global step 6220 loss 0.38946 batch 938/1640 lr 0.001 accuracy 92.89844 wps 31795.01 step time 0.53s\n","=>> Epoch 3  global step 6240 loss 0.35989 batch 958/1640 lr 0.001 accuracy 93.33008 wps 30379.47 step time 0.41s\n","=>> Epoch 3  global step 6260 loss 0.37693 batch 978/1640 lr 0.001 accuracy 93.05078 wps 32723.36 step time 0.49s\n","=>> Epoch 3  global step 6280 loss 0.36149 batch 998/1640 lr 0.001 accuracy 93.53906 wps 30808.64 step time 0.43s\n","=>> Epoch 3  global step 6300 loss 0.36011 batch 1018/1640 lr 0.001 accuracy 93.44336 wps 31792.86 step time 0.46s\n","=>> Epoch 3  global step 6320 loss 0.37154 batch 1038/1640 lr 0.001 accuracy 93.27930 wps 32019.20 step time 0.47s\n","=>> Epoch 3  global step 6340 loss 0.37799 batch 1058/1640 lr 0.001 accuracy 93.17578 wps 30340.74 step time 0.43s\n","=>> Epoch 3  global step 6360 loss 0.38136 batch 1078/1640 lr 0.001 accuracy 93.15039 wps 31533.13 step time 0.43s\n","=>> Epoch 3  global step 6380 loss 0.36844 batch 1098/1640 lr 0.001 accuracy 93.24414 wps 31932.10 step time 0.46s\n","=>> Epoch 3  global step 6400 loss 0.35285 batch 1118/1640 lr 0.001 accuracy 93.64844 wps 29258.94 step time 0.38s\n","=>> Epoch 3  global step 6420 loss 0.38212 batch 1138/1640 lr 0.001 accuracy 93.00977 wps 32126.30 step time 0.45s\n","=>> Epoch 3  global step 6440 loss 0.37487 batch 1158/1640 lr 0.001 accuracy 93.17773 wps 30640.62 step time 0.42s\n","=>> Epoch 3  global step 6460 loss 0.37017 batch 1178/1640 lr 0.001 accuracy 93.30664 wps 31214.11 step time 0.43s\n","=>> Epoch 3  global step 6480 loss 0.38808 batch 1198/1640 lr 0.001 accuracy 92.98438 wps 31226.09 step time 0.44s\n","=>> Epoch 3  global step 6500 loss 0.38733 batch 1218/1640 lr 0.001 accuracy 92.89844 wps 33396.42 step time 0.50s\n","=>> Epoch 3  global step 6520 loss 0.38141 batch 1238/1640 lr 0.001 accuracy 93.01172 wps 30177.59 step time 0.53s\n","=>> Epoch 3  global step 6540 loss 0.38232 batch 1258/1640 lr 0.001 accuracy 93.06055 wps 31947.17 step time 0.50s\n","=>> Epoch 3  global step 6560 loss 0.39608 batch 1278/1640 lr 0.001 accuracy 92.68359 wps 32360.61 step time 0.52s\n","=>> Epoch 3  global step 6580 loss 0.38901 batch 1298/1640 lr 0.001 accuracy 92.94922 wps 31470.68 step time 0.52s\n","=>> Epoch 3  global step 6600 loss 0.36089 batch 1318/1640 lr 0.001 accuracy 93.50977 wps 31472.25 step time 0.44s\n","=>> Epoch 3  global step 6620 loss 0.37707 batch 1338/1640 lr 0.001 accuracy 93.08203 wps 31875.79 step time 0.45s\n","=>> Epoch 3  global step 6640 loss 0.35717 batch 1358/1640 lr 0.001 accuracy 93.45703 wps 30994.22 step time 0.42s\n","=>> Epoch 3  global step 6660 loss 0.36780 batch 1378/1640 lr 0.001 accuracy 93.31250 wps 31173.33 step time 0.43s\n","=>> Epoch 3  global step 6680 loss 0.38601 batch 1398/1640 lr 0.001 accuracy 92.92969 wps 31415.32 step time 0.51s\n","=>> Epoch 3  global step 6700 loss 0.37427 batch 1418/1640 lr 0.001 accuracy 93.27734 wps 31106.52 step time 0.49s\n","=>> Epoch 3  global step 6720 loss 0.37354 batch 1438/1640 lr 0.001 accuracy 93.09766 wps 30965.12 step time 0.49s\n","=>> Epoch 3  global step 6740 loss 0.38320 batch 1458/1640 lr 0.001 accuracy 92.80273 wps 32663.97 step time 0.49s\n","=>> Epoch 3  global step 6760 loss 0.35461 batch 1478/1640 lr 0.001 accuracy 93.58594 wps 30984.73 step time 0.43s\n","=>> Epoch 3  global step 6780 loss 0.36733 batch 1498/1640 lr 0.001 accuracy 93.36719 wps 31867.24 step time 0.46s\n","=>> Epoch 3  global step 6800 loss 0.36809 batch 1518/1640 lr 0.001 accuracy 93.41406 wps 29772.29 step time 0.46s\n","=>> Epoch 3  global step 6820 loss 0.37656 batch 1538/1640 lr 0.001 accuracy 93.11328 wps 31264.24 step time 0.52s\n","=>> Epoch 3  global step 6840 loss 0.34584 batch 1558/1640 lr 0.001 accuracy 93.71875 wps 25311.12 step time 0.48s\n","=>> Epoch 3  global step 6860 loss 0.35157 batch 1578/1640 lr 0.001 accuracy 93.68164 wps 28008.46 step time 0.41s\n","=>> Epoch 3  global step 6880 loss 0.36301 batch 1598/1640 lr 0.001 accuracy 93.33984 wps 25684.54 step time 0.52s\n","=>> Epoch 3  global step 6900 loss 0.37797 batch 1618/1640 lr 0.001 accuracy 93.16992 wps 27469.36 step time 0.50s\n","=>> Epoch 3  global step 6920 loss 0.37877 batch 1638/1640 lr 0.001 accuracy 93.07617 wps 27731.70 step time 0.54s\n","=>> Finsh epoch 3, global step 6923\n","=>> Epoch 4  global step 6940 loss 0.30627 batch 17/1640 lr 0.001 accuracy 79.43555 wps 31094.79 step time 0.43s\n","=>> Epoch 4  global step 6960 loss 0.34170 batch 37/1640 lr 0.001 accuracy 93.74023 wps 30494.20 step time 0.42s\n","=>> Epoch 4  global step 6980 loss 0.36994 batch 57/1640 lr 0.001 accuracy 93.18555 wps 32309.42 step time 0.54s\n","=>> Epoch 4  global step 7000 loss 0.35796 batch 77/1640 lr 0.001 accuracy 93.52148 wps 31200.83 step time 0.44s\n","=>> global step 7000, eval result: \n","=>> location_traffic_convenience - 0.6457968495346393\n","=>> location_distance_from_business_district - 0.5267016443461352\n","=>> location_easy_to_find - 0.6921763913350463\n","=>> service_wait_time - 0.6448369050352638\n","=>> service_waiters_attitude - 0.7938172982828117\n","=>> service_parking_convenience - 0.707434301734545\n","=>> service_serving_speed - 0.7463742954627642\n","=>> price_level - 0.7726021587161838\n","=>> price_cost_effective - 0.7076451218673154\n","=>> price_discount - 0.6562397786784394\n","=>> environment_decoration - 0.7193337406859541\n","=>> environment_noise - 0.7574994859830669\n","=>> environment_space - 0.761861668414029\n","=>> environment_cleaness - 0.7451166228726371\n","=>> dish_portion - 0.7169550634268871\n","=>> dish_taste - 0.7198546290025126\n","=>> dish_look - 0.5554636824401474\n","=>> dish_recommendation - 0.7222797062523609\n","=>> others_overall_experience - 0.5821655109874523\n","=>> others_willing_to_consume_again - 0.6738477519410142\n","=>> Eval loss 1.73159, f1 0.69240\n","=>> current result -0.6924001303499604, previous best result -0.6854293658954924\n","=>> Epoch 4  global step 7020 loss 0.35081 batch 97/1640 lr 0.001 accuracy 93.64648 wps 29795.64 step time 0.43s\n","=>> Epoch 4  global step 7040 loss 0.34379 batch 117/1640 lr 0.001 accuracy 93.72070 wps 30025.67 step time 0.41s\n","=>> Epoch 4  global step 7060 loss 0.37095 batch 137/1640 lr 0.001 accuracy 93.13867 wps 32560.53 step time 0.48s\n","=>> Epoch 4  global step 7080 loss 0.37274 batch 157/1640 lr 0.001 accuracy 93.24023 wps 31694.09 step time 0.45s\n","=>> Epoch 4  global step 7100 loss 0.36125 batch 177/1640 lr 0.001 accuracy 93.29297 wps 31872.92 step time 0.45s\n","=>> Epoch 4  global step 7120 loss 0.34164 batch 197/1640 lr 0.001 accuracy 93.89453 wps 29709.21 step time 0.42s\n","=>> Epoch 4  global step 7140 loss 0.37300 batch 217/1640 lr 0.001 accuracy 93.33984 wps 25694.21 step time 0.63s\n","=>> Epoch 4  global step 7160 loss 0.35613 batch 237/1640 lr 0.001 accuracy 93.37109 wps 27180.44 step time 0.47s\n","=>> Epoch 4  global step 7180 loss 0.37242 batch 257/1640 lr 0.001 accuracy 93.24219 wps 28202.69 step time 0.53s\n","=>> Epoch 4  global step 7200 loss 0.37324 batch 277/1640 lr 0.001 accuracy 93.18359 wps 28215.51 step time 0.53s\n","=>> Epoch 4  global step 7220 loss 0.36124 batch 297/1640 lr 0.001 accuracy 93.53516 wps 26378.17 step time 0.52s\n","=>> Epoch 4  global step 7240 loss 0.35576 batch 317/1640 lr 0.001 accuracy 93.48242 wps 27753.81 step time 0.43s\n","=>> Epoch 4  global step 7260 loss 0.34842 batch 337/1640 lr 0.001 accuracy 93.70703 wps 27915.15 step time 0.46s\n","=>> Epoch 4  global step 7280 loss 0.37626 batch 357/1640 lr 0.001 accuracy 93.17773 wps 26651.38 step time 0.74s\n","=>> Epoch 4  global step 7300 loss 0.36060 batch 377/1640 lr 0.001 accuracy 93.44141 wps 26945.08 step time 0.56s\n","=>> Epoch 4  global step 7320 loss 0.37380 batch 397/1640 lr 0.001 accuracy 93.21680 wps 27066.58 step time 0.66s\n","=>> Epoch 4  global step 7340 loss 0.35653 batch 417/1640 lr 0.001 accuracy 93.60156 wps 27942.53 step time 0.47s\n","=>> Epoch 4  global step 7360 loss 0.35420 batch 437/1640 lr 0.001 accuracy 93.58203 wps 27597.42 step time 0.49s\n","=>> Epoch 4  global step 7380 loss 0.36026 batch 457/1640 lr 0.001 accuracy 93.41406 wps 28162.16 step time 0.46s\n","=>> Epoch 4  global step 7400 loss 0.36354 batch 477/1640 lr 0.001 accuracy 93.32617 wps 26230.07 step time 0.51s\n","=>> Epoch 4  global step 7420 loss 0.36161 batch 497/1640 lr 0.001 accuracy 93.49609 wps 27648.32 step time 0.49s\n","=>> Epoch 4  global step 7440 loss 0.36629 batch 517/1640 lr 0.001 accuracy 93.33203 wps 27105.86 step time 0.56s\n","=>> Epoch 4  global step 7460 loss 0.37818 batch 537/1640 lr 0.001 accuracy 93.21289 wps 27449.92 step time 0.55s\n","=>> Epoch 4  global step 7480 loss 0.36954 batch 557/1640 lr 0.001 accuracy 93.26953 wps 28264.79 step time 0.48s\n","=>> Epoch 4  global step 7500 loss 0.36529 batch 577/1640 lr 0.001 accuracy 93.34961 wps 26891.70 step time 0.55s\n","=>> Epoch 4  global step 7520 loss 0.35192 batch 597/1640 lr 0.001 accuracy 93.67383 wps 27220.17 step time 0.50s\n","=>> Epoch 4  global step 7540 loss 0.37215 batch 617/1640 lr 0.001 accuracy 93.29492 wps 26984.70 step time 0.56s\n","=>> Epoch 4  global step 7560 loss 0.38528 batch 637/1640 lr 0.001 accuracy 93.08789 wps 27289.07 step time 0.57s\n","=>> Epoch 4  global step 7580 loss 0.36938 batch 657/1640 lr 0.001 accuracy 93.24805 wps 25432.52 step time 0.54s\n","=>> Epoch 4  global step 7600 loss 0.36036 batch 677/1640 lr 0.001 accuracy 93.35938 wps 28103.59 step time 0.52s\n","=>> Epoch 4  global step 7620 loss 0.37045 batch 697/1640 lr 0.001 accuracy 93.18750 wps 26753.90 step time 0.70s\n","=>> Epoch 4  global step 7640 loss 0.38275 batch 717/1640 lr 0.001 accuracy 92.92188 wps 28195.39 step time 0.57s\n","=>> Epoch 4  global step 7660 loss 0.36714 batch 737/1640 lr 0.001 accuracy 93.36523 wps 26826.83 step time 0.57s\n","=>> Epoch 4  global step 7680 loss 0.38782 batch 757/1640 lr 0.001 accuracy 93.00586 wps 27609.56 step time 0.67s\n","=>> Epoch 4  global step 7700 loss 0.36311 batch 777/1640 lr 0.001 accuracy 93.34570 wps 27182.33 step time 0.55s\n","=>> Epoch 4  global step 7720 loss 0.37082 batch 797/1640 lr 0.001 accuracy 93.23437 wps 26023.27 step time 0.54s\n","=>> Epoch 4  global step 7740 loss 0.34942 batch 817/1640 lr 0.001 accuracy 93.62891 wps 24223.76 step time 0.58s\n","=>> Epoch 4  global step 7760 loss 0.35611 batch 837/1640 lr 0.001 accuracy 93.48242 wps 26916.38 step time 0.50s\n","=>> Epoch 4  global step 7780 loss 0.38244 batch 857/1640 lr 0.001 accuracy 93.06836 wps 28544.95 step time 0.56s\n","=>> Epoch 4  global step 7800 loss 0.37061 batch 877/1640 lr 0.001 accuracy 93.33008 wps 27655.51 step time 0.53s\n","=>> Epoch 4  global step 7820 loss 0.37272 batch 897/1640 lr 0.001 accuracy 93.17578 wps 27914.19 step time 0.48s\n","=>> Epoch 4  global step 7840 loss 0.37411 batch 917/1640 lr 0.001 accuracy 93.26562 wps 29057.74 step time 0.51s\n","=>> Epoch 4  global step 7860 loss 0.37049 batch 937/1640 lr 0.001 accuracy 93.25195 wps 28146.16 step time 0.53s\n","=>> Epoch 4  global step 7880 loss 0.35288 batch 957/1640 lr 0.001 accuracy 93.56445 wps 27719.44 step time 0.50s\n","=>> Epoch 4  global step 7900 loss 0.37070 batch 977/1640 lr 0.001 accuracy 93.19336 wps 27479.80 step time 0.55s\n","=>> Epoch 4  global step 7920 loss 0.39560 batch 997/1640 lr 0.001 accuracy 92.67773 wps 27029.81 step time 0.71s\n","=>> Epoch 4  global step 7940 loss 0.35765 batch 1017/1640 lr 0.001 accuracy 93.46680 wps 27568.51 step time 0.44s\n","=>> Epoch 4  global step 7960 loss 0.36980 batch 1037/1640 lr 0.001 accuracy 93.31836 wps 25734.92 step time 0.66s\n","=>> Epoch 4  global step 7980 loss 0.38198 batch 1057/1640 lr 0.001 accuracy 93.05664 wps 27069.47 step time 0.58s\n","=>> Epoch 4  global step 8000 loss 0.35498 batch 1077/1640 lr 0.001 accuracy 93.53125 wps 27884.57 step time 0.51s\n","=>> global step 8000, eval result: \n","=>> location_traffic_convenience - 0.6523631487647993\n","=>> location_distance_from_business_district - 0.5233866232629438\n","=>> location_easy_to_find - 0.6911324388369253\n","=>> service_wait_time - 0.6468491126190805\n","=>> service_waiters_attitude - 0.7967579993737779\n","=>> service_parking_convenience - 0.7081865780197723\n","=>> service_serving_speed - 0.7488096092460984\n","=>> price_level - 0.7752600950008495\n","=>> price_cost_effective - 0.7089732241303788\n","=>> price_discount - 0.6609106734039822\n","=>> environment_decoration - 0.7303751917353691\n","=>> environment_noise - 0.7580163921980613\n","=>> environment_space - 0.7644406975868915\n","=>> environment_cleaness - 0.7481334195201721\n","=>> dish_portion - 0.7221327695069297\n","=>> dish_taste - 0.7232507252931989\n","=>> dish_look - 0.5615274718037693\n","=>> dish_recommendation - 0.7326847987819525\n","=>> others_overall_experience - 0.5835716682985176\n","=>> others_willing_to_consume_again - 0.6831708991638139\n","=>> Eval loss 1.71774, f1 0.69600\n","=>> current result -0.6959966768273642, previous best result -0.6924001303499604\n","=>> Epoch 4  global step 8020 loss 0.35728 batch 1097/1640 lr 0.001 accuracy 93.45117 wps 26623.47 step time 0.49s\n","=>> Epoch 4  global step 8040 loss 0.35128 batch 1117/1640 lr 0.001 accuracy 93.59570 wps 27432.01 step time 0.58s\n","=>> Epoch 4  global step 8060 loss 0.35609 batch 1137/1640 lr 0.001 accuracy 93.42187 wps 27524.24 step time 0.54s\n","=>> Epoch 4  global step 8080 loss 0.35857 batch 1157/1640 lr 0.001 accuracy 93.46094 wps 28226.02 step time 0.49s\n","=>> Epoch 4  global step 8100 loss 0.35770 batch 1177/1640 lr 0.001 accuracy 93.47070 wps 27849.96 step time 0.52s\n","=>> Epoch 4  global step 8120 loss 0.37279 batch 1197/1640 lr 0.001 accuracy 93.17578 wps 28376.33 step time 0.57s\n","=>> Epoch 4  global step 8140 loss 0.35883 batch 1217/1640 lr 0.001 accuracy 93.37109 wps 27864.66 step time 0.53s\n","=>> Epoch 4  global step 8160 loss 0.36705 batch 1237/1640 lr 0.001 accuracy 93.27930 wps 26649.67 step time 0.58s\n","=>> Epoch 4  global step 8180 loss 0.35672 batch 1257/1640 lr 0.001 accuracy 93.55469 wps 27289.72 step time 0.49s\n","=>> Epoch 4  global step 8200 loss 0.34890 batch 1277/1640 lr 0.001 accuracy 93.72266 wps 27656.40 step time 0.44s\n","=>> Epoch 4  global step 8220 loss 0.36687 batch 1297/1640 lr 0.001 accuracy 93.25586 wps 28812.00 step time 0.52s\n","=>> Epoch 4  global step 8240 loss 0.36274 batch 1317/1640 lr 0.001 accuracy 93.41406 wps 27455.85 step time 0.53s\n","=>> Epoch 4  global step 8260 loss 0.37476 batch 1337/1640 lr 0.001 accuracy 93.14063 wps 26159.31 step time 0.63s\n","=>> Epoch 4  global step 8280 loss 0.36708 batch 1357/1640 lr 0.001 accuracy 93.36914 wps 27033.34 step time 0.45s\n","=>> Epoch 4  global step 8300 loss 0.36493 batch 1377/1640 lr 0.001 accuracy 93.29492 wps 27902.75 step time 0.51s\n","=>> Epoch 4  global step 8320 loss 0.34706 batch 1397/1640 lr 0.001 accuracy 93.76953 wps 27144.20 step time 0.45s\n","=>> Epoch 4  global step 8340 loss 0.37908 batch 1417/1640 lr 0.001 accuracy 93.03906 wps 28745.16 step time 0.54s\n","=>> Epoch 4  global step 8360 loss 0.36572 batch 1437/1640 lr 0.001 accuracy 93.28125 wps 26113.33 step time 0.48s\n","=>> Epoch 4  global step 8380 loss 0.36387 batch 1457/1640 lr 0.001 accuracy 93.28320 wps 28882.29 step time 0.49s\n","=>> Epoch 4  global step 8400 loss 0.35496 batch 1477/1640 lr 0.001 accuracy 93.40430 wps 26338.28 step time 0.53s\n","=>> Epoch 4  global step 8420 loss 0.38216 batch 1497/1640 lr 0.001 accuracy 92.91406 wps 26809.43 step time 0.63s\n","=>> Epoch 4  global step 8440 loss 0.38537 batch 1517/1640 lr 0.001 accuracy 93.01758 wps 28305.85 step time 0.64s\n","=>> Epoch 4  global step 8460 loss 0.37078 batch 1537/1640 lr 0.001 accuracy 93.20312 wps 27997.69 step time 0.50s\n","=>> Epoch 4  global step 8480 loss 0.37408 batch 1557/1640 lr 0.001 accuracy 93.11914 wps 28673.74 step time 0.49s\n","=>> Epoch 4  global step 8500 loss 0.36777 batch 1577/1640 lr 0.001 accuracy 93.25000 wps 27322.50 step time 0.58s\n","=>> Epoch 4  global step 8520 loss 0.36661 batch 1597/1640 lr 0.001 accuracy 93.32422 wps 27883.05 step time 0.52s\n","=>> Epoch 4  global step 8540 loss 0.36068 batch 1617/1640 lr 0.001 accuracy 93.38867 wps 26259.80 step time 0.60s\n","=>> Epoch 4  global step 8560 loss 0.34996 batch 1637/1640 lr 0.001 accuracy 93.75391 wps 26618.10 step time 0.43s\n","=>> Finsh epoch 4, global step 8564\n","=>> Epoch 5  global step 8580 loss 0.27709 batch 16/1640 lr 0.001 accuracy 74.89258 wps 31711.88 step time 0.43s\n","=>> Epoch 5  global step 8600 loss 0.33853 batch 36/1640 lr 0.001 accuracy 93.94141 wps 30908.81 step time 0.43s\n","=>> Epoch 5  global step 8620 loss 0.34503 batch 56/1640 lr 0.001 accuracy 93.66797 wps 32363.36 step time 0.48s\n","=>> Epoch 5  global step 8640 loss 0.35049 batch 76/1640 lr 0.001 accuracy 93.63281 wps 30801.71 step time 0.49s\n","=>> Epoch 5  global step 8660 loss 0.35088 batch 96/1640 lr 0.001 accuracy 93.61523 wps 31416.27 step time 0.45s\n","=>> Epoch 5  global step 8680 loss 0.36043 batch 116/1640 lr 0.001 accuracy 93.46484 wps 31366.01 step time 0.45s\n","=>> Epoch 5  global step 8700 loss 0.33287 batch 136/1640 lr 0.001 accuracy 93.83789 wps 30541.75 step time 0.42s\n","=>> Epoch 5  global step 8720 loss 0.34994 batch 156/1640 lr 0.001 accuracy 93.69531 wps 31248.33 step time 0.43s\n","=>> Epoch 5  global step 8740 loss 0.35024 batch 176/1640 lr 0.001 accuracy 93.65234 wps 30589.40 step time 0.49s\n","=>> Epoch 5  global step 8760 loss 0.35168 batch 196/1640 lr 0.001 accuracy 93.69727 wps 32083.89 step time 0.46s\n","=>> Epoch 5  global step 8780 loss 0.36661 batch 216/1640 lr 0.001 accuracy 93.26367 wps 29954.70 step time 0.55s\n","=>> Epoch 5  global step 8800 loss 0.34986 batch 236/1640 lr 0.001 accuracy 93.58789 wps 28100.23 step time 0.49s\n","=>> Epoch 5  global step 8820 loss 0.37261 batch 256/1640 lr 0.001 accuracy 93.23438 wps 28334.27 step time 0.58s\n","=>> Epoch 5  global step 8840 loss 0.35397 batch 276/1640 lr 0.001 accuracy 93.57031 wps 27791.28 step time 0.48s\n","=>> Epoch 5  global step 8860 loss 0.34697 batch 296/1640 lr 0.001 accuracy 93.75000 wps 26957.70 step time 0.42s\n","=>> Epoch 5  global step 8880 loss 0.35534 batch 316/1640 lr 0.001 accuracy 93.48828 wps 27994.32 step time 0.51s\n","=>> Epoch 5  global step 8900 loss 0.35290 batch 336/1640 lr 0.001 accuracy 93.61133 wps 27180.50 step time 0.49s\n","=>> Epoch 5  global step 8920 loss 0.33699 batch 356/1640 lr 0.001 accuracy 93.79883 wps 28050.97 step time 0.46s\n","=>> Epoch 5  global step 8940 loss 0.34167 batch 376/1640 lr 0.001 accuracy 93.83008 wps 27433.35 step time 0.47s\n","=>> Epoch 5  global step 8960 loss 0.34741 batch 396/1640 lr 0.001 accuracy 93.75391 wps 28026.23 step time 0.52s\n","=>> Epoch 5  global step 8980 loss 0.37136 batch 416/1640 lr 0.001 accuracy 93.08594 wps 27519.55 step time 0.63s\n","=>> Epoch 5  global step 9000 loss 0.33490 batch 436/1640 lr 0.001 accuracy 93.85547 wps 27769.46 step time 0.48s\n","=>> global step 9000, eval result: \n","=>> location_traffic_convenience - 0.6506167003989626\n","=>> location_distance_from_business_district - 0.5366467515675827\n","=>> location_easy_to_find - 0.6976830399016908\n","=>> service_wait_time - 0.6496527109000777\n","=>> service_waiters_attitude - 0.7961773064873404\n","=>> service_parking_convenience - 0.7193356310979699\n","=>> service_serving_speed - 0.7561747154873423\n","=>> price_level - 0.7780530495667195\n","=>> price_cost_effective - 0.7120185362945378\n","=>> price_discount - 0.665139287074458\n","=>> environment_decoration - 0.7293298792771064\n","=>> environment_noise - 0.7613647382555718\n","=>> environment_space - 0.7639920429754417\n","=>> environment_cleaness - 0.7530312850137642\n","=>> dish_portion - 0.7257700237114737\n","=>> dish_taste - 0.7252424777425819\n","=>> dish_look - 0.5709195353283605\n","=>> dish_recommendation - 0.7338982901903688\n","=>> others_overall_experience - 0.5831613347198621\n","=>> others_willing_to_consume_again - 0.6872608951759542\n","=>> Eval loss 1.70799, f1 0.69977\n","=>> current result -0.6997734115583584, previous best result -0.6959966768273642\n","=>> Epoch 5  global step 9020 loss 0.38423 batch 456/1640 lr 0.001 accuracy 92.89648 wps 25652.72 step time 0.81s\n","=>> Epoch 5  global step 9040 loss 0.34212 batch 476/1640 lr 0.001 accuracy 93.88672 wps 27591.17 step time 0.47s\n","=>> Epoch 5  global step 9060 loss 0.34435 batch 496/1640 lr 0.001 accuracy 93.83398 wps 27905.47 step time 0.46s\n","=>> Epoch 5  global step 9080 loss 0.35356 batch 516/1640 lr 0.001 accuracy 93.46094 wps 27003.19 step time 0.57s\n","=>> Epoch 5  global step 9100 loss 0.35287 batch 536/1640 lr 0.001 accuracy 93.56641 wps 27702.90 step time 0.43s\n","=>> Epoch 5  global step 9120 loss 0.35231 batch 556/1640 lr 0.001 accuracy 93.61523 wps 27329.73 step time 0.47s\n","=>> Epoch 5  global step 9140 loss 0.36079 batch 576/1640 lr 0.001 accuracy 93.30859 wps 26701.85 step time 0.62s\n","=>> Epoch 5  global step 9160 loss 0.34742 batch 596/1640 lr 0.001 accuracy 93.54883 wps 27272.45 step time 0.49s\n","=>> Epoch 5  global step 9180 loss 0.36631 batch 616/1640 lr 0.001 accuracy 93.21875 wps 26643.71 step time 0.60s\n","=>> Epoch 5  global step 9200 loss 0.36612 batch 636/1640 lr 0.001 accuracy 93.24023 wps 27031.11 step time 0.57s\n","=>> Epoch 5  global step 9220 loss 0.34267 batch 656/1640 lr 0.001 accuracy 93.78906 wps 27375.92 step time 0.46s\n","=>> Epoch 5  global step 9240 loss 0.34857 batch 676/1640 lr 0.001 accuracy 93.66211 wps 27883.02 step time 0.49s\n","=>> Epoch 5  global step 9260 loss 0.35876 batch 696/1640 lr 0.001 accuracy 93.47461 wps 25766.13 step time 0.59s\n","=>> Epoch 5  global step 9280 loss 0.37285 batch 716/1640 lr 0.001 accuracy 93.11719 wps 27937.98 step time 0.55s\n","=>> Epoch 5  global step 9300 loss 0.34427 batch 736/1640 lr 0.001 accuracy 93.68555 wps 28563.23 step time 0.47s\n","=>> Epoch 5  global step 9320 loss 0.38123 batch 756/1640 lr 0.001 accuracy 93.05469 wps 28060.05 step time 0.60s\n","=>> Epoch 5  global step 9340 loss 0.36775 batch 776/1640 lr 0.001 accuracy 93.35938 wps 27547.44 step time 0.54s\n","=>> Epoch 5  global step 9360 loss 0.35182 batch 796/1640 lr 0.001 accuracy 93.49023 wps 25793.86 step time 0.56s\n","=>> Epoch 5  global step 9380 loss 0.34852 batch 816/1640 lr 0.001 accuracy 93.64453 wps 28072.33 step time 0.53s\n","=>> Epoch 5  global step 9400 loss 0.36369 batch 836/1640 lr 0.001 accuracy 93.30273 wps 27154.93 step time 0.61s\n","=>> Epoch 5  global step 9420 loss 0.36134 batch 856/1640 lr 0.001 accuracy 93.36133 wps 26189.56 step time 0.59s\n","=>> Epoch 5  global step 9440 loss 0.34662 batch 876/1640 lr 0.001 accuracy 93.60938 wps 27017.07 step time 0.54s\n","=>> Epoch 5  global step 9460 loss 0.36904 batch 896/1640 lr 0.001 accuracy 93.25000 wps 24844.62 step time 0.69s\n","=>> Epoch 5  global step 9480 loss 0.34641 batch 916/1640 lr 0.001 accuracy 93.47852 wps 24977.99 step time 0.64s\n","=>> Epoch 5  global step 9500 loss 0.35603 batch 936/1640 lr 0.001 accuracy 93.36914 wps 27091.66 step time 0.54s\n","=>> Epoch 5  global step 9520 loss 0.38234 batch 956/1640 lr 0.001 accuracy 93.05469 wps 26853.69 step time 0.74s\n","=>> Epoch 5  global step 9540 loss 0.36542 batch 976/1640 lr 0.001 accuracy 93.33984 wps 26476.69 step time 0.62s\n","=>> Epoch 5  global step 9560 loss 0.37042 batch 996/1640 lr 0.001 accuracy 93.08399 wps 28845.96 step time 0.59s\n","=>> Epoch 5  global step 9580 loss 0.34938 batch 1016/1640 lr 0.001 accuracy 93.61523 wps 24301.54 step time 0.60s\n","=>> Epoch 5  global step 9600 loss 0.35803 batch 1036/1640 lr 0.001 accuracy 93.42383 wps 28278.87 step time 0.51s\n","=>> Epoch 5  global step 9620 loss 0.34577 batch 1056/1640 lr 0.001 accuracy 93.56445 wps 28512.51 step time 0.45s\n","=>> Epoch 5  global step 9640 loss 0.34313 batch 1076/1640 lr 0.001 accuracy 93.68945 wps 26792.74 step time 0.53s\n","=>> Epoch 5  global step 9660 loss 0.35848 batch 1096/1640 lr 0.001 accuracy 93.43359 wps 27613.86 step time 0.55s\n","=>> Epoch 5  global step 9680 loss 0.33924 batch 1116/1640 lr 0.001 accuracy 93.79297 wps 26877.79 step time 0.44s\n","=>> Epoch 5  global step 9700 loss 0.36295 batch 1136/1640 lr 0.001 accuracy 93.40820 wps 28399.01 step time 0.55s\n","=>> Epoch 5  global step 9720 loss 0.34378 batch 1156/1640 lr 0.001 accuracy 93.75195 wps 26266.69 step time 0.54s\n","=>> Epoch 5  global step 9740 loss 0.34336 batch 1176/1640 lr 0.001 accuracy 93.58984 wps 25434.89 step time 0.52s\n","=>> Epoch 5  global step 9760 loss 0.35485 batch 1196/1640 lr 0.001 accuracy 93.53320 wps 27262.77 step time 0.45s\n","=>> Epoch 5  global step 9780 loss 0.38155 batch 1216/1640 lr 0.001 accuracy 92.91211 wps 27751.85 step time 0.63s\n","=>> Epoch 5  global step 9800 loss 0.36796 batch 1236/1640 lr 0.001 accuracy 93.41797 wps 27638.18 step time 0.54s\n","=>> Epoch 5  global step 9820 loss 0.35457 batch 1256/1640 lr 0.001 accuracy 93.34766 wps 28346.10 step time 0.47s\n","=>> Epoch 5  global step 9840 loss 0.35005 batch 1276/1640 lr 0.001 accuracy 93.61719 wps 27614.34 step time 0.54s\n","=>> Epoch 5  global step 9860 loss 0.34172 batch 1296/1640 lr 0.001 accuracy 93.90625 wps 26832.47 step time 0.51s\n","=>> Epoch 5  global step 9880 loss 0.35252 batch 1316/1640 lr 0.001 accuracy 93.67969 wps 27804.43 step time 0.49s\n","=>> Epoch 5  global step 9900 loss 0.34726 batch 1336/1640 lr 0.001 accuracy 93.59766 wps 24908.33 step time 0.53s\n","=>> Epoch 5  global step 9920 loss 0.37054 batch 1356/1640 lr 0.001 accuracy 93.12695 wps 26591.44 step time 0.60s\n","=>> Epoch 5  global step 9940 loss 0.34546 batch 1376/1640 lr 0.001 accuracy 93.72851 wps 28081.79 step time 0.44s\n","=>> Epoch 5  global step 9960 loss 0.33528 batch 1396/1640 lr 0.001 accuracy 93.94531 wps 25722.01 step time 0.52s\n","=>> Epoch 5  global step 9980 loss 0.35644 batch 1416/1640 lr 0.001 accuracy 93.51562 wps 28145.07 step time 0.54s\n","=>> Epoch 5  global step 10000 loss 0.36314 batch 1436/1640 lr 0.001 accuracy 93.32617 wps 26018.31 step time 0.64s\n","=>> global step 10000, eval result: \n","=>> location_traffic_convenience - 0.6500562636003343\n","=>> location_distance_from_business_district - 0.5415499345525107\n","=>> location_easy_to_find - 0.69940279498492\n","=>> service_wait_time - 0.6565156115909246\n","=>> service_waiters_attitude - 0.799534892567193\n","=>> service_parking_convenience - 0.7332176137633675\n","=>> service_serving_speed - 0.7503237569872999\n","=>> price_level - 0.7792176603253165\n","=>> price_cost_effective - 0.7136023261825989\n","=>> price_discount - 0.6623456120951158\n","=>> environment_decoration - 0.7284792526122754\n","=>> environment_noise - 0.7648319733776714\n","=>> environment_space - 0.7676726392907242\n","=>> environment_cleaness - 0.7548308143609909\n","=>> dish_portion - 0.7246011413988842\n","=>> dish_taste - 0.7248295625996587\n","=>> dish_look - 0.5778546332965357\n","=>> dish_recommendation - 0.7390427808399893\n","=>> others_overall_experience - 0.5904118463556631\n","=>> others_willing_to_consume_again - 0.6943253074029456\n","=>> Eval loss 1.70244, f1 0.70263\n","=>> current result -0.702632320909246, previous best result -0.6997734115583584\n","=>> Epoch 5  global step 10020 loss 0.35936 batch 1456/1640 lr 0.001 accuracy 93.45313 wps 26413.90 step time 0.53s\n","=>> Epoch 5  global step 10040 loss 0.38400 batch 1476/1640 lr 0.001 accuracy 92.94922 wps 27747.79 step time 0.64s\n","=>> Epoch 5  global step 10060 loss 0.36323 batch 1496/1640 lr 0.001 accuracy 93.32227 wps 26913.40 step time 0.54s\n","=>> Epoch 5  global step 10080 loss 0.35400 batch 1516/1640 lr 0.001 accuracy 93.59375 wps 27802.43 step time 0.49s\n","=>> Epoch 5  global step 10100 loss 0.34758 batch 1536/1640 lr 0.001 accuracy 93.69141 wps 26797.68 step time 0.46s\n","=>> Epoch 5  global step 10120 loss 0.35881 batch 1556/1640 lr 0.001 accuracy 93.48047 wps 28364.78 step time 0.46s\n","=>> Epoch 5  global step 10140 loss 0.36183 batch 1576/1640 lr 0.001 accuracy 93.38281 wps 26553.61 step time 0.57s\n","=>> Epoch 5  global step 10160 loss 0.36604 batch 1596/1640 lr 0.001 accuracy 93.21875 wps 28315.36 step time 0.55s\n","=>> Epoch 5  global step 10180 loss 0.36529 batch 1616/1640 lr 0.001 accuracy 93.16797 wps 27495.27 step time 0.52s\n","=>> Epoch 5  global step 10200 loss 0.36201 batch 1636/1640 lr 0.001 accuracy 93.42578 wps 27489.98 step time 0.57s\n","=>> Finsh epoch 5, global step 10205\n","=>> Epoch 6  global step 10220 loss 0.25831 batch 15/1640 lr 0.001 accuracy 70.38477 wps 30723.16 step time 0.40s\n","=>> Epoch 6  global step 10240 loss 0.31541 batch 35/1640 lr 0.001 accuracy 94.26758 wps 28534.83 step time 0.38s\n","=>> Epoch 6  global step 10260 loss 0.35643 batch 55/1640 lr 0.001 accuracy 93.49414 wps 32003.88 step time 0.46s\n","=>> Epoch 6  global step 10280 loss 0.33760 batch 75/1640 lr 0.001 accuracy 93.87891 wps 30633.04 step time 0.43s\n","=>> Epoch 6  global step 10300 loss 0.35079 batch 95/1640 lr 0.001 accuracy 93.59961 wps 32583.48 step time 0.50s\n","=>> Epoch 6  global step 10320 loss 0.34553 batch 115/1640 lr 0.001 accuracy 93.65820 wps 30424.12 step time 0.50s\n","=>> Epoch 6  global step 10340 loss 0.32822 batch 135/1640 lr 0.001 accuracy 94.02734 wps 29605.30 step time 0.40s\n","=>> Epoch 6  global step 10360 loss 0.32551 batch 155/1640 lr 0.001 accuracy 94.12500 wps 29253.60 step time 0.40s\n","=>> Epoch 6  global step 10380 loss 0.33076 batch 175/1640 lr 0.001 accuracy 93.98047 wps 30273.01 step time 0.42s\n","=>> Epoch 6  global step 10400 loss 0.35066 batch 195/1640 lr 0.001 accuracy 93.50000 wps 32250.09 step time 0.47s\n","=>> Epoch 6  global step 10420 loss 0.34029 batch 215/1640 lr 0.001 accuracy 93.91406 wps 31202.82 step time 0.44s\n","=>> Epoch 6  global step 10440 loss 0.34763 batch 235/1640 lr 0.001 accuracy 93.70117 wps 31768.40 step time 0.46s\n","=>> Epoch 6  global step 10460 loss 0.33973 batch 255/1640 lr 0.001 accuracy 93.73633 wps 31388.33 step time 0.45s\n","=>> Epoch 6  global step 10480 loss 0.36121 batch 275/1640 lr 0.001 accuracy 93.36523 wps 32080.20 step time 0.47s\n","=>> Epoch 6  global step 10500 loss 0.33378 batch 295/1640 lr 0.001 accuracy 93.83008 wps 30372.49 step time 0.43s\n","=>> Epoch 6  global step 10520 loss 0.35025 batch 315/1640 lr 0.001 accuracy 93.66797 wps 31796.64 step time 0.45s\n","=>> Epoch 6  global step 10540 loss 0.34631 batch 335/1640 lr 0.001 accuracy 93.69531 wps 30756.91 step time 0.50s\n","=>> Epoch 6  global step 10560 loss 0.33605 batch 355/1640 lr 0.001 accuracy 93.97656 wps 30570.95 step time 0.50s\n","=>> Epoch 6  global step 10580 loss 0.35068 batch 375/1640 lr 0.001 accuracy 93.48828 wps 31250.05 step time 0.44s\n","=>> Epoch 6  global step 10600 loss 0.35778 batch 395/1640 lr 0.001 accuracy 93.49805 wps 32681.84 step time 0.50s\n","=>> Epoch 6  global step 10620 loss 0.35666 batch 415/1640 lr 0.001 accuracy 93.42187 wps 32459.43 step time 0.49s\n","=>> Epoch 6  global step 10640 loss 0.33181 batch 435/1640 lr 0.001 accuracy 93.94727 wps 30692.90 step time 0.43s\n","=>> Epoch 6  global step 10660 loss 0.32190 batch 455/1640 lr 0.001 accuracy 94.11133 wps 30838.91 step time 0.43s\n","=>> Epoch 6  global step 10680 loss 0.34173 batch 475/1640 lr 0.001 accuracy 93.76953 wps 31871.66 step time 0.45s\n","=>> Epoch 6  global step 10700 loss 0.33917 batch 495/1640 lr 0.001 accuracy 93.76172 wps 30492.33 step time 0.41s\n","=>> Epoch 6  global step 10720 loss 0.35490 batch 515/1640 lr 0.001 accuracy 93.63672 wps 31065.37 step time 0.50s\n","=>> Epoch 6  global step 10740 loss 0.32728 batch 535/1640 lr 0.001 accuracy 94.06445 wps 29549.68 step time 0.39s\n","=>> Epoch 6  global step 10760 loss 0.34513 batch 555/1640 lr 0.001 accuracy 93.80664 wps 31256.84 step time 0.43s\n","=>> Epoch 6  global step 10780 loss 0.33606 batch 575/1640 lr 0.001 accuracy 93.80273 wps 31043.50 step time 0.42s\n","=>> Epoch 6  global step 10800 loss 0.36144 batch 595/1640 lr 0.001 accuracy 93.43164 wps 32466.21 step time 0.47s\n","=>> Epoch 6  global step 10820 loss 0.36947 batch 615/1640 lr 0.001 accuracy 93.22852 wps 32107.27 step time 0.52s\n","=>> Epoch 6  global step 10840 loss 0.37386 batch 635/1640 lr 0.001 accuracy 93.03320 wps 32821.51 step time 0.54s\n","=>> Epoch 6  global step 10860 loss 0.33732 batch 655/1640 lr 0.001 accuracy 93.87500 wps 31186.85 step time 0.43s\n","=>> Epoch 6  global step 10880 loss 0.34237 batch 675/1640 lr 0.001 accuracy 93.72461 wps 31531.43 step time 0.44s\n","=>> Epoch 6  global step 10900 loss 0.34829 batch 695/1640 lr 0.001 accuracy 93.69336 wps 31776.02 step time 0.45s\n","=>> Epoch 6  global step 10920 loss 0.33557 batch 715/1640 lr 0.001 accuracy 93.91211 wps 32188.30 step time 0.46s\n","=>> Epoch 6  global step 10940 loss 0.34884 batch 735/1640 lr 0.001 accuracy 93.71875 wps 31535.95 step time 0.44s\n","=>> Epoch 6  global step 10960 loss 0.35451 batch 755/1640 lr 0.001 accuracy 93.54102 wps 30377.72 step time 0.48s\n","=>> Epoch 6  global step 10980 loss 0.35002 batch 775/1640 lr 0.001 accuracy 93.56445 wps 32899.25 step time 0.50s\n","=>> Epoch 6  global step 11000 loss 0.34992 batch 795/1640 lr 0.001 accuracy 93.63672 wps 31366.18 step time 0.52s\n","=>> global step 11000, eval result: \n","=>> location_traffic_convenience - 0.6566290301321034\n","=>> location_distance_from_business_district - 0.5511377308774353\n","=>> location_easy_to_find - 0.7024944408414836\n","=>> service_wait_time - 0.6622370576052513\n","=>> service_waiters_attitude - 0.7972539565674307\n","=>> service_parking_convenience - 0.7315149147007588\n","=>> service_serving_speed - 0.7537354280505318\n","=>> price_level - 0.7826668664954993\n","=>> price_cost_effective - 0.7174988321870797\n","=>> price_discount - 0.6650565170480947\n","=>> environment_decoration - 0.7296266988315723\n","=>> environment_noise - 0.7635641979397058\n","=>> environment_space - 0.7676859743509444\n","=>> environment_cleaness - 0.7543053824299872\n","=>> dish_portion - 0.7251196964792279\n","=>> dish_taste - 0.7261591945297499\n","=>> dish_look - 0.5750185043880259\n","=>> dish_recommendation - 0.7377734838025225\n","=>> others_overall_experience - 0.5922961678281878\n","=>> others_willing_to_consume_again - 0.6957421233263048\n","=>> Eval loss 1.70167, f1 0.70438\n","=>> current result -0.7043758099205949, previous best result -0.702632320909246\n","=>> Epoch 6  global step 11020 loss 0.33861 batch 815/1640 lr 0.001 accuracy 93.77734 wps 30142.83 step time 0.45s\n","=>> Epoch 6  global step 11040 loss 0.35338 batch 835/1640 lr 0.001 accuracy 93.57422 wps 31533.73 step time 0.45s\n","=>> Epoch 6  global step 11060 loss 0.34557 batch 855/1640 lr 0.001 accuracy 93.71094 wps 31380.25 step time 0.44s\n","=>> Epoch 6  global step 11080 loss 0.34955 batch 875/1640 lr 0.001 accuracy 93.70508 wps 30179.66 step time 0.42s\n","=>> Epoch 6  global step 11100 loss 0.34822 batch 895/1640 lr 0.001 accuracy 93.45898 wps 31976.13 step time 0.47s\n","=>> Epoch 6  global step 11120 loss 0.33931 batch 915/1640 lr 0.001 accuracy 93.82422 wps 30408.39 step time 0.50s\n","=>> Epoch 6  global step 11140 loss 0.36314 batch 935/1640 lr 0.001 accuracy 93.34961 wps 30635.67 step time 0.50s\n","=>> Epoch 6  global step 11160 loss 0.35269 batch 955/1640 lr 0.001 accuracy 93.35156 wps 31621.56 step time 0.53s\n","=>> Epoch 6  global step 11180 loss 0.35321 batch 975/1640 lr 0.001 accuracy 93.61524 wps 31639.17 step time 0.46s\n","=>> Epoch 6  global step 11200 loss 0.33605 batch 995/1640 lr 0.001 accuracy 93.81055 wps 30367.35 step time 0.43s\n","=>> Epoch 6  global step 11220 loss 0.34982 batch 1015/1640 lr 0.001 accuracy 93.56055 wps 31302.95 step time 0.45s\n","=>> Epoch 6  global step 11240 loss 0.36169 batch 1035/1640 lr 0.001 accuracy 93.34570 wps 31877.46 step time 0.47s\n","=>> Epoch 6  global step 11260 loss 0.35287 batch 1055/1640 lr 0.001 accuracy 93.53906 wps 32106.75 step time 0.48s\n","=>> Epoch 6  global step 11280 loss 0.34284 batch 1075/1640 lr 0.001 accuracy 93.77344 wps 30962.59 step time 0.44s\n","=>> Epoch 6  global step 11300 loss 0.34403 batch 1095/1640 lr 0.001 accuracy 93.77344 wps 30703.78 step time 0.44s\n","=>> Epoch 6  global step 11320 loss 0.36998 batch 1115/1640 lr 0.001 accuracy 93.25781 wps 31888.99 step time 0.46s\n","=>> Epoch 6  global step 11340 loss 0.35279 batch 1135/1640 lr 0.001 accuracy 93.53125 wps 30576.70 step time 0.51s\n","=>> Epoch 6  global step 11360 loss 0.35604 batch 1155/1640 lr 0.001 accuracy 93.50586 wps 32238.23 step time 0.48s\n","=>> Epoch 6  global step 11380 loss 0.34611 batch 1175/1640 lr 0.001 accuracy 93.69141 wps 28297.90 step time 0.47s\n","=>> Epoch 6  global step 11400 loss 0.33941 batch 1195/1640 lr 0.001 accuracy 93.87305 wps 27621.49 step time 0.45s\n","=>> Epoch 6  global step 11420 loss 0.36360 batch 1215/1640 lr 0.001 accuracy 93.43164 wps 26541.29 step time 0.69s\n","=>> Epoch 6  global step 11440 loss 0.36118 batch 1235/1640 lr 0.001 accuracy 93.22852 wps 27970.27 step time 0.54s\n","=>> Epoch 6  global step 11460 loss 0.33750 batch 1255/1640 lr 0.001 accuracy 93.96680 wps 28061.66 step time 0.50s\n","=>> Epoch 6  global step 11480 loss 0.37243 batch 1275/1640 lr 0.001 accuracy 93.17969 wps 26398.36 step time 0.70s\n","=>> Epoch 6  global step 11500 loss 0.34843 batch 1295/1640 lr 0.001 accuracy 93.82617 wps 27359.98 step time 0.53s\n","=>> Epoch 6  global step 11520 loss 0.37838 batch 1315/1640 lr 0.001 accuracy 93.07422 wps 25944.52 step time 0.64s\n","=>> Epoch 6  global step 11540 loss 0.36340 batch 1335/1640 lr 0.001 accuracy 93.33398 wps 27530.41 step time 0.63s\n","=>> Epoch 6  global step 11560 loss 0.35485 batch 1355/1640 lr 0.001 accuracy 93.48437 wps 26207.00 step time 0.64s\n","=>> Epoch 6  global step 11580 loss 0.34831 batch 1375/1640 lr 0.001 accuracy 93.60156 wps 27159.19 step time 0.47s\n","=>> Epoch 6  global step 11600 loss 0.36252 batch 1395/1640 lr 0.001 accuracy 93.28320 wps 26403.88 step time 0.68s\n","=>> Epoch 6  global step 11620 loss 0.36305 batch 1415/1640 lr 0.001 accuracy 93.27734 wps 24343.39 step time 0.76s\n","=>> Epoch 6  global step 11640 loss 0.33692 batch 1435/1640 lr 0.001 accuracy 93.91016 wps 27465.83 step time 0.46s\n","=>> Epoch 6  global step 11660 loss 0.34613 batch 1455/1640 lr 0.001 accuracy 93.58398 wps 26256.29 step time 0.54s\n","=>> Epoch 6  global step 11680 loss 0.34422 batch 1475/1640 lr 0.001 accuracy 93.82031 wps 25799.44 step time 0.55s\n","=>> Epoch 6  global step 11700 loss 0.34670 batch 1495/1640 lr 0.001 accuracy 93.61914 wps 25999.73 step time 0.60s\n","=>> Epoch 6  global step 11720 loss 0.35306 batch 1515/1640 lr 0.001 accuracy 93.48438 wps 26079.95 step time 0.58s\n","=>> Epoch 6  global step 11740 loss 0.34719 batch 1535/1640 lr 0.001 accuracy 93.56250 wps 24833.74 step time 0.52s\n","=>> Epoch 6  global step 11760 loss 0.35444 batch 1555/1640 lr 0.001 accuracy 93.50586 wps 26195.44 step time 0.61s\n","=>> Epoch 6  global step 11780 loss 0.33880 batch 1575/1640 lr 0.001 accuracy 93.74414 wps 27592.91 step time 0.49s\n","=>> Epoch 6  global step 11800 loss 0.36290 batch 1595/1640 lr 0.001 accuracy 93.52148 wps 26645.37 step time 0.66s\n","=>> Epoch 6  global step 11820 loss 0.35300 batch 1615/1640 lr 0.001 accuracy 93.54297 wps 27991.88 step time 0.49s\n","=>> Epoch 6  global step 11840 loss 0.34466 batch 1635/1640 lr 0.001 accuracy 93.59766 wps 25117.37 step time 0.57s\n","=>> Finsh epoch 6, global step 11846\n","=>> Epoch 7  global step 11860 loss 0.24145 batch 14/1640 lr 0.001 accuracy 65.55859 wps 30891.77 step time 0.36s\n","=>> Epoch 7  global step 11880 loss 0.34458 batch 34/1640 lr 0.001 accuracy 93.58203 wps 31474.65 step time 0.45s\n","=>> Epoch 7  global step 11900 loss 0.34870 batch 54/1640 lr 0.001 accuracy 93.56250 wps 32973.75 step time 0.51s\n","=>> Epoch 7  global step 11920 loss 0.33183 batch 74/1640 lr 0.001 accuracy 93.90039 wps 30529.50 step time 0.49s\n","=>> Epoch 7  global step 11940 loss 0.32785 batch 94/1640 lr 0.001 accuracy 94.03320 wps 29394.48 step time 0.40s\n","=>> Epoch 7  global step 11960 loss 0.32860 batch 114/1640 lr 0.001 accuracy 93.95898 wps 29766.19 step time 0.47s\n","=>> Epoch 7  global step 11980 loss 0.33216 batch 134/1640 lr 0.001 accuracy 93.85742 wps 30753.84 step time 0.44s\n","=>> Epoch 7  global step 12000 loss 0.33305 batch 154/1640 lr 0.001 accuracy 93.91016 wps 32106.33 step time 0.47s\n","=>> global step 12000, eval result: \n","=>> location_traffic_convenience - 0.6566889121169028\n","=>> location_distance_from_business_district - 0.5458274714102842\n","=>> location_easy_to_find - 0.7037584312682272\n","=>> service_wait_time - 0.6659452055515038\n","=>> service_waiters_attitude - 0.7973679233704378\n","=>> service_parking_convenience - 0.736125838456066\n","=>> service_serving_speed - 0.7525012630298452\n","=>> price_level - 0.7824290347579439\n","=>> price_cost_effective - 0.7163189902830411\n","=>> price_discount - 0.6672179942203325\n","=>> environment_decoration - 0.7319393631295512\n","=>> environment_noise - 0.7676975212146495\n","=>> environment_space - 0.7696740144451746\n","=>> environment_cleaness - 0.7557324002877004\n","=>> dish_portion - 0.7265761482277918\n","=>> dish_taste - 0.7265415289522563\n","=>> dish_look - 0.5751460572099863\n","=>> dish_recommendation - 0.7378676608951705\n","=>> others_overall_experience - 0.5934274996298865\n","=>> others_willing_to_consume_again - 0.7027294392492119\n","=>> Eval loss 1.69978, f1 0.70558\n","=>> current result -0.7055756348852982, previous best result -0.7043758099205949\n","=>> Epoch 7  global step 12020 loss 0.34510 batch 174/1640 lr 0.001 accuracy 93.67383 wps 30763.70 step time 0.47s\n","=>> Epoch 7  global step 12040 loss 0.33120 batch 194/1640 lr 0.001 accuracy 93.93164 wps 31066.68 step time 0.45s\n","=>> Epoch 7  global step 12060 loss 0.32697 batch 214/1640 lr 0.001 accuracy 93.94336 wps 30399.50 step time 0.49s\n","=>> Epoch 7  global step 12080 loss 0.33741 batch 234/1640 lr 0.001 accuracy 93.88867 wps 32615.99 step time 0.50s\n","=>> Epoch 7  global step 12100 loss 0.35239 batch 254/1640 lr 0.001 accuracy 93.44727 wps 32044.28 step time 0.53s\n","=>> Epoch 7  global step 12120 loss 0.31831 batch 274/1640 lr 0.001 accuracy 94.12500 wps 29938.41 step time 0.48s\n","=>> Epoch 7  global step 12140 loss 0.33488 batch 294/1640 lr 0.001 accuracy 93.88281 wps 31062.57 step time 0.52s\n","=>> Epoch 7  global step 12160 loss 0.33849 batch 314/1640 lr 0.001 accuracy 93.84570 wps 31398.44 step time 0.46s\n","=>> Epoch 7  global step 12180 loss 0.34254 batch 334/1640 lr 0.001 accuracy 93.81836 wps 31222.19 step time 0.44s\n","=>> Epoch 7  global step 12200 loss 0.34178 batch 354/1640 lr 0.001 accuracy 93.68945 wps 32825.96 step time 0.51s\n","=>> Epoch 7  global step 12220 loss 0.34058 batch 374/1640 lr 0.001 accuracy 93.77734 wps 30510.16 step time 0.44s\n","=>> Epoch 7  global step 12240 loss 0.36411 batch 394/1640 lr 0.001 accuracy 93.32226 wps 31845.22 step time 0.63s\n","=>> Epoch 7  global step 12260 loss 0.34415 batch 414/1640 lr 0.001 accuracy 93.73437 wps 31904.18 step time 0.48s\n","=>> Epoch 7  global step 12280 loss 0.35225 batch 434/1640 lr 0.001 accuracy 93.52344 wps 31002.01 step time 0.43s\n","=>> Epoch 7  global step 12300 loss 0.33558 batch 454/1640 lr 0.001 accuracy 94.00391 wps 30536.53 step time 0.49s\n","=>> Epoch 7  global step 12320 loss 0.34345 batch 474/1640 lr 0.001 accuracy 93.69531 wps 31074.44 step time 0.51s\n","=>> Epoch 7  global step 12340 loss 0.34314 batch 494/1640 lr 0.001 accuracy 93.70898 wps 31283.11 step time 0.51s\n","=>> Epoch 7  global step 12360 loss 0.33508 batch 514/1640 lr 0.001 accuracy 93.77734 wps 30637.67 step time 0.43s\n","=>> Epoch 7  global step 12380 loss 0.31430 batch 534/1640 lr 0.001 accuracy 94.21289 wps 30303.68 step time 0.48s\n","=>> Epoch 7  global step 12400 loss 0.31414 batch 554/1640 lr 0.001 accuracy 94.41992 wps 28943.95 step time 0.39s\n","=>> Epoch 7  global step 12420 loss 0.32038 batch 574/1640 lr 0.001 accuracy 94.16406 wps 29375.04 step time 0.40s\n","=>> Epoch 7  global step 12440 loss 0.34434 batch 594/1640 lr 0.001 accuracy 93.67383 wps 32215.17 step time 0.47s\n","=>> Epoch 7  global step 12460 loss 0.33257 batch 614/1640 lr 0.001 accuracy 93.83789 wps 31462.07 step time 0.45s\n","=>> Epoch 7  global step 12480 loss 0.35459 batch 634/1640 lr 0.001 accuracy 93.51563 wps 32989.53 step time 0.51s\n","=>> Epoch 7  global step 12500 loss 0.33984 batch 654/1640 lr 0.001 accuracy 93.73437 wps 30099.83 step time 0.53s\n","=>> Epoch 7  global step 12520 loss 0.35257 batch 674/1640 lr 0.001 accuracy 93.53516 wps 31719.34 step time 0.54s\n","=>> Epoch 7  global step 12540 loss 0.35660 batch 694/1640 lr 0.001 accuracy 93.37305 wps 33020.74 step time 0.56s\n","=>> Epoch 7  global step 12560 loss 0.34802 batch 714/1640 lr 0.001 accuracy 93.60547 wps 31925.73 step time 0.46s\n","=>> Epoch 7  global step 12580 loss 0.34956 batch 734/1640 lr 0.001 accuracy 93.64062 wps 31364.57 step time 0.53s\n","=>> Epoch 7  global step 12600 loss 0.35361 batch 754/1640 lr 0.001 accuracy 93.53711 wps 32047.71 step time 0.55s\n","=>> Epoch 7  global step 12620 loss 0.33992 batch 774/1640 lr 0.001 accuracy 93.81641 wps 31529.13 step time 0.45s\n","=>> Epoch 7  global step 12640 loss 0.33478 batch 794/1640 lr 0.001 accuracy 93.99219 wps 30604.98 step time 0.42s\n","=>> Epoch 7  global step 12660 loss 0.35116 batch 814/1640 lr 0.001 accuracy 93.56055 wps 32080.03 step time 0.48s\n","=>> Epoch 7  global step 12680 loss 0.32833 batch 834/1640 lr 0.001 accuracy 93.95508 wps 29316.98 step time 0.48s\n","=>> Epoch 7  global step 12700 loss 0.33800 batch 854/1640 lr 0.001 accuracy 93.92969 wps 30106.88 step time 0.42s\n","=>> Epoch 7  global step 12720 loss 0.33253 batch 874/1640 lr 0.001 accuracy 93.92578 wps 30479.62 step time 0.42s\n","=>> Epoch 7  global step 12740 loss 0.35979 batch 894/1640 lr 0.001 accuracy 93.54297 wps 32129.14 step time 0.55s\n","=>> Epoch 7  global step 12760 loss 0.33082 batch 914/1640 lr 0.001 accuracy 93.98437 wps 30835.62 step time 0.43s\n","=>> Epoch 7  global step 12780 loss 0.32945 batch 934/1640 lr 0.001 accuracy 93.92969 wps 29924.66 step time 0.41s\n","=>> Epoch 7  global step 12800 loss 0.34484 batch 954/1640 lr 0.001 accuracy 93.71484 wps 31675.41 step time 0.45s\n","=>> Epoch 7  global step 12820 loss 0.34688 batch 974/1640 lr 0.001 accuracy 93.68164 wps 31029.37 step time 0.43s\n","=>> Epoch 7  global step 12840 loss 0.33730 batch 994/1640 lr 0.001 accuracy 93.79297 wps 30535.60 step time 0.43s\n","=>> Epoch 7  global step 12860 loss 0.34375 batch 1014/1640 lr 0.001 accuracy 93.72852 wps 28498.89 step time 0.51s\n","=>> Epoch 7  global step 12880 loss 0.32510 batch 1034/1640 lr 0.001 accuracy 94.02734 wps 27551.01 step time 0.45s\n","=>> Epoch 7  global step 12900 loss 0.34653 batch 1054/1640 lr 0.001 accuracy 93.53516 wps 27572.61 step time 0.60s\n","=>> Epoch 7  global step 12920 loss 0.34933 batch 1074/1640 lr 0.001 accuracy 93.46680 wps 27385.88 step time 0.47s\n","=>> Epoch 7  global step 12940 loss 0.35773 batch 1094/1640 lr 0.001 accuracy 93.39453 wps 26669.43 step time 0.64s\n","=>> Epoch 7  global step 12960 loss 0.33558 batch 1114/1640 lr 0.001 accuracy 93.96094 wps 28250.61 step time 0.48s\n","=>> Epoch 7  global step 12980 loss 0.33449 batch 1134/1640 lr 0.001 accuracy 93.97852 wps 25576.82 step time 0.54s\n","=>> Epoch 7  global step 13000 loss 0.33033 batch 1154/1640 lr 0.001 accuracy 93.97070 wps 27101.55 step time 0.47s\n","=>> global step 13000, eval result: \n","=>> location_traffic_convenience - 0.6567735567407584\n","=>> location_distance_from_business_district - 0.5526684633585385\n","=>> location_easy_to_find - 0.7075322703604433\n","=>> service_wait_time - 0.659320830000854\n","=>> service_waiters_attitude - 0.7998162683938282\n","=>> service_parking_convenience - 0.7323778556704994\n","=>> service_serving_speed - 0.7566508579816209\n","=>> price_level - 0.7832833422002854\n","=>> price_cost_effective - 0.7179313656493774\n","=>> price_discount - 0.6680232007551344\n","=>> environment_decoration - 0.7291691401647824\n","=>> environment_noise - 0.7631631850995396\n","=>> environment_space - 0.768704982028678\n","=>> environment_cleaness - 0.7566072017963335\n","=>> dish_portion - 0.7255025410301157\n","=>> dish_taste - 0.7306122500684108\n","=>> dish_look - 0.5820700214589104\n","=>> dish_recommendation - 0.7353057062086437\n","=>> others_overall_experience - 0.5933745220078455\n","=>> others_willing_to_consume_again - 0.710995236504241\n","=>> Eval loss 1.70457, f1 0.70649\n","=>> current result -0.706494139873942, previous best result -0.7055756348852982\n","=>> Epoch 7  global step 13020 loss 0.34640 batch 1174/1640 lr 0.001 accuracy 93.50000 wps 25810.81 step time 0.68s\n","=>> Epoch 7  global step 13040 loss 0.34575 batch 1194/1640 lr 0.001 accuracy 93.63281 wps 27716.51 step time 0.54s\n","=>> Epoch 7  global step 13060 loss 0.33641 batch 1214/1640 lr 0.001 accuracy 93.75391 wps 26907.26 step time 0.50s\n","=>> Epoch 7  global step 13080 loss 0.33353 batch 1234/1640 lr 0.001 accuracy 93.96680 wps 27079.98 step time 0.50s\n","=>> Epoch 7  global step 13100 loss 0.34238 batch 1254/1640 lr 0.001 accuracy 93.70703 wps 26961.84 step time 0.56s\n","=>> Epoch 7  global step 13120 loss 0.36178 batch 1274/1640 lr 0.001 accuracy 93.28125 wps 27684.63 step time 0.60s\n","=>> Epoch 7  global step 13140 loss 0.34381 batch 1294/1640 lr 0.001 accuracy 93.78516 wps 26751.17 step time 0.53s\n","=>> Epoch 7  global step 13160 loss 0.33628 batch 1314/1640 lr 0.001 accuracy 93.88281 wps 25978.31 step time 0.55s\n","=>> Epoch 7  global step 13180 loss 0.33272 batch 1334/1640 lr 0.001 accuracy 93.98242 wps 27481.83 step time 0.54s\n","=>> Epoch 7  global step 13200 loss 0.33781 batch 1354/1640 lr 0.001 accuracy 93.71289 wps 28811.71 step time 0.47s\n","=>> Epoch 7  global step 13220 loss 0.34329 batch 1374/1640 lr 0.001 accuracy 93.76953 wps 28278.28 step time 0.50s\n","=>> Epoch 7  global step 13240 loss 0.33535 batch 1394/1640 lr 0.001 accuracy 93.81055 wps 27351.92 step time 0.47s\n","=>> Epoch 7  global step 13260 loss 0.34704 batch 1414/1640 lr 0.001 accuracy 93.60938 wps 27372.58 step time 0.59s\n","=>> Epoch 7  global step 13280 loss 0.35569 batch 1434/1640 lr 0.001 accuracy 93.46094 wps 26587.32 step time 0.65s\n","=>> Epoch 7  global step 13300 loss 0.33396 batch 1454/1640 lr 0.001 accuracy 93.94531 wps 27111.64 step time 0.57s\n","=>> Epoch 7  global step 13320 loss 0.34212 batch 1474/1640 lr 0.001 accuracy 93.78320 wps 28411.44 step time 0.50s\n","=>> Epoch 7  global step 13340 loss 0.34768 batch 1494/1640 lr 0.001 accuracy 93.52930 wps 27157.98 step time 0.56s\n","=>> Epoch 7  global step 13360 loss 0.33991 batch 1514/1640 lr 0.001 accuracy 93.84375 wps 25961.88 step time 0.56s\n","=>> Epoch 7  global step 13380 loss 0.35374 batch 1534/1640 lr 0.001 accuracy 93.54688 wps 27212.25 step time 0.55s\n","=>> Epoch 7  global step 13400 loss 0.32622 batch 1554/1640 lr 0.001 accuracy 94.02539 wps 27065.57 step time 0.48s\n","=>> Epoch 7  global step 13420 loss 0.32606 batch 1574/1640 lr 0.001 accuracy 94.06445 wps 26475.70 step time 0.51s\n","=>> Epoch 7  global step 13440 loss 0.32631 batch 1594/1640 lr 0.001 accuracy 94.15430 wps 27502.19 step time 0.43s\n","=>> Epoch 7  global step 13460 loss 0.32114 batch 1614/1640 lr 0.001 accuracy 94.16406 wps 27296.33 step time 0.43s\n","=>> Epoch 7  global step 13480 loss 0.33455 batch 1634/1640 lr 0.001 accuracy 93.99414 wps 27591.12 step time 0.47s\n","=>> Finsh epoch 7, global step 13487\n","=>> Epoch 8  global step 13500 loss 0.22477 batch 13/1640 lr 0.001 accuracy 60.82422 wps 32713.72 step time 0.41s\n","=>> Epoch 8  global step 13520 loss 0.32880 batch 33/1640 lr 0.001 accuracy 93.88867 wps 30793.27 step time 0.42s\n","=>> Epoch 8  global step 13540 loss 0.33332 batch 53/1640 lr 0.001 accuracy 93.94336 wps 30781.71 step time 0.50s\n","=>> Epoch 8  global step 13560 loss 0.33613 batch 73/1640 lr 0.001 accuracy 93.73828 wps 31506.87 step time 0.53s\n","=>> Epoch 8  global step 13580 loss 0.33158 batch 93/1640 lr 0.001 accuracy 93.99414 wps 30999.37 step time 0.50s\n","=>> Epoch 8  global step 13600 loss 0.32827 batch 113/1640 lr 0.001 accuracy 94.03711 wps 31243.33 step time 0.45s\n","=>> Epoch 8  global step 13620 loss 0.32389 batch 133/1640 lr 0.001 accuracy 94.04883 wps 30588.15 step time 0.43s\n","=>> Epoch 8  global step 13640 loss 0.33628 batch 153/1640 lr 0.001 accuracy 93.83008 wps 31643.72 step time 0.46s\n","=>> Epoch 8  global step 13660 loss 0.32813 batch 173/1640 lr 0.001 accuracy 94.04492 wps 30307.84 step time 0.48s\n","=>> Epoch 8  global step 13680 loss 0.31987 batch 193/1640 lr 0.001 accuracy 94.00000 wps 31295.58 step time 0.44s\n","=>> Epoch 8  global step 13700 loss 0.31006 batch 213/1640 lr 0.001 accuracy 94.23633 wps 30480.49 step time 0.43s\n","=>> Epoch 8  global step 13720 loss 0.31631 batch 233/1640 lr 0.001 accuracy 94.26953 wps 30851.38 step time 0.49s\n","=>> Epoch 8  global step 13740 loss 0.32342 batch 253/1640 lr 0.001 accuracy 94.13281 wps 31289.06 step time 0.45s\n","=>> Epoch 8  global step 13760 loss 0.33212 batch 273/1640 lr 0.001 accuracy 94.08008 wps 31427.32 step time 0.45s\n","=>> Epoch 8  global step 13780 loss 0.32629 batch 293/1640 lr 0.001 accuracy 94.01172 wps 29780.77 step time 0.53s\n","=>> Epoch 8  global step 13800 loss 0.32062 batch 313/1640 lr 0.001 accuracy 94.05273 wps 30516.99 step time 0.42s\n","=>> Epoch 8  global step 13820 loss 0.31683 batch 333/1640 lr 0.001 accuracy 94.25391 wps 29992.08 step time 0.41s\n","=>> Epoch 8  global step 13840 loss 0.33767 batch 353/1640 lr 0.001 accuracy 93.79883 wps 31283.13 step time 0.51s\n","=>> Epoch 8  global step 13860 loss 0.31782 batch 373/1640 lr 0.001 accuracy 94.24805 wps 29797.05 step time 0.40s\n","=>> Epoch 8  global step 13880 loss 0.33367 batch 393/1640 lr 0.001 accuracy 93.93750 wps 31738.14 step time 0.46s\n","=>> Epoch 8  global step 13900 loss 0.34099 batch 413/1640 lr 0.001 accuracy 93.63477 wps 33234.04 step time 0.52s\n","=>> Epoch 8  global step 13920 loss 0.32200 batch 433/1640 lr 0.001 accuracy 94.12500 wps 30998.64 step time 0.43s\n","=>> Epoch 8  global step 13940 loss 0.33595 batch 453/1640 lr 0.001 accuracy 93.76953 wps 31348.62 step time 0.50s\n","=>> Epoch 8  global step 13960 loss 0.34693 batch 473/1640 lr 0.001 accuracy 93.60937 wps 33004.46 step time 0.52s\n","=>> Epoch 8  global step 13980 loss 0.33001 batch 493/1640 lr 0.001 accuracy 94.01953 wps 30528.94 step time 0.50s\n","=>> Epoch 8  global step 14000 loss 0.32943 batch 513/1640 lr 0.001 accuracy 93.97070 wps 31420.89 step time 0.45s\n","=>> global step 14000, eval result: \n","=>> location_traffic_convenience - 0.6491200385554876\n","=>> location_distance_from_business_district - 0.5544651443922652\n","=>> location_easy_to_find - 0.7052671930690095\n","=>> service_wait_time - 0.6682226804907686\n","=>> service_waiters_attitude - 0.7985251137018485\n","=>> service_parking_convenience - 0.7432371708513776\n","=>> service_serving_speed - 0.7569377375870587\n","=>> price_level - 0.7827866303259916\n","=>> price_cost_effective - 0.7169041227608907\n","=>> price_discount - 0.6724252421795218\n","=>> environment_decoration - 0.7288800551526884\n","=>> environment_noise - 0.7651462726976557\n","=>> environment_space - 0.767674155172871\n","=>> environment_cleaness - 0.759359966518191\n","=>> dish_portion - 0.7275935245377838\n","=>> dish_taste - 0.7320397895908966\n","=>> dish_look - 0.5801526244789332\n","=>> dish_recommendation - 0.7375463831373663\n","=>> others_overall_experience - 0.5949546761118538\n","=>> others_willing_to_consume_again - 0.7102190725384675\n","=>> Eval loss 1.70894, f1 0.70757\n","=>> current result -0.7075728796925465, previous best result -0.706494139873942\n","=>> Epoch 8  global step 14020 loss 0.34291 batch 533/1640 lr 0.001 accuracy 93.79297 wps 31358.66 step time 0.49s\n","=>> Epoch 8  global step 14040 loss 0.32039 batch 553/1640 lr 0.001 accuracy 94.18945 wps 30523.85 step time 0.42s\n","=>> Epoch 8  global step 14060 loss 0.30989 batch 573/1640 lr 0.001 accuracy 94.39648 wps 29906.03 step time 0.40s\n","=>> Epoch 8  global step 14080 loss 0.34408 batch 593/1640 lr 0.001 accuracy 93.80664 wps 32357.75 step time 0.54s\n","=>> Epoch 8  global step 14100 loss 0.33355 batch 613/1640 lr 0.001 accuracy 93.82031 wps 29057.23 step time 0.49s\n","=>> Epoch 8  global step 14120 loss 0.34598 batch 633/1640 lr 0.001 accuracy 93.65820 wps 27810.75 step time 0.62s\n","=>> Epoch 8  global step 14140 loss 0.33334 batch 653/1640 lr 0.001 accuracy 93.92188 wps 27218.36 step time 0.50s\n","=>> Epoch 8  global step 14160 loss 0.34444 batch 673/1640 lr 0.001 accuracy 93.69727 wps 27984.50 step time 0.53s\n","=>> Epoch 8  global step 14180 loss 0.33215 batch 693/1640 lr 0.001 accuracy 93.92774 wps 26514.62 step time 0.64s\n","=>> Epoch 8  global step 14200 loss 0.33139 batch 713/1640 lr 0.001 accuracy 93.95703 wps 26146.48 step time 0.53s\n","=>> Epoch 8  global step 14220 loss 0.33338 batch 733/1640 lr 0.001 accuracy 93.95117 wps 28192.61 step time 0.52s\n","=>> Epoch 8  global step 14240 loss 0.33299 batch 753/1640 lr 0.001 accuracy 93.90820 wps 28853.60 step time 0.47s\n","=>> Epoch 8  global step 14260 loss 0.33483 batch 773/1640 lr 0.001 accuracy 93.84570 wps 25427.90 step time 0.61s\n","=>> Epoch 8  global step 14280 loss 0.32438 batch 793/1640 lr 0.001 accuracy 94.14062 wps 25801.89 step time 0.55s\n","=>> Epoch 8  global step 14300 loss 0.32556 batch 813/1640 lr 0.001 accuracy 94.02930 wps 27634.77 step time 0.49s\n","=>> Epoch 8  global step 14320 loss 0.33163 batch 833/1640 lr 0.001 accuracy 93.96875 wps 27018.79 step time 0.60s\n","=>> Epoch 8  global step 14340 loss 0.32542 batch 853/1640 lr 0.001 accuracy 94.14453 wps 27713.17 step time 0.54s\n","=>> Epoch 8  global step 14360 loss 0.32115 batch 873/1640 lr 0.001 accuracy 94.18164 wps 27317.25 step time 0.50s\n","=>> Epoch 8  global step 14380 loss 0.34465 batch 893/1640 lr 0.001 accuracy 93.71875 wps 27107.77 step time 0.61s\n","=>> Epoch 8  global step 14400 loss 0.32796 batch 913/1640 lr 0.001 accuracy 93.91602 wps 28291.58 step time 0.50s\n","=>> Epoch 8  global step 14420 loss 0.32521 batch 933/1640 lr 0.001 accuracy 94.04297 wps 26433.31 step time 0.52s\n","=>> Epoch 8  global step 14440 loss 0.33334 batch 953/1640 lr 0.001 accuracy 93.89453 wps 26403.07 step time 0.54s\n","=>> Epoch 8  global step 14460 loss 0.33253 batch 973/1640 lr 0.001 accuracy 93.92578 wps 28952.30 step time 0.51s\n","=>> Epoch 8  global step 14480 loss 0.32163 batch 993/1640 lr 0.001 accuracy 94.09766 wps 27339.07 step time 0.47s\n","=>> Epoch 8  global step 14500 loss 0.32779 batch 1013/1640 lr 0.001 accuracy 94.06445 wps 27730.73 step time 0.51s\n","=>> Epoch 8  global step 14520 loss 0.32562 batch 1033/1640 lr 0.001 accuracy 94.07812 wps 27656.87 step time 0.51s\n","=>> Epoch 8  global step 14540 loss 0.32139 batch 1053/1640 lr 0.001 accuracy 94.16211 wps 27468.55 step time 0.42s\n","=>> Epoch 8  global step 14560 loss 0.33518 batch 1073/1640 lr 0.001 accuracy 93.82422 wps 25894.69 step time 0.58s\n","=>> Epoch 8  global step 14580 loss 0.33731 batch 1093/1640 lr 0.001 accuracy 93.84570 wps 28064.51 step time 0.48s\n","=>> Epoch 8  global step 14600 loss 0.34031 batch 1113/1640 lr 0.001 accuracy 93.77539 wps 27339.33 step time 0.55s\n","=>> Epoch 8  global step 14620 loss 0.33694 batch 1133/1640 lr 0.001 accuracy 93.84766 wps 27234.81 step time 0.53s\n","=>> Epoch 8  global step 14640 loss 0.34890 batch 1153/1640 lr 0.001 accuracy 93.51953 wps 25883.14 step time 0.69s\n","=>> Epoch 8  global step 14660 loss 0.33802 batch 1173/1640 lr 0.001 accuracy 93.88281 wps 27688.94 step time 0.56s\n","=>> Epoch 8  global step 14680 loss 0.32370 batch 1193/1640 lr 0.001 accuracy 94.14453 wps 27222.98 step time 0.43s\n","=>> Epoch 8  global step 14700 loss 0.32744 batch 1213/1640 lr 0.001 accuracy 94.04102 wps 28285.59 step time 0.48s\n","=>> Epoch 8  global step 14720 loss 0.33212 batch 1233/1640 lr 0.001 accuracy 93.88086 wps 27619.31 step time 0.52s\n","=>> Epoch 8  global step 14740 loss 0.34387 batch 1253/1640 lr 0.001 accuracy 93.64453 wps 28083.31 step time 0.54s\n","=>> Epoch 8  global step 14760 loss 0.34819 batch 1273/1640 lr 0.001 accuracy 93.51758 wps 28503.47 step time 0.56s\n","=>> Epoch 8  global step 14780 loss 0.33024 batch 1293/1640 lr 0.001 accuracy 93.92969 wps 27361.19 step time 0.53s\n","=>> Epoch 8  global step 14800 loss 0.33693 batch 1313/1640 lr 0.001 accuracy 93.84766 wps 26587.06 step time 0.57s\n","=>> Epoch 8  global step 14820 loss 0.35433 batch 1333/1640 lr 0.001 accuracy 93.50586 wps 26773.03 step time 0.66s\n","=>> Epoch 8  global step 14840 loss 0.34136 batch 1353/1640 lr 0.001 accuracy 93.78516 wps 27075.81 step time 0.55s\n","=>> Epoch 8  global step 14860 loss 0.35983 batch 1373/1640 lr 0.001 accuracy 93.27344 wps 26175.07 step time 0.68s\n","=>> Epoch 8  global step 14880 loss 0.33461 batch 1393/1640 lr 0.001 accuracy 93.72852 wps 27659.95 step time 0.51s\n","=>> Epoch 8  global step 14900 loss 0.33325 batch 1413/1640 lr 0.001 accuracy 93.94922 wps 27972.31 step time 0.47s\n","=>> Epoch 8  global step 14920 loss 0.32246 batch 1433/1640 lr 0.001 accuracy 94.12695 wps 27580.81 step time 0.50s\n","=>> Epoch 8  global step 14940 loss 0.33119 batch 1453/1640 lr 0.001 accuracy 93.95313 wps 26822.95 step time 0.62s\n","=>> Epoch 8  global step 14960 loss 0.31603 batch 1473/1640 lr 0.001 accuracy 94.17188 wps 27202.40 step time 0.47s\n","=>> Epoch 8  global step 14980 loss 0.33497 batch 1493/1640 lr 0.001 accuracy 93.93359 wps 25987.97 step time 0.58s\n","=>> Epoch 8  global step 15000 loss 0.34364 batch 1513/1640 lr 0.001 accuracy 93.56055 wps 27532.23 step time 0.52s\n","=>> global step 15000, eval result: \n","=>> location_traffic_convenience - 0.6549013791897478\n","=>> location_distance_from_business_district - 0.5521293143181442\n","=>> location_easy_to_find - 0.7074953804314439\n","=>> service_wait_time - 0.6657276569631514\n","=>> service_waiters_attitude - 0.8028641989755305\n","=>> service_parking_convenience - 0.7373401839129603\n","=>> service_serving_speed - 0.7549363874139735\n","=>> price_level - 0.7838491973136974\n","=>> price_cost_effective - 0.7150889676793337\n","=>> price_discount - 0.6737181959211451\n","=>> environment_decoration - 0.7292539982043075\n","=>> environment_noise - 0.7654318915399935\n","=>> environment_space - 0.7677187651008013\n","=>> environment_cleaness - 0.7565327768212671\n","=>> dish_portion - 0.7272045135785213\n","=>> dish_taste - 0.734021400597806\n","=>> dish_look - 0.5786569137155615\n","=>> dish_recommendation - 0.7357893233430708\n","=>> others_overall_experience - 0.5907435336496972\n","=>> others_willing_to_consume_again - 0.713416009194742\n","=>> Eval loss 1.71265, f1 0.70734\n","=>> current result -0.7073409993932447, previous best result -0.7075728796925465\n","=>> Epoch 8  global step 15020 loss 0.32387 batch 1533/1640 lr 0.001 accuracy 94.15625 wps 26304.57 step time 0.47s\n","=>> Epoch 8  global step 15040 loss 0.33598 batch 1553/1640 lr 0.001 accuracy 93.97070 wps 27188.79 step time 0.53s\n","=>> Epoch 8  global step 15060 loss 0.34741 batch 1573/1640 lr 0.001 accuracy 93.70508 wps 26545.35 step time 0.61s\n","=>> Epoch 8  global step 15080 loss 0.32590 batch 1593/1640 lr 0.001 accuracy 94.09961 wps 26125.18 step time 0.57s\n","=>> Epoch 8  global step 15100 loss 0.33143 batch 1613/1640 lr 0.001 accuracy 94.00977 wps 25547.65 step time 0.53s\n","=>> Epoch 8  global step 15120 loss 0.33123 batch 1633/1640 lr 0.001 accuracy 94.00977 wps 27520.59 step time 0.56s\n","=>> Finsh epoch 8, global step 15128\n","=>> Epoch 9  global step 15140 loss 0.19182 batch 12/1640 lr 0.001 accuracy 56.46289 wps 31054.40 step time 0.33s\n","=>> Epoch 9  global step 15160 loss 0.32813 batch 32/1640 lr 0.001 accuracy 93.87891 wps 31538.44 step time 0.46s\n","=>> Epoch 9  global step 15180 loss 0.30464 batch 52/1640 lr 0.001 accuracy 94.46094 wps 29720.56 step time 0.41s\n","=>> Epoch 9  global step 15200 loss 0.33740 batch 72/1640 lr 0.001 accuracy 93.74805 wps 31692.25 step time 0.54s\n","=>> Epoch 9  global step 15220 loss 0.31380 batch 92/1640 lr 0.001 accuracy 94.29492 wps 31804.12 step time 0.46s\n","=>> Epoch 9  global step 15240 loss 0.32658 batch 112/1640 lr 0.001 accuracy 94.07813 wps 31964.33 step time 0.47s\n","=>> Epoch 9  global step 15260 loss 0.31998 batch 132/1640 lr 0.001 accuracy 94.05469 wps 32530.66 step time 0.49s\n","=>> Epoch 9  global step 15280 loss 0.31449 batch 152/1640 lr 0.001 accuracy 94.29688 wps 30702.47 step time 0.44s\n","=>> Epoch 9  global step 15300 loss 0.32872 batch 172/1640 lr 0.001 accuracy 94.02539 wps 31196.14 step time 0.51s\n","=>> Epoch 9  global step 15320 loss 0.29963 batch 192/1640 lr 0.001 accuracy 94.54883 wps 28266.02 step time 0.38s\n","=>> Epoch 9  global step 15340 loss 0.30417 batch 212/1640 lr 0.001 accuracy 94.43164 wps 30268.56 step time 0.49s\n","=>> Epoch 9  global step 15360 loss 0.30390 batch 232/1640 lr 0.001 accuracy 94.52930 wps 30671.48 step time 0.43s\n","=>> Epoch 9  global step 15380 loss 0.31388 batch 252/1640 lr 0.001 accuracy 94.39453 wps 30761.44 step time 0.43s\n","=>> Epoch 9  global step 15400 loss 0.31003 batch 272/1640 lr 0.001 accuracy 94.28906 wps 31565.80 step time 0.46s\n","=>> Epoch 9  global step 15420 loss 0.33299 batch 292/1640 lr 0.001 accuracy 93.91992 wps 30642.05 step time 0.50s\n","=>> Epoch 9  global step 15440 loss 0.33577 batch 312/1640 lr 0.001 accuracy 93.87891 wps 33219.21 step time 0.52s\n","=>> Epoch 9  global step 15460 loss 0.31965 batch 332/1640 lr 0.001 accuracy 94.30469 wps 30998.97 step time 0.52s\n","=>> Epoch 9  global step 15480 loss 0.33871 batch 352/1640 lr 0.001 accuracy 93.77930 wps 33125.18 step time 0.51s\n","=>> Epoch 9  global step 15500 loss 0.33538 batch 372/1640 lr 0.001 accuracy 93.78516 wps 32011.43 step time 0.54s\n","=>> Epoch 9  global step 15520 loss 0.32181 batch 392/1640 lr 0.001 accuracy 94.17578 wps 29743.07 step time 0.41s\n","=>> Epoch 9  global step 15540 loss 0.33257 batch 412/1640 lr 0.001 accuracy 93.88867 wps 30622.06 step time 0.43s\n","=>> Epoch 9  global step 15560 loss 0.32797 batch 432/1640 lr 0.001 accuracy 93.95117 wps 30038.93 step time 0.55s\n","=>> Epoch 9  global step 15580 loss 0.33238 batch 452/1640 lr 0.001 accuracy 93.96484 wps 32454.44 step time 0.50s\n","=>> Epoch 9  global step 15600 loss 0.31579 batch 472/1640 lr 0.001 accuracy 94.26172 wps 29799.69 step time 0.41s\n","=>> Epoch 9  global step 15620 loss 0.32437 batch 492/1640 lr 0.001 accuracy 94.06836 wps 31549.83 step time 0.46s\n","=>> Epoch 9  global step 15640 loss 0.33997 batch 512/1640 lr 0.001 accuracy 93.72461 wps 30279.33 step time 0.62s\n","=>> Epoch 9  global step 15660 loss 0.32428 batch 532/1640 lr 0.001 accuracy 93.98438 wps 30775.86 step time 0.51s\n","=>> Epoch 9  global step 15680 loss 0.31537 batch 552/1640 lr 0.001 accuracy 94.27539 wps 30215.12 step time 0.49s\n","=>> Epoch 9  global step 15700 loss 0.32716 batch 572/1640 lr 0.001 accuracy 93.93945 wps 31354.21 step time 0.50s\n","=>> Epoch 9  global step 15720 loss 0.31238 batch 592/1640 lr 0.001 accuracy 94.31836 wps 31506.71 step time 0.45s\n","=>> Epoch 9  global step 15740 loss 0.33794 batch 612/1640 lr 0.001 accuracy 93.76172 wps 30876.94 step time 0.44s\n","=>> Epoch 9  global step 15760 loss 0.31859 batch 632/1640 lr 0.001 accuracy 94.21094 wps 30500.02 step time 0.50s\n","=>> Epoch 9  global step 15780 loss 0.32704 batch 652/1640 lr 0.001 accuracy 93.92578 wps 31316.44 step time 0.45s\n","=>> Epoch 9  global step 15800 loss 0.28876 batch 672/1640 lr 0.001 accuracy 94.80664 wps 29315.17 step time 0.39s\n","=>> Epoch 9  global step 15820 loss 0.32586 batch 692/1640 lr 0.001 accuracy 94.02148 wps 30495.80 step time 0.49s\n","=>> Epoch 9  global step 15840 loss 0.31946 batch 712/1640 lr 0.001 accuracy 94.13086 wps 30613.91 step time 0.44s\n","=>> Epoch 9  global step 15860 loss 0.32059 batch 732/1640 lr 0.001 accuracy 94.00977 wps 30902.46 step time 0.43s\n","=>> Epoch 9  global step 15880 loss 0.32410 batch 752/1640 lr 0.001 accuracy 94.03125 wps 32567.41 step time 0.49s\n","=>> Epoch 9  global step 15900 loss 0.31375 batch 772/1640 lr 0.001 accuracy 94.40039 wps 30485.74 step time 0.43s\n","=>> Epoch 9  global step 15920 loss 0.33395 batch 792/1640 lr 0.001 accuracy 93.87109 wps 30885.15 step time 0.50s\n","=>> Epoch 9  global step 15940 loss 0.32773 batch 812/1640 lr 0.001 accuracy 93.97461 wps 31711.83 step time 0.46s\n","=>> Epoch 9  global step 15960 loss 0.32479 batch 832/1640 lr 0.001 accuracy 94.17188 wps 31358.95 step time 0.45s\n","=>> Epoch 9  global step 15980 loss 0.33332 batch 852/1640 lr 0.001 accuracy 93.83008 wps 31449.99 step time 0.53s\n","=>> Epoch 9  global step 16000 loss 0.32006 batch 872/1640 lr 0.001 accuracy 94.20898 wps 29753.75 step time 0.46s\n","=>> global step 16000, eval result: \n","=>> location_traffic_convenience - 0.6526386698096216\n","=>> location_distance_from_business_district - 0.5397134849611469\n","=>> location_easy_to_find - 0.7014629538468259\n","=>> service_wait_time - 0.6678318709478491\n","=>> service_waiters_attitude - 0.8025775260865056\n","=>> service_parking_convenience - 0.7349057000755745\n","=>> service_serving_speed - 0.7547041734595485\n","=>> price_level - 0.7836210832547759\n","=>> price_cost_effective - 0.7156662125341339\n","=>> price_discount - 0.6661559736301921\n","=>> environment_decoration - 0.7257199045241536\n","=>> environment_noise - 0.7647859779729557\n","=>> environment_space - 0.7673764710396406\n","=>> environment_cleaness - 0.7580695321481685\n","=>> dish_portion - 0.7254394957988386\n","=>> dish_taste - 0.7324858684244664\n","=>> dish_look - 0.5819994149583934\n","=>> dish_recommendation - 0.7320297560976871\n","=>> others_overall_experience - 0.59341558900567\n","=>> others_willing_to_consume_again - 0.7102077341926012\n","=>> Eval loss 1.72489, f1 0.70554\n","=>> current result -0.7055403696384372, previous best result -0.7075728796925465\n","=>> Epoch 9  global step 16020 loss 0.32097 batch 892/1640 lr 0.001 accuracy 94.20117 wps 27398.13 step time 0.50s\n","=>> Epoch 9  global step 16040 loss 0.31843 batch 912/1640 lr 0.001 accuracy 94.14648 wps 26955.50 step time 0.56s\n","=>> Epoch 9  global step 16060 loss 0.31596 batch 932/1640 lr 0.001 accuracy 94.24414 wps 26651.24 step time 0.47s\n","Traceback (most recent call last):\n","  File \"elmo_run.py\", line 350, in <module>\n","    train_clf(flags)\n","  File \"elmo_run.py\", line 205, in train_clf\n","    run_info=add_summary and flags.debug\n","  File \"/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/mycode/elmo.py\", line 495, in train_clf_one_step\n","    feed_dict=feed_dict)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n","    run_metadata_ptr)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n","    run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n","    return fn(*args)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n","    options, feed_dict, fetch_list, target_list, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4Cjkg5LqpyM0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591264271017,"user_tz":-480,"elapsed":2169151,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"d7ad218b-82fa-4648-bf98-b02ef35d0584"},"source":["# softmax global loss + fix embedding\n","!python elmo_run.py \\\n","--mode=train \\\n","--data_files=data/train_not_aug.json \\\n","--eval_files=data/validation.json \\\n","--label_file=data/labels.txt \\\n","--vocab_file=data/vocab.txt \\\n","--embed_file=data/embedding.txt \\\n","--num_layers=3 \\\n","--batch_size=64 \\\n","--max_len=1200 \\\n","--rnn_cell_name='WEIGHT_LSTM' \\\n","--fix_embedding=True \\\n","--linear_dropout=0.5 \\\n","--num_classes_each_label=4 \\\n","--num_labels=20 \\\n","--steps_per_eval=1000 \\\n","--optimizer='rms' \\\n","--learning_rate=0.001 \\\n","--focal_loss_gamma=0 \\\n","--label_smoothing=0 \\\n","--checkpoint_dir=output/elmo_origin_loss_fix \\\n","--checkpoint_load_step=12000 \\\n","--previous_best_eval=-0.7055756348852982"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/train_not_aug.json ...\n","# Got 105000 data items with 1640 batches\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/validation.json ...\n","# Got 15000 data items with 117 batches\n","  saving config to output/elmo_origin_loss_fix/config\n","mode=train,data_files=['data/train_not_aug.json'],eval_files=['data/validation.json'],label_file=data/labels.txt,vocab_file=data/vocab.txt,embed_file=data/embedding.txt,out_file=None,split_word=True,reverse=False,weight_file=None,prob=False,max_len=1200,batch_size=64,num_layers=3,optimizer=rms,learning_rate=0.001,decay_schema=hand,decay_steps=10000,loss_name=softmax,focal_loss_gamma=0.0,max_gradient_norm=2.0,l2_loss_ratio=0.0,label_smoothing=0.0,embedding_dropout=0.1,dropout_keep_prob=0.8,weight_keep_drop=0.8,linear_dropout=0.5,rnn_cell_name=WEIGHT_LSTM,embedding_size=300,num_units=300,num_classes_each_label=4,num_labels=20,fix_embedding=True,need_early_stop=True,patient=5,debug=False,num_train_epoch=50,steps_per_stats=20,steps_per_summary=50,steps_per_eval=1000,checkpoint_dir=output/elmo_origin_loss_fix,checkpoint_load_step=12000,previous_best_eval=-0.7055756348852982,vocab_size=50000\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/weight:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","variable <tf.Variable 'elmo_encoder/scalar:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/memory_layer/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/attention_op/dense/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/dense_1/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense_1/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/drop_connect_layer/kernel:0' shape=(300, 1200) dtype=float32_ref> with parameter number 360000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/luong_attention/attention_g:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/attention_layer/kernel:0' shape=(900, 300) dtype=float32_ref> with parameter number 270000\n","variable <tf.Variable 'classifier/predict_clf/dense/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/predict_clf/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/predict_clf/dense_1/kernel:0' shape=(300, 4) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/predict_clf/dense_1/bias:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","# total parameter number 8560510\n","loading config from output/elmo_origin_loss_fix/config\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","output/elmo_origin_loss_fix/model.ckpt-12000\n","\n","!!! Restored model\n","=>> Start to train with learning rate 0.001\n","=>> Global step 12000\n","=>> Epoch 1  global step 12020 loss 0.32251 batch 20/1640 lr 0.001 accuracy 94.19922 wps 21407.24 step time 0.65s\n","=>> Epoch 1  global step 12040 loss 0.31598 batch 40/1640 lr 0.001 accuracy 94.24414 wps 35485.84 step time 0.36s\n","=>> Epoch 1  global step 12060 loss 0.34094 batch 60/1640 lr 0.001 accuracy 93.77930 wps 34543.77 step time 0.44s\n","=>> Epoch 1  global step 12080 loss 0.31392 batch 80/1640 lr 0.001 accuracy 94.31836 wps 35402.26 step time 0.35s\n","=>> Epoch 1  global step 12100 loss 0.32145 batch 100/1640 lr 0.001 accuracy 94.09375 wps 35993.91 step time 0.38s\n","=>> Epoch 1  global step 12120 loss 0.33097 batch 120/1640 lr 0.001 accuracy 93.91406 wps 34603.40 step time 0.42s\n","=>> Epoch 1  global step 12140 loss 0.33219 batch 140/1640 lr 0.001 accuracy 93.96289 wps 36503.59 step time 0.40s\n","=>> Epoch 1  global step 12160 loss 0.33459 batch 160/1640 lr 0.001 accuracy 93.91797 wps 36141.53 step time 0.38s\n","=>> Epoch 1  global step 12180 loss 0.32545 batch 180/1640 lr 0.001 accuracy 94.06250 wps 35654.12 step time 0.37s\n","=>> Epoch 1  global step 12200 loss 0.33217 batch 200/1640 lr 0.001 accuracy 93.96680 wps 35388.56 step time 0.36s\n","=>> Epoch 1  global step 12220 loss 0.34737 batch 220/1640 lr 0.001 accuracy 93.68359 wps 35409.94 step time 0.53s\n","=>> Epoch 1  global step 12240 loss 0.33600 batch 240/1640 lr 0.001 accuracy 93.87891 wps 36682.77 step time 0.40s\n","=>> Epoch 1  global step 12260 loss 0.33857 batch 260/1640 lr 0.001 accuracy 93.84180 wps 34147.09 step time 0.41s\n","=>> Epoch 1  global step 12280 loss 0.32346 batch 280/1640 lr 0.001 accuracy 94.10937 wps 36291.07 step time 0.39s\n","=>> Epoch 1  global step 12300 loss 0.34854 batch 300/1640 lr 0.001 accuracy 93.56445 wps 37057.91 step time 0.43s\n","=>> Epoch 1  global step 12320 loss 0.35081 batch 320/1640 lr 0.001 accuracy 93.56836 wps 37225.52 step time 0.44s\n","=>> Epoch 1  global step 12340 loss 0.34446 batch 340/1640 lr 0.001 accuracy 93.64453 wps 36862.73 step time 0.41s\n","=>> Epoch 1  global step 12360 loss 0.33314 batch 360/1640 lr 0.001 accuracy 93.95898 wps 36241.70 step time 0.38s\n","=>> Epoch 1  global step 12380 loss 0.35750 batch 380/1640 lr 0.001 accuracy 93.42969 wps 36448.44 step time 0.51s\n","=>> Epoch 1  global step 12400 loss 0.35395 batch 400/1640 lr 0.001 accuracy 93.54688 wps 35209.64 step time 0.43s\n","=>> Epoch 1  global step 12420 loss 0.32773 batch 420/1640 lr 0.001 accuracy 94.12695 wps 35334.91 step time 0.35s\n","=>> Epoch 1  global step 12440 loss 0.33515 batch 440/1640 lr 0.001 accuracy 93.90234 wps 34695.02 step time 0.42s\n","=>> Epoch 1  global step 12460 loss 0.32575 batch 460/1640 lr 0.001 accuracy 94.11719 wps 32622.37 step time 0.44s\n","=>> Epoch 1  global step 12480 loss 0.32948 batch 480/1640 lr 0.001 accuracy 93.95898 wps 32841.02 step time 0.42s\n","=>> Epoch 1  global step 12500 loss 0.36855 batch 500/1640 lr 0.001 accuracy 93.18555 wps 30822.78 step time 0.59s\n","=>> Epoch 1  global step 12520 loss 0.33764 batch 520/1640 lr 0.001 accuracy 93.81445 wps 31959.06 step time 0.44s\n","=>> Epoch 1  global step 12540 loss 0.32754 batch 540/1640 lr 0.001 accuracy 94.08203 wps 29988.57 step time 0.46s\n","=>> Epoch 1  global step 12560 loss 0.34165 batch 560/1640 lr 0.001 accuracy 93.74805 wps 32807.90 step time 0.45s\n","=>> Epoch 1  global step 12580 loss 0.33191 batch 580/1640 lr 0.001 accuracy 94.01563 wps 31119.16 step time 0.43s\n","=>> Epoch 1  global step 12600 loss 0.32415 batch 600/1640 lr 0.001 accuracy 94.05859 wps 31147.04 step time 0.46s\n","=>> Epoch 1  global step 12620 loss 0.33356 batch 620/1640 lr 0.001 accuracy 93.87695 wps 32607.39 step time 0.44s\n","=>> Epoch 1  global step 12640 loss 0.32271 batch 640/1640 lr 0.001 accuracy 94.12109 wps 31686.40 step time 0.38s\n","=>> Epoch 1  global step 12660 loss 0.34482 batch 660/1640 lr 0.001 accuracy 93.74219 wps 30368.04 step time 0.53s\n","=>> Epoch 1  global step 12680 loss 0.32226 batch 680/1640 lr 0.001 accuracy 94.09961 wps 32258.30 step time 0.43s\n","=>> Epoch 1  global step 12700 loss 0.32928 batch 700/1640 lr 0.001 accuracy 94.16016 wps 31780.47 step time 0.49s\n","=>> Epoch 1  global step 12720 loss 0.31940 batch 720/1640 lr 0.001 accuracy 94.10742 wps 31132.51 step time 0.40s\n","=>> Epoch 1  global step 12740 loss 0.33793 batch 740/1640 lr 0.001 accuracy 93.77148 wps 31356.97 step time 0.48s\n","=>> Epoch 1  global step 12760 loss 0.34131 batch 760/1640 lr 0.001 accuracy 93.82617 wps 29264.39 step time 0.54s\n","=>> Epoch 1  global step 12780 loss 0.34049 batch 780/1640 lr 0.001 accuracy 93.84180 wps 32298.94 step time 0.45s\n","=>> Epoch 1  global step 12800 loss 0.32032 batch 800/1640 lr 0.001 accuracy 94.16406 wps 31709.49 step time 0.38s\n","=>> Epoch 1  global step 12820 loss 0.32949 batch 820/1640 lr 0.001 accuracy 93.90820 wps 30849.63 step time 0.50s\n","=>> Epoch 1  global step 12840 loss 0.32638 batch 840/1640 lr 0.001 accuracy 94.12109 wps 29917.39 step time 0.44s\n","=>> Epoch 1  global step 12860 loss 0.35036 batch 860/1640 lr 0.001 accuracy 93.52148 wps 31741.79 step time 0.45s\n","=>> Epoch 1  global step 12880 loss 0.31584 batch 880/1640 lr 0.001 accuracy 94.20898 wps 30869.95 step time 0.37s\n","=>> Epoch 1  global step 12900 loss 0.33520 batch 900/1640 lr 0.001 accuracy 93.86133 wps 31905.90 step time 0.46s\n","=>> Epoch 1  global step 12920 loss 0.32911 batch 920/1640 lr 0.001 accuracy 94.00000 wps 27282.67 step time 0.58s\n","=>> Epoch 1  global step 12940 loss 0.33019 batch 940/1640 lr 0.001 accuracy 94.03125 wps 30579.64 step time 0.44s\n","=>> Epoch 1  global step 12960 loss 0.35449 batch 960/1640 lr 0.001 accuracy 93.42969 wps 32255.93 step time 0.54s\n","=>> Epoch 1  global step 12980 loss 0.33432 batch 980/1640 lr 0.001 accuracy 93.91602 wps 30757.78 step time 0.48s\n","=>> Epoch 1  global step 13000 loss 0.33489 batch 1000/1640 lr 0.001 accuracy 93.91211 wps 31211.66 step time 0.43s\n","=>> global step 13000, eval result: \n","=>> location_traffic_convenience - 0.6578522231594791\n","=>> location_distance_from_business_district - 0.5484248465641791\n","=>> location_easy_to_find - 0.710120074264978\n","=>> service_wait_time - 0.6613575689948931\n","=>> service_waiters_attitude - 0.7998687434519531\n","=>> service_parking_convenience - 0.735824816247383\n","=>> service_serving_speed - 0.7528377237574155\n","=>> price_level - 0.782406617215668\n","=>> price_cost_effective - 0.7165319192797377\n","=>> price_discount - 0.6656107452884892\n","=>> environment_decoration - 0.7269707228275137\n","=>> environment_noise - 0.7679451702357561\n","=>> environment_space - 0.7684826558822577\n","=>> environment_cleaness - 0.7594846432019868\n","=>> dish_portion - 0.7249702679765972\n","=>> dish_taste - 0.7265776456179102\n","=>> dish_look - 0.5859437314851702\n","=>> dish_recommendation - 0.7426784651975991\n","=>> others_overall_experience - 0.5940638773057232\n","=>> others_willing_to_consume_again - 0.7047717253985077\n","=>> Eval loss 1.70534, f1 0.70664\n","=>> current result -0.7066362091676599, previous best result -0.7055756348852982\n","=>> Epoch 1  global step 13020 loss 0.35461 batch 1020/1640 lr 0.001 accuracy 93.43359 wps 30215.77 step time 0.58s\n","=>> Epoch 1  global step 13040 loss 0.34304 batch 1040/1640 lr 0.001 accuracy 93.64453 wps 31648.25 step time 0.45s\n","=>> Epoch 1  global step 13060 loss 0.33217 batch 1060/1640 lr 0.001 accuracy 93.89648 wps 29944.52 step time 0.48s\n","=>> Epoch 1  global step 13080 loss 0.34529 batch 1080/1640 lr 0.001 accuracy 93.67383 wps 30997.43 step time 0.46s\n","=>> Epoch 1  global step 13100 loss 0.34669 batch 1100/1640 lr 0.001 accuracy 93.61914 wps 30067.99 step time 0.50s\n","=>> Epoch 1  global step 13120 loss 0.33955 batch 1120/1640 lr 0.001 accuracy 93.77344 wps 28466.05 step time 0.54s\n","=>> Epoch 1  global step 13140 loss 0.34780 batch 1140/1640 lr 0.001 accuracy 93.65039 wps 29950.66 step time 0.53s\n","=>> Epoch 1  global step 13160 loss 0.34046 batch 1160/1640 lr 0.001 accuracy 93.70898 wps 32248.05 step time 0.48s\n","=>> Epoch 1  global step 13180 loss 0.32596 batch 1180/1640 lr 0.001 accuracy 93.97852 wps 30997.84 step time 0.49s\n","=>> Epoch 1  global step 13200 loss 0.33867 batch 1200/1640 lr 0.001 accuracy 93.80664 wps 29925.39 step time 0.50s\n","=>> Epoch 1  global step 13220 loss 0.30567 batch 1220/1640 lr 0.001 accuracy 94.40625 wps 32281.48 step time 0.36s\n","=>> Epoch 1  global step 13240 loss 0.32839 batch 1240/1640 lr 0.001 accuracy 93.97070 wps 28424.36 step time 0.53s\n","=>> Epoch 1  global step 13260 loss 0.31910 batch 1260/1640 lr 0.001 accuracy 94.07812 wps 31647.58 step time 0.41s\n","=>> Epoch 1  global step 13280 loss 0.33463 batch 1280/1640 lr 0.001 accuracy 93.73633 wps 30571.84 step time 0.45s\n","=>> Epoch 1  global step 13300 loss 0.36050 batch 1300/1640 lr 0.001 accuracy 93.28711 wps 31132.83 step time 0.58s\n","=>> Epoch 1  global step 13320 loss 0.34625 batch 1320/1640 lr 0.001 accuracy 93.66602 wps 30742.58 step time 0.53s\n","=>> Epoch 1  global step 13340 loss 0.33473 batch 1340/1640 lr 0.001 accuracy 93.81641 wps 30907.21 step time 0.47s\n","=>> Epoch 1  global step 13360 loss 0.32852 batch 1360/1640 lr 0.001 accuracy 94.06055 wps 30434.80 step time 0.44s\n","=>> Epoch 1  global step 13380 loss 0.32629 batch 1380/1640 lr 0.001 accuracy 94.04883 wps 31313.17 step time 0.46s\n","=>> Epoch 1  global step 13400 loss 0.34281 batch 1400/1640 lr 0.001 accuracy 93.67773 wps 28653.33 step time 0.54s\n","=>> Epoch 1  global step 13420 loss 0.35281 batch 1420/1640 lr 0.001 accuracy 93.58008 wps 31722.01 step time 0.47s\n","=>> Epoch 1  global step 13440 loss 0.34680 batch 1440/1640 lr 0.001 accuracy 93.63281 wps 29742.26 step time 0.53s\n","=>> Epoch 1  global step 13460 loss 0.33989 batch 1460/1640 lr 0.001 accuracy 93.75000 wps 31944.47 step time 0.39s\n","=>> Epoch 1  global step 13480 loss 0.35452 batch 1480/1640 lr 0.001 accuracy 93.58594 wps 29661.52 step time 0.53s\n","=>> Epoch 1  global step 13500 loss 0.34014 batch 1500/1640 lr 0.001 accuracy 93.83008 wps 27070.61 step time 0.65s\n","=>> Epoch 1  global step 13520 loss 0.35802 batch 1520/1640 lr 0.001 accuracy 93.44922 wps 30620.34 step time 0.57s\n","=>> Epoch 1  global step 13540 loss 0.32926 batch 1540/1640 lr 0.001 accuracy 94.00781 wps 30724.20 step time 0.43s\n","=>> Epoch 1  global step 13560 loss 0.34445 batch 1560/1640 lr 0.001 accuracy 93.64453 wps 30788.26 step time 0.44s\n","=>> Epoch 1  global step 13580 loss 0.36522 batch 1580/1640 lr 0.001 accuracy 93.19726 wps 27744.35 step time 0.59s\n","=>> Epoch 1  global step 13600 loss 0.36529 batch 1600/1640 lr 0.001 accuracy 93.21875 wps 30381.18 step time 0.58s\n","=>> Epoch 1  global step 13620 loss 0.34820 batch 1620/1640 lr 0.001 accuracy 93.59766 wps 32114.04 step time 0.48s\n","=>> Epoch 1  global step 13640 loss 0.33485 batch 1640/1640 lr 0.001 accuracy 93.85938 wps 31100.43 step time 0.39s\n","=>> Finsh epoch 1, global step 13641\n","=>> Epoch 2  global step 13660 loss 0.32193 batch 19/1640 lr 0.001 accuracy 89.01367 wps 36179.16 step time 0.46s\n","=>> Epoch 2  global step 13680 loss 0.32660 batch 39/1640 lr 0.001 accuracy 94.01367 wps 35271.33 step time 0.39s\n","=>> Epoch 2  global step 13700 loss 0.33170 batch 59/1640 lr 0.001 accuracy 93.82422 wps 35652.08 step time 0.45s\n","=>> Epoch 2  global step 13720 loss 0.32748 batch 79/1640 lr 0.001 accuracy 93.96094 wps 36289.61 step time 0.38s\n","=>> Epoch 2  global step 13740 loss 0.34091 batch 99/1640 lr 0.001 accuracy 93.78125 wps 36137.23 step time 0.46s\n","=>> Epoch 2  global step 13760 loss 0.33586 batch 119/1640 lr 0.001 accuracy 93.72461 wps 34229.39 step time 0.49s\n","=>> Epoch 2  global step 13780 loss 0.32671 batch 139/1640 lr 0.001 accuracy 93.91797 wps 36297.46 step time 0.39s\n","=>> Epoch 2  global step 13800 loss 0.32944 batch 159/1640 lr 0.001 accuracy 94.07617 wps 36871.05 step time 0.40s\n","=>> Epoch 2  global step 13820 loss 0.34353 batch 179/1640 lr 0.001 accuracy 93.60156 wps 37499.17 step time 0.43s\n","=>> Epoch 2  global step 13840 loss 0.33401 batch 199/1640 lr 0.001 accuracy 93.75391 wps 35560.14 step time 0.46s\n","=>> Epoch 2  global step 13860 loss 0.32192 batch 219/1640 lr 0.001 accuracy 94.16992 wps 35871.10 step time 0.36s\n","=>> Epoch 2  global step 13880 loss 0.34084 batch 239/1640 lr 0.001 accuracy 93.66016 wps 35899.60 step time 0.48s\n","=>> Epoch 2  global step 13900 loss 0.35841 batch 259/1640 lr 0.001 accuracy 93.44336 wps 36130.07 step time 0.58s\n","=>> Epoch 2  global step 13920 loss 0.32041 batch 279/1640 lr 0.001 accuracy 94.15625 wps 36063.77 step time 0.38s\n","=>> Epoch 2  global step 13940 loss 0.33028 batch 299/1640 lr 0.001 accuracy 94.02344 wps 36945.00 step time 0.41s\n","=>> Epoch 2  global step 13960 loss 0.32056 batch 319/1640 lr 0.001 accuracy 94.07813 wps 36221.86 step time 0.39s\n","=>> Epoch 2  global step 13980 loss 0.32459 batch 339/1640 lr 0.001 accuracy 93.99023 wps 35572.27 step time 0.35s\n","=>> Epoch 2  global step 14000 loss 0.31665 batch 359/1640 lr 0.001 accuracy 94.25781 wps 35666.70 step time 0.37s\n","=>> global step 14000, eval result: \n","=>> location_traffic_convenience - 0.662093636693699\n","=>> location_distance_from_business_district - 0.5468036231473099\n","=>> location_easy_to_find - 0.7064220021165969\n","=>> service_wait_time - 0.662025806102245\n","=>> service_waiters_attitude - 0.7980560825087556\n","=>> service_parking_convenience - 0.7411497567586184\n","=>> service_serving_speed - 0.755672309688454\n","=>> price_level - 0.7841650934050723\n","=>> price_cost_effective - 0.7171360238505545\n","=>> price_discount - 0.6672261938543237\n","=>> environment_decoration - 0.7275872014423436\n","=>> environment_noise - 0.7672964451142171\n","=>> environment_space - 0.7690219716688096\n","=>> environment_cleaness - 0.7573838580094462\n","=>> dish_portion - 0.7267486854449742\n","=>> dish_taste - 0.7292953248173856\n","=>> dish_look - 0.5819108897985716\n","=>> dish_recommendation - 0.7384728206535732\n","=>> others_overall_experience - 0.5949163040899151\n","=>> others_willing_to_consume_again - 0.7036095319521916\n","=>> Eval loss 1.70932, f1 0.70685\n","=>> current result -0.7068496780558529, previous best result -0.7066362091676599\n","=>> Epoch 2  global step 14020 loss 0.35554 batch 379/1640 lr 0.001 accuracy 93.47656 wps 34876.02 step time 0.48s\n","=>> Epoch 2  global step 14040 loss 0.32243 batch 399/1640 lr 0.001 accuracy 94.09961 wps 34915.40 step time 0.42s\n","=>> Epoch 2  global step 14060 loss 0.34190 batch 419/1640 lr 0.001 accuracy 93.60938 wps 36674.39 step time 0.41s\n","=>> Epoch 2  global step 14080 loss 0.30269 batch 439/1640 lr 0.001 accuracy 94.37891 wps 35497.13 step time 0.35s\n","=>> Epoch 2  global step 14100 loss 0.34130 batch 459/1640 lr 0.001 accuracy 93.78516 wps 36074.41 step time 0.47s\n","=>> Epoch 2  global step 14120 loss 0.31109 batch 479/1640 lr 0.001 accuracy 94.37891 wps 35774.12 step time 0.37s\n","=>> Epoch 2  global step 14140 loss 0.31799 batch 499/1640 lr 0.001 accuracy 94.27539 wps 35626.08 step time 0.37s\n","=>> Epoch 2  global step 14160 loss 0.31469 batch 519/1640 lr 0.001 accuracy 94.38281 wps 35317.71 step time 0.36s\n","=>> Epoch 2  global step 14180 loss 0.35480 batch 539/1640 lr 0.001 accuracy 93.57031 wps 36232.48 step time 0.50s\n","=>> Epoch 2  global step 14200 loss 0.33798 batch 559/1640 lr 0.001 accuracy 93.78516 wps 35804.93 step time 0.47s\n","=>> Epoch 2  global step 14220 loss 0.34949 batch 579/1640 lr 0.001 accuracy 93.51367 wps 37627.94 step time 0.44s\n","=>> Epoch 2  global step 14240 loss 0.32554 batch 599/1640 lr 0.001 accuracy 94.11133 wps 36027.38 step time 0.39s\n","=>> Epoch 2  global step 14260 loss 0.31425 batch 619/1640 lr 0.001 accuracy 94.31641 wps 35141.71 step time 0.34s\n","=>> Epoch 2  global step 14280 loss 0.32896 batch 639/1640 lr 0.001 accuracy 93.92969 wps 35444.86 step time 0.36s\n","=>> Epoch 2  global step 14300 loss 0.33325 batch 659/1640 lr 0.001 accuracy 93.82031 wps 36819.62 step time 0.40s\n","=>> Epoch 2  global step 14320 loss 0.34187 batch 679/1640 lr 0.001 accuracy 93.75391 wps 35861.24 step time 0.46s\n","=>> Epoch 2  global step 14340 loss 0.36141 batch 699/1640 lr 0.001 accuracy 93.28906 wps 33804.30 step time 0.50s\n","=>> Epoch 2  global step 14360 loss 0.32227 batch 719/1640 lr 0.001 accuracy 94.07031 wps 35603.22 step time 0.43s\n","=>> Epoch 2  global step 14380 loss 0.33601 batch 739/1640 lr 0.001 accuracy 93.92773 wps 36865.31 step time 0.40s\n","=>> Epoch 2  global step 14400 loss 0.32454 batch 759/1640 lr 0.001 accuracy 94.06836 wps 36870.79 step time 0.40s\n","=>> Epoch 2  global step 14420 loss 0.32506 batch 779/1640 lr 0.001 accuracy 94.10547 wps 35759.60 step time 0.37s\n","=>> Epoch 2  global step 14440 loss 0.30902 batch 799/1640 lr 0.001 accuracy 94.33398 wps 35305.88 step time 0.35s\n","=>> Epoch 2  global step 14460 loss 0.33412 batch 819/1640 lr 0.001 accuracy 93.91016 wps 37147.93 step time 0.40s\n","=>> Epoch 2  global step 14480 loss 0.32428 batch 839/1640 lr 0.001 accuracy 94.02539 wps 36996.80 step time 0.41s\n","=>> Epoch 2  global step 14500 loss 0.32444 batch 859/1640 lr 0.001 accuracy 94.00586 wps 36302.51 step time 0.38s\n","=>> Epoch 2  global step 14520 loss 0.32513 batch 879/1640 lr 0.001 accuracy 94.10742 wps 34900.31 step time 0.41s\n","=>> Epoch 2  global step 14540 loss 0.33716 batch 899/1640 lr 0.001 accuracy 93.84766 wps 37079.61 step time 0.41s\n","=>> Epoch 2  global step 14560 loss 0.33580 batch 919/1640 lr 0.001 accuracy 93.85937 wps 34848.86 step time 0.42s\n","=>> Epoch 2  global step 14580 loss 0.33832 batch 939/1640 lr 0.001 accuracy 93.73828 wps 35802.84 step time 0.37s\n","=>> Epoch 2  global step 14600 loss 0.32329 batch 959/1640 lr 0.001 accuracy 94.02148 wps 35723.57 step time 0.37s\n","=>> Epoch 2  global step 14620 loss 0.33188 batch 979/1640 lr 0.001 accuracy 93.97461 wps 35356.63 step time 0.43s\n","=>> Epoch 2  global step 14640 loss 0.35010 batch 999/1640 lr 0.001 accuracy 93.58789 wps 34821.88 step time 0.46s\n","=>> Epoch 2  global step 14660 loss 0.32476 batch 1019/1640 lr 0.001 accuracy 94.21484 wps 31262.42 step time 0.43s\n","=>> Epoch 2  global step 14680 loss 0.34995 batch 1039/1640 lr 0.001 accuracy 93.59180 wps 30739.22 step time 0.52s\n","=>> Epoch 2  global step 14700 loss 0.33955 batch 1059/1640 lr 0.001 accuracy 93.82812 wps 31532.57 step time 0.49s\n","=>> Epoch 2  global step 14720 loss 0.32202 batch 1079/1640 lr 0.001 accuracy 94.12109 wps 31308.57 step time 0.38s\n","=>> Epoch 2  global step 14740 loss 0.33261 batch 1099/1640 lr 0.001 accuracy 93.91406 wps 27814.00 step time 0.56s\n","=>> Epoch 2  global step 14760 loss 0.33329 batch 1119/1640 lr 0.001 accuracy 93.82422 wps 32473.50 step time 0.46s\n","=>> Epoch 2  global step 14780 loss 0.30955 batch 1139/1640 lr 0.001 accuracy 94.35156 wps 31266.62 step time 0.40s\n","=>> Epoch 2  global step 14800 loss 0.31756 batch 1159/1640 lr 0.001 accuracy 94.30273 wps 30936.64 step time 0.40s\n","=>> Epoch 2  global step 14820 loss 0.32719 batch 1179/1640 lr 0.001 accuracy 94.06641 wps 30640.16 step time 0.42s\n","=>> Epoch 2  global step 14840 loss 0.33532 batch 1199/1640 lr 0.001 accuracy 93.90820 wps 32168.09 step time 0.41s\n","=>> Epoch 2  global step 14860 loss 0.32788 batch 1219/1640 lr 0.001 accuracy 93.96680 wps 32081.30 step time 0.42s\n","=>> Epoch 2  global step 14880 loss 0.31475 batch 1239/1640 lr 0.001 accuracy 94.31055 wps 30666.75 step time 0.40s\n","=>> Epoch 2  global step 14900 loss 0.31652 batch 1259/1640 lr 0.001 accuracy 94.26367 wps 31840.67 step time 0.40s\n","=>> Epoch 2  global step 14920 loss 0.33647 batch 1279/1640 lr 0.001 accuracy 93.87109 wps 30911.71 step time 0.46s\n","=>> Epoch 2  global step 14940 loss 0.32096 batch 1299/1640 lr 0.001 accuracy 94.08984 wps 31638.01 step time 0.43s\n","=>> Epoch 2  global step 14960 loss 0.33116 batch 1319/1640 lr 0.001 accuracy 93.88477 wps 29299.82 step time 0.49s\n","=>> Epoch 2  global step 14980 loss 0.35239 batch 1339/1640 lr 0.001 accuracy 93.50391 wps 30135.15 step time 0.55s\n","=>> Epoch 2  global step 15000 loss 0.31727 batch 1359/1640 lr 0.001 accuracy 94.22070 wps 31206.29 step time 0.45s\n","=>> global step 15000, eval result: \n","=>> location_traffic_convenience - 0.6626198430795363\n","=>> location_distance_from_business_district - 0.5506741811864989\n","=>> location_easy_to_find - 0.7085641246988184\n","=>> service_wait_time - 0.6715437018681039\n","=>> service_waiters_attitude - 0.7994151971202739\n","=>> service_parking_convenience - 0.7424709432820481\n","=>> service_serving_speed - 0.7547217568744513\n","=>> price_level - 0.7823300213234015\n","=>> price_cost_effective - 0.7144205234900123\n","=>> price_discount - 0.6692375046669445\n","=>> environment_decoration - 0.732673116771658\n","=>> environment_noise - 0.7625725070036944\n","=>> environment_space - 0.7697994511991959\n","=>> environment_cleaness - 0.7569342580159577\n","=>> dish_portion - 0.7272180543209047\n","=>> dish_taste - 0.7296741353051347\n","=>> dish_look - 0.5819864551537628\n","=>> dish_recommendation - 0.7373385765616974\n","=>> others_overall_experience - 0.5960510771078215\n","=>> others_willing_to_consume_again - 0.7106286245129843\n","=>> Eval loss 1.71420, f1 0.70804\n","=>> current result -0.7080437026771451, previous best result -0.7068496780558529\n","=>> Epoch 2  global step 15020 loss 0.32117 batch 1379/1640 lr 0.001 accuracy 94.07227 wps 30541.89 step time 0.45s\n","=>> Epoch 2  global step 15040 loss 0.34125 batch 1399/1640 lr 0.001 accuracy 93.74805 wps 30692.11 step time 0.44s\n","=>> Epoch 2  global step 15060 loss 0.33992 batch 1419/1640 lr 0.001 accuracy 93.78320 wps 31890.27 step time 0.49s\n","=>> Epoch 2  global step 15080 loss 0.34377 batch 1439/1640 lr 0.001 accuracy 93.82031 wps 29293.79 step time 0.53s\n","=>> Epoch 2  global step 15100 loss 0.33775 batch 1459/1640 lr 0.001 accuracy 93.87500 wps 32619.50 step time 0.42s\n","=>> Epoch 2  global step 15120 loss 0.33689 batch 1479/1640 lr 0.001 accuracy 93.79297 wps 32398.66 step time 0.45s\n","=>> Epoch 2  global step 15140 loss 0.32846 batch 1499/1640 lr 0.001 accuracy 93.86328 wps 27712.99 step time 0.53s\n","=>> Epoch 2  global step 15160 loss 0.34333 batch 1519/1640 lr 0.001 accuracy 93.69141 wps 28799.11 step time 0.56s\n","=>> Epoch 2  global step 15180 loss 0.32820 batch 1539/1640 lr 0.001 accuracy 93.99805 wps 28445.48 step time 0.59s\n","=>> Epoch 2  global step 15200 loss 0.32598 batch 1559/1640 lr 0.001 accuracy 94.01758 wps 29239.85 step time 0.49s\n","=>> Epoch 2  global step 15220 loss 0.35679 batch 1579/1640 lr 0.001 accuracy 93.19141 wps 29610.35 step time 0.60s\n","=>> Epoch 2  global step 15240 loss 0.31649 batch 1599/1640 lr 0.001 accuracy 94.40820 wps 31372.73 step time 0.39s\n","=>> Epoch 2  global step 15260 loss 0.32684 batch 1619/1640 lr 0.001 accuracy 94.01562 wps 31548.50 step time 0.42s\n","=>> Epoch 2  global step 15280 loss 0.33386 batch 1639/1640 lr 0.001 accuracy 93.85937 wps 31148.65 step time 0.48s\n","=>> Finsh epoch 2, global step 15282\n","=>> Epoch 3  global step 15300 loss 0.30385 batch 18/1640 lr 0.001 accuracy 84.39453 wps 35247.53 step time 0.41s\n","=>> Epoch 3  global step 15320 loss 0.31256 batch 38/1640 lr 0.001 accuracy 94.30078 wps 36497.85 step time 0.41s\n","=>> Epoch 3  global step 15340 loss 0.30236 batch 58/1640 lr 0.001 accuracy 94.48242 wps 35369.03 step time 0.35s\n","=>> Epoch 3  global step 15360 loss 0.32910 batch 78/1640 lr 0.001 accuracy 93.90039 wps 36034.62 step time 0.47s\n","=>> Epoch 3  global step 15380 loss 0.31681 batch 98/1640 lr 0.001 accuracy 94.25781 wps 34497.73 step time 0.41s\n","=>> Epoch 3  global step 15400 loss 0.31252 batch 118/1640 lr 0.001 accuracy 94.36914 wps 35111.75 step time 0.35s\n","=>> Epoch 3  global step 15420 loss 0.32303 batch 138/1640 lr 0.001 accuracy 94.04687 wps 35213.19 step time 0.35s\n","=>> Epoch 3  global step 15440 loss 0.30697 batch 158/1640 lr 0.001 accuracy 94.34180 wps 36144.50 step time 0.37s\n","=>> Epoch 3  global step 15460 loss 0.31876 batch 178/1640 lr 0.001 accuracy 94.20508 wps 35137.80 step time 0.46s\n","=>> Epoch 3  global step 15480 loss 0.31093 batch 198/1640 lr 0.001 accuracy 94.39649 wps 35818.55 step time 0.37s\n","=>> Epoch 3  global step 15500 loss 0.31087 batch 218/1640 lr 0.001 accuracy 94.33789 wps 34093.77 step time 0.41s\n","=>> Epoch 3  global step 15520 loss 0.32670 batch 238/1640 lr 0.001 accuracy 93.92578 wps 34449.24 step time 0.51s\n","=>> Epoch 3  global step 15540 loss 0.30585 batch 258/1640 lr 0.001 accuracy 94.52148 wps 35021.61 step time 0.34s\n","=>> Epoch 3  global step 15560 loss 0.33129 batch 278/1640 lr 0.001 accuracy 93.96680 wps 36342.73 step time 0.40s\n","=>> Epoch 3  global step 15580 loss 0.33553 batch 298/1640 lr 0.001 accuracy 93.82617 wps 36361.74 step time 0.49s\n","=>> Epoch 3  global step 15600 loss 0.31160 batch 318/1640 lr 0.001 accuracy 94.34180 wps 35276.87 step time 0.36s\n","=>> Epoch 3  global step 15620 loss 0.31196 batch 338/1640 lr 0.001 accuracy 94.35937 wps 35841.86 step time 0.38s\n","=>> Epoch 3  global step 15640 loss 0.31337 batch 358/1640 lr 0.001 accuracy 94.36328 wps 35761.39 step time 0.37s\n","=>> Epoch 3  global step 15660 loss 0.33760 batch 378/1640 lr 0.001 accuracy 93.90234 wps 36498.84 step time 0.40s\n","=>> Epoch 3  global step 15680 loss 0.32700 batch 398/1640 lr 0.001 accuracy 93.94141 wps 36405.66 step time 0.40s\n","=>> Epoch 3  global step 15700 loss 0.33827 batch 418/1640 lr 0.001 accuracy 93.82031 wps 37299.90 step time 0.43s\n","=>> Epoch 3  global step 15720 loss 0.32625 batch 438/1640 lr 0.001 accuracy 93.95508 wps 36856.53 step time 0.42s\n","=>> Epoch 3  global step 15740 loss 0.32971 batch 458/1640 lr 0.001 accuracy 93.86328 wps 35435.01 step time 0.45s\n","=>> Epoch 3  global step 15760 loss 0.31636 batch 478/1640 lr 0.001 accuracy 94.19336 wps 35381.19 step time 0.36s\n","=>> Epoch 3  global step 15780 loss 0.35048 batch 498/1640 lr 0.001 accuracy 93.45508 wps 36179.36 step time 0.58s\n","=>> Epoch 3  global step 15800 loss 0.31995 batch 518/1640 lr 0.001 accuracy 94.29297 wps 36373.50 step time 0.39s\n","=>> Epoch 3  global step 15820 loss 0.31087 batch 538/1640 lr 0.001 accuracy 94.36328 wps 36159.17 step time 0.37s\n","=>> Epoch 3  global step 15840 loss 0.33232 batch 558/1640 lr 0.001 accuracy 94.08398 wps 35892.14 step time 0.36s\n","=>> Epoch 3  global step 15860 loss 0.33047 batch 578/1640 lr 0.001 accuracy 93.90625 wps 35568.39 step time 0.45s\n","=>> Epoch 3  global step 15880 loss 0.31809 batch 598/1640 lr 0.001 accuracy 94.19727 wps 36338.21 step time 0.39s\n","=>> Epoch 3  global step 15900 loss 0.32117 batch 618/1640 lr 0.001 accuracy 94.07422 wps 35639.03 step time 0.35s\n","=>> Epoch 3  global step 15920 loss 0.31777 batch 638/1640 lr 0.001 accuracy 94.09766 wps 35767.39 step time 0.37s\n","=>> Epoch 3  global step 15940 loss 0.31146 batch 658/1640 lr 0.001 accuracy 94.28516 wps 36522.95 step time 0.39s\n","=>> Epoch 3  global step 15960 loss 0.32798 batch 678/1640 lr 0.001 accuracy 93.89648 wps 35697.02 step time 0.48s\n","=>> Epoch 3  global step 15980 loss 0.32926 batch 698/1640 lr 0.001 accuracy 94.12305 wps 36079.22 step time 0.37s\n","=>> Epoch 3  global step 16000 loss 0.32024 batch 718/1640 lr 0.001 accuracy 94.07227 wps 34725.48 step time 0.42s\n","=>> global step 16000, eval result: \n","=>> location_traffic_convenience - 0.6612737063153314\n","=>> location_distance_from_business_district - 0.5521428096544871\n","=>> location_easy_to_find - 0.7086511759805293\n","=>> service_wait_time - 0.6683283818068786\n","=>> service_waiters_attitude - 0.8031776476784543\n","=>> service_parking_convenience - 0.7382334820993018\n","=>> service_serving_speed - 0.7539277783473461\n","=>> price_level - 0.7815484373780408\n","=>> price_cost_effective - 0.7128336484562533\n","=>> price_discount - 0.6736165160758294\n","=>> environment_decoration - 0.7295647527197202\n","=>> environment_noise - 0.7656252510933894\n","=>> environment_space - 0.7694071594538703\n","=>> environment_cleaness - 0.751687363636063\n","=>> dish_portion - 0.7268229113311159\n","=>> dish_taste - 0.734222890907825\n","=>> dish_look - 0.5873331893700764\n","=>> dish_recommendation - 0.7354438430347607\n","=>> others_overall_experience - 0.5961614319302091\n","=>> others_willing_to_consume_again - 0.707152884104433\n","=>> Eval loss 1.72296, f1 0.70786\n","=>> current result -0.7078577630686956, previous best result -0.7080437026771451\n","=>> Epoch 3  global step 16020 loss 0.33475 batch 738/1640 lr 0.001 accuracy 93.86328 wps 36877.94 step time 0.44s\n","=>> Epoch 3  global step 16040 loss 0.34716 batch 758/1640 lr 0.001 accuracy 93.60547 wps 34692.21 step time 0.52s\n","=>> Epoch 3  global step 16060 loss 0.32630 batch 778/1640 lr 0.001 accuracy 94.01953 wps 36534.87 step time 0.40s\n","=>> Epoch 3  global step 16080 loss 0.32772 batch 798/1640 lr 0.001 accuracy 94.00195 wps 36311.34 step time 0.39s\n","=>> Epoch 3  global step 16100 loss 0.32278 batch 818/1640 lr 0.001 accuracy 94.03320 wps 36540.78 step time 0.39s\n","Traceback (most recent call last):\n","  File \"elmo_run.py\", line 350, in <module>\n","    train_clf(flags)\n","  File \"elmo_run.py\", line 205, in train_clf\n","    run_info=add_summary and flags.debug\n","  File \"/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/mycode/elmo.py\", line 495, in train_clf_one_step\n","    feed_dict=feed_dict)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n","    run_metadata_ptr)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n","    run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n","    return fn(*args)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n","    options, feed_dict, fetch_list, target_list, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bg7pWa1Mn7lw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591271498892,"user_tz":-480,"elapsed":6951110,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"6924ca56-b1c0-49a8-ad46-fe7b92144b9b"},"source":["# softmax global loss + fix embedding: at 14000\n","!python elmo_run.py \\\n","--mode=train \\\n","--data_files=data/train_not_aug.json \\\n","--eval_files=data/validation.json \\\n","--label_file=data/labels.txt \\\n","--vocab_file=data/vocab.txt \\\n","--embed_file=data/embedding.txt \\\n","--num_layers=3 \\\n","--batch_size=64 \\\n","--max_len=1200 \\\n","--rnn_cell_name='WEIGHT_LSTM' \\\n","--fix_embedding=True \\\n","--linear_dropout=0.5 \\\n","--num_classes_each_label=4 \\\n","--num_labels=20 \\\n","--steps_per_eval=1000 \\\n","--optimizer='rms' \\\n","--learning_rate=0.001 \\\n","--focal_loss_gamma=0 \\\n","--label_smoothing=0 \\\n","--checkpoint_dir=output/elmo_origin_loss_fix_at_best \\\n","--checkpoint_load_step=14000 \\\n","--previous_best_eval=-0.7075728796925465"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/train_not_aug.json ...\n","# Got 105000 data items with 1640 batches\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/validation.json ...\n","# Got 15000 data items with 117 batches\n","  saving config to output/elmo_origin_loss_fix_at_best/config\n","mode=train,data_files=['data/train_not_aug.json'],eval_files=['data/validation.json'],label_file=data/labels.txt,vocab_file=data/vocab.txt,embed_file=data/embedding.txt,out_file=None,split_word=True,reverse=False,weight_file=None,prob=False,max_len=1200,batch_size=64,num_layers=3,optimizer=rms,learning_rate=0.001,decay_schema=hand,decay_steps=10000,loss_name=softmax,focal_loss_gamma=0.0,max_gradient_norm=2.0,l2_loss_ratio=0.0,label_smoothing=0.0,embedding_dropout=0.1,dropout_keep_prob=0.8,weight_keep_drop=0.8,linear_dropout=0.5,rnn_cell_name=WEIGHT_LSTM,embedding_size=300,num_units=300,num_classes_each_label=4,num_labels=20,fix_embedding=True,need_early_stop=True,patient=5,debug=False,num_train_epoch=50,steps_per_stats=20,steps_per_summary=50,steps_per_eval=1000,checkpoint_dir=output/elmo_origin_loss_fix_at_best,checkpoint_load_step=14000,previous_best_eval=-0.7075728796925465,vocab_size=50000\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/weight:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","variable <tf.Variable 'elmo_encoder/scalar:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/memory_layer/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/attention_op/dense/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/dense_1/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classifier/attention_op/dense_1/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/dense/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/weight_drop_lstm_cell/drop_connect_layer/kernel:0' shape=(300, 1200) dtype=float32_ref> with parameter number 360000\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/luong_attention/attention_g:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classifier/attention_op/attention_wrapper/attention_layer/kernel:0' shape=(900, 300) dtype=float32_ref> with parameter number 270000\n","variable <tf.Variable 'classifier/predict_clf/dense/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classifier/predict_clf/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classifier/predict_clf/dense_1/kernel:0' shape=(300, 4) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classifier/predict_clf/dense_1/bias:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","# total parameter number 8560510\n","loading config from output/elmo_origin_loss_fix_at_best/config\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","output/elmo_origin_loss_fix_at_best/model.ckpt-14000\n","\n","!!! Restored model\n","=>> Start to train with learning rate 0.001\n","=>> Global step 14000\n","=>> Epoch 1  global step 14020 loss 0.32221 batch 20/1640 lr 0.001 accuracy 94.13672 wps 21542.24 step time 0.66s\n","=>> Epoch 1  global step 14040 loss 0.31992 batch 40/1640 lr 0.001 accuracy 94.15625 wps 35456.30 step time 0.38s\n","=>> Epoch 1  global step 14060 loss 0.31936 batch 60/1640 lr 0.001 accuracy 94.22852 wps 35173.98 step time 0.38s\n","=>> Epoch 1  global step 14080 loss 0.33085 batch 80/1640 lr 0.001 accuracy 94.03906 wps 36666.00 step time 0.41s\n","=>> Epoch 1  global step 14100 loss 0.31702 batch 100/1640 lr 0.001 accuracy 94.29492 wps 35145.11 step time 0.37s\n","=>> Epoch 1  global step 14120 loss 0.31977 batch 120/1640 lr 0.001 accuracy 94.18555 wps 36300.90 step time 0.39s\n","=>> Epoch 1  global step 14140 loss 0.32194 batch 140/1640 lr 0.001 accuracy 94.14453 wps 35249.47 step time 0.38s\n","=>> Epoch 1  global step 14160 loss 0.30620 batch 160/1640 lr 0.001 accuracy 94.47852 wps 34760.67 step time 0.35s\n","=>> Epoch 1  global step 14180 loss 0.33536 batch 180/1640 lr 0.001 accuracy 93.80078 wps 35437.10 step time 0.47s\n","=>> Epoch 1  global step 14200 loss 0.32480 batch 200/1640 lr 0.001 accuracy 94.03125 wps 36408.25 step time 0.42s\n","=>> Epoch 1  global step 14220 loss 0.32834 batch 220/1640 lr 0.001 accuracy 93.94727 wps 33498.32 step time 0.47s\n","=>> Epoch 1  global step 14240 loss 0.32260 batch 240/1640 lr 0.001 accuracy 94.18945 wps 36574.24 step time 0.39s\n","=>> Epoch 1  global step 14260 loss 0.33206 batch 260/1640 lr 0.001 accuracy 93.89648 wps 35326.50 step time 0.46s\n","=>> Epoch 1  global step 14280 loss 0.34691 batch 280/1640 lr 0.001 accuracy 93.69336 wps 37244.73 step time 0.46s\n","=>> Epoch 1  global step 14300 loss 0.30738 batch 300/1640 lr 0.001 accuracy 94.52539 wps 35844.84 step time 0.37s\n","=>> Epoch 1  global step 14320 loss 0.30889 batch 320/1640 lr 0.001 accuracy 94.49805 wps 35088.17 step time 0.35s\n","=>> Epoch 1  global step 14340 loss 0.32603 batch 340/1640 lr 0.001 accuracy 94.00391 wps 35864.06 step time 0.39s\n","=>> Epoch 1  global step 14360 loss 0.32709 batch 360/1640 lr 0.001 accuracy 94.00000 wps 34945.32 step time 0.43s\n","=>> Epoch 1  global step 14380 loss 0.30872 batch 380/1640 lr 0.001 accuracy 94.36133 wps 36060.46 step time 0.40s\n","=>> Epoch 1  global step 14400 loss 0.32968 batch 400/1640 lr 0.001 accuracy 93.99414 wps 35564.16 step time 0.38s\n","=>> Epoch 1  global step 14420 loss 0.33654 batch 420/1640 lr 0.001 accuracy 93.88867 wps 35987.12 step time 0.38s\n","=>> Epoch 1  global step 14440 loss 0.32028 batch 440/1640 lr 0.001 accuracy 94.18555 wps 36480.10 step time 0.39s\n","=>> Epoch 1  global step 14460 loss 0.31801 batch 460/1640 lr 0.001 accuracy 94.23438 wps 34557.59 step time 0.41s\n","=>> Epoch 1  global step 14480 loss 0.31635 batch 480/1640 lr 0.001 accuracy 94.25391 wps 34759.01 step time 0.44s\n","=>> Epoch 1  global step 14500 loss 0.31994 batch 500/1640 lr 0.001 accuracy 94.00781 wps 34816.90 step time 0.36s\n","=>> Epoch 1  global step 14520 loss 0.33832 batch 520/1640 lr 0.001 accuracy 93.81250 wps 36256.11 step time 0.42s\n","=>> Epoch 1  global step 14540 loss 0.31989 batch 540/1640 lr 0.001 accuracy 94.22461 wps 33931.71 step time 0.37s\n","=>> Epoch 1  global step 14560 loss 0.31439 batch 560/1640 lr 0.001 accuracy 94.31641 wps 30904.83 step time 0.42s\n","=>> Epoch 1  global step 14580 loss 0.32563 batch 580/1640 lr 0.001 accuracy 94.04687 wps 32107.25 step time 0.43s\n","=>> Epoch 1  global step 14600 loss 0.32125 batch 600/1640 lr 0.001 accuracy 94.16797 wps 29578.44 step time 0.51s\n","=>> Epoch 1  global step 14620 loss 0.32033 batch 620/1640 lr 0.001 accuracy 94.18164 wps 30803.21 step time 0.43s\n","=>> Epoch 1  global step 14640 loss 0.32069 batch 640/1640 lr 0.001 accuracy 94.19141 wps 30927.52 step time 0.48s\n","=>> Epoch 1  global step 14660 loss 0.34390 batch 660/1640 lr 0.001 accuracy 93.64258 wps 33305.50 step time 0.47s\n","=>> Epoch 1  global step 14680 loss 0.33739 batch 680/1640 lr 0.001 accuracy 93.76758 wps 31338.59 step time 0.51s\n","=>> Epoch 1  global step 14700 loss 0.31811 batch 700/1640 lr 0.001 accuracy 94.12305 wps 31324.72 step time 0.42s\n","=>> Epoch 1  global step 14720 loss 0.33929 batch 720/1640 lr 0.001 accuracy 93.67383 wps 29058.48 step time 0.55s\n","=>> Epoch 1  global step 14740 loss 0.33889 batch 740/1640 lr 0.001 accuracy 93.80273 wps 30225.60 step time 0.57s\n","=>> Epoch 1  global step 14760 loss 0.33286 batch 760/1640 lr 0.001 accuracy 93.80859 wps 31167.65 step time 0.53s\n","=>> Epoch 1  global step 14780 loss 0.32260 batch 780/1640 lr 0.001 accuracy 93.96484 wps 30189.22 step time 0.51s\n","=>> Epoch 1  global step 14800 loss 0.33715 batch 800/1640 lr 0.001 accuracy 93.83984 wps 30797.25 step time 0.50s\n","=>> Epoch 1  global step 14820 loss 0.33181 batch 820/1640 lr 0.001 accuracy 94.03320 wps 29072.29 step time 0.49s\n","=>> Epoch 1  global step 14840 loss 0.33251 batch 840/1640 lr 0.001 accuracy 93.88672 wps 27670.45 step time 0.63s\n","=>> Epoch 1  global step 14860 loss 0.33376 batch 860/1640 lr 0.001 accuracy 93.82617 wps 31921.05 step time 0.53s\n","=>> Epoch 1  global step 14880 loss 0.34111 batch 880/1640 lr 0.001 accuracy 93.81445 wps 31684.98 step time 0.50s\n","=>> Epoch 1  global step 14900 loss 0.35315 batch 900/1640 lr 0.001 accuracy 93.51172 wps 28757.07 step time 0.66s\n","=>> Epoch 1  global step 14920 loss 0.32289 batch 920/1640 lr 0.001 accuracy 93.89648 wps 29623.97 step time 0.45s\n","=>> Epoch 1  global step 14940 loss 0.32422 batch 940/1640 lr 0.001 accuracy 94.05859 wps 31285.39 step time 0.46s\n","=>> Epoch 1  global step 14960 loss 0.33429 batch 960/1640 lr 0.001 accuracy 93.87305 wps 32511.58 step time 0.47s\n","=>> Epoch 1  global step 14980 loss 0.33243 batch 980/1640 lr 0.001 accuracy 93.70898 wps 27987.83 step time 0.65s\n","=>> Epoch 1  global step 15000 loss 0.33412 batch 1000/1640 lr 0.001 accuracy 93.99023 wps 30362.09 step time 0.53s\n","=>> global step 15000, eval result: \n","=>> location_traffic_convenience - 0.6636399611757312\n","=>> location_distance_from_business_district - 0.5568707017767804\n","=>> location_easy_to_find - 0.7092906627022478\n","=>> service_wait_time - 0.6669453430364303\n","=>> service_waiters_attitude - 0.8010110793994403\n","=>> service_parking_convenience - 0.7375393510196406\n","=>> service_serving_speed - 0.7531698264650551\n","=>> price_level - 0.7839394375682488\n","=>> price_cost_effective - 0.711246831116447\n","=>> price_discount - 0.6693393221829002\n","=>> environment_decoration - 0.7303740116428198\n","=>> environment_noise - 0.7646344540321852\n","=>> environment_space - 0.7706782158083266\n","=>> environment_cleaness - 0.7611965798060735\n","=>> dish_portion - 0.7270364434168308\n","=>> dish_taste - 0.732109777071575\n","=>> dish_look - 0.5831878884008435\n","=>> dish_recommendation - 0.7355870777864011\n","=>> others_overall_experience - 0.5949937533676709\n","=>> others_willing_to_consume_again - 0.7126861200745624\n","=>> Eval loss 1.71954, f1 0.70827\n","=>> current result -0.7082738418925103, previous best result -0.7075728796925465\n","=>> Epoch 1  global step 15020 loss 0.34528 batch 1020/1640 lr 0.001 accuracy 93.64453 wps 29080.02 step time 0.62s\n","=>> Epoch 1  global step 15040 loss 0.32768 batch 1040/1640 lr 0.001 accuracy 93.98828 wps 28670.23 step time 0.50s\n","=>> Epoch 1  global step 15060 loss 0.33704 batch 1060/1640 lr 0.001 accuracy 93.86523 wps 33639.64 step time 0.42s\n","=>> Epoch 1  global step 15080 loss 0.34149 batch 1080/1640 lr 0.001 accuracy 93.75195 wps 33198.04 step time 0.45s\n","=>> Epoch 1  global step 15100 loss 0.34248 batch 1100/1640 lr 0.001 accuracy 93.60156 wps 31385.23 step time 0.48s\n","=>> Epoch 1  global step 15120 loss 0.31738 batch 1120/1640 lr 0.001 accuracy 94.24805 wps 31999.81 step time 0.41s\n","=>> Epoch 1  global step 15140 loss 0.31403 batch 1140/1640 lr 0.001 accuracy 94.28711 wps 30702.05 step time 0.42s\n","=>> Epoch 1  global step 15160 loss 0.31365 batch 1160/1640 lr 0.001 accuracy 94.31250 wps 32479.29 step time 0.37s\n","=>> Epoch 1  global step 15180 loss 0.32619 batch 1180/1640 lr 0.001 accuracy 94.06055 wps 29197.21 step time 0.51s\n","=>> Epoch 1  global step 15200 loss 0.31280 batch 1200/1640 lr 0.001 accuracy 94.32422 wps 31167.36 step time 0.39s\n","=>> Epoch 1  global step 15220 loss 0.32945 batch 1220/1640 lr 0.001 accuracy 94.00195 wps 31574.76 step time 0.43s\n","=>> Epoch 1  global step 15240 loss 0.33309 batch 1240/1640 lr 0.001 accuracy 93.94922 wps 29713.52 step time 0.49s\n","=>> Epoch 1  global step 15260 loss 0.32536 batch 1260/1640 lr 0.001 accuracy 93.96484 wps 28990.93 step time 0.53s\n","=>> Epoch 1  global step 15280 loss 0.34024 batch 1280/1640 lr 0.001 accuracy 93.74414 wps 27913.88 step time 0.64s\n","=>> Epoch 1  global step 15300 loss 0.32922 batch 1300/1640 lr 0.001 accuracy 93.95508 wps 30914.76 step time 0.44s\n","=>> Epoch 1  global step 15320 loss 0.30669 batch 1320/1640 lr 0.001 accuracy 94.36328 wps 30761.93 step time 0.37s\n","=>> Epoch 1  global step 15340 loss 0.33004 batch 1340/1640 lr 0.001 accuracy 94.03320 wps 32514.89 step time 0.44s\n","=>> Epoch 1  global step 15360 loss 0.34102 batch 1360/1640 lr 0.001 accuracy 93.67383 wps 31307.72 step time 0.49s\n","=>> Epoch 1  global step 15380 loss 0.33206 batch 1380/1640 lr 0.001 accuracy 93.88867 wps 29562.52 step time 0.49s\n","=>> Epoch 1  global step 15400 loss 0.33833 batch 1400/1640 lr 0.001 accuracy 93.70313 wps 32290.21 step time 0.45s\n","=>> Epoch 1  global step 15420 loss 0.33672 batch 1420/1640 lr 0.001 accuracy 93.95898 wps 31122.18 step time 0.47s\n","=>> Epoch 1  global step 15440 loss 0.33045 batch 1440/1640 lr 0.001 accuracy 93.92773 wps 30743.05 step time 0.46s\n","=>> Epoch 1  global step 15460 loss 0.33876 batch 1460/1640 lr 0.001 accuracy 93.85547 wps 31359.01 step time 0.49s\n","=>> Epoch 1  global step 15480 loss 0.31636 batch 1480/1640 lr 0.001 accuracy 94.19727 wps 30045.23 step time 0.50s\n","=>> Epoch 1  global step 15500 loss 0.33177 batch 1500/1640 lr 0.001 accuracy 93.97851 wps 30887.47 step time 0.41s\n","=>> Epoch 1  global step 15520 loss 0.32920 batch 1520/1640 lr 0.001 accuracy 94.12500 wps 31561.78 step time 0.37s\n","=>> Epoch 1  global step 15540 loss 0.33272 batch 1540/1640 lr 0.001 accuracy 93.91016 wps 31965.16 step time 0.45s\n","=>> Epoch 1  global step 15560 loss 0.32048 batch 1560/1640 lr 0.001 accuracy 94.01172 wps 28617.10 step time 0.51s\n","=>> Epoch 1  global step 15580 loss 0.31999 batch 1580/1640 lr 0.001 accuracy 94.26367 wps 29093.41 step time 0.46s\n","=>> Epoch 1  global step 15600 loss 0.34477 batch 1600/1640 lr 0.001 accuracy 93.78711 wps 29162.17 step time 0.57s\n","=>> Epoch 1  global step 15620 loss 0.32145 batch 1620/1640 lr 0.001 accuracy 94.17969 wps 29691.69 step time 0.47s\n","=>> Epoch 1  global step 15640 loss 0.34233 batch 1640/1640 lr 0.001 accuracy 93.75391 wps 30366.91 step time 0.54s\n","=>> Finsh epoch 1, global step 15641\n","=>> Epoch 2  global step 15660 loss 0.30410 batch 19/1640 lr 0.001 accuracy 89.51172 wps 35361.93 step time 0.41s\n","=>> Epoch 2  global step 15680 loss 0.32010 batch 39/1640 lr 0.001 accuracy 94.08984 wps 34611.92 step time 0.43s\n","=>> Epoch 2  global step 15700 loss 0.30262 batch 59/1640 lr 0.001 accuracy 94.45703 wps 33941.52 step time 0.41s\n","=>> Epoch 2  global step 15720 loss 0.31904 batch 79/1640 lr 0.001 accuracy 94.13477 wps 36901.79 step time 0.41s\n","=>> Epoch 2  global step 15740 loss 0.30425 batch 99/1640 lr 0.001 accuracy 94.44531 wps 35031.83 step time 0.34s\n","=>> Epoch 2  global step 15760 loss 0.31603 batch 119/1640 lr 0.001 accuracy 94.07813 wps 35110.00 step time 0.45s\n","=>> Epoch 2  global step 15780 loss 0.31848 batch 139/1640 lr 0.001 accuracy 94.12891 wps 34465.65 step time 0.49s\n","=>> Epoch 2  global step 15800 loss 0.32246 batch 159/1640 lr 0.001 accuracy 94.05664 wps 36335.06 step time 0.39s\n","=>> Epoch 2  global step 15820 loss 0.31906 batch 179/1640 lr 0.001 accuracy 94.22070 wps 34243.41 step time 0.41s\n","=>> Epoch 2  global step 15840 loss 0.30902 batch 199/1640 lr 0.001 accuracy 94.37305 wps 36222.19 step time 0.39s\n","=>> Epoch 2  global step 15860 loss 0.32365 batch 219/1640 lr 0.001 accuracy 94.08203 wps 35555.90 step time 0.48s\n","=>> Epoch 2  global step 15880 loss 0.30896 batch 239/1640 lr 0.001 accuracy 94.35352 wps 36243.41 step time 0.39s\n","=>> Epoch 2  global step 15900 loss 0.31188 batch 259/1640 lr 0.001 accuracy 94.37891 wps 35773.91 step time 0.37s\n","=>> Epoch 2  global step 15920 loss 0.33003 batch 279/1640 lr 0.001 accuracy 93.90625 wps 37507.48 step time 0.43s\n","=>> Epoch 2  global step 15940 loss 0.31304 batch 299/1640 lr 0.001 accuracy 94.30469 wps 35784.31 step time 0.36s\n","=>> Epoch 2  global step 15960 loss 0.31635 batch 319/1640 lr 0.001 accuracy 94.21484 wps 35942.79 step time 0.37s\n","=>> Epoch 2  global step 15980 loss 0.32390 batch 339/1640 lr 0.001 accuracy 94.16992 wps 35503.41 step time 0.35s\n","=>> Epoch 2  global step 16000 loss 0.32340 batch 359/1640 lr 0.001 accuracy 94.14844 wps 36754.18 step time 0.41s\n","=>> global step 16000, eval result: \n","=>> location_traffic_convenience - 0.6558859289037386\n","=>> location_distance_from_business_district - 0.549274417629914\n","=>> location_easy_to_find - 0.7088494681323467\n","=>> service_wait_time - 0.6692170166032383\n","=>> service_waiters_attitude - 0.8038694632996295\n","=>> service_parking_convenience - 0.7447607469516911\n","=>> service_serving_speed - 0.7543230990781186\n","=>> price_level - 0.7828880720943929\n","=>> price_cost_effective - 0.7112021353813001\n","=>> price_discount - 0.6688703813444724\n","=>> environment_decoration - 0.7303472446248335\n","=>> environment_noise - 0.7646048996752438\n","=>> environment_space - 0.7685057115975338\n","=>> environment_cleaness - 0.7609708729435314\n","=>> dish_portion - 0.723690859398999\n","=>> dish_taste - 0.7314856692239857\n","=>> dish_look - 0.5811478629450728\n","=>> dish_recommendation - 0.740570668266831\n","=>> others_overall_experience - 0.5970283111253049\n","=>> others_willing_to_consume_again - 0.7095751912611008\n","=>> Eval loss 1.72539, f1 0.70785\n","=>> current result -0.7078534010240639, previous best result -0.7082738418925103\n","=>> Epoch 2  global step 16020 loss 0.32295 batch 379/1640 lr 0.001 accuracy 94.03906 wps 37098.16 step time 0.42s\n","=>> Epoch 2  global step 16040 loss 0.34668 batch 399/1640 lr 0.001 accuracy 93.58203 wps 36188.25 step time 0.49s\n","=>> Epoch 2  global step 16060 loss 0.31838 batch 419/1640 lr 0.001 accuracy 94.27734 wps 35632.35 step time 0.37s\n","=>> Epoch 2  global step 16080 loss 0.30889 batch 439/1640 lr 0.001 accuracy 94.25391 wps 31735.38 step time 0.42s\n","=>> Epoch 2  global step 16100 loss 0.31518 batch 459/1640 lr 0.001 accuracy 94.29688 wps 31525.35 step time 0.45s\n","=>> Epoch 2  global step 16120 loss 0.33147 batch 479/1640 lr 0.001 accuracy 94.01758 wps 29315.76 step time 0.52s\n","=>> Epoch 2  global step 16140 loss 0.33353 batch 499/1640 lr 0.001 accuracy 93.85547 wps 32589.03 step time 0.46s\n","=>> Epoch 2  global step 16160 loss 0.33426 batch 519/1640 lr 0.001 accuracy 93.83398 wps 28747.74 step time 0.63s\n","=>> Epoch 2  global step 16180 loss 0.30651 batch 539/1640 lr 0.001 accuracy 94.44336 wps 30326.07 step time 0.41s\n","=>> Epoch 2  global step 16200 loss 0.32658 batch 559/1640 lr 0.001 accuracy 93.95508 wps 30543.20 step time 0.44s\n","=>> Epoch 2  global step 16220 loss 0.33359 batch 579/1640 lr 0.001 accuracy 93.88477 wps 27438.32 step time 0.66s\n","=>> Epoch 2  global step 16240 loss 0.32356 batch 599/1640 lr 0.001 accuracy 94.08008 wps 29906.79 step time 0.55s\n","=>> Epoch 2  global step 16260 loss 0.34231 batch 619/1640 lr 0.001 accuracy 93.70117 wps 30268.54 step time 0.60s\n","=>> Epoch 2  global step 16280 loss 0.33624 batch 639/1640 lr 0.001 accuracy 93.79883 wps 28476.29 step time 0.55s\n","=>> Epoch 2  global step 16300 loss 0.30885 batch 659/1640 lr 0.001 accuracy 94.30469 wps 30843.75 step time 0.45s\n","=>> Epoch 2  global step 16320 loss 0.33232 batch 679/1640 lr 0.001 accuracy 93.83008 wps 29082.82 step time 0.55s\n","=>> Epoch 2  global step 16340 loss 0.31611 batch 699/1640 lr 0.001 accuracy 94.13867 wps 31602.52 step time 0.45s\n","=>> Epoch 2  global step 16360 loss 0.31115 batch 719/1640 lr 0.001 accuracy 94.47656 wps 30782.26 step time 0.41s\n","=>> Epoch 2  global step 16380 loss 0.31539 batch 739/1640 lr 0.001 accuracy 94.31055 wps 31689.90 step time 0.41s\n","=>> Epoch 2  global step 16400 loss 0.31050 batch 759/1640 lr 0.001 accuracy 94.25391 wps 32196.57 step time 0.40s\n","=>> Epoch 2  global step 16420 loss 0.31486 batch 779/1640 lr 0.001 accuracy 94.18359 wps 32739.26 step time 0.41s\n","=>> Epoch 2  global step 16440 loss 0.32002 batch 799/1640 lr 0.001 accuracy 94.16406 wps 32607.01 step time 0.43s\n","=>> Epoch 2  global step 16460 loss 0.31299 batch 819/1640 lr 0.001 accuracy 94.37109 wps 32836.97 step time 0.38s\n","=>> Epoch 2  global step 16480 loss 0.30865 batch 839/1640 lr 0.001 accuracy 94.37500 wps 31348.55 step time 0.47s\n","=>> Epoch 2  global step 16500 loss 0.30947 batch 859/1640 lr 0.001 accuracy 94.44336 wps 30797.97 step time 0.43s\n","=>> Epoch 2  global step 16520 loss 0.32897 batch 879/1640 lr 0.001 accuracy 93.91992 wps 31402.08 step time 0.52s\n","=>> Epoch 2  global step 16540 loss 0.34309 batch 899/1640 lr 0.001 accuracy 93.78320 wps 29496.45 step time 0.59s\n","=>> Epoch 2  global step 16560 loss 0.32025 batch 919/1640 lr 0.001 accuracy 94.11719 wps 31652.98 step time 0.40s\n","=>> Epoch 2  global step 16580 loss 0.31873 batch 939/1640 lr 0.001 accuracy 94.22266 wps 31584.75 step time 0.45s\n","=>> Epoch 2  global step 16600 loss 0.32741 batch 959/1640 lr 0.001 accuracy 93.99219 wps 30817.18 step time 0.45s\n","=>> Epoch 2  global step 16620 loss 0.30583 batch 979/1640 lr 0.001 accuracy 94.33398 wps 31903.20 step time 0.43s\n","=>> Epoch 2  global step 16640 loss 0.31846 batch 999/1640 lr 0.001 accuracy 94.17773 wps 31603.74 step time 0.43s\n","=>> Epoch 2  global step 16660 loss 0.32517 batch 1019/1640 lr 0.001 accuracy 94.10742 wps 30525.59 step time 0.45s\n","=>> Epoch 2  global step 16680 loss 0.33023 batch 1039/1640 lr 0.001 accuracy 93.97070 wps 25419.01 step time 0.71s\n","=>> Epoch 2  global step 16700 loss 0.31462 batch 1059/1640 lr 0.001 accuracy 94.26953 wps 30503.69 step time 0.44s\n","=>> Epoch 2  global step 16720 loss 0.31332 batch 1079/1640 lr 0.001 accuracy 94.25586 wps 31183.21 step time 0.46s\n","=>> Epoch 2  global step 16740 loss 0.32056 batch 1099/1640 lr 0.001 accuracy 94.28516 wps 32201.92 step time 0.42s\n","=>> Epoch 2  global step 16760 loss 0.33009 batch 1119/1640 lr 0.001 accuracy 93.87305 wps 28360.59 step time 0.61s\n","=>> Epoch 2  global step 16780 loss 0.31983 batch 1139/1640 lr 0.001 accuracy 94.16016 wps 31850.58 step time 0.45s\n","=>> Epoch 2  global step 16800 loss 0.31833 batch 1159/1640 lr 0.001 accuracy 94.12500 wps 31454.05 step time 0.44s\n","=>> Epoch 2  global step 16820 loss 0.33130 batch 1179/1640 lr 0.001 accuracy 94.01367 wps 29316.27 step time 0.56s\n","=>> Epoch 2  global step 16840 loss 0.31357 batch 1199/1640 lr 0.001 accuracy 94.18945 wps 31883.22 step time 0.42s\n","=>> Epoch 2  global step 16860 loss 0.34183 batch 1219/1640 lr 0.001 accuracy 93.70898 wps 31919.22 step time 0.53s\n","=>> Epoch 2  global step 16880 loss 0.33023 batch 1239/1640 lr 0.001 accuracy 93.96875 wps 29453.57 step time 0.49s\n","=>> Epoch 2  global step 16900 loss 0.31992 batch 1259/1640 lr 0.001 accuracy 94.02148 wps 29142.96 step time 0.56s\n","=>> Epoch 2  global step 16920 loss 0.32789 batch 1279/1640 lr 0.001 accuracy 94.07422 wps 31859.14 step time 0.47s\n","=>> Epoch 2  global step 16940 loss 0.32322 batch 1299/1640 lr 0.001 accuracy 93.96680 wps 30216.63 step time 0.53s\n","=>> Epoch 2  global step 16960 loss 0.31370 batch 1319/1640 lr 0.001 accuracy 94.30469 wps 31832.82 step time 0.37s\n","=>> Epoch 2  global step 16980 loss 0.33776 batch 1339/1640 lr 0.001 accuracy 93.78320 wps 30872.31 step time 0.51s\n","=>> Epoch 2  global step 17000 loss 0.30302 batch 1359/1640 lr 0.001 accuracy 94.40625 wps 30677.07 step time 0.40s\n","=>> global step 17000, eval result: \n","=>> location_traffic_convenience - 0.6670212686683896\n","=>> location_distance_from_business_district - 0.5486613162183502\n","=>> location_easy_to_find - 0.7074740662420348\n","=>> service_wait_time - 0.6679107267511406\n","=>> service_waiters_attitude - 0.8034333761090191\n","=>> service_parking_convenience - 0.7452474253870445\n","=>> service_serving_speed - 0.7500440639517714\n","=>> price_level - 0.7832805889376045\n","=>> price_cost_effective - 0.7083862862499594\n","=>> price_discount - 0.6747012352803093\n","=>> environment_decoration - 0.7304931668266751\n","=>> environment_noise - 0.7592092327885401\n","=>> environment_space - 0.7657149426092047\n","=>> environment_cleaness - 0.7580561748541985\n","=>> dish_portion - 0.7233445079456544\n","=>> dish_taste - 0.7342469682580632\n","=>> dish_look - 0.5808673871736063\n","=>> dish_recommendation - 0.7377401520107569\n","=>> others_overall_experience - 0.5945432667568364\n","=>> others_willing_to_consume_again - 0.7088528626801668\n","=>> Eval loss 1.73390, f1 0.70746\n","=>> current result -0.7074614507849664, previous best result -0.7082738418925103\n","=>> Epoch 2  global step 17020 loss 0.33306 batch 1379/1640 lr 0.001 accuracy 93.75586 wps 30485.18 step time 0.52s\n","=>> Epoch 2  global step 17040 loss 0.32404 batch 1399/1640 lr 0.001 accuracy 94.14062 wps 31921.96 step time 0.48s\n","=>> Epoch 2  global step 17060 loss 0.32152 batch 1419/1640 lr 0.001 accuracy 94.07812 wps 28931.92 step time 0.58s\n","=>> Epoch 2  global step 17080 loss 0.30988 batch 1439/1640 lr 0.001 accuracy 94.38086 wps 32105.89 step time 0.39s\n","=>> Epoch 2  global step 17100 loss 0.33442 batch 1459/1640 lr 0.001 accuracy 93.92578 wps 29404.47 step time 0.54s\n","=>> Epoch 2  global step 17120 loss 0.31894 batch 1479/1640 lr 0.001 accuracy 94.19922 wps 32637.81 step time 0.40s\n","=>> Epoch 2  global step 17140 loss 0.32994 batch 1499/1640 lr 0.001 accuracy 93.88672 wps 28800.24 step time 0.58s\n","=>> Epoch 2  global step 17160 loss 0.32229 batch 1519/1640 lr 0.001 accuracy 94.03711 wps 31096.55 step time 0.44s\n","=>> Epoch 2  global step 17180 loss 0.33775 batch 1539/1640 lr 0.001 accuracy 93.78906 wps 32082.56 step time 0.44s\n","=>> Epoch 2  global step 17200 loss 0.31424 batch 1559/1640 lr 0.001 accuracy 94.24023 wps 31755.88 step time 0.42s\n","=>> Epoch 2  global step 17220 loss 0.32790 batch 1579/1640 lr 0.001 accuracy 93.85937 wps 31291.74 step time 0.46s\n","=>> Epoch 2  global step 17240 loss 0.32123 batch 1599/1640 lr 0.001 accuracy 94.08789 wps 29828.38 step time 0.52s\n","=>> Epoch 2  global step 17260 loss 0.33339 batch 1619/1640 lr 0.001 accuracy 93.96680 wps 32203.86 step time 0.44s\n","=>> Epoch 2  global step 17280 loss 0.32768 batch 1639/1640 lr 0.001 accuracy 93.92773 wps 31077.19 step time 0.45s\n","=>> Finsh epoch 2, global step 17282\n","=>> Epoch 3  global step 17300 loss 0.29124 batch 18/1640 lr 0.001 accuracy 84.57422 wps 35416.74 step time 0.42s\n","=>> Epoch 3  global step 17320 loss 0.31597 batch 38/1640 lr 0.001 accuracy 94.19531 wps 35507.99 step time 0.44s\n","=>> Epoch 3  global step 17340 loss 0.32551 batch 58/1640 lr 0.001 accuracy 94.01367 wps 35147.69 step time 0.52s\n","=>> Epoch 3  global step 17360 loss 0.30723 batch 78/1640 lr 0.001 accuracy 94.48437 wps 33916.86 step time 0.46s\n","=>> Epoch 3  global step 17380 loss 0.29678 batch 98/1640 lr 0.001 accuracy 94.66797 wps 35161.93 step time 0.33s\n","=>> Epoch 3  global step 17400 loss 0.31446 batch 118/1640 lr 0.001 accuracy 94.12695 wps 35464.58 step time 0.36s\n","=>> Epoch 3  global step 17420 loss 0.31177 batch 138/1640 lr 0.001 accuracy 94.36328 wps 37374.01 step time 0.42s\n","=>> Epoch 3  global step 17440 loss 0.31208 batch 158/1640 lr 0.001 accuracy 94.30664 wps 37150.36 step time 0.41s\n","=>> Epoch 3  global step 17460 loss 0.30603 batch 178/1640 lr 0.001 accuracy 94.33008 wps 34699.54 step time 0.42s\n","=>> Epoch 3  global step 17480 loss 0.29910 batch 198/1640 lr 0.001 accuracy 94.45703 wps 37002.01 step time 0.39s\n","=>> Epoch 3  global step 17500 loss 0.30815 batch 218/1640 lr 0.001 accuracy 94.41406 wps 36341.37 step time 0.37s\n","=>> Epoch 3  global step 17520 loss 0.31405 batch 238/1640 lr 0.001 accuracy 94.27344 wps 36699.31 step time 0.41s\n","=>> Epoch 3  global step 17540 loss 0.30950 batch 258/1640 lr 0.001 accuracy 94.29297 wps 35910.46 step time 0.46s\n","=>> Epoch 3  global step 17560 loss 0.31502 batch 278/1640 lr 0.001 accuracy 94.27344 wps 36504.53 step time 0.39s\n","=>> Epoch 3  global step 17580 loss 0.32884 batch 298/1640 lr 0.001 accuracy 93.94141 wps 35924.90 step time 0.44s\n","=>> Epoch 3  global step 17600 loss 0.29045 batch 318/1640 lr 0.001 accuracy 94.79102 wps 35330.74 step time 0.34s\n","=>> Epoch 3  global step 17620 loss 0.28955 batch 338/1640 lr 0.001 accuracy 94.71094 wps 33906.53 step time 0.31s\n","=>> Epoch 3  global step 17640 loss 0.34683 batch 358/1640 lr 0.001 accuracy 93.58984 wps 35457.26 step time 0.54s\n","=>> Epoch 3  global step 17660 loss 0.32938 batch 378/1640 lr 0.001 accuracy 93.85352 wps 35582.19 step time 0.45s\n","=>> Epoch 3  global step 17680 loss 0.30030 batch 398/1640 lr 0.001 accuracy 94.66406 wps 35255.12 step time 0.34s\n","=>> Epoch 3  global step 17700 loss 0.33916 batch 418/1640 lr 0.001 accuracy 93.77734 wps 37153.45 step time 0.50s\n","=>> Epoch 3  global step 17720 loss 0.34401 batch 438/1640 lr 0.001 accuracy 93.59375 wps 36777.58 step time 0.51s\n","=>> Epoch 3  global step 17740 loss 0.31568 batch 458/1640 lr 0.001 accuracy 94.19531 wps 36421.64 step time 0.39s\n","=>> Epoch 3  global step 17760 loss 0.30382 batch 478/1640 lr 0.001 accuracy 94.42969 wps 36841.11 step time 0.40s\n","=>> Epoch 3  global step 17780 loss 0.30681 batch 498/1640 lr 0.001 accuracy 94.42187 wps 35921.45 step time 0.36s\n","=>> Epoch 3  global step 17800 loss 0.32199 batch 518/1640 lr 0.001 accuracy 94.00977 wps 37686.90 step time 0.45s\n","=>> Epoch 3  global step 17820 loss 0.30692 batch 538/1640 lr 0.001 accuracy 94.42969 wps 35649.30 step time 0.42s\n","=>> Epoch 3  global step 17840 loss 0.31212 batch 558/1640 lr 0.001 accuracy 94.31641 wps 35522.22 step time 0.34s\n","=>> Epoch 3  global step 17860 loss 0.30888 batch 578/1640 lr 0.001 accuracy 94.36523 wps 35875.99 step time 0.44s\n","=>> Epoch 3  global step 17880 loss 0.31105 batch 598/1640 lr 0.001 accuracy 94.32617 wps 35911.34 step time 0.36s\n","=>> Epoch 3  global step 17900 loss 0.33708 batch 618/1640 lr 0.001 accuracy 93.81836 wps 37437.86 step time 0.43s\n","=>> Epoch 3  global step 17920 loss 0.30601 batch 638/1640 lr 0.001 accuracy 94.42188 wps 35941.37 step time 0.36s\n","=>> Epoch 3  global step 17940 loss 0.31953 batch 658/1640 lr 0.001 accuracy 94.13867 wps 37686.58 step time 0.43s\n","=>> Epoch 3  global step 17960 loss 0.30602 batch 678/1640 lr 0.001 accuracy 94.45313 wps 35249.56 step time 0.35s\n","=>> Epoch 3  global step 17980 loss 0.31211 batch 698/1640 lr 0.001 accuracy 94.38672 wps 35135.17 step time 0.35s\n","=>> Epoch 3  global step 18000 loss 0.30939 batch 718/1640 lr 0.001 accuracy 94.35937 wps 35027.49 step time 0.41s\n","=>> global step 18000, eval result: \n","=>> location_traffic_convenience - 0.6636484212627849\n","=>> location_distance_from_business_district - 0.5499595826106027\n","=>> location_easy_to_find - 0.7074360370687974\n","=>> service_wait_time - 0.669300259634191\n","=>> service_waiters_attitude - 0.8021364711916328\n","=>> service_parking_convenience - 0.7482201202255933\n","=>> service_serving_speed - 0.7543085279503292\n","=>> price_level - 0.7815201966457108\n","=>> price_cost_effective - 0.7131922413554425\n","=>> price_discount - 0.6675480105364228\n","=>> environment_decoration - 0.7309702899624877\n","=>> environment_noise - 0.7599329607573881\n","=>> environment_space - 0.7640707159493528\n","=>> environment_cleaness - 0.7546784680338913\n","=>> dish_portion - 0.7226203721784303\n","=>> dish_taste - 0.7307825572245161\n","=>> dish_look - 0.5790561850706493\n","=>> dish_recommendation - 0.7362886821642738\n","=>> others_overall_experience - 0.5938878774297087\n","=>> others_willing_to_consume_again - 0.7101943724700486\n","=>> Eval loss 1.74403, f1 0.70699\n","=>> current result -0.7069876174861128, previous best result -0.7082738418925103\n","=>> Epoch 3  global step 18020 loss 0.32133 batch 738/1640 lr 0.001 accuracy 94.08398 wps 36555.66 step time 0.41s\n","=>> Epoch 3  global step 18040 loss 0.33141 batch 758/1640 lr 0.001 accuracy 93.95898 wps 35563.80 step time 0.47s\n","=>> Epoch 3  global step 18060 loss 0.30911 batch 778/1640 lr 0.001 accuracy 94.31641 wps 34697.19 step time 0.42s\n","=>> Epoch 3  global step 18080 loss 0.30702 batch 798/1640 lr 0.001 accuracy 94.31836 wps 36016.42 step time 0.39s\n","=>> Epoch 3  global step 18100 loss 0.31677 batch 818/1640 lr 0.001 accuracy 94.19336 wps 36929.67 step time 0.41s\n","=>> Epoch 3  global step 18120 loss 0.30438 batch 838/1640 lr 0.001 accuracy 94.43750 wps 35719.54 step time 0.38s\n","=>> Epoch 3  global step 18140 loss 0.32353 batch 858/1640 lr 0.001 accuracy 94.06641 wps 35091.34 step time 0.44s\n","=>> Epoch 3  global step 18160 loss 0.34105 batch 878/1640 lr 0.001 accuracy 93.68164 wps 36855.41 step time 0.41s\n","=>> Epoch 3  global step 18180 loss 0.32381 batch 898/1640 lr 0.001 accuracy 94.04297 wps 36223.67 step time 0.38s\n","=>> Epoch 3  global step 18200 loss 0.31994 batch 918/1640 lr 0.001 accuracy 94.20313 wps 34950.10 step time 0.50s\n","=>> Epoch 3  global step 18220 loss 0.30480 batch 938/1640 lr 0.001 accuracy 94.43555 wps 35284.85 step time 0.35s\n","=>> Epoch 3  global step 18240 loss 0.33066 batch 958/1640 lr 0.001 accuracy 93.90820 wps 37608.39 step time 0.43s\n","=>> Epoch 3  global step 18260 loss 0.30482 batch 978/1640 lr 0.001 accuracy 94.34961 wps 35189.67 step time 0.35s\n","=>> Epoch 3  global step 18280 loss 0.32833 batch 998/1640 lr 0.001 accuracy 93.91406 wps 35466.71 step time 0.45s\n","=>> Epoch 3  global step 18300 loss 0.32778 batch 1018/1640 lr 0.001 accuracy 94.03711 wps 36569.11 step time 0.39s\n","=>> Epoch 3  global step 18320 loss 0.30771 batch 1038/1640 lr 0.001 accuracy 94.26758 wps 37215.84 step time 0.42s\n","=>> Epoch 3  global step 18340 loss 0.32228 batch 1058/1640 lr 0.001 accuracy 94.12891 wps 29624.90 step time 0.54s\n","=>> Epoch 3  global step 18360 loss 0.32456 batch 1078/1640 lr 0.001 accuracy 93.97656 wps 32478.31 step time 0.50s\n","=>> Epoch 3  global step 18380 loss 0.30407 batch 1098/1640 lr 0.001 accuracy 94.52344 wps 33232.40 step time 0.40s\n","=>> Epoch 3  global step 18400 loss 0.29820 batch 1118/1640 lr 0.001 accuracy 94.57226 wps 32128.62 step time 0.34s\n","=>> Epoch 3  global step 18420 loss 0.34074 batch 1138/1640 lr 0.001 accuracy 93.66602 wps 30925.54 step time 0.50s\n","=>> Epoch 3  global step 18440 loss 0.31627 batch 1158/1640 lr 0.001 accuracy 94.19727 wps 31416.71 step time 0.49s\n","=>> Epoch 3  global step 18460 loss 0.32736 batch 1178/1640 lr 0.001 accuracy 94.04883 wps 31878.38 step time 0.52s\n","=>> Epoch 3  global step 18480 loss 0.31763 batch 1198/1640 lr 0.001 accuracy 94.28125 wps 29784.11 step time 0.49s\n","=>> Epoch 3  global step 18500 loss 0.32855 batch 1218/1640 lr 0.001 accuracy 94.03711 wps 27549.47 step time 0.59s\n","=>> Epoch 3  global step 18520 loss 0.31026 batch 1238/1640 lr 0.001 accuracy 94.29102 wps 31399.02 step time 0.43s\n","=>> Epoch 3  global step 18540 loss 0.32566 batch 1258/1640 lr 0.001 accuracy 94.11719 wps 30812.38 step time 0.45s\n","=>> Epoch 3  global step 18560 loss 0.30660 batch 1278/1640 lr 0.001 accuracy 94.35547 wps 31866.32 step time 0.43s\n","=>> Epoch 3  global step 18580 loss 0.32073 batch 1298/1640 lr 0.001 accuracy 94.19531 wps 30156.04 step time 0.52s\n","=>> Epoch 3  global step 18600 loss 0.32481 batch 1318/1640 lr 0.001 accuracy 94.06055 wps 29724.11 step time 0.50s\n","=>> Epoch 3  global step 18620 loss 0.31608 batch 1338/1640 lr 0.001 accuracy 94.25195 wps 31164.90 step time 0.48s\n","=>> Epoch 3  global step 18640 loss 0.31846 batch 1358/1640 lr 0.001 accuracy 94.22852 wps 31810.93 step time 0.42s\n","=>> Epoch 3  global step 18660 loss 0.31058 batch 1378/1640 lr 0.001 accuracy 94.29492 wps 32277.89 step time 0.40s\n","=>> Epoch 3  global step 18680 loss 0.32416 batch 1398/1640 lr 0.001 accuracy 94.03711 wps 31830.72 step time 0.42s\n","=>> Epoch 3  global step 18700 loss 0.32202 batch 1418/1640 lr 0.001 accuracy 94.14453 wps 29418.02 step time 0.54s\n","=>> Epoch 3  global step 18720 loss 0.32366 batch 1438/1640 lr 0.001 accuracy 93.99805 wps 32004.95 step time 0.44s\n","=>> Epoch 3  global step 18740 loss 0.31449 batch 1458/1640 lr 0.001 accuracy 94.30859 wps 29499.65 step time 0.50s\n","=>> Epoch 3  global step 18760 loss 0.33248 batch 1478/1640 lr 0.001 accuracy 93.96875 wps 29619.67 step time 0.57s\n","=>> Epoch 3  global step 18780 loss 0.31721 batch 1498/1640 lr 0.001 accuracy 94.24414 wps 28555.94 step time 0.49s\n","=>> Epoch 3  global step 18800 loss 0.30524 batch 1518/1640 lr 0.001 accuracy 94.38672 wps 28345.65 step time 0.49s\n","=>> Epoch 3  global step 18820 loss 0.31554 batch 1538/1640 lr 0.001 accuracy 94.25391 wps 32674.59 step time 0.40s\n","=>> Epoch 3  global step 18840 loss 0.30594 batch 1558/1640 lr 0.001 accuracy 94.36719 wps 31710.67 step time 0.41s\n","=>> Epoch 3  global step 18860 loss 0.31119 batch 1578/1640 lr 0.001 accuracy 94.31641 wps 31154.37 step time 0.46s\n","=>> Epoch 3  global step 18880 loss 0.31144 batch 1598/1640 lr 0.001 accuracy 94.30469 wps 31386.49 step time 0.44s\n","=>> Epoch 3  global step 18900 loss 0.30313 batch 1618/1640 lr 0.001 accuracy 94.53125 wps 32373.88 step time 0.41s\n","=>> Epoch 3  global step 18920 loss 0.30604 batch 1638/1640 lr 0.001 accuracy 94.31836 wps 31696.45 step time 0.43s\n","=>> Finsh epoch 3, global step 18923\n","=>> Epoch 4  global step 18940 loss 0.24801 batch 17/1640 lr 0.001 accuracy 80.44141 wps 34047.81 step time 0.37s\n","=>> Epoch 4  global step 18960 loss 0.31877 batch 37/1640 lr 0.001 accuracy 94.23437 wps 37404.69 step time 0.44s\n","=>> Epoch 4  global step 18980 loss 0.30712 batch 57/1640 lr 0.001 accuracy 94.36133 wps 31731.92 step time 0.42s\n","=>> Epoch 4  global step 19000 loss 0.28540 batch 77/1640 lr 0.001 accuracy 94.72852 wps 32146.47 step time 0.38s\n","=>> global step 19000, eval result: \n","=>> location_traffic_convenience - 0.6598278553948436\n","=>> location_distance_from_business_district - 0.5472322646330412\n","=>> location_easy_to_find - 0.714978364780667\n","=>> service_wait_time - 0.6641093786902418\n","=>> service_waiters_attitude - 0.8027602899030182\n","=>> service_parking_convenience - 0.7456383027484603\n","=>> service_serving_speed - 0.7550667377224574\n","=>> price_level - 0.7809186711414908\n","=>> price_cost_effective - 0.7150828494100092\n","=>> price_discount - 0.6717748250494576\n","=>> environment_decoration - 0.7289775879014412\n","=>> environment_noise - 0.7615168767163277\n","=>> environment_space - 0.7647153286956667\n","=>> environment_cleaness - 0.7552825985860259\n","=>> dish_portion - 0.7251199326295292\n","=>> dish_taste - 0.7306814639938304\n","=>> dish_look - 0.5793397451108097\n","=>> dish_recommendation - 0.7322356601457763\n","=>> others_overall_experience - 0.5939259128195581\n","=>> others_willing_to_consume_again - 0.7148212530219866\n","=>> Eval loss 1.75080, f1 0.70720\n","=>> current result -0.7072002949547318, previous best result -0.7082738418925103\n","=>> No loss decrease, restore previous best model and set learning rate to half of previous one\n","=>> Epoch 4  global step 15020 loss 0.30556 batch 97/1640 lr 0.0001 accuracy 94.46484 wps 31169.07 step time 0.43s\n","=>> Epoch 4  global step 15040 loss 0.32683 batch 117/1640 lr 0.0001 accuracy 94.04492 wps 29737.32 step time 0.52s\n","=>> Epoch 4  global step 15060 loss 0.30944 batch 137/1640 lr 0.0001 accuracy 94.36719 wps 29046.20 step time 0.52s\n","=>> Epoch 4  global step 15080 loss 0.31549 batch 157/1640 lr 0.0001 accuracy 94.26562 wps 32219.27 step time 0.45s\n","=>> Epoch 4  global step 15100 loss 0.31964 batch 177/1640 lr 0.0001 accuracy 94.11719 wps 29781.35 step time 0.51s\n","=>> Epoch 4  global step 15120 loss 0.30341 batch 197/1640 lr 0.0001 accuracy 94.39258 wps 30148.90 step time 0.45s\n","=>> Epoch 4  global step 15140 loss 0.30792 batch 217/1640 lr 0.0001 accuracy 94.32813 wps 30634.00 step time 0.45s\n","=>> Epoch 4  global step 15160 loss 0.29803 batch 237/1640 lr 0.0001 accuracy 94.41797 wps 29058.42 step time 0.48s\n","=>> Epoch 4  global step 15180 loss 0.30755 batch 257/1640 lr 0.0001 accuracy 94.36719 wps 29515.49 step time 0.54s\n","=>> Epoch 4  global step 15200 loss 0.30589 batch 277/1640 lr 0.0001 accuracy 94.35547 wps 30394.60 step time 0.47s\n","=>> Epoch 4  global step 15220 loss 0.30554 batch 297/1640 lr 0.0001 accuracy 94.41406 wps 32290.54 step time 0.43s\n","=>> Epoch 4  global step 15240 loss 0.30219 batch 317/1640 lr 0.0001 accuracy 94.43555 wps 32185.13 step time 0.45s\n","=>> Epoch 4  global step 15260 loss 0.33878 batch 337/1640 lr 0.0001 accuracy 93.85938 wps 31102.87 step time 0.52s\n","=>> Epoch 4  global step 15280 loss 0.32186 batch 357/1640 lr 0.0001 accuracy 94.20508 wps 31358.05 step time 0.51s\n","=>> Epoch 4  global step 15300 loss 0.31939 batch 377/1640 lr 0.0001 accuracy 94.05859 wps 29903.92 step time 0.57s\n","=>> Epoch 4  global step 15320 loss 0.31046 batch 397/1640 lr 0.0001 accuracy 94.42578 wps 31927.73 step time 0.39s\n","=>> Epoch 4  global step 15340 loss 0.30198 batch 417/1640 lr 0.0001 accuracy 94.51758 wps 29199.09 step time 0.46s\n","=>> Epoch 4  global step 15360 loss 0.31399 batch 437/1640 lr 0.0001 accuracy 94.26758 wps 29655.07 step time 0.52s\n","=>> Epoch 4  global step 15380 loss 0.32597 batch 457/1640 lr 0.0001 accuracy 93.90430 wps 31928.38 step time 0.47s\n","=>> Epoch 4  global step 15400 loss 0.29409 batch 477/1640 lr 0.0001 accuracy 94.60352 wps 31943.85 step time 0.37s\n","=>> Epoch 4  global step 15420 loss 0.31520 batch 497/1640 lr 0.0001 accuracy 94.26953 wps 28755.43 step time 0.54s\n","=>> Epoch 4  global step 15440 loss 0.32319 batch 517/1640 lr 0.0001 accuracy 94.01563 wps 33217.74 step time 0.52s\n","=>> Epoch 4  global step 15460 loss 0.29134 batch 537/1640 lr 0.0001 accuracy 94.78125 wps 30827.27 step time 0.37s\n","=>> Epoch 4  global step 15480 loss 0.30520 batch 557/1640 lr 0.0001 accuracy 94.44141 wps 31301.90 step time 0.42s\n","=>> Epoch 4  global step 15500 loss 0.31693 batch 577/1640 lr 0.0001 accuracy 94.18945 wps 31426.79 step time 0.49s\n","=>> Epoch 4  global step 15520 loss 0.31560 batch 597/1640 lr 0.0001 accuracy 94.07813 wps 31999.08 step time 0.45s\n","=>> Epoch 4  global step 15540 loss 0.28733 batch 617/1640 lr 0.0001 accuracy 94.79883 wps 31575.29 step time 0.39s\n","=>> Epoch 4  global step 15560 loss 0.31246 batch 637/1640 lr 0.0001 accuracy 94.30859 wps 31852.90 step time 0.44s\n","=>> Epoch 4  global step 15580 loss 0.30255 batch 657/1640 lr 0.0001 accuracy 94.42383 wps 31956.49 step time 0.42s\n","=>> Epoch 4  global step 15600 loss 0.31548 batch 677/1640 lr 0.0001 accuracy 94.24023 wps 32430.14 step time 0.47s\n","=>> Epoch 4  global step 15620 loss 0.30101 batch 697/1640 lr 0.0001 accuracy 94.52344 wps 32439.18 step time 0.43s\n","=>> Epoch 4  global step 15640 loss 0.31332 batch 717/1640 lr 0.0001 accuracy 94.26172 wps 28572.16 step time 0.51s\n","=>> Epoch 4  global step 15660 loss 0.30929 batch 737/1640 lr 0.0001 accuracy 94.36133 wps 31604.01 step time 0.41s\n","=>> Epoch 4  global step 15680 loss 0.31568 batch 757/1640 lr 0.0001 accuracy 94.11328 wps 29646.98 step time 0.54s\n","=>> Epoch 4  global step 15700 loss 0.30895 batch 777/1640 lr 0.0001 accuracy 94.29297 wps 31545.31 step time 0.48s\n","=>> Epoch 4  global step 15720 loss 0.32514 batch 797/1640 lr 0.0001 accuracy 94.05664 wps 30341.47 step time 0.55s\n","=>> Epoch 4  global step 15740 loss 0.30451 batch 817/1640 lr 0.0001 accuracy 94.34375 wps 31701.33 step time 0.42s\n","=>> Epoch 4  global step 15760 loss 0.30572 batch 837/1640 lr 0.0001 accuracy 94.35352 wps 31065.05 step time 0.51s\n","=>> Epoch 4  global step 15780 loss 0.32777 batch 857/1640 lr 0.0001 accuracy 93.85547 wps 30950.12 step time 0.61s\n","=>> Epoch 4  global step 15800 loss 0.29692 batch 877/1640 lr 0.0001 accuracy 94.52930 wps 31950.07 step time 0.39s\n","=>> Epoch 4  global step 15820 loss 0.30515 batch 897/1640 lr 0.0001 accuracy 94.37500 wps 31061.48 step time 0.42s\n","=>> Epoch 4  global step 15840 loss 0.31825 batch 917/1640 lr 0.0001 accuracy 94.26367 wps 29314.54 step time 0.50s\n","=>> Epoch 4  global step 15860 loss 0.29914 batch 937/1640 lr 0.0001 accuracy 94.49414 wps 29816.89 step time 0.47s\n","=>> Epoch 4  global step 15880 loss 0.31910 batch 957/1640 lr 0.0001 accuracy 94.13281 wps 28608.57 step time 0.58s\n","=>> Epoch 4  global step 15900 loss 0.31102 batch 977/1640 lr 0.0001 accuracy 94.32812 wps 28841.84 step time 0.53s\n","=>> Epoch 4  global step 15920 loss 0.30644 batch 997/1640 lr 0.0001 accuracy 94.36719 wps 30681.98 step time 0.48s\n","=>> Epoch 4  global step 15940 loss 0.31234 batch 1017/1640 lr 0.0001 accuracy 94.27344 wps 29784.50 step time 0.56s\n","=>> Epoch 4  global step 15960 loss 0.31997 batch 1037/1640 lr 0.0001 accuracy 94.18164 wps 29254.92 step time 0.57s\n","=>> Epoch 4  global step 15980 loss 0.31544 batch 1057/1640 lr 0.0001 accuracy 94.32227 wps 31051.56 step time 0.47s\n","=>> Epoch 4  global step 16000 loss 0.31273 batch 1077/1640 lr 0.0001 accuracy 94.25000 wps 32587.57 step time 0.45s\n","=>> global step 16000, eval result: \n","=>> location_traffic_convenience - 0.671182151803276\n","=>> location_distance_from_business_district - 0.5469384319315135\n","=>> location_easy_to_find - 0.7118795222918636\n","=>> service_wait_time - 0.6702143349644434\n","=>> service_waiters_attitude - 0.7988841166924825\n","=>> service_parking_convenience - 0.7356828173827933\n","=>> service_serving_speed - 0.7559374190744496\n","=>> price_level - 0.7814796181823715\n","=>> price_cost_effective - 0.7127874218734567\n","=>> price_discount - 0.6702767313222892\n","=>> environment_decoration - 0.7282006337359752\n","=>> environment_noise - 0.7653057477893483\n","=>> environment_space - 0.7680251208297175\n","=>> environment_cleaness - 0.7575246141745394\n","=>> dish_portion - 0.7260837691806573\n","=>> dish_taste - 0.7322800680507074\n","=>> dish_look - 0.5741959417729097\n","=>> dish_recommendation - 0.7386648654209631\n","=>> others_overall_experience - 0.5985462940089104\n","=>> others_willing_to_consume_again - 0.7090776052968679\n","=>> Eval loss 1.73244, f1 0.70766\n","=>> current result -0.7076583612889767, previous best result -0.7082738418925103\n","=>> Epoch 4  global step 16020 loss 0.31240 batch 1097/1640 lr 0.0001 accuracy 94.25391 wps 30862.33 step time 0.46s\n","=>> Epoch 4  global step 16040 loss 0.30621 batch 1117/1640 lr 0.0001 accuracy 94.37695 wps 31267.77 step time 0.45s\n","=>> Epoch 4  global step 16060 loss 0.31800 batch 1137/1640 lr 0.0001 accuracy 94.17773 wps 29954.61 step time 0.64s\n","=>> Epoch 4  global step 16080 loss 0.31215 batch 1157/1640 lr 0.0001 accuracy 94.12500 wps 29506.02 step time 0.53s\n","=>> Epoch 4  global step 16100 loss 0.30337 batch 1177/1640 lr 0.0001 accuracy 94.33203 wps 30318.75 step time 0.48s\n","=>> Epoch 4  global step 16120 loss 0.30427 batch 1197/1640 lr 0.0001 accuracy 94.44531 wps 31424.93 step time 0.43s\n","=>> Epoch 4  global step 16140 loss 0.30592 batch 1217/1640 lr 0.0001 accuracy 94.35352 wps 30405.18 step time 0.47s\n","=>> Epoch 4  global step 16160 loss 0.31445 batch 1237/1640 lr 0.0001 accuracy 94.31836 wps 32363.04 step time 0.45s\n","=>> Epoch 4  global step 16180 loss 0.30948 batch 1257/1640 lr 0.0001 accuracy 94.26953 wps 32497.46 step time 0.48s\n","=>> Epoch 4  global step 16200 loss 0.29747 batch 1277/1640 lr 0.0001 accuracy 94.55859 wps 30451.90 step time 0.45s\n","=>> Epoch 4  global step 16220 loss 0.31505 batch 1297/1640 lr 0.0001 accuracy 94.20508 wps 30584.31 step time 0.55s\n","=>> Epoch 4  global step 16240 loss 0.30644 batch 1317/1640 lr 0.0001 accuracy 94.43164 wps 30011.07 step time 0.52s\n","=>> Epoch 4  global step 16260 loss 0.30177 batch 1337/1640 lr 0.0001 accuracy 94.38281 wps 31158.15 step time 0.44s\n","=>> Epoch 4  global step 16280 loss 0.30920 batch 1357/1640 lr 0.0001 accuracy 94.26172 wps 30190.15 step time 0.47s\n","=>> Epoch 4  global step 16300 loss 0.29938 batch 1377/1640 lr 0.0001 accuracy 94.51367 wps 28518.23 step time 0.49s\n","=>> Epoch 4  global step 16320 loss 0.30797 batch 1397/1640 lr 0.0001 accuracy 94.41015 wps 31948.20 step time 0.45s\n","=>> Epoch 4  global step 16340 loss 0.30006 batch 1417/1640 lr 0.0001 accuracy 94.49805 wps 30746.83 step time 0.38s\n","=>> Epoch 4  global step 16360 loss 0.31441 batch 1437/1640 lr 0.0001 accuracy 94.15625 wps 29268.92 step time 0.48s\n","=>> Epoch 4  global step 16380 loss 0.31650 batch 1457/1640 lr 0.0001 accuracy 94.17773 wps 32312.46 step time 0.47s\n","=>> Epoch 4  global step 16400 loss 0.31446 batch 1477/1640 lr 0.0001 accuracy 94.15430 wps 31063.63 step time 0.48s\n","=>> Epoch 4  global step 16420 loss 0.29182 batch 1497/1640 lr 0.0001 accuracy 94.58984 wps 30923.14 step time 0.41s\n","=>> Epoch 4  global step 16440 loss 0.31377 batch 1517/1640 lr 0.0001 accuracy 94.20313 wps 29521.94 step time 0.55s\n","=>> Epoch 4  global step 16460 loss 0.30606 batch 1537/1640 lr 0.0001 accuracy 94.43555 wps 31196.03 step time 0.45s\n","=>> Epoch 4  global step 16480 loss 0.32605 batch 1557/1640 lr 0.0001 accuracy 94.09961 wps 32290.66 step time 0.46s\n","=>> Epoch 4  global step 16500 loss 0.30472 batch 1577/1640 lr 0.0001 accuracy 94.36328 wps 31767.50 step time 0.44s\n","=>> Epoch 4  global step 16520 loss 0.31357 batch 1597/1640 lr 0.0001 accuracy 94.23047 wps 31538.86 step time 0.45s\n","=>> Epoch 4  global step 16540 loss 0.31747 batch 1617/1640 lr 0.0001 accuracy 94.11133 wps 28100.59 step time 0.59s\n","=>> Epoch 4  global step 16560 loss 0.31426 batch 1637/1640 lr 0.0001 accuracy 94.18359 wps 29226.08 step time 0.50s\n","=>> Finsh epoch 4, global step 16564\n","=>> Epoch 5  global step 16580 loss 0.23830 batch 16/1640 lr 0.0001 accuracy 75.61328 wps 33742.67 step time 0.34s\n","=>> Epoch 5  global step 16600 loss 0.32465 batch 36/1640 lr 0.0001 accuracy 93.97461 wps 34735.72 step time 0.52s\n","=>> Epoch 5  global step 16620 loss 0.29058 batch 56/1640 lr 0.0001 accuracy 94.69727 wps 34856.05 step time 0.34s\n","=>> Epoch 5  global step 16640 loss 0.32112 batch 76/1640 lr 0.0001 accuracy 94.00195 wps 36093.87 step time 0.39s\n","=>> Epoch 5  global step 16660 loss 0.29535 batch 96/1640 lr 0.0001 accuracy 94.59961 wps 34992.39 step time 0.35s\n","=>> Epoch 5  global step 16680 loss 0.29082 batch 116/1640 lr 0.0001 accuracy 94.64062 wps 35699.57 step time 0.36s\n","=>> Epoch 5  global step 16700 loss 0.28909 batch 136/1640 lr 0.0001 accuracy 94.63672 wps 34679.04 step time 0.43s\n","=>> Epoch 5  global step 16720 loss 0.29982 batch 156/1640 lr 0.0001 accuracy 94.42383 wps 35177.65 step time 0.43s\n","=>> Epoch 5  global step 16740 loss 0.30340 batch 176/1640 lr 0.0001 accuracy 94.47070 wps 36368.64 step time 0.39s\n","=>> Epoch 5  global step 16760 loss 0.31885 batch 196/1640 lr 0.0001 accuracy 94.13477 wps 37695.70 step time 0.44s\n","=>> Epoch 5  global step 16780 loss 0.31146 batch 216/1640 lr 0.0001 accuracy 94.28320 wps 37082.75 step time 0.41s\n","=>> Epoch 5  global step 16800 loss 0.30334 batch 236/1640 lr 0.0001 accuracy 94.46094 wps 35768.42 step time 0.36s\n","=>> Epoch 5  global step 16820 loss 0.30917 batch 256/1640 lr 0.0001 accuracy 94.39648 wps 37257.41 step time 0.39s\n","=>> Epoch 5  global step 16840 loss 0.29931 batch 276/1640 lr 0.0001 accuracy 94.53711 wps 35305.41 step time 0.42s\n","=>> Epoch 5  global step 16860 loss 0.31699 batch 296/1640 lr 0.0001 accuracy 94.18945 wps 36648.70 step time 0.40s\n","=>> Epoch 5  global step 16880 loss 0.29965 batch 316/1640 lr 0.0001 accuracy 94.52148 wps 35841.24 step time 0.37s\n","=>> Epoch 5  global step 16900 loss 0.29251 batch 336/1640 lr 0.0001 accuracy 94.68164 wps 35477.26 step time 0.36s\n","=>> Epoch 5  global step 16920 loss 0.31128 batch 356/1640 lr 0.0001 accuracy 94.25781 wps 36679.52 step time 0.39s\n","=>> Epoch 5  global step 16940 loss 0.31721 batch 376/1640 lr 0.0001 accuracy 94.08398 wps 35335.10 step time 0.44s\n","=>> Epoch 5  global step 16960 loss 0.30811 batch 396/1640 lr 0.0001 accuracy 94.40430 wps 37166.54 step time 0.43s\n","=>> Epoch 5  global step 16980 loss 0.31711 batch 416/1640 lr 0.0001 accuracy 94.10547 wps 36044.72 step time 0.49s\n","=>> Epoch 5  global step 17000 loss 0.29224 batch 436/1640 lr 0.0001 accuracy 94.75586 wps 34936.39 step time 0.35s\n","=>> global step 17000, eval result: \n","=>> location_traffic_convenience - 0.6688299115400178\n","=>> location_distance_from_business_district - 0.5496999015607333\n","=>> location_easy_to_find - 0.713716340998924\n","=>> service_wait_time - 0.6712180996576396\n","=>> service_waiters_attitude - 0.8007363597255683\n","=>> service_parking_convenience - 0.7353795374023849\n","=>> service_serving_speed - 0.7571243141292121\n","=>> price_level - 0.7811716525157751\n","=>> price_cost_effective - 0.7137052943093901\n","=>> price_discount - 0.6686847197537428\n","=>> environment_decoration - 0.727499666478465\n","=>> environment_noise - 0.7632128256246455\n","=>> environment_space - 0.7672380510789347\n","=>> environment_cleaness - 0.7557988704576408\n","=>> dish_portion - 0.7237710219258524\n","=>> dish_taste - 0.7318918146159368\n","=>> dish_look - 0.5724994257378094\n","=>> dish_recommendation - 0.7391140876926633\n","=>> others_overall_experience - 0.5977603310799833\n","=>> others_willing_to_consume_again - 0.7086352713093392\n","=>> Eval loss 1.74193, f1 0.70738\n","=>> current result -0.707384374879733, previous best result -0.7082738418925103\n","=>> Epoch 5  global step 17020 loss 0.30642 batch 456/1640 lr 0.0001 accuracy 94.34766 wps 37409.72 step time 0.43s\n","=>> Epoch 5  global step 17040 loss 0.30650 batch 476/1640 lr 0.0001 accuracy 94.48047 wps 36541.13 step time 0.41s\n","=>> Epoch 5  global step 17060 loss 0.31191 batch 496/1640 lr 0.0001 accuracy 94.28711 wps 35298.61 step time 0.53s\n","=>> Epoch 5  global step 17080 loss 0.28424 batch 516/1640 lr 0.0001 accuracy 94.75195 wps 33500.78 step time 0.37s\n","=>> Epoch 5  global step 17100 loss 0.30969 batch 536/1640 lr 0.0001 accuracy 94.21289 wps 30216.78 step time 0.46s\n","=>> Epoch 5  global step 17120 loss 0.28286 batch 556/1640 lr 0.0001 accuracy 94.88281 wps 30011.08 step time 0.41s\n","=>> Epoch 5  global step 17140 loss 0.32089 batch 576/1640 lr 0.0001 accuracy 94.05078 wps 31771.37 step time 0.51s\n","=>> Epoch 5  global step 17160 loss 0.32031 batch 596/1640 lr 0.0001 accuracy 94.14258 wps 29610.17 step time 0.58s\n","=>> Epoch 5  global step 17180 loss 0.32556 batch 616/1640 lr 0.0001 accuracy 94.04492 wps 30730.00 step time 0.52s\n","=>> Epoch 5  global step 17200 loss 0.29844 batch 636/1640 lr 0.0001 accuracy 94.44922 wps 31382.84 step time 0.45s\n","=>> Epoch 5  global step 17220 loss 0.31212 batch 656/1640 lr 0.0001 accuracy 94.27148 wps 31798.23 step time 0.45s\n","=>> Epoch 5  global step 17240 loss 0.30987 batch 676/1640 lr 0.0001 accuracy 94.48437 wps 31701.19 step time 0.47s\n","=>> Epoch 5  global step 17260 loss 0.31214 batch 696/1640 lr 0.0001 accuracy 94.25000 wps 32086.48 step time 0.45s\n","=>> Epoch 5  global step 17280 loss 0.29981 batch 716/1640 lr 0.0001 accuracy 94.48633 wps 27738.30 step time 0.60s\n","=>> Epoch 5  global step 17300 loss 0.32901 batch 736/1640 lr 0.0001 accuracy 93.98047 wps 30918.22 step time 0.56s\n","=>> Epoch 5  global step 17320 loss 0.28878 batch 756/1640 lr 0.0001 accuracy 94.77539 wps 31584.84 step time 0.41s\n","=>> Epoch 5  global step 17340 loss 0.29787 batch 776/1640 lr 0.0001 accuracy 94.53906 wps 31684.46 step time 0.40s\n","=>> Epoch 5  global step 17360 loss 0.30783 batch 796/1640 lr 0.0001 accuracy 94.36328 wps 31808.03 step time 0.45s\n","=>> Epoch 5  global step 17380 loss 0.31730 batch 816/1640 lr 0.0001 accuracy 94.21094 wps 32358.51 step time 0.49s\n","=>> Epoch 5  global step 17400 loss 0.30561 batch 836/1640 lr 0.0001 accuracy 94.28711 wps 29256.27 step time 0.53s\n","=>> Epoch 5  global step 17420 loss 0.29658 batch 856/1640 lr 0.0001 accuracy 94.67188 wps 30469.04 step time 0.44s\n","=>> Epoch 5  global step 17440 loss 0.31033 batch 876/1640 lr 0.0001 accuracy 94.35742 wps 29539.02 step time 0.55s\n","=>> Epoch 5  global step 17460 loss 0.31351 batch 896/1640 lr 0.0001 accuracy 94.10547 wps 30268.42 step time 0.60s\n","=>> Epoch 5  global step 17480 loss 0.31600 batch 916/1640 lr 0.0001 accuracy 94.20703 wps 30470.78 step time 0.52s\n","=>> Epoch 5  global step 17500 loss 0.30893 batch 936/1640 lr 0.0001 accuracy 94.38477 wps 31018.48 step time 0.46s\n","=>> Epoch 5  global step 17520 loss 0.30897 batch 956/1640 lr 0.0001 accuracy 94.30859 wps 32784.02 step time 0.43s\n","=>> Epoch 5  global step 17540 loss 0.29401 batch 976/1640 lr 0.0001 accuracy 94.61523 wps 29302.07 step time 0.47s\n","=>> Epoch 5  global step 17560 loss 0.30202 batch 996/1640 lr 0.0001 accuracy 94.43945 wps 30450.85 step time 0.47s\n","=>> Epoch 5  global step 17580 loss 0.31058 batch 1016/1640 lr 0.0001 accuracy 94.34766 wps 29062.18 step time 0.55s\n","=>> Epoch 5  global step 17600 loss 0.29673 batch 1036/1640 lr 0.0001 accuracy 94.45508 wps 30889.16 step time 0.45s\n","=>> Epoch 5  global step 17620 loss 0.29092 batch 1056/1640 lr 0.0001 accuracy 94.65039 wps 28984.69 step time 0.52s\n","=>> Epoch 5  global step 17640 loss 0.29397 batch 1076/1640 lr 0.0001 accuracy 94.58789 wps 32706.61 step time 0.40s\n","=>> Epoch 5  global step 17660 loss 0.30280 batch 1096/1640 lr 0.0001 accuracy 94.46484 wps 31293.92 step time 0.44s\n","=>> Epoch 5  global step 17680 loss 0.30576 batch 1116/1640 lr 0.0001 accuracy 94.47852 wps 31897.70 step time 0.40s\n","=>> Epoch 5  global step 17700 loss 0.30618 batch 1136/1640 lr 0.0001 accuracy 94.42187 wps 31168.06 step time 0.42s\n","=>> Epoch 5  global step 17720 loss 0.31756 batch 1156/1640 lr 0.0001 accuracy 94.16016 wps 29398.33 step time 0.50s\n","=>> Epoch 5  global step 17740 loss 0.30273 batch 1176/1640 lr 0.0001 accuracy 94.44727 wps 31597.45 step time 0.44s\n","=>> Epoch 5  global step 17760 loss 0.28429 batch 1196/1640 lr 0.0001 accuracy 94.75000 wps 30743.18 step time 0.40s\n","=>> Epoch 5  global step 17780 loss 0.31444 batch 1216/1640 lr 0.0001 accuracy 94.24609 wps 28382.93 step time 0.65s\n","=>> Epoch 5  global step 17800 loss 0.30783 batch 1236/1640 lr 0.0001 accuracy 94.32031 wps 32099.00 step time 0.43s\n","=>> Epoch 5  global step 17820 loss 0.28859 batch 1256/1640 lr 0.0001 accuracy 94.61328 wps 32065.37 step time 0.39s\n","=>> Epoch 5  global step 17840 loss 0.28679 batch 1276/1640 lr 0.0001 accuracy 94.80078 wps 31117.29 step time 0.40s\n","=>> Epoch 5  global step 17860 loss 0.30577 batch 1296/1640 lr 0.0001 accuracy 94.45508 wps 28292.81 step time 0.58s\n","=>> Epoch 5  global step 17880 loss 0.28703 batch 1316/1640 lr 0.0001 accuracy 94.73438 wps 31054.40 step time 0.38s\n","=>> Epoch 5  global step 17900 loss 0.29746 batch 1336/1640 lr 0.0001 accuracy 94.59766 wps 31579.73 step time 0.43s\n","=>> Epoch 5  global step 17920 loss 0.32230 batch 1356/1640 lr 0.0001 accuracy 94.12109 wps 31719.32 step time 0.50s\n","=>> Epoch 5  global step 17940 loss 0.31709 batch 1376/1640 lr 0.0001 accuracy 94.11719 wps 30812.09 step time 0.51s\n","=>> Epoch 5  global step 17960 loss 0.30912 batch 1396/1640 lr 0.0001 accuracy 94.39844 wps 31881.00 step time 0.43s\n","=>> Epoch 5  global step 17980 loss 0.29505 batch 1416/1640 lr 0.0001 accuracy 94.58398 wps 30696.29 step time 0.40s\n","=>> Epoch 5  global step 18000 loss 0.31240 batch 1436/1640 lr 0.0001 accuracy 94.14258 wps 31688.90 step time 0.50s\n","=>> global step 18000, eval result: \n","=>> location_traffic_convenience - 0.6647060222288395\n","=>> location_distance_from_business_district - 0.5521559230332631\n","=>> location_easy_to_find - 0.7138122708761778\n","=>> service_wait_time - 0.6673172860212522\n","=>> service_waiters_attitude - 0.7998526386196193\n","=>> service_parking_convenience - 0.7400340852897941\n","=>> service_serving_speed - 0.7527212237645561\n","=>> price_level - 0.7807510296474212\n","=>> price_cost_effective - 0.712658352938431\n","=>> price_discount - 0.6733445219230709\n","=>> environment_decoration - 0.7274926226616669\n","=>> environment_noise - 0.7602688519973642\n","=>> environment_space - 0.7654762628893328\n","=>> environment_cleaness - 0.7529912686265173\n","=>> dish_portion - 0.7232840621804085\n","=>> dish_taste - 0.7345006792823621\n","=>> dish_look - 0.5733770889793506\n","=>> dish_recommendation - 0.7383515871616618\n","=>> others_overall_experience - 0.5976533458407405\n","=>> others_willing_to_consume_again - 0.7080648807532861\n","=>> Eval loss 1.74904, f1 0.70694\n","=>> current result -0.7069407002357557, previous best result -0.7082738418925103\n","=>> Epoch 5  global step 18020 loss 0.29639 batch 1456/1640 lr 0.0001 accuracy 94.58594 wps 30889.48 step time 0.47s\n","=>> Epoch 5  global step 18040 loss 0.32181 batch 1476/1640 lr 0.0001 accuracy 94.14844 wps 27308.70 step time 0.62s\n","=>> Epoch 5  global step 18060 loss 0.28949 batch 1496/1640 lr 0.0001 accuracy 94.79883 wps 31258.13 step time 0.41s\n","=>> Epoch 5  global step 18080 loss 0.28327 batch 1516/1640 lr 0.0001 accuracy 94.83398 wps 31184.88 step time 0.35s\n","=>> Epoch 5  global step 18100 loss 0.31373 batch 1536/1640 lr 0.0001 accuracy 94.31055 wps 28684.72 step time 0.67s\n","=>> Epoch 5  global step 18120 loss 0.30886 batch 1556/1640 lr 0.0001 accuracy 94.36914 wps 29102.62 step time 0.56s\n","=>> Epoch 5  global step 18140 loss 0.30825 batch 1576/1640 lr 0.0001 accuracy 94.36133 wps 31637.92 step time 0.45s\n","=>> Epoch 5  global step 18160 loss 0.30595 batch 1596/1640 lr 0.0001 accuracy 94.39453 wps 30378.01 step time 0.51s\n","=>> Epoch 5  global step 18180 loss 0.30099 batch 1616/1640 lr 0.0001 accuracy 94.41406 wps 29542.71 step time 0.52s\n","=>> Epoch 5  global step 18200 loss 0.30507 batch 1636/1640 lr 0.0001 accuracy 94.45703 wps 31339.88 step time 0.43s\n","=>> Finsh epoch 5, global step 18205\n","=>> Epoch 6  global step 18220 loss 0.24586 batch 15/1640 lr 0.0001 accuracy 70.32227 wps 34416.11 step time 0.42s\n","=>> Epoch 6  global step 18240 loss 0.30358 batch 35/1640 lr 0.0001 accuracy 94.51562 wps 35152.28 step time 0.44s\n","=>> Epoch 6  global step 18260 loss 0.29096 batch 55/1640 lr 0.0001 accuracy 94.76367 wps 34783.90 step time 0.43s\n","=>> Epoch 6  global step 18280 loss 0.30144 batch 75/1640 lr 0.0001 accuracy 94.47070 wps 34007.52 step time 0.49s\n","=>> Epoch 6  global step 18300 loss 0.29211 batch 95/1640 lr 0.0001 accuracy 94.67969 wps 36579.18 step time 0.40s\n","=>> Epoch 6  global step 18320 loss 0.29257 batch 115/1640 lr 0.0001 accuracy 94.62695 wps 35901.40 step time 0.39s\n","=>> Epoch 6  global step 18340 loss 0.29381 batch 135/1640 lr 0.0001 accuracy 94.60547 wps 30795.52 step time 0.46s\n","=>> Epoch 6  global step 18360 loss 0.29046 batch 155/1640 lr 0.0001 accuracy 94.70508 wps 31772.58 step time 0.42s\n","=>> Epoch 6  global step 18380 loss 0.30038 batch 175/1640 lr 0.0001 accuracy 94.46875 wps 30690.69 step time 0.41s\n","=>> Epoch 6  global step 18400 loss 0.30674 batch 195/1640 lr 0.0001 accuracy 94.34766 wps 32567.78 step time 0.43s\n","=>> Epoch 6  global step 18420 loss 0.29660 batch 215/1640 lr 0.0001 accuracy 94.55469 wps 30879.46 step time 0.46s\n","=>> Epoch 6  global step 18440 loss 0.30537 batch 235/1640 lr 0.0001 accuracy 94.34570 wps 32298.97 step time 0.41s\n","=>> Epoch 6  global step 18460 loss 0.30711 batch 255/1640 lr 0.0001 accuracy 94.46680 wps 31147.30 step time 0.50s\n","=>> Epoch 6  global step 18480 loss 0.30851 batch 275/1640 lr 0.0001 accuracy 94.41406 wps 30994.84 step time 0.52s\n","=>> Epoch 6  global step 18500 loss 0.30837 batch 295/1640 lr 0.0001 accuracy 94.41406 wps 30887.23 step time 0.50s\n","=>> Epoch 6  global step 18520 loss 0.28861 batch 315/1640 lr 0.0001 accuracy 94.80078 wps 30690.78 step time 0.41s\n","=>> Epoch 6  global step 18540 loss 0.30471 batch 335/1640 lr 0.0001 accuracy 94.54297 wps 29893.99 step time 0.49s\n","=>> Epoch 6  global step 18560 loss 0.31082 batch 355/1640 lr 0.0001 accuracy 94.31055 wps 31849.60 step time 0.49s\n","=>> Epoch 6  global step 18580 loss 0.31407 batch 375/1640 lr 0.0001 accuracy 94.18359 wps 29709.87 step time 0.58s\n","=>> Epoch 6  global step 18600 loss 0.30848 batch 395/1640 lr 0.0001 accuracy 94.28516 wps 29873.40 step time 0.54s\n","=>> Epoch 6  global step 18620 loss 0.29909 batch 415/1640 lr 0.0001 accuracy 94.42187 wps 31989.63 step time 0.48s\n","=>> Epoch 6  global step 18640 loss 0.29445 batch 435/1640 lr 0.0001 accuracy 94.67383 wps 32375.93 step time 0.37s\n","=>> Epoch 6  global step 18660 loss 0.31336 batch 455/1640 lr 0.0001 accuracy 94.30078 wps 28158.56 step time 0.60s\n","=>> Epoch 6  global step 18680 loss 0.33236 batch 475/1640 lr 0.0001 accuracy 93.85742 wps 27542.82 step time 0.82s\n","=>> Epoch 6  global step 18700 loss 0.28781 batch 495/1640 lr 0.0001 accuracy 94.79492 wps 31988.50 step time 0.37s\n","=>> Epoch 6  global step 18720 loss 0.30596 batch 515/1640 lr 0.0001 accuracy 94.38086 wps 31149.47 step time 0.48s\n","=>> Epoch 6  global step 18740 loss 0.30016 batch 535/1640 lr 0.0001 accuracy 94.52344 wps 31774.59 step time 0.45s\n","=>> Epoch 6  global step 18760 loss 0.30576 batch 555/1640 lr 0.0001 accuracy 94.20898 wps 30859.31 step time 0.50s\n","=>> Epoch 6  global step 18780 loss 0.30240 batch 575/1640 lr 0.0001 accuracy 94.39648 wps 31025.77 step time 0.44s\n","=>> Epoch 6  global step 18800 loss 0.30595 batch 595/1640 lr 0.0001 accuracy 94.37500 wps 29977.39 step time 0.55s\n","=>> Epoch 6  global step 18820 loss 0.30163 batch 615/1640 lr 0.0001 accuracy 94.43555 wps 31708.96 step time 0.47s\n","=>> Epoch 6  global step 18840 loss 0.32015 batch 635/1640 lr 0.0001 accuracy 94.08984 wps 29599.11 step time 0.59s\n","=>> Epoch 6  global step 18860 loss 0.30628 batch 655/1640 lr 0.0001 accuracy 94.36914 wps 31810.51 step time 0.45s\n","=>> Epoch 6  global step 18880 loss 0.29975 batch 675/1640 lr 0.0001 accuracy 94.55859 wps 30017.04 step time 0.51s\n","=>> Epoch 6  global step 18900 loss 0.30333 batch 695/1640 lr 0.0001 accuracy 94.51953 wps 30563.00 step time 0.44s\n","=>> Epoch 6  global step 18920 loss 0.29968 batch 715/1640 lr 0.0001 accuracy 94.44922 wps 30796.24 step time 0.47s\n","=>> Epoch 6  global step 18940 loss 0.31054 batch 735/1640 lr 0.0001 accuracy 94.30469 wps 31374.39 step time 0.47s\n","=>> Epoch 6  global step 18960 loss 0.30498 batch 755/1640 lr 0.0001 accuracy 94.37305 wps 28873.06 step time 0.61s\n","=>> Epoch 6  global step 18980 loss 0.30979 batch 775/1640 lr 0.0001 accuracy 94.35156 wps 29037.84 step time 0.55s\n","=>> Epoch 6  global step 19000 loss 0.28497 batch 795/1640 lr 0.0001 accuracy 94.83398 wps 31124.02 step time 0.36s\n","=>> global step 19000, eval result: \n","=>> location_traffic_convenience - 0.6691550204552029\n","=>> location_distance_from_business_district - 0.550035824760356\n","=>> location_easy_to_find - 0.7155443594524402\n","=>> service_wait_time - 0.6684273059260705\n","=>> service_waiters_attitude - 0.8002061274706955\n","=>> service_parking_convenience - 0.7390485909716408\n","=>> service_serving_speed - 0.7523804551915827\n","=>> price_level - 0.7810336753004231\n","=>> price_cost_effective - 0.7130159789249763\n","=>> price_discount - 0.6756519562375495\n","=>> environment_decoration - 0.7274774379973724\n","=>> environment_noise - 0.758514428077864\n","=>> environment_space - 0.7651081960448044\n","=>> environment_cleaness - 0.7515526443013203\n","=>> dish_portion - 0.7222533203796326\n","=>> dish_taste - 0.7323522199816804\n","=>> dish_look - 0.5742920856470274\n","=>> dish_recommendation - 0.7384100476660888\n","=>> others_overall_experience - 0.5974797149050182\n","=>> others_willing_to_consume_again - 0.7101587138992935\n","=>> Eval loss 1.75511, f1 0.70710\n","=>> current result -0.7071049051795519, previous best result -0.7082738418925103\n","=>> No loss decrease, restore previous best model and set learning rate to half of previous one\n","=>> Epoch 6  global step 15020 loss 0.27443 batch 815/1640 lr 1e-05 accuracy 94.88281 wps 30420.10 step time 0.54s\n","=>> Epoch 6  global step 15040 loss 0.25735 batch 835/1640 lr 1e-05 accuracy 95.24219 wps 30872.07 step time 0.45s\n","=>> Epoch 6  global step 15060 loss 0.25606 batch 855/1640 lr 1e-05 accuracy 95.29492 wps 32052.11 step time 0.41s\n","=>> Epoch 6  global step 15080 loss 0.25203 batch 875/1640 lr 1e-05 accuracy 95.42578 wps 30811.01 step time 0.38s\n","=>> Epoch 6  global step 15100 loss 0.25870 batch 895/1640 lr 1e-05 accuracy 95.20508 wps 31285.22 step time 0.40s\n","=>> Epoch 6  global step 15120 loss 0.26104 batch 915/1640 lr 1e-05 accuracy 95.16406 wps 31453.79 step time 0.45s\n","=>> Epoch 6  global step 15140 loss 0.25503 batch 935/1640 lr 1e-05 accuracy 95.27539 wps 31605.53 step time 0.41s\n","=>> Epoch 6  global step 15160 loss 0.24975 batch 955/1640 lr 1e-05 accuracy 95.38477 wps 31096.88 step time 0.45s\n","=>> Epoch 6  global step 15180 loss 0.27046 batch 975/1640 lr 1e-05 accuracy 95.07422 wps 31102.42 step time 0.47s\n","=>> Epoch 6  global step 15200 loss 0.27144 batch 995/1640 lr 1e-05 accuracy 94.95117 wps 32057.73 step time 0.45s\n","=>> Epoch 6  global step 15220 loss 0.26493 batch 1015/1640 lr 1e-05 accuracy 95.15234 wps 28893.38 step time 0.53s\n","=>> Epoch 6  global step 15240 loss 0.25976 batch 1035/1640 lr 1e-05 accuracy 95.08594 wps 31249.38 step time 0.45s\n","=>> Epoch 6  global step 15260 loss 0.26555 batch 1055/1640 lr 1e-05 accuracy 95.08594 wps 31132.31 step time 0.47s\n","=>> Epoch 6  global step 15280 loss 0.26088 batch 1075/1640 lr 1e-05 accuracy 95.27930 wps 31064.30 step time 0.43s\n","=>> Epoch 6  global step 15300 loss 0.27418 batch 1095/1640 lr 1e-05 accuracy 94.96289 wps 31428.97 step time 0.47s\n","=>> Epoch 6  global step 15320 loss 0.25614 batch 1115/1640 lr 1e-05 accuracy 95.30078 wps 31143.31 step time 0.41s\n","=>> Epoch 6  global step 15340 loss 0.24646 batch 1135/1640 lr 1e-05 accuracy 95.49609 wps 28995.80 step time 0.48s\n","=>> Epoch 6  global step 15360 loss 0.27202 batch 1155/1640 lr 1e-05 accuracy 94.96680 wps 29157.20 step time 0.56s\n","=>> Epoch 6  global step 15380 loss 0.26356 batch 1175/1640 lr 1e-05 accuracy 95.11328 wps 29406.61 step time 0.56s\n","=>> Epoch 6  global step 15400 loss 0.28483 batch 1195/1640 lr 1e-05 accuracy 94.73437 wps 32135.28 step time 0.48s\n","=>> Epoch 6  global step 15420 loss 0.26638 batch 1215/1640 lr 1e-05 accuracy 94.99609 wps 31737.73 step time 0.41s\n","=>> Epoch 6  global step 15440 loss 0.26024 batch 1235/1640 lr 1e-05 accuracy 95.16406 wps 30270.99 step time 0.44s\n","=>> Epoch 6  global step 15460 loss 0.26150 batch 1255/1640 lr 1e-05 accuracy 95.13672 wps 30427.13 step time 0.48s\n","=>> Epoch 6  global step 15480 loss 0.26788 batch 1275/1640 lr 1e-05 accuracy 95.08984 wps 30992.47 step time 0.48s\n","=>> Epoch 6  global step 15500 loss 0.26594 batch 1295/1640 lr 1e-05 accuracy 95.12109 wps 30591.89 step time 0.42s\n","=>> Epoch 6  global step 15520 loss 0.27273 batch 1315/1640 lr 1e-05 accuracy 94.96875 wps 27810.10 step time 0.65s\n","=>> Epoch 6  global step 15540 loss 0.26031 batch 1335/1640 lr 1e-05 accuracy 95.14648 wps 28410.57 step time 0.52s\n","=>> Epoch 6  global step 15560 loss 0.27805 batch 1355/1640 lr 1e-05 accuracy 94.78711 wps 32705.76 step time 0.47s\n","=>> Epoch 6  global step 15580 loss 0.25722 batch 1375/1640 lr 1e-05 accuracy 95.30859 wps 32190.02 step time 0.41s\n","=>> Epoch 6  global step 15600 loss 0.25587 batch 1395/1640 lr 1e-05 accuracy 95.27344 wps 31798.78 step time 0.45s\n","=>> Epoch 6  global step 15620 loss 0.26594 batch 1415/1640 lr 1e-05 accuracy 94.99609 wps 28497.83 step time 0.54s\n","=>> Epoch 6  global step 15640 loss 0.25021 batch 1435/1640 lr 1e-05 accuracy 95.36523 wps 28692.69 step time 0.48s\n","=>> Epoch 6  global step 15660 loss 0.28261 batch 1455/1640 lr 1e-05 accuracy 94.66797 wps 30902.90 step time 0.58s\n","=>> Epoch 6  global step 15680 loss 0.26537 batch 1475/1640 lr 1e-05 accuracy 95.05469 wps 29245.91 step time 0.51s\n","=>> Epoch 6  global step 15700 loss 0.25946 batch 1495/1640 lr 1e-05 accuracy 95.21289 wps 31429.59 step time 0.41s\n","=>> Epoch 6  global step 15720 loss 0.26075 batch 1515/1640 lr 1e-05 accuracy 95.24024 wps 29963.68 step time 0.47s\n","=>> Epoch 6  global step 15740 loss 0.25677 batch 1535/1640 lr 1e-05 accuracy 95.31445 wps 31494.49 step time 0.43s\n","=>> Epoch 6  global step 15760 loss 0.25979 batch 1555/1640 lr 1e-05 accuracy 95.30664 wps 31146.83 step time 0.44s\n","=>> Epoch 6  global step 15780 loss 0.25959 batch 1575/1640 lr 1e-05 accuracy 95.20703 wps 31217.64 step time 0.44s\n","=>> Epoch 6  global step 15800 loss 0.24910 batch 1595/1640 lr 1e-05 accuracy 95.43555 wps 30282.70 step time 0.42s\n","=>> Epoch 6  global step 15820 loss 0.25853 batch 1615/1640 lr 1e-05 accuracy 95.25977 wps 31976.54 step time 0.42s\n","=>> Epoch 6  global step 15840 loss 0.26674 batch 1635/1640 lr 1e-05 accuracy 95.15820 wps 31407.63 step time 0.45s\n","=>> Finsh epoch 6, global step 15846\n","=>> Epoch 7  global step 15860 loss 0.20059 batch 14/1640 lr 1e-05 accuracy 66.32812 wps 33554.69 step time 0.46s\n","=>> Epoch 7  global step 15880 loss 0.24665 batch 34/1640 lr 1e-05 accuracy 95.43555 wps 36461.85 step time 0.39s\n","=>> Epoch 7  global step 15900 loss 0.26414 batch 54/1640 lr 1e-05 accuracy 95.09961 wps 36115.52 step time 0.38s\n","=>> Epoch 7  global step 15920 loss 0.27248 batch 74/1640 lr 1e-05 accuracy 95.07812 wps 37183.88 step time 0.43s\n","=>> Epoch 7  global step 15940 loss 0.26202 batch 94/1640 lr 1e-05 accuracy 95.31055 wps 36597.03 step time 0.42s\n","=>> Epoch 7  global step 15960 loss 0.24639 batch 114/1640 lr 1e-05 accuracy 95.45312 wps 34011.76 step time 0.41s\n","=>> Epoch 7  global step 15980 loss 0.26098 batch 134/1640 lr 1e-05 accuracy 95.11719 wps 35227.29 step time 0.44s\n","=>> Epoch 7  global step 16000 loss 0.27853 batch 154/1640 lr 1e-05 accuracy 94.87305 wps 34515.36 step time 0.58s\n","=>> global step 16000, eval result: \n","=>> location_traffic_convenience - 0.6666481282713297\n","=>> location_distance_from_business_district - 0.5333194516404309\n","=>> location_easy_to_find - 0.716227724607384\n","=>> service_wait_time - 0.6736563467027156\n","=>> service_waiters_attitude - 0.797775812779652\n","=>> service_parking_convenience - 0.7342573996784725\n","=>> service_serving_speed - 0.7541859806561138\n","=>> price_level - 0.7801034010912739\n","=>> price_cost_effective - 0.715599629221662\n","=>> price_discount - 0.6661737392370377\n","=>> environment_decoration - 0.7262726367050246\n","=>> environment_noise - 0.7671304661286855\n","=>> environment_space - 0.7674431224520212\n","=>> environment_cleaness - 0.7557411974232073\n","=>> dish_portion - 0.7245008754615189\n","=>> dish_taste - 0.7323301475374318\n","=>> dish_look - 0.5739509625023484\n","=>> dish_recommendation - 0.7374031058238847\n","=>> others_overall_experience - 0.5949835037102505\n","=>> others_willing_to_consume_again - 0.7039016900944368\n","=>> Eval loss 1.73558, f1 0.70608\n","=>> current result -0.7060802660862441, previous best result -0.7082738418925103\n","=>> Epoch 7  global step 16020 loss 0.27136 batch 174/1640 lr 1e-05 accuracy 95.06445 wps 37018.35 step time 0.43s\n","=>> Epoch 7  global step 16040 loss 0.25288 batch 194/1640 lr 1e-05 accuracy 95.34766 wps 34406.70 step time 0.34s\n","=>> Epoch 7  global step 16060 loss 0.26325 batch 214/1640 lr 1e-05 accuracy 95.11523 wps 36584.95 step time 0.41s\n","=>> Epoch 7  global step 16080 loss 0.26993 batch 234/1640 lr 1e-05 accuracy 94.94531 wps 35505.21 step time 0.47s\n","=>> Epoch 7  global step 16100 loss 0.27309 batch 254/1640 lr 1e-05 accuracy 94.91016 wps 34228.24 step time 0.50s\n","=>> Epoch 7  global step 16120 loss 0.26576 batch 274/1640 lr 1e-05 accuracy 95.22070 wps 34634.35 step time 0.40s\n","=>> Epoch 7  global step 16140 loss 0.25138 batch 294/1640 lr 1e-05 accuracy 95.32422 wps 31647.09 step time 0.42s\n","=>> Epoch 7  global step 16160 loss 0.25895 batch 314/1640 lr 1e-05 accuracy 95.29883 wps 30317.40 step time 0.47s\n","=>> Epoch 7  global step 16180 loss 0.26970 batch 334/1640 lr 1e-05 accuracy 95.09180 wps 28917.80 step time 0.60s\n","=>> Epoch 7  global step 16200 loss 0.25828 batch 354/1640 lr 1e-05 accuracy 95.29102 wps 30019.06 step time 0.42s\n","=>> Epoch 7  global step 16220 loss 0.26880 batch 374/1640 lr 1e-05 accuracy 95.08594 wps 30086.96 step time 0.53s\n","=>> Epoch 7  global step 16240 loss 0.24647 batch 394/1640 lr 1e-05 accuracy 95.45703 wps 31674.03 step time 0.41s\n","=>> Epoch 7  global step 16260 loss 0.26222 batch 414/1640 lr 1e-05 accuracy 95.06641 wps 32334.18 step time 0.42s\n","=>> Epoch 7  global step 16280 loss 0.27182 batch 434/1640 lr 1e-05 accuracy 94.89648 wps 29884.93 step time 0.57s\n","=>> Epoch 7  global step 16300 loss 0.26575 batch 454/1640 lr 1e-05 accuracy 95.17774 wps 33861.91 step time 0.49s\n","=>> Epoch 7  global step 16320 loss 0.26269 batch 474/1640 lr 1e-05 accuracy 95.10938 wps 30666.27 step time 0.49s\n","=>> Epoch 7  global step 16340 loss 0.26203 batch 494/1640 lr 1e-05 accuracy 95.00977 wps 30725.31 step time 0.48s\n","=>> Epoch 7  global step 16360 loss 0.24930 batch 514/1640 lr 1e-05 accuracy 95.33789 wps 30816.72 step time 0.41s\n","=>> Epoch 7  global step 16380 loss 0.25941 batch 534/1640 lr 1e-05 accuracy 95.12109 wps 29786.67 step time 0.51s\n","=>> Epoch 7  global step 16400 loss 0.25328 batch 554/1640 lr 1e-05 accuracy 95.29297 wps 30019.41 step time 0.47s\n","=>> Epoch 7  global step 16420 loss 0.27256 batch 574/1640 lr 1e-05 accuracy 94.92187 wps 30100.07 step time 0.53s\n","=>> Epoch 7  global step 16440 loss 0.26289 batch 594/1640 lr 1e-05 accuracy 95.07617 wps 32287.20 step time 0.45s\n","=>> Epoch 7  global step 16460 loss 0.26165 batch 614/1640 lr 1e-05 accuracy 95.20117 wps 30880.38 step time 0.43s\n","=>> Epoch 7  global step 16480 loss 0.26081 batch 634/1640 lr 1e-05 accuracy 95.08984 wps 30631.99 step time 0.47s\n","=>> Epoch 7  global step 16500 loss 0.27677 batch 654/1640 lr 1e-05 accuracy 94.94141 wps 30003.64 step time 0.58s\n","=>> Epoch 7  global step 16520 loss 0.24582 batch 674/1640 lr 1e-05 accuracy 95.41602 wps 31512.82 step time 0.37s\n","=>> Epoch 7  global step 16540 loss 0.26141 batch 694/1640 lr 1e-05 accuracy 95.21094 wps 30605.30 step time 0.48s\n","=>> Epoch 7  global step 16560 loss 0.26284 batch 714/1640 lr 1e-05 accuracy 95.14453 wps 32003.36 step time 0.43s\n","=>> Epoch 7  global step 16580 loss 0.26909 batch 734/1640 lr 1e-05 accuracy 94.97461 wps 32074.31 step time 0.49s\n","=>> Epoch 7  global step 16600 loss 0.27277 batch 754/1640 lr 1e-05 accuracy 94.91602 wps 29912.21 step time 0.54s\n","=>> Epoch 7  global step 16620 loss 0.26915 batch 774/1640 lr 1e-05 accuracy 95.06250 wps 31046.57 step time 0.51s\n","=>> Epoch 7  global step 16640 loss 0.25722 batch 794/1640 lr 1e-05 accuracy 95.14453 wps 32056.73 step time 0.41s\n","=>> Epoch 7  global step 16660 loss 0.27345 batch 814/1640 lr 1e-05 accuracy 94.93555 wps 28286.19 step time 0.65s\n","=>> Epoch 7  global step 16680 loss 0.26744 batch 834/1640 lr 1e-05 accuracy 95.11523 wps 29564.98 step time 0.55s\n","=>> Epoch 7  global step 16700 loss 0.25450 batch 854/1640 lr 1e-05 accuracy 95.32031 wps 28280.22 step time 0.50s\n","=>> Epoch 7  global step 16720 loss 0.25719 batch 874/1640 lr 1e-05 accuracy 95.30469 wps 31503.57 step time 0.45s\n","=>> Epoch 7  global step 16740 loss 0.25101 batch 894/1640 lr 1e-05 accuracy 95.45508 wps 32026.15 step time 0.41s\n","=>> Epoch 7  global step 16760 loss 0.25284 batch 914/1640 lr 1e-05 accuracy 95.34375 wps 32257.44 step time 0.39s\n","=>> Epoch 7  global step 16780 loss 0.25489 batch 934/1640 lr 1e-05 accuracy 95.38086 wps 31852.82 step time 0.42s\n","=>> Epoch 7  global step 16800 loss 0.24810 batch 954/1640 lr 1e-05 accuracy 95.34961 wps 30099.37 step time 0.47s\n","=>> Epoch 7  global step 16820 loss 0.24945 batch 974/1640 lr 1e-05 accuracy 95.46680 wps 32043.73 step time 0.40s\n","=>> Epoch 7  global step 16840 loss 0.27489 batch 994/1640 lr 1e-05 accuracy 94.98633 wps 31498.62 step time 0.46s\n","=>> Epoch 7  global step 16860 loss 0.25423 batch 1014/1640 lr 1e-05 accuracy 95.22852 wps 30946.61 step time 0.43s\n","=>> Epoch 7  global step 16880 loss 0.24663 batch 1034/1640 lr 1e-05 accuracy 95.59375 wps 32482.24 step time 0.37s\n","=>> Epoch 7  global step 16900 loss 0.25457 batch 1054/1640 lr 1e-05 accuracy 95.32422 wps 29646.59 step time 0.45s\n","=>> Epoch 7  global step 16920 loss 0.25836 batch 1074/1640 lr 1e-05 accuracy 95.25977 wps 30307.36 step time 0.54s\n","=>> Epoch 7  global step 16940 loss 0.24504 batch 1094/1640 lr 1e-05 accuracy 95.37500 wps 30694.26 step time 0.47s\n","=>> Epoch 7  global step 16960 loss 0.23875 batch 1114/1640 lr 1e-05 accuracy 95.66602 wps 31877.35 step time 0.38s\n","=>> Epoch 7  global step 16980 loss 0.26743 batch 1134/1640 lr 1e-05 accuracy 95.02148 wps 31088.44 step time 0.52s\n","=>> Epoch 7  global step 17000 loss 0.26924 batch 1154/1640 lr 1e-05 accuracy 94.97656 wps 30154.59 step time 0.52s\n","=>> global step 17000, eval result: \n","=>> location_traffic_convenience - 0.6679400230410166\n","=>> location_distance_from_business_district - 0.5266514772207682\n","=>> location_easy_to_find - 0.7123659747410239\n","=>> service_wait_time - 0.665270302264084\n","=>> service_waiters_attitude - 0.7961692855678095\n","=>> service_parking_convenience - 0.7316285639998888\n","=>> service_serving_speed - 0.7539276760587872\n","=>> price_level - 0.7781118825936384\n","=>> price_cost_effective - 0.7166792160864095\n","=>> price_discount - 0.6627241606986423\n","=>> environment_decoration - 0.7270328612325078\n","=>> environment_noise - 0.7673287296421486\n","=>> environment_space - 0.7656733014918901\n","=>> environment_cleaness - 0.7525298568266426\n","=>> dish_portion - 0.7221238468930495\n","=>> dish_taste - 0.7317491455499483\n","=>> dish_look - 0.5756921755762481\n","=>> dish_recommendation - 0.7371633081400646\n","=>> others_overall_experience - 0.5950015271145987\n","=>> others_willing_to_consume_again - 0.7034892722481494\n","=>> Eval loss 1.74847, f1 0.70446\n","=>> current result -0.7044626293493659, previous best result -0.7082738418925103\n","=>> Epoch 7  global step 17020 loss 0.26234 batch 1174/1640 lr 1e-05 accuracy 95.07812 wps 29924.53 step time 0.50s\n","=>> Epoch 7  global step 17040 loss 0.24419 batch 1194/1640 lr 1e-05 accuracy 95.47852 wps 32573.14 step time 0.38s\n","=>> Epoch 7  global step 17060 loss 0.27454 batch 1214/1640 lr 1e-05 accuracy 94.93750 wps 29384.53 step time 0.54s\n","=>> Epoch 7  global step 17080 loss 0.25867 batch 1234/1640 lr 1e-05 accuracy 95.25391 wps 31168.58 step time 0.44s\n","=>> Epoch 7  global step 17100 loss 0.27158 batch 1254/1640 lr 1e-05 accuracy 95.01563 wps 29612.59 step time 0.58s\n","=>> Epoch 7  global step 17120 loss 0.24236 batch 1274/1640 lr 1e-05 accuracy 95.56055 wps 30552.62 step time 0.39s\n","=>> Epoch 7  global step 17140 loss 0.24891 batch 1294/1640 lr 1e-05 accuracy 95.43555 wps 31525.46 step time 0.43s\n","=>> Epoch 7  global step 17160 loss 0.24931 batch 1314/1640 lr 1e-05 accuracy 95.43750 wps 30787.67 step time 0.48s\n","=>> Epoch 7  global step 17180 loss 0.27506 batch 1334/1640 lr 1e-05 accuracy 94.83594 wps 28692.65 step time 0.58s\n","=>> Epoch 7  global step 17200 loss 0.24821 batch 1354/1640 lr 1e-05 accuracy 95.43555 wps 30339.52 step time 0.43s\n","=>> Epoch 7  global step 17220 loss 0.26384 batch 1374/1640 lr 1e-05 accuracy 95.08399 wps 31566.46 step time 0.44s\n","=>> Epoch 7  global step 17240 loss 0.25618 batch 1394/1640 lr 1e-05 accuracy 95.29297 wps 31238.94 step time 0.44s\n","=>> Epoch 7  global step 17260 loss 0.26337 batch 1414/1640 lr 1e-05 accuracy 95.17188 wps 29336.74 step time 0.58s\n","=>> Epoch 7  global step 17280 loss 0.26583 batch 1434/1640 lr 1e-05 accuracy 95.02734 wps 31713.79 step time 0.48s\n","=>> Epoch 7  global step 17300 loss 0.23284 batch 1454/1640 lr 1e-05 accuracy 95.73633 wps 30485.22 step time 0.37s\n","=>> Epoch 7  global step 17320 loss 0.24449 batch 1474/1640 lr 1e-05 accuracy 95.54297 wps 30794.69 step time 0.37s\n","=>> Epoch 7  global step 17340 loss 0.27443 batch 1494/1640 lr 1e-05 accuracy 94.90625 wps 33204.47 step time 0.50s\n","=>> Epoch 7  global step 17360 loss 0.26396 batch 1514/1640 lr 1e-05 accuracy 95.15039 wps 32065.12 step time 0.43s\n","=>> Epoch 7  global step 17380 loss 0.24015 batch 1534/1640 lr 1e-05 accuracy 95.64453 wps 31121.67 step time 0.36s\n","=>> Epoch 7  global step 17400 loss 0.26631 batch 1554/1640 lr 1e-05 accuracy 95.13281 wps 31689.50 step time 0.48s\n","=>> Epoch 7  global step 17420 loss 0.24981 batch 1574/1640 lr 1e-05 accuracy 95.39062 wps 28927.00 step time 0.51s\n","=>> Epoch 7  global step 17440 loss 0.28271 batch 1594/1640 lr 1e-05 accuracy 94.75000 wps 33084.00 step time 0.49s\n","=>> Epoch 7  global step 17460 loss 0.25514 batch 1614/1640 lr 1e-05 accuracy 95.34570 wps 28032.40 step time 0.51s\n","=>> Epoch 7  global step 17480 loss 0.25095 batch 1634/1640 lr 1e-05 accuracy 95.32422 wps 31210.07 step time 0.44s\n","=>> Finsh epoch 7, global step 17487\n","=>> Epoch 8  global step 17500 loss 0.18028 batch 13/1640 lr 1e-05 accuracy 61.65039 wps 35584.51 step time 0.33s\n","=>> Epoch 8  global step 17520 loss 0.25056 batch 33/1640 lr 1e-05 accuracy 95.32812 wps 36104.14 step time 0.38s\n","=>> Epoch 8  global step 17540 loss 0.26542 batch 53/1640 lr 1e-05 accuracy 95.08203 wps 35000.47 step time 0.51s\n","=>> Epoch 8  global step 17560 loss 0.27319 batch 73/1640 lr 1e-05 accuracy 95.00391 wps 37071.76 step time 0.44s\n","=>> Epoch 8  global step 17580 loss 0.24836 batch 93/1640 lr 1e-05 accuracy 95.46094 wps 35858.17 step time 0.37s\n","=>> Epoch 8  global step 17600 loss 0.26597 batch 113/1640 lr 1e-05 accuracy 95.00977 wps 36472.25 step time 0.40s\n","=>> Epoch 8  global step 17620 loss 0.26064 batch 133/1640 lr 1e-05 accuracy 95.31836 wps 35769.50 step time 0.45s\n","=>> Epoch 8  global step 17640 loss 0.24788 batch 153/1640 lr 1e-05 accuracy 95.38867 wps 34426.14 step time 0.43s\n","=>> Epoch 8  global step 17660 loss 0.25823 batch 173/1640 lr 1e-05 accuracy 95.15820 wps 36224.39 step time 0.40s\n","=>> Epoch 8  global step 17680 loss 0.25911 batch 193/1640 lr 1e-05 accuracy 95.16797 wps 35880.74 step time 0.38s\n","=>> Epoch 8  global step 17700 loss 0.26386 batch 213/1640 lr 1e-05 accuracy 95.13672 wps 36355.50 step time 0.39s\n","=>> Epoch 8  global step 17720 loss 0.27565 batch 233/1640 lr 1e-05 accuracy 94.97852 wps 35686.41 step time 0.47s\n","=>> Epoch 8  global step 17740 loss 0.25863 batch 253/1640 lr 1e-05 accuracy 95.22461 wps 36760.41 step time 0.42s\n","=>> Epoch 8  global step 17760 loss 0.24106 batch 273/1640 lr 1e-05 accuracy 95.50977 wps 34867.22 step time 0.35s\n","=>> Epoch 8  global step 17780 loss 0.24945 batch 293/1640 lr 1e-05 accuracy 95.46680 wps 35802.87 step time 0.39s\n","=>> Epoch 8  global step 17800 loss 0.26260 batch 313/1640 lr 1e-05 accuracy 95.14453 wps 35488.20 step time 0.37s\n","=>> Epoch 8  global step 17820 loss 0.23932 batch 333/1640 lr 1e-05 accuracy 95.62695 wps 35383.08 step time 0.36s\n","=>> Epoch 8  global step 17840 loss 0.25400 batch 353/1640 lr 1e-05 accuracy 95.34961 wps 35910.01 step time 0.38s\n","=>> Epoch 8  global step 17860 loss 0.26430 batch 373/1640 lr 1e-05 accuracy 95.22070 wps 36396.96 step time 0.40s\n","=>> Epoch 8  global step 17880 loss 0.26483 batch 393/1640 lr 1e-05 accuracy 95.17578 wps 36337.66 step time 0.39s\n","=>> Epoch 8  global step 17900 loss 0.26323 batch 413/1640 lr 1e-05 accuracy 95.12305 wps 37750.42 step time 0.46s\n","=>> Epoch 8  global step 17920 loss 0.26166 batch 433/1640 lr 1e-05 accuracy 95.16797 wps 36795.29 step time 0.43s\n","=>> Epoch 8  global step 17940 loss 0.25352 batch 453/1640 lr 1e-05 accuracy 95.33984 wps 35884.85 step time 0.37s\n","=>> Epoch 8  global step 17960 loss 0.24777 batch 473/1640 lr 1e-05 accuracy 95.39844 wps 30524.60 step time 0.43s\n","=>> Epoch 8  global step 17980 loss 0.24333 batch 493/1640 lr 1e-05 accuracy 95.41211 wps 31492.22 step time 0.41s\n","=>> Epoch 8  global step 18000 loss 0.24311 batch 513/1640 lr 1e-05 accuracy 95.48633 wps 31134.27 step time 0.45s\n","=>> global step 18000, eval result: \n","=>> location_traffic_convenience - 0.6732111084727966\n","=>> location_distance_from_business_district - 0.537961567641762\n","=>> location_easy_to_find - 0.711597855418854\n","=>> service_wait_time - 0.6693814217064973\n","=>> service_waiters_attitude - 0.796980849803113\n","=>> service_parking_convenience - 0.7304904564955289\n","=>> service_serving_speed - 0.7565016640744432\n","=>> price_level - 0.7783104665870697\n","=>> price_cost_effective - 0.7160269942813147\n","=>> price_discount - 0.6693835193338357\n","=>> environment_decoration - 0.7263227723998342\n","=>> environment_noise - 0.7628046672291302\n","=>> environment_space - 0.7639123981309694\n","=>> environment_cleaness - 0.7535652994450983\n","=>> dish_portion - 0.722479751313846\n","=>> dish_taste - 0.7310265514511276\n","=>> dish_look - 0.5761430863790049\n","=>> dish_recommendation - 0.7385056690410637\n","=>> others_overall_experience - 0.5964837636075183\n","=>> others_willing_to_consume_again - 0.7027110637085218\n","=>> Eval loss 1.75665, f1 0.70569\n","=>> current result -0.7056900463260665, previous best result -0.7082738418925103\n","=>> Epoch 8  global step 18020 loss 0.25834 batch 533/1640 lr 1e-05 accuracy 95.38477 wps 30645.19 step time 0.47s\n","=>> Epoch 8  global step 18040 loss 0.25903 batch 553/1640 lr 1e-05 accuracy 95.19922 wps 31414.30 step time 0.49s\n","=>> Epoch 8  global step 18060 loss 0.25501 batch 573/1640 lr 1e-05 accuracy 95.21875 wps 32189.19 step time 0.44s\n","=>> Epoch 8  global step 18080 loss 0.26004 batch 593/1640 lr 1e-05 accuracy 95.19336 wps 32207.20 step time 0.44s\n","=>> Epoch 8  global step 18100 loss 0.26228 batch 613/1640 lr 1e-05 accuracy 95.17383 wps 27742.80 step time 0.54s\n","=>> Epoch 8  global step 18120 loss 0.26093 batch 633/1640 lr 1e-05 accuracy 95.25586 wps 30748.93 step time 0.49s\n","=>> Epoch 8  global step 18140 loss 0.26648 batch 653/1640 lr 1e-05 accuracy 95.14648 wps 29798.09 step time 0.59s\n","=>> Epoch 8  global step 18160 loss 0.26947 batch 673/1640 lr 1e-05 accuracy 95.10352 wps 27549.52 step time 0.69s\n","=>> Epoch 8  global step 18180 loss 0.25477 batch 693/1640 lr 1e-05 accuracy 95.33594 wps 31051.41 step time 0.45s\n","=>> Epoch 8  global step 18200 loss 0.24144 batch 713/1640 lr 1e-05 accuracy 95.57031 wps 31659.41 step time 0.40s\n","=>> Epoch 8  global step 18220 loss 0.26054 batch 733/1640 lr 1e-05 accuracy 95.23242 wps 30139.04 step time 0.54s\n","=>> Epoch 8  global step 18240 loss 0.25436 batch 753/1640 lr 1e-05 accuracy 95.37109 wps 33499.07 step time 0.46s\n","=>> Epoch 8  global step 18260 loss 0.26616 batch 773/1640 lr 1e-05 accuracy 95.14258 wps 31735.11 step time 0.49s\n","=>> Epoch 8  global step 18280 loss 0.26614 batch 793/1640 lr 1e-05 accuracy 95.10742 wps 29943.95 step time 0.54s\n","=>> Epoch 8  global step 18300 loss 0.25698 batch 813/1640 lr 1e-05 accuracy 95.24023 wps 30966.07 step time 0.43s\n","=>> Epoch 8  global step 18320 loss 0.24771 batch 833/1640 lr 1e-05 accuracy 95.38086 wps 32267.48 step time 0.42s\n","=>> Epoch 8  global step 18340 loss 0.25088 batch 853/1640 lr 1e-05 accuracy 95.41016 wps 30940.96 step time 0.40s\n","=>> Epoch 8  global step 18360 loss 0.25862 batch 873/1640 lr 1e-05 accuracy 95.14844 wps 32024.23 step time 0.46s\n","=>> Epoch 8  global step 18380 loss 0.25563 batch 893/1640 lr 1e-05 accuracy 95.24219 wps 29518.46 step time 0.51s\n","=>> Epoch 8  global step 18400 loss 0.24094 batch 913/1640 lr 1e-05 accuracy 95.60547 wps 31855.46 step time 0.40s\n","=>> Epoch 8  global step 18420 loss 0.25154 batch 933/1640 lr 1e-05 accuracy 95.32813 wps 30584.31 step time 0.42s\n","=>> Epoch 8  global step 18440 loss 0.26162 batch 953/1640 lr 1e-05 accuracy 95.18750 wps 32396.01 step time 0.44s\n","=>> Epoch 8  global step 18460 loss 0.25819 batch 973/1640 lr 1e-05 accuracy 95.19336 wps 29410.57 step time 0.49s\n","=>> Epoch 8  global step 18480 loss 0.25544 batch 993/1640 lr 1e-05 accuracy 95.20703 wps 31374.32 step time 0.45s\n","=>> Epoch 8  global step 18500 loss 0.24820 batch 1013/1640 lr 1e-05 accuracy 95.46680 wps 30806.99 step time 0.42s\n","=>> Epoch 8  global step 18520 loss 0.24681 batch 1033/1640 lr 1e-05 accuracy 95.35938 wps 31660.58 step time 0.43s\n","=>> Epoch 8  global step 18540 loss 0.25755 batch 1053/1640 lr 1e-05 accuracy 95.19531 wps 30863.18 step time 0.44s\n","=>> Epoch 8  global step 18560 loss 0.25077 batch 1073/1640 lr 1e-05 accuracy 95.35547 wps 32138.02 step time 0.45s\n","=>> Epoch 8  global step 18580 loss 0.24741 batch 1093/1640 lr 1e-05 accuracy 95.46289 wps 30829.96 step time 0.41s\n","=>> Epoch 8  global step 18600 loss 0.26246 batch 1113/1640 lr 1e-05 accuracy 95.11914 wps 30658.33 step time 0.51s\n","=>> Epoch 8  global step 18620 loss 0.25153 batch 1133/1640 lr 1e-05 accuracy 95.42383 wps 28695.99 step time 0.49s\n","=>> Epoch 8  global step 18640 loss 0.26155 batch 1153/1640 lr 1e-05 accuracy 95.10742 wps 31336.85 step time 0.43s\n","=>> Epoch 8  global step 18660 loss 0.26252 batch 1173/1640 lr 1e-05 accuracy 95.25000 wps 30339.78 step time 0.43s\n","=>> Epoch 8  global step 18680 loss 0.25854 batch 1193/1640 lr 1e-05 accuracy 95.22266 wps 31832.31 step time 0.47s\n","=>> Epoch 8  global step 18700 loss 0.26837 batch 1213/1640 lr 1e-05 accuracy 95.10352 wps 29625.50 step time 0.64s\n","=>> Epoch 8  global step 18720 loss 0.23856 batch 1233/1640 lr 1e-05 accuracy 95.62891 wps 30429.29 step time 0.38s\n","=>> Epoch 8  global step 18740 loss 0.25538 batch 1253/1640 lr 1e-05 accuracy 95.31250 wps 31178.94 step time 0.48s\n","=>> Epoch 8  global step 18760 loss 0.24921 batch 1273/1640 lr 1e-05 accuracy 95.44141 wps 31911.06 step time 0.39s\n","=>> Epoch 8  global step 18780 loss 0.25643 batch 1293/1640 lr 1e-05 accuracy 95.19922 wps 31485.57 step time 0.45s\n","=>> Epoch 8  global step 18800 loss 0.25320 batch 1313/1640 lr 1e-05 accuracy 95.37891 wps 32020.46 step time 0.41s\n","=>> Epoch 8  global step 18820 loss 0.26307 batch 1333/1640 lr 1e-05 accuracy 95.18750 wps 29217.33 step time 0.56s\n","=>> Epoch 8  global step 18840 loss 0.25885 batch 1353/1640 lr 1e-05 accuracy 95.18750 wps 31353.40 step time 0.47s\n","=>> Epoch 8  global step 18860 loss 0.24989 batch 1373/1640 lr 1e-05 accuracy 95.46680 wps 32012.88 step time 0.37s\n","=>> Epoch 8  global step 18880 loss 0.27048 batch 1393/1640 lr 1e-05 accuracy 94.99805 wps 32343.36 step time 0.49s\n","=>> Epoch 8  global step 18900 loss 0.26373 batch 1413/1640 lr 1e-05 accuracy 95.10547 wps 29186.19 step time 0.53s\n","=>> Epoch 8  global step 18920 loss 0.26610 batch 1433/1640 lr 1e-05 accuracy 95.07227 wps 28699.13 step time 0.65s\n","=>> Epoch 8  global step 18940 loss 0.25670 batch 1453/1640 lr 1e-05 accuracy 95.19336 wps 31303.77 step time 0.50s\n","=>> Epoch 8  global step 18960 loss 0.24232 batch 1473/1640 lr 1e-05 accuracy 95.53125 wps 30239.67 step time 0.40s\n","=>> Epoch 8  global step 18980 loss 0.24721 batch 1493/1640 lr 1e-05 accuracy 95.36523 wps 30798.65 step time 0.42s\n","=>> Epoch 8  global step 19000 loss 0.27209 batch 1513/1640 lr 1e-05 accuracy 94.97852 wps 27926.21 step time 0.71s\n","=>> global step 19000, eval result: \n","=>> location_traffic_convenience - 0.6684616878751006\n","=>> location_distance_from_business_district - 0.5454315178293633\n","=>> location_easy_to_find - 0.7091028814069432\n","=>> service_wait_time - 0.6690038609767021\n","=>> service_waiters_attitude - 0.79629537869638\n","=>> service_parking_convenience - 0.7325906440924445\n","=>> service_serving_speed - 0.7567933240769564\n","=>> price_level - 0.7778692701253397\n","=>> price_cost_effective - 0.7146200453422314\n","=>> price_discount - 0.6685891928726757\n","=>> environment_decoration - 0.7283266148921193\n","=>> environment_noise - 0.7607263797392676\n","=>> environment_space - 0.763035540561688\n","=>> environment_cleaness - 0.7547706888037211\n","=>> dish_portion - 0.7231846050531802\n","=>> dish_taste - 0.7319223225719983\n","=>> dish_look - 0.573933908304679\n","=>> dish_recommendation - 0.7387469006153723\n","=>> others_overall_experience - 0.59681206381255\n","=>> others_willing_to_consume_again - 0.7073480754661263\n","=>> Eval loss 1.76243, f1 0.70588\n","=>> current result -0.7058782451557419, previous best result -0.7082738418925103\n","=>> No loss decrease, restore previous best model and set learning rate to half of previous one\n","=>> Early stop, exit\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-tJdkKQp1kVl","colab_type":"code","colab":{}},"source":["!python elmo_run.py \\\n","--mode=inference \\\n","--data_files=data/testa.json \\\n","--label_file=data/labels.txt \\\n","--vocab_file=data/vocab.txt \\\n","--out_file=output/out.testa.json \\\n","--prob=False \\\n","--batch_size=100 \\\n","--num_classes_each_label 4 \\\n","--num_labels=20 \\\n","--checkpoint_dir=output/elmo_ema_smoothing_aug"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"giUIB2Bi2bpe","colab_type":"text"},"source":["# 效果测试"]},{"cell_type":"code","metadata":{"id":"7nmA4g4p5jOa","colab_type":"code","colab":{}},"source":["% cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/nlp_task/fine_gained/mycode\n","! pwd\n","! ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jbfC_CTS619m","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593699958480,"user_tz":-480,"elapsed":1036,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}}},"source":["import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 忽略警告\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","try:\n","    from tensorflow.python.util import module_wrapper as deprecation\n","except ImportError:\n","    from tensorflow.python.util import deprecation_wrapper as deprecation\n","deprecation._PER_MODULE_WARNING_LIMIT = 0"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"S2jhIjpA2ayR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593701996397,"user_tz":-480,"elapsed":22397,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"442050c8-3f2b-4b9a-8219-86e49f03f620"},"source":["import tensorflow as tf\n","from elmo import Model\n","from dataset import DataSet\n","from collections import defaultdict\n","from elmo_utils import *\n","\n","\n","checkpoint_dir = \"output/elmo_origin_loss_fix_at_best\"\n","\n","\n","eval_dataset = DataSet([\"data/validation.json\"],\n","                           \"data/vocab.txt\",\n","                           \"data/labels.txt\",\n","                           128,\n","                           reverse=False,\n","                           split_word=True,\n","                           max_len=1200)\n","\n","eval_graph = tf.Graph()\n","\n","with eval_graph.as_default():\n","    eval_config = load_config(checkpoint_dir,\n","                                {\"mode\":'eval','checkpoint_dir':checkpoint_dir+\"/best_eval\"})\n","    eval_model = Model(eval_config)\n","    eval_model.build()"],"execution_count":32,"outputs":[{"output_type":"stream","text":["# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from data/validation.json ...\n","# Got 15000 data items with 117 batches\n","loading config from output/elmo_origin_loss_fix_at_best/config\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 44512 300\n","构建elmo静态图...\n","WARNING: Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fab983c7a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fab983c7a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fabacb23d68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fabacb23518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fabacb23518>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fab983c7a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fab983c7a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fabc5b117f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fabc5b117f0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fabacb23198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fabacb23198>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc5464c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc5464c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66247b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66247b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc6646940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc6646940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fabc5464f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WeightDropLSTMCell.call of <weight_drop_lstm.WeightDropLSTMCell object at 0x7fab983c7780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabacb23d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <weight_drop_lstm.DropConnectLayer object at 0x7fabb48ffb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabc66240b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2dbbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fabb3bdd048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabb2973128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kCnMdJLa8-Kt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593702006453,"user_tz":-480,"elapsed":1592,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}}},"source":["def cal_f1_and_more(class_num, predicted, truth):\n","    \"class_num -- 4; predicted -- [15000, 4]; truth -- [15000, 4]\"\n","    results = []\n","    for i in range(class_num):\n","        results.append({\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0})\n","\n","    for i, p in enumerate(predicted):\n","        t = truth[i]\n","        for j in range(class_num):\n","            if p[j] == 1:\n","                if t[j] == 1:\n","                    results[j]['TP'] += 1\n","                else:\n","                    results[j]['FP'] += 1\n","            else:\n","                if t[j] == 1:\n","                    results[j]['FN'] += 1\n","                else:\n","                    results[j]['TN'] += 1\n","\n","    precision = [0.0] * class_num\n","    recall = [0.0] * class_num\n","    f1 = [0.0] * class_num\n","    for i in range(class_num):\n","        if results[i]['TP'] == 0:\n","            if results[i]['FP'] == 0 and results[i]['FN'] == 0:\n","                precision[i] = 1.0\n","                recall[i] = 1.0\n","                f1[i] = 1.0\n","            else:\n","                precision[i] = 0.0\n","                recall[i] = 0.0\n","                f1[i] = 0.0\n","        else:\n","            precision[i] = results[i]['TP'] / (results[i]['TP'] +\n","                                               results[i]['FP'])\n","            recall[i] = results[i]['TP'] / (results[i]['TP'] +\n","                                            results[i]['FN'])\n","            f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n","\n","    return f1, precision, recall, results"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1ofGnew5Iyb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"status":"ok","timestamp":1593702127039,"user_tz":-480,"elapsed":95294,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"c8a0ff38-9712-4046-c7ab-3285281239b7"},"source":["with tf.Session(graph=eval_graph, config=get_config_proto(log_device_placement=False)) as eval_sess:\n","    eval_model.init_model(eval_sess)\n","    eval_model.restore_model(eval_sess)\n","\n","    checkpoint_loss, acc = 0.0, 0.0\n","    predicts, truths = defaultdict(list), defaultdict(list)\n","    for idx,(source, lengths, targets, _) in enumerate(eval_dataset.get_next(shuffle=False)):\n","        batch_loss, accuracy, batch_size, predict, logits = eval_model.eval_clf_one_step(eval_sess,\n","                                                                            source,\n","                                                                            lengths,\n","                                                                            targets)\n","        # predict： batch * 20 * 4\n","        for i, p in enumerate(predict):\n","            for j in range(eval_model.config.num_labels):\n","                label_name = eval_dataset.i2l[j]\n","                truths[label_name].append(targets[i][j])\n","                predicts[label_name].append(p[j])\n","        checkpoint_loss += batch_loss\n","        acc += accuracy\n","        if (idx+1) % 100 == 0:\n","            print(\"=>> batch %d/%d\" %(i+1,eval_dataset.num_batches))\n","\n","    results = {}\n","    total_f1 = 0.0\n","    metrics = {}\n","    for label_name in eval_dataset.label_names:\n","        f1, precision, recall, results_detail = cal_f1_and_more(eval_model.config.num_classes_each_label,\n","                                        np.asarray(predicts[label_name]),\n","                                        np.asarray(truths[label_name]))\n","        metrics[label_name] = {'f1': f1, 'precision': precision, 'recall': recall, 'details': results_detail}\n","\n","        results[label_name] = sum(f1) / len(f1)\n","        total_f1 += sum(f1) / len(f1)\n","        print(\"=>> {0} - {1}\".format(label_name, sum(f1) / len(f1)))\n","\n","    final_f1 = total_f1 / len(results)\n","\n","    print(\"=>> Eval loss %.5f, f1 %.5f\" % (checkpoint_loss / i, final_f1))"],"execution_count":34,"outputs":[{"output_type":"stream","text":["\n","!!! Restored model\n","=>> batch 128/117\n","=>> location_traffic_convenience - 0.6635102376478403\n","=>> location_distance_from_business_district - 0.5557976118306553\n","=>> location_easy_to_find - 0.7079569694634317\n","=>> service_wait_time - 0.6655879854095182\n","=>> service_waiters_attitude - 0.8004459100756012\n","=>> service_parking_convenience - 0.7401540201622667\n","=>> service_serving_speed - 0.7545546848035559\n","=>> price_level - 0.7839641368574368\n","=>> price_cost_effective - 0.7114510835772092\n","=>> price_discount - 0.6700410389334504\n","=>> environment_decoration - 0.7316427629299174\n","=>> environment_noise - 0.7646429859399563\n","=>> environment_space - 0.7706049828527306\n","=>> environment_cleaness - 0.7613696422917533\n","=>> dish_portion - 0.7274114163940149\n","=>> dish_taste - 0.7318627889496034\n","=>> dish_look - 0.5813661673335402\n","=>> dish_recommendation - 0.7345278066715328\n","=>> others_overall_experience - 0.59359095187412\n","=>> others_willing_to_consume_again - 0.7120806764505359\n","=>> Eval loss 1.71949, f1 0.70813\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M8SpPOGS_owS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593702149776,"user_tz":-480,"elapsed":677,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"fb0c4040-e4ad-42d8-c848-a1d45c9fb26d"},"source":["metrics"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'dish_look': {'details': [{'FN': 1189, 'FP': 755, 'TN': 11119, 'TP': 1937},\n","   {'FN': 515, 'FP': 168, 'TN': 14171, 'TP': 146},\n","   {'FN': 267, 'FP': 150, 'TN': 14395, 'TP': 188},\n","   {'FN': 829, 'FP': 1727, 'TN': 2515, 'TP': 9929}],\n","  'f1': [0.6658645582674458,\n","   0.29948717948717946,\n","   0.4741488020176544,\n","   0.885964129561881],\n","  'precision': [0.7195393759286776,\n","   0.46496815286624205,\n","   0.5562130177514792,\n","   0.8518359643102265],\n","  'recall': [0.6196417146513116,\n","   0.22087745839636913,\n","   0.41318681318681316,\n","   0.9229410671128463]},\n"," 'dish_portion': {'details': [{'FN': 830, 'FP': 935, 'TN': 10024, 'TP': 3211},\n","   {'FN': 707, 'FP': 487, 'TN': 13108, 'TP': 698},\n","   {'FN': 410, 'FP': 415, 'TN': 13143, 'TP': 1032},\n","   {'FN': 992, 'FP': 1102, 'TN': 5786, 'TP': 7120}],\n","  'f1': [0.7844143153780385,\n","   0.5389961389961389,\n","   0.7144340602284527,\n","   0.8718011509734296],\n","  'precision': [0.7744814278822962,\n","   0.5890295358649789,\n","   0.7131997235659986,\n","   0.8659693505229871],\n","  'recall': [0.7946052957188815,\n","   0.496797153024911,\n","   0.7156726768377254,\n","   0.8777120315581854]},\n"," 'dish_recommendation': {'details': [{'FN': 660,\n","    'FP': 415,\n","    'TN': 12290,\n","    'TP': 1635},\n","   {'FN': 159, 'FP': 63, 'TN': 14650, 'TP': 128},\n","   {'FN': 102, 'FP': 96, 'TN': 14569, 'TP': 233},\n","   {'FN': 462, 'FP': 809, 'TN': 2108, 'TP': 11621}],\n","  'f1': [0.7525891829689298,\n","   0.5355648535564853,\n","   0.7018072289156626,\n","   0.9481499612450537],\n","  'precision': [0.7975609756097561,\n","   0.6701570680628273,\n","   0.7082066869300911,\n","   0.9349155269509252],\n","  'recall': [0.7124183006535948,\n","   0.445993031358885,\n","   0.6955223880597015,\n","   0.9617644624679301]},\n"," 'dish_taste': {'details': [{'FN': 1347, 'FP': 1181, 'TN': 5975, 'TP': 6497},\n","   {'FN': 1126, 'FP': 1543, 'TN': 7637, 'TP': 4694},\n","   {'FN': 201, 'FP': 161, 'TN': 14259, 'TP': 379},\n","   {'FN': 343, 'FP': 132, 'TN': 14112, 'TP': 413}],\n","  'f1': [0.8371343898982089,\n","   0.77863481794808,\n","   0.6767857142857142,\n","   0.6348962336664105],\n","  'precision': [0.8461839020578276,\n","   0.7526054192720859,\n","   0.7018518518518518,\n","   0.7577981651376147],\n","  'recall': [0.8282763895971443,\n","   0.8065292096219931,\n","   0.653448275862069,\n","   0.5462962962962963]},\n"," 'environment_cleaness': {'details': [{'FN': 543,\n","    'FP': 689,\n","    'TN': 10159,\n","    'TP': 3609},\n","   {'FN': 331, 'FP': 140, 'TN': 14233, 'TP': 296},\n","   {'FN': 168, 'FP': 206, 'TN': 14169, 'TP': 457},\n","   {'FN': 719, 'FP': 726, 'TN': 4678, 'TP': 8877}],\n","  'f1': [0.8542011834319526,\n","   0.5569143932267168,\n","   0.7096273291925465,\n","   0.9247356633157977],\n","  'precision': [0.8396928804094927,\n","   0.6788990825688074,\n","   0.6892911010558069,\n","   0.9243986254295533],\n","  'recall': [0.8692196531791907,\n","   0.47208931419457734,\n","   0.7312,\n","   0.9250729470612755]},\n"," 'environment_decoration': {'details': [{'FN': 455,\n","    'FP': 905,\n","    'TN': 8483,\n","    'TP': 5157},\n","   {'FN': 575, 'FP': 377, 'TN': 13311, 'TP': 737},\n","   {'FN': 132, 'FP': 137, 'TN': 14588, 'TP': 143},\n","   {'FN': 741, 'FP': 484, 'TN': 6715, 'TP': 7060}],\n","  'f1': [0.883501798869282,\n","   0.6075845012366035,\n","   0.5153153153153153,\n","   0.9201694362984687],\n","  'precision': [0.8507093368525239,\n","   0.6615798922800719,\n","   0.5107142857142857,\n","   0.9358430540827147],\n","  'recall': [0.9189237348538846,\n","   0.5617378048780488,\n","   0.52,\n","   0.9050121779259069]},\n"," 'environment_noise': {'details': [{'FN': 675,\n","    'FP': 753,\n","    'TN': 10914,\n","    'TP': 2658},\n","   {'FN': 344, 'FP': 162, 'TN': 14173, 'TP': 321},\n","   {'FN': 97, 'FP': 110, 'TN': 14409, 'TP': 384},\n","   {'FN': 764, 'FP': 855, 'TN': 3624, 'TP': 9757}],\n","  'f1': [0.7882562277580072,\n","   0.5592334494773519,\n","   0.7876923076923077,\n","   0.9233899588321582],\n","  'precision': [0.7792436235708003,\n","   0.6645962732919255,\n","   0.7773279352226721,\n","   0.9194308330192236],\n","  'recall': [0.7974797479747975,\n","   0.48270676691729325,\n","   0.7983367983367984,\n","   0.9273833285809334]},\n"," 'environment_space': {'details': [{'FN': 649,\n","    'FP': 792,\n","    'TN': 10783,\n","    'TP': 2776},\n","   {'FN': 461, 'FP': 430, 'TN': 13260, 'TP': 849},\n","   {'FN': 201, 'FP': 233, 'TN': 13993, 'TP': 573},\n","   {'FN': 945, 'FP': 801, 'TN': 4708, 'TP': 8546}],\n","  'f1': [0.793936793936794,\n","   0.6558516801853997,\n","   0.7253164556962026,\n","   0.9073150015925258],\n","  'precision': [0.7780269058295964,\n","   0.6637998436278343,\n","   0.7109181141439206,\n","   0.9143040547769338],\n","  'recall': [0.8105109489051094,\n","   0.6480916030534352,\n","   0.7403100775193798,\n","   0.9004319881993468]},\n"," 'location_distance_from_business_district': {'details': [{'FN': 747,\n","    'FP': 763,\n","    'TN': 11439,\n","    'TP': 2051},\n","   {'FN': 78, 'FP': 5, 'TN': 14915, 'TP': 2},\n","   {'FN': 48, 'FP': 32, 'TN': 14878, 'TP': 42},\n","   {'FN': 759, 'FP': 832, 'TN': 2136, 'TP': 11273}],\n","  'f1': [0.7309337134711333,\n","   0.04597701149425287,\n","   0.5121951219512195,\n","   0.9340846004060157],\n","  'precision': [0.7288557213930348,\n","   0.2857142857142857,\n","   0.5675675675675675,\n","   0.9312680710450227],\n","  'recall': [0.7330235882773409,\n","   0.025,\n","   0.4666666666666667,\n","   0.9369182180851063]},\n"," 'location_easy_to_find': {'details': [{'FN': 389,\n","    'FP': 339,\n","    'TN': 12058,\n","    'TP': 2214},\n","   {'FN': 267, 'FP': 80, 'TN': 14591, 'TP': 62},\n","   {'FN': 125, 'FP': 160, 'TN': 14288, 'TP': 427},\n","   {'FN': 364, 'FP': 566, 'TN': 2918, 'TP': 11152}],\n","  'f1': [0.8588052754072926,\n","   0.2632696390658174,\n","   0.7497805092186128,\n","   0.9599724541620039],\n","  'precision': [0.8672150411280846,\n","   0.43661971830985913,\n","   0.727427597955707,\n","   0.9516982420208226],\n","  'recall': [0.8505570495582021,\n","   0.1884498480243161,\n","   0.7735507246376812,\n","   0.968391802709274]},\n"," 'location_traffic_convenience': {'details': [{'FN': 229,\n","    'FP': 350,\n","    'TN': 11725,\n","    'TP': 2696},\n","   {'FN': 122, 'FP': 24, 'TN': 14840, 'TP': 14},\n","   {'FN': 76, 'FP': 55, 'TN': 14763, 'TP': 106},\n","   {'FN': 330, 'FP': 328, 'TN': 2915, 'TP': 11427}],\n","  'f1': [0.9030313180371797,\n","   0.16091954022988503,\n","   0.6180758017492711,\n","   0.9720142905750254],\n","  'precision': [0.8850952068286277,\n","   0.3684210526315789,\n","   0.6583850931677019,\n","   0.972096980008507],\n","  'recall': [0.9217094017094017,\n","   0.10294117647058823,\n","   0.5824175824175825,\n","   0.9719316152079612]},\n"," 'others_overall_experience': {'details': [{'FN': 703,\n","    'FP': 1285,\n","    'TN': 3640,\n","    'TP': 9372},\n","   {'FN': 1301, 'FP': 1011, 'TN': 10632, 'TP': 2056},\n","   {'FN': 249, 'FP': 238, 'TN': 13479, 'TP': 1034},\n","   {'FN': 282, 'FP': 1, 'TN': 14714, 'TP': 3}],\n","  'f1': [0.9041095890410958,\n","   0.6400996264009963,\n","   0.8093933463796478,\n","   0.020761245674740483],\n","  'precision': [0.8794219761659003,\n","   0.6703619171829149,\n","   0.8128930817610063,\n","   0.75],\n","  'recall': [0.9302233250620348,\n","   0.6124515936848377,\n","   0.8059236165237724,\n","   0.010526315789473684]},\n"," 'others_willing_to_consume_again': {'details': [{'FN': 611,\n","    'FP': 756,\n","    'TN': 9570,\n","    'TP': 4063},\n","   {'FN': 307, 'FP': 103, 'TN': 14502, 'TP': 88},\n","   {'FN': 92, 'FP': 194, 'TN': 14229, 'TP': 485},\n","   {'FN': 771, 'FP': 728, 'TN': 4918, 'TP': 8583}],\n","  'f1': [0.8559991572737806,\n","   0.3003412969283276,\n","   0.7722929936305732,\n","   0.9196892579694615],\n","  'precision': [0.8431209794563187,\n","   0.4607329842931937,\n","   0.7142857142857143,\n","   0.9218129094619267],\n","  'recall': [0.8692768506632434,\n","   0.22278481012658227,\n","   0.8405545927209706,\n","   0.9175753688261706]},\n"," 'price_cost_effective': {'details': [{'FN': 365,\n","    'FP': 828,\n","    'TN': 11443,\n","    'TP': 2364},\n","   {'FN': 249, 'FP': 85, 'TN': 14517, 'TP': 149},\n","   {'FN': 175, 'FP': 134, 'TN': 14421, 'TP': 270},\n","   {'FN': 810, 'FP': 552, 'TN': 3020, 'TP': 10618}],\n","  'f1': [0.7985137645667961,\n","   0.47151898734177217,\n","   0.6360424028268552,\n","   0.9397291795734135],\n","  'precision': [0.7406015037593985,\n","   0.6367521367521367,\n","   0.6683168316831684,\n","   0.9505819158460161],\n","  'recall': [0.8662513741297179,\n","   0.3743718592964824,\n","   0.6067415730337079,\n","   0.9291214560728036]},\n"," 'price_discount': {'details': [{'FN': 617,\n","    'FP': 692,\n","    'TN': 11467,\n","    'TP': 2224},\n","   {'FN': 689, 'FP': 1100, 'TN': 11269, 'TP': 1942},\n","   {'FN': 211, 'FP': 63, 'TN': 14671, 'TP': 55},\n","   {'FN': 747, 'FP': 409, 'TN': 5329, 'TP': 8515}],\n","  'f1': [0.772624630884141,\n","   0.6846465714789354,\n","   0.28645833333333337,\n","   0.9364346200373914],\n","  'precision': [0.7626886145404664,\n","   0.6383957922419461,\n","   0.4661016949152542,\n","   0.9541685342895563],\n","  'recall': [0.7828229496656107,\n","   0.7381223869251236,\n","   0.20676691729323307,\n","   0.9193478730295832]},\n"," 'price_level': {'details': [{'FN': 709, 'FP': 613, 'TN': 12159, 'TP': 1519},\n","   {'FN': 906, 'FP': 1189, 'TN': 10296, 'TP': 2609},\n","   {'FN': 239, 'FP': 414, 'TN': 12826, 'TP': 1521},\n","   {'FN': 896, 'FP': 534, 'TN': 6969, 'TP': 6601}],\n","  'f1': [0.6967889908256882,\n","   0.7135238616162998,\n","   0.823274695534506,\n","   0.9022689994532531],\n","  'precision': [0.7124765478424016,\n","   0.6869404949973671,\n","   0.786046511627907,\n","   0.9251576734407848],\n","  'recall': [0.6817773788150808,\n","   0.7422475106685633,\n","   0.8642045454545455,\n","   0.880485527544351]},\n"," 'service_parking_convenience': {'details': [{'FN': 96,\n","    'FP': 139,\n","    'TN': 14299,\n","    'TP': 466},\n","   {'FN': 146, 'FP': 66, 'TN': 14730, 'TP': 58},\n","   {'FN': 27, 'FP': 47, 'TN': 14765, 'TP': 161},\n","   {'FN': 59, 'FP': 76, 'TN': 878, 'TP': 13987}],\n","  'f1': [0.7986289631533848,\n","   0.35365853658536583,\n","   0.8131313131313131,\n","   0.9951972677790031],\n","  'precision': [0.7702479338842976,\n","   0.46774193548387094,\n","   0.7740384615384616,\n","   0.9945957477067482],\n","  'recall': [0.8291814946619217,\n","   0.28431372549019607,\n","   0.8563829787234043,\n","   0.9957995158764061]},\n"," 'service_serving_speed': {'details': [{'FN': 206,\n","    'FP': 211,\n","    'TN': 13639,\n","    'TP': 944},\n","   {'FN': 233, 'FP': 118, 'TN': 14514, 'TP': 135},\n","   {'FN': 146, 'FP': 204, 'TN': 13991, 'TP': 659},\n","   {'FN': 302, 'FP': 354, 'TN': 1969, 'TP': 12375}],\n","  'f1': [0.8190889370932755,\n","   0.4347826086956522,\n","   0.790167865707434,\n","   0.974179327717862],\n","  'precision': [0.8173160173160173,\n","   0.5335968379446641,\n","   0.7636152954808807,\n","   0.9721894885694085],\n","  'recall': [0.8208695652173913,\n","   0.36684782608695654,\n","   0.8186335403726708,\n","   0.9761773290210618]},\n"," 'service_wait_time': {'details': [{'FN': 312,\n","    'FP': 227,\n","    'TN': 14074,\n","    'TP': 387},\n","   {'FN': 329, 'FP': 318, 'TN': 14083, 'TP': 270},\n","   {'FN': 169, 'FP': 144, 'TN': 14391, 'TP': 296},\n","   {'FN': 421, 'FP': 542, 'TN': 1221, 'TP': 12816}],\n","  'f1': [0.5894897182025894,\n","   0.45492839090143217,\n","   0.6541436464088398,\n","   0.9637901861252115],\n","  'precision': [0.6302931596091205,\n","   0.45918367346938777,\n","   0.6727272727272727,\n","   0.9594250636322803],\n","  'recall': [0.5536480686695279,\n","   0.4507512520868113,\n","   0.6365591397849463,\n","   0.9681952103951046]},\n"," 'service_waiters_attitude': {'details': [{'FN': 374,\n","    'FP': 555,\n","    'TN': 8476,\n","    'TP': 5595},\n","   {'FN': 853, 'FP': 532, 'TN': 12638, 'TP': 977},\n","   {'FN': 245, 'FP': 311, 'TN': 13480, 'TP': 964},\n","   {'FN': 463, 'FP': 537, 'TN': 8471, 'TP': 5529}],\n","  'f1': [0.9233435101906098,\n","   0.5852051512428871,\n","   0.7761674718196457,\n","   0.9170675070492619],\n","  'precision': [0.9097560975609756,\n","   0.6474486414844268,\n","   0.756078431372549,\n","   0.9114737883283878],\n","  'recall': [0.9373429385156643,\n","   0.533879781420765,\n","   0.7973531844499586,\n","   0.9227303070761015]}}"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"Zg1R9Cy2-oyX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593701310192,"user_tz":-480,"elapsed":1143,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}}},"source":["import pickle\n","\n","with open(\"data/class_weights.bin\", 'rb') as f:\n","    class_weight = pickle.load(f, encoding='utf-8')"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"0R4fxWFo_yzH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593702490945,"user_tz":-480,"elapsed":1122,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}}},"source":["class_weight_inverse = [[1 / num for num in list_a] for list_a in class_weight]"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ZqOwjWc_9fg","colab_type":"code","colab":{}},"source":["class_weight_inverse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9qQYB21EjJh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593703257346,"user_tz":-480,"elapsed":1704,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"678e92c8-6b53-4931-d4d1-e051676be335"},"source":["f1_list = []\n","precision_list = []\n","recall_list = []\n","\n","for i, label_name in enumerate(eval_dataset.label_names):\n","    weighted_f1 = sum([a * b for a, b in zip(class_weight_inverse[i], metrics[label_name]['f1'])])\n","    weighted_precision = sum([a * b for a, b in zip(class_weight_inverse[i], metrics[label_name]['precision'])])\n","    weighted_recall = sum([a * b for a, b in zip(class_weight_inverse[i], metrics[label_name]['recall'])])\n","    print(\"=\" * 30)\n","    print(label_name)\n","    print(\"weighted f1:\", weighted_f1)\n","    print(\"weighted precision:\", weighted_precision)\n","    print(\"weighted recall:\", weighted_recall)\n","    f1_list.append(weighted_f1)\n","    precision_list.append(weighted_precision)\n","    recall_list.append(weighted_recall)\n","\n","print(\"#\" * 30)\n","print(\"averaged f1:\", sum(f1_list) / len(f1_list))\n","print(\"averaged precision:\", sum(precision_list) / len(precision_list))\n","print(\"averaged recall:\", sum(recall_list) / len(recall_list))"],"execution_count":44,"outputs":[{"output_type":"stream","text":["==============================\n","location_traffic_convenience\n","weighted f1: 0.9455280035707135\n","weighted precision: 0.9445345707317678\n","weighted recall: 0.9482195540666717\n","==============================\n","location_distance_from_business_district\n","weighted f1: 0.8881375561561486\n","weighted precision: 0.8870191135408428\n","weighted recall: 0.8904373110744659\n","==============================\n","location_easy_to_find\n","weighted f1: 0.9183189314084468\n","weighted precision: 0.9166392496604572\n","weighted recall: 0.9225109936312154\n","==============================\n","service_wait_time\n","weighted f1: 0.9164206380868779\n","weighted precision: 0.9151522657408971\n","weighted recall: 0.917984210073227\n","==============================\n","service_waiters_attitude\n","weighted f1: 0.8682723844621453\n","weighted precision: 0.866427990492425\n","weighted recall: 0.8717010343757985\n","==============================\n","service_parking_convenience\n","weighted f1: 0.9766218809945754\n","weighted precision: 0.9760819518426841\n","weighted recall: 0.97791685413215\n","==============================\n","service_serving_speed\n","weighted f1: 0.9398847749451685\n","weighted precision: 0.9389127136183217\n","weighted recall: 0.9416639639190226\n","==============================\n","price_level\n","weighted f1: 0.818927262571223\n","weighted precision: 0.8222386584187158\n","weighted recall: 0.8172025295804002\n","==============================\n","price_cost_effective\n","weighted f1: 0.8922059369965747\n","weighted precision: 0.8959593496983178\n","weighted recall: 0.8924645477926635\n","==============================\n","price_discount\n","weighted f1: 0.8496088622377892\n","weighted precision: 0.8533870512957467\n","weighted recall: 0.8491681851636401\n","==============================\n","environment_decoration\n","weighted f1: 0.8698866985090984\n","weighted precision: 0.8704007977660151\n","weighted recall: 0.8713640946680566\n","==============================\n","environment_noise\n","weighted f1: 0.872199126629299\n","weighted precision: 0.8719571274516733\n","weighted recall: 0.873850796301655\n","==============================\n","environment_space\n","weighted f1: 0.8486436134673017\n","weighted precision: 0.8491827074618514\n","weighted recall: 0.8483753486609127\n","==============================\n","environment_cleaness\n","weighted f1: 0.8794093140299207\n","weighted precision: 0.8797524684607123\n","weighted recall: 0.8809256625783979\n","==============================\n","dish_portion\n","weighted f1: 0.8028884976714479\n","weighted precision: 0.8014375449640383\n","weighted recall: 0.8051621995604563\n","==============================\n","dish_taste\n","weighted f1: 0.7983093278392783\n","weighted precision: 0.8000916027319137\n","weighted recall: 0.7990701918041679\n","==============================\n","dish_look\n","weighted f1: 0.8030072059811137\n","weighted precision: 0.7989875822923361\n","weighted recall: 0.8150972549219693\n","==============================\n","dish_recommendation\n","weighted f1: 0.905257060871683\n","weighted precision: 0.9040997687391594\n","weighted recall: 0.9083062479108023\n","==============================\n","others_overall_experience\n","weighted f1: 0.8189665441832729\n","weighted precision: 0.824213223240845\n","weighted recall: 0.8297063017187571\n","==============================\n","others_willing_to_consume_again\n","weighted f1: 0.8770592079771926\n","weighted precision: 0.8765730605097773\n","weighted recall: 0.8803787135834608\n","##############################\n","averaged f1: 0.8744776414294637\n","averaged precision: 0.8746524399329247\n","averaged recall: 0.8770752997758946\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0Z9uepUEHqFq","colab_type":"text"},"source":["# 结论\n","\n","单模型最高macro f1达到 70.81%。多模型集成会有提高。\n","\n","数据极度不平衡。计算weighted平均precision 最好达到0.8746，weighted平均f1最高达到0.8744，weighted平均recall最高达到0.8770。\n","\n","效果相比于基本的Albert classification有很大的提升。相比于多个单模型集成的fasttext，结果有更好。\n","\n","基本的Albert classification一是难以完成长文本的信息的完整获取，而是模型在少量fine tuning的基础之上，拼接上层更复杂的模型，以提升拟合能力可能会有效果的提升。"]},{"cell_type":"code","metadata":{"id":"QvkprTIUGH2w","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}
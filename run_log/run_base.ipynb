{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_base.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP27Qj1v6lBZxnExGVPoNWL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jyiQxRwqjmY5","colab_type":"code","outputId":"9a679106-f9b9-4af6-8d67-a7bb5713d63d","executionInfo":{"status":"ok","timestamp":1590484547169,"user_tz":-480,"elapsed":33773,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yjjcH1bgIdo2","colab_type":"code","colab":{}},"source":["!pip uninstall tensorflow\n","!pip install tensorflow-gpu==1.14.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MsWDOiAHZCE","colab_type":"code","colab":{}},"source":["!pip install pyhanlp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HdTHhzRlHmOT","colab_type":"code","outputId":"494b14b5-06f7-457e-ad51-065f257480c3","executionInfo":{"status":"ok","timestamp":1590315241343,"user_tz":-480,"elapsed":927733,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["from pyhanlp import *"],"execution_count":0,"outputs":[{"output_type":"stream","text":["下载 https://file.hankcs.com/bin/hanlp-1.7.7-release.zip 到 /usr/local/lib/python3.6/dist-packages/pyhanlp/static/hanlp-1.7.7-release.zip\n","100.00%, 1 MB, 711 KB/s, 还有 0 分  0 秒   \n","下载 http://114.115.185.60/file/data-for-1.7.5.zip 到 /usr/local/lib/python3.6/dist-packages/pyhanlp/static/data-for-1.7.5.zip\n","100.00%, 637 MB, 715 KB/s, 还有 0 分  0 秒   \n","解压 data.zip...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A0cZ9QApjnzH","colab_type":"code","outputId":"dee0957e-05e7-4442-c09b-829dd7564914","executionInfo":{"status":"ok","timestamp":1590483295951,"user_tz":-480,"elapsed":137346,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["% cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/nlp_task/fine_gained\n","! pwd\n","! ls\n","# ! git clone https://github.com/xueyouluo/fsauor2018.git\n","# ! ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained\n","/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained\n","fsauor2018  run_base.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b4kxzOWij9SA","colab_type":"code","outputId":"ef5f4f74-b869-43d8-b96e-eae9e155f3d5","executionInfo":{"status":"ok","timestamp":1590484016325,"user_tz":-480,"elapsed":1175,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["% cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/nlp_task/fine_gained/fsauor2018/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cUts4IHmu1iH","colab_type":"code","colab":{}},"source":["!bzip2 -dk ./wordvec/sgns.sogou.word.bz2\n","!pwd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sS7oLNx0z9H9","colab_type":"code","outputId":"87ed9459-111b-477d-f33a-0cc016298ba7","executionInfo":{"status":"ok","timestamp":1590317585414,"user_tz":-480,"elapsed":660727,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"colab":{"base_uri":"https://localhost:8080/","height":386}},"source":["%cd scripts/\n","!bash preprocess.sh\n","%cd .."],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/scripts\n","Process training file ...\n","# processed -- 10000 --\n","# processed -- 20000 --\n","# processed -- 30000 --\n","# processed -- 40000 --\n","# processed -- 50000 --\n","# processed -- 60000 --\n","# processed -- 70000 --\n","# processed -- 80000 --\n","# processed -- 90000 --\n","# processed -- 100000 --\n","# Start to create vocab ...\n","# Created vocab file data/vocab.txt with vocab size 50000\n","Process validation file ...\n","# processed -- 10000 --\n","Process testa file ...\n","# processed -- 10000 --\n","Get pretrained embedding ...\n","Get label file ...\n","[Errno 2] No such file or directory: 'fsauor2018/'\n","/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/scripts\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fvrcfvyc3siJ","colab_type":"code","outputId":"8f4518e6-f964-46ee-9f51-155d3f729095","executionInfo":{"status":"ok","timestamp":1590318951603,"user_tz":-480,"elapsed":866,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd .."],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-geYEtHx11yK","colab_type":"code","outputId":"73717369-db69-4d61-e311-1fe8ae3f29ce","executionInfo":{"status":"ok","timestamp":1590169245550,"user_tz":-480,"elapsed":26888616,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!bash bash/elmo_train.sh"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:142: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:33: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from scripts/data/train.json ...\n","# Got 105000 data items with 3281 batches\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from scripts/data/validation.json ...\n","# Got 15000 data items with 93 batches\n","  saving hparams to scripts/data/elmo_ema_0120/hparams\n","mode=train,data_files=['scripts/data/train.json'],eval_files=['scripts/data/validation.json'],label_file=scripts/data/labels.txt,vocab_file=scripts/data/vocab.txt,embed_file=scripts/data/embedding.txt,out_file=None,split_word=True,max_len=1200,batch_size=32,reverse=False,prob=False,num_layers=3,decay_schema=hand,encoder=elmo,decay_steps=10000,learning_rate=0.001,focal_loss=0.0,embedding_dropout=0.1,max_gradient_norm=5.0,dropout_keep_prob=0.8,weight_keep_drop=0.8,l2_loss_ratio=0.0,rnn_cell_name=lstm,embedding_size=300,num_units=300,double_decoder=False,variational_dropout=True,target_label_num=4,feature_num=20,need_early_stop=True,patient=5,debug=False,num_train_epoch=50,steps_per_stats=20,steps_per_summary=50,steps_per_eval=2000,checkpoint_dir=scripts/data/elmo_ema_0120,vocab_size=50000\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:72: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:89: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:134: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 33871 300\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:114: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:106: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","build elmo encoder\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:40: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","seq_dim is deprecated, use seq_axis instead\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","batch_dim is deprecated, use batch_axis instead\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f8410e2cd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f8410e2cd30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f8461c7c198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f8461c7c198>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f84180e5a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f84180e5a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f842709fac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f842709fac8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f842709ff98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f842709ff98>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f841841e160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f841841e160>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:264: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:64: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f842727a240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f842727a240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:242: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84268a2908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84268a2908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84268a2908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84268a2908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1372: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py:2078: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f84268a2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f842729efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8425f20e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84272c0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:284: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:291: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:291: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n","\n","variable <tf.Variable 'embedding/embedding:0' shape=(50000, 300) dtype=float32_ref> with parameter number 15000000\n","variable <tf.Variable 'embedding/feature_embedding:0' shape=(20, 300) dtype=float32_ref> with parameter number 6000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/scalar:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","variable <tf.Variable 'elmo_encoder/weight:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classification/attention_semantic/memory_layer/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classification/attention_semantic/dense/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classification/attention_semantic/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classification/attention_semantic/dense_1/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classification/attention_semantic/dense_1/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classification/attention_semantic/attention_wrapper/lstm_cell/kernel:0' shape=(1200, 1200) dtype=float32_ref> with parameter number 1440000\n","variable <tf.Variable 'classification/attention_semantic/attention_wrapper/lstm_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classification/attention_semantic/attention_wrapper/luong_attention/attention_g:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classification/predict_clf/dense/kernel:0' shape=(900, 300) dtype=float32_ref> with parameter number 270000\n","variable <tf.Variable 'classification/predict_clf/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classification/predict_clf/dense_1/kernel:0' shape=(300, 4) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classification/predict_clf/dense_1/bias:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","# total parameter number 23746510\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:326: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:295: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:297: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:302: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:41: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","loading hparams from scripts/data/elmo_ema_0120/hparams\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 33871 300\n","build elmo encoder\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f8424ca83c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f8424ca83c8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f840d654208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f840d654208>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f8424b84b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f8424b84b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f842727ac88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f842727ac88>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f840d654358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f840d654358>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f84245c3208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f84245c3208>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84245c3208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84245c3208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f841bf4a780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f841bf4a780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f841bf4a780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f841bf4a780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f8424b9ad30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8424b84a20>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8424b84ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f84274eec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","2020-05-22 10:13:52.344920: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2020-05-22 10:13:52.350828: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n","2020-05-22 10:13:52.514059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:13:52.515158: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x21239c0 executing computations on platform CUDA. Devices:\n","2020-05-22 10:13:52.515193: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-22 10:13:52.518705: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n","2020-05-22 10:13:52.518955: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2122a00 executing computations on platform Host. Devices:\n","2020-05-22 10:13:52.519002: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n","2020-05-22 10:13:52.519218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:13:52.520143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 10:13:52.535611: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 10:13:52.732989: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 10:13:52.813327: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 10:13:52.842169: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 10:13:53.087314: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 10:13:53.214133: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 10:13:53.733792: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 10:13:53.734110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:13:53.735187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:13:53.736053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 10:13:53.736168: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 10:13:53.737880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 10:13:53.737913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 10:13:53.737930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 10:13:53.738138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:13:53.739104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:13:53.740038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","2020-05-22 10:13:53.884890: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\n","  /job:localhost/replica:0/task:0/device:CPU:0].\n","See below for details of this colocation group:\n","Colocation Debug Info:\n","Colocation group had the following types and supported devices: \n","Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\n","AssignSub: GPU CPU \n","Merge: GPU CPU XLA_CPU XLA_GPU \n","Switch: GPU CPU XLA_CPU XLA_GPU \n","L2Loss: GPU CPU XLA_CPU XLA_GPU \n","Add: GPU CPU XLA_CPU XLA_GPU \n","Assign: GPU CPU \n","Identity: GPU CPU XLA_CPU XLA_GPU \n","VariableV2: GPU CPU \n","Const: GPU CPU XLA_CPU XLA_GPU \n","Fill: GPU CPU XLA_CPU XLA_GPU \n","Sub: GPU CPU XLA_CPU XLA_GPU \n","UnsortedSegmentSum: GPU CPU XLA_CPU XLA_GPU \n","Reshape: GPU CPU XLA_CPU XLA_GPU \n","RefSwitch: GPU CPU \n","GatherV2: GPU CPU XLA_CPU XLA_GPU \n","ExpandDims: GPU CPU XLA_CPU XLA_GPU \n","RandomUniform: GPU CPU XLA_CPU XLA_GPU \n","Cast: GPU CPU XLA_CPU XLA_GPU \n","Mul: GPU CPU XLA_CPU XLA_GPU \n","Unique: GPU CPU \n","SparseApplyRMSProp: CPU \n","StridedSlice: GPU CPU XLA_CPU XLA_GPU \n","ConcatV2: GPU CPU XLA_CPU XLA_GPU \n","Shape: GPU CPU XLA_CPU XLA_GPU \n","IsVariableInitialized: GPU CPU \n","\n","Colocation members, user-requested devices, and framework assigned devices, if any:\n","  embedding/feature_embedding/Initializer/random_uniform/shape (Const) \n","  embedding/feature_embedding/Initializer/random_uniform/min (Const) \n","  embedding/feature_embedding/Initializer/random_uniform/max (Const) \n","  embedding/feature_embedding/Initializer/random_uniform/RandomUniform (RandomUniform) \n","  embedding/feature_embedding/Initializer/random_uniform/sub (Sub) \n","  embedding/feature_embedding/Initializer/random_uniform/mul (Mul) \n","  embedding/feature_embedding/Initializer/random_uniform (Add) \n","  embedding/feature_embedding (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/read (Identity) /device:GPU:0\n","  embedding_lookup_1/axis (Const) /device:GPU:0\n","  embedding_lookup_1 (GatherV2) /device:GPU:0\n","  global_norm/L2Loss_1 (L2Loss) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Shape (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Cast (Cast) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Size (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/ExpandDims/dim (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/ExpandDims (ExpandDims) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice/stack (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice/stack_1 (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice/stack_2 (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice (StridedSlice) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/concat/axis (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/concat (ConcatV2) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Reshape (Reshape) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Reshape_1 (Reshape) /device:GPU:0\n","  global_norm_1/L2Loss_1 (L2Loss) /device:GPU:0\n","  clip_by_global_norm/mul_2 (Mul) /device:GPU:0\n","  clip_by_global_norm/clip_by_global_norm/_1 (Identity) /device:GPU:0\n","  global_norm_2/L2Loss_1 (L2Loss) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Initializer/ones/shape_as_tensor (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Initializer/ones/Const (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Initializer/ones (Fill) /device:GPU:0\n","  embedding/feature_embedding/RMSProp (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/read (Identity) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Initializer/zeros/Const (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Initializer/zeros (Fill) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1 (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/read (Identity) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/Unique (Unique) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/Shape (Shape) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice/stack (Const) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice/stack_1 (Const) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice/stack_2 (Const) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice (StridedSlice) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/UnsortedSegmentSum (UnsortedSegmentSum) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/SparseApplyRMSProp (SparseApplyRMSProp) /device:GPU:0\n","  IsVariableInitialized_1 (IsVariableInitialized) /device:GPU:0\n","  cond_1/read/Switch (RefSwitch) /device:GPU:0\n","  cond_1/Switch_1 (Switch) \n","  embedding/feature_embedding/ExponentialMovingAverage (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/IsVariableInitialized (IsVariableInitialized) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/Switch (Switch) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/switch_t (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/switch_f (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/pred_id (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/read/Switch (RefSwitch) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/read (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/Switch_1 (Switch) \n","  embedding/feature_embedding/ExponentialMovingAverage/cond/Merge (Merge) /device:GPU:0\n","  cond_1/read/Switch_embedding/feature_embedding/ExponentialMovingAverage (Switch) /device:GPU:0\n","  cond_1/read_embedding/feature_embedding/ExponentialMovingAverage (Identity) /device:GPU:0\n","  cond_1/Merge_embedding/feature_embedding/ExponentialMovingAverage (Merge) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/read (Identity) /device:GPU:0\n","  ExponentialMovingAverage/AssignMovingAvg_1 (AssignSub) /device:GPU:0\n","  save/Assign_111 (Assign) /device:GPU:0\n","  save/Assign_112 (Assign) /device:GPU:0\n","  save/Assign_113 (Assign) /device:GPU:0\n","  save/Assign_114 (Assign) /device:GPU:0\n","\n","2020-05-22 10:13:54.759649: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n","unable to restore model, train from scratch\n","# Start to train with learning rate 0.001, Fri May 22 10:13:55 2020\n","# Global step 0\n","2020-05-22 10:13:58.983557: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","# Epoch 1  global step 20 loss 1.38087 batch 20/3281 lr 0.001 accuracy 68.85547 wps 12459.42 step time 0.65s\n","# Epoch 1  global step 40 loss 1.35833 batch 40/3281 lr 0.001 accuracy 80.13672 wps 19834.60 step time 0.44s\n","# Epoch 1  global step 60 loss 1.30366 batch 60/3281 lr 0.001 accuracy 81.34375 wps 19424.28 step time 0.38s\n","# Epoch 1  global step 80 loss 1.17736 batch 80/3281 lr 0.001 accuracy 81.24219 wps 18901.74 step time 0.36s\n","# Epoch 1  global step 100 loss 1.01712 batch 100/3281 lr 0.001 accuracy 80.77734 wps 19518.63 step time 0.39s\n","# Epoch 1  global step 120 loss 0.99207 batch 120/3281 lr 0.001 accuracy 80.62500 wps 20112.76 step time 0.40s\n","# Epoch 1  global step 140 loss 0.98520 batch 140/3281 lr 0.001 accuracy 80.51563 wps 19758.80 step time 0.45s\n","# Epoch 1  global step 160 loss 0.92956 batch 160/3281 lr 0.001 accuracy 82.47656 wps 18286.21 step time 0.34s\n","# Epoch 1  global step 180 loss 0.92242 batch 180/3281 lr 0.001 accuracy 82.74609 wps 20384.45 step time 0.46s\n","# Epoch 1  global step 200 loss 0.87165 batch 200/3281 lr 0.001 accuracy 83.68359 wps 19668.86 step time 0.39s\n","# Epoch 1  global step 220 loss 0.83118 batch 220/3281 lr 0.001 accuracy 84.67969 wps 18747.49 step time 0.36s\n","# Epoch 1  global step 240 loss 0.82741 batch 240/3281 lr 0.001 accuracy 84.34375 wps 18181.03 step time 0.38s\n","# Epoch 1  global step 260 loss 0.85306 batch 260/3281 lr 0.001 accuracy 83.97266 wps 18941.46 step time 0.41s\n","# Epoch 1  global step 280 loss 0.84586 batch 280/3281 lr 0.001 accuracy 83.79688 wps 19297.52 step time 0.47s\n","# Epoch 1  global step 300 loss 0.84952 batch 300/3281 lr 0.001 accuracy 83.35938 wps 18761.68 step time 0.46s\n","# Epoch 1  global step 320 loss 0.81132 batch 320/3281 lr 0.001 accuracy 84.67578 wps 18303.27 step time 0.38s\n","# Epoch 1  global step 340 loss 0.86317 batch 340/3281 lr 0.001 accuracy 83.83984 wps 19007.95 step time 0.41s\n","# Epoch 1  global step 360 loss 0.81474 batch 360/3281 lr 0.001 accuracy 84.41016 wps 18542.44 step time 0.34s\n","# Epoch 1  global step 380 loss 0.81690 batch 380/3281 lr 0.001 accuracy 84.62500 wps 19036.51 step time 0.36s\n","# Epoch 1  global step 400 loss 0.79039 batch 400/3281 lr 0.001 accuracy 85.23828 wps 17984.32 step time 0.33s\n","# Epoch 1  global step 420 loss 0.79739 batch 420/3281 lr 0.001 accuracy 84.37500 wps 18878.28 step time 0.36s\n","# Epoch 1  global step 440 loss 0.80030 batch 440/3281 lr 0.001 accuracy 84.45703 wps 18190.67 step time 0.33s\n","# Epoch 1  global step 460 loss 0.80264 batch 460/3281 lr 0.001 accuracy 84.45703 wps 19003.74 step time 0.43s\n","# Epoch 1  global step 480 loss 0.81628 batch 480/3281 lr 0.001 accuracy 84.04297 wps 20287.75 step time 0.44s\n","# Epoch 1  global step 500 loss 0.79927 batch 500/3281 lr 0.001 accuracy 84.61719 wps 18507.83 step time 0.39s\n","# Epoch 1  global step 520 loss 0.78561 batch 520/3281 lr 0.001 accuracy 84.67188 wps 18443.52 step time 0.39s\n","# Epoch 1  global step 540 loss 0.79603 batch 540/3281 lr 0.001 accuracy 84.58984 wps 18971.32 step time 0.36s\n","# Epoch 1  global step 560 loss 0.80169 batch 560/3281 lr 0.001 accuracy 84.26953 wps 19550.60 step time 0.37s\n","# Epoch 1  global step 580 loss 0.79188 batch 580/3281 lr 0.001 accuracy 84.45703 wps 19071.41 step time 0.37s\n","# Epoch 1  global step 600 loss 0.75556 batch 600/3281 lr 0.001 accuracy 85.33984 wps 17939.35 step time 0.33s\n","# Epoch 1  global step 620 loss 0.76256 batch 620/3281 lr 0.001 accuracy 84.94141 wps 18644.39 step time 0.40s\n","# Epoch 1  global step 640 loss 0.77817 batch 640/3281 lr 0.001 accuracy 84.89453 wps 18936.82 step time 0.37s\n","# Epoch 1  global step 660 loss 0.78195 batch 660/3281 lr 0.001 accuracy 84.63281 wps 18450.90 step time 0.44s\n","# Epoch 1  global step 680 loss 0.76420 batch 680/3281 lr 0.001 accuracy 85.13281 wps 18789.23 step time 0.36s\n","# Epoch 1  global step 700 loss 0.75321 batch 700/3281 lr 0.001 accuracy 85.50000 wps 18392.65 step time 0.35s\n","# Epoch 1  global step 720 loss 0.78399 batch 720/3281 lr 0.001 accuracy 84.71484 wps 18896.25 step time 0.37s\n","# Epoch 1  global step 740 loss 0.78471 batch 740/3281 lr 0.001 accuracy 84.69922 wps 19983.15 step time 0.41s\n","# Epoch 1  global step 760 loss 0.74892 batch 760/3281 lr 0.001 accuracy 85.65234 wps 18338.33 step time 0.34s\n","# Epoch 1  global step 780 loss 0.72190 batch 780/3281 lr 0.001 accuracy 85.96094 wps 18295.48 step time 0.35s\n","# Epoch 1  global step 800 loss 0.77800 batch 800/3281 lr 0.001 accuracy 84.72656 wps 19990.63 step time 0.41s\n","# Epoch 1  global step 820 loss 0.75765 batch 820/3281 lr 0.001 accuracy 85.35156 wps 19308.64 step time 0.37s\n","# Epoch 1  global step 840 loss 0.75682 batch 840/3281 lr 0.001 accuracy 85.29687 wps 19442.63 step time 0.39s\n","# Epoch 1  global step 860 loss 0.75262 batch 860/3281 lr 0.001 accuracy 85.38281 wps 18526.27 step time 0.40s\n","# Epoch 1  global step 880 loss 0.78166 batch 880/3281 lr 0.001 accuracy 84.75391 wps 19725.33 step time 0.39s\n","# Epoch 1  global step 900 loss 0.73958 batch 900/3281 lr 0.001 accuracy 85.83594 wps 18515.74 step time 0.35s\n","# Epoch 1  global step 920 loss 0.74548 batch 920/3281 lr 0.001 accuracy 85.37891 wps 19117.77 step time 0.36s\n","# Epoch 1  global step 940 loss 0.75430 batch 940/3281 lr 0.001 accuracy 85.63281 wps 18674.18 step time 0.44s\n","# Epoch 1  global step 960 loss 0.75433 batch 960/3281 lr 0.001 accuracy 85.16406 wps 18942.97 step time 0.37s\n","# Epoch 1  global step 980 loss 0.75390 batch 980/3281 lr 0.001 accuracy 85.48437 wps 19188.34 step time 0.38s\n","# Epoch 1  global step 1000 loss 0.72319 batch 1000/3281 lr 0.001 accuracy 86.39844 wps 18651.92 step time 0.37s\n","# Epoch 1  global step 1020 loss 0.72324 batch 1020/3281 lr 0.001 accuracy 86.17969 wps 19235.78 step time 0.42s\n","# Epoch 1  global step 1040 loss 0.70927 batch 1040/3281 lr 0.001 accuracy 86.39844 wps 18925.34 step time 0.42s\n","# Epoch 1  global step 1060 loss 0.69495 batch 1060/3281 lr 0.001 accuracy 86.56250 wps 18974.06 step time 0.40s\n","# Epoch 1  global step 1080 loss 0.68959 batch 1080/3281 lr 0.001 accuracy 86.98828 wps 18458.97 step time 0.34s\n","# Epoch 1  global step 1100 loss 0.72631 batch 1100/3281 lr 0.001 accuracy 85.99609 wps 19887.10 step time 0.45s\n","# Epoch 1  global step 1120 loss 0.68643 batch 1120/3281 lr 0.001 accuracy 86.71094 wps 18832.90 step time 0.39s\n","# Epoch 1  global step 1140 loss 0.64257 batch 1140/3281 lr 0.001 accuracy 87.84375 wps 18292.28 step time 0.34s\n","# Epoch 1  global step 1160 loss 0.69389 batch 1160/3281 lr 0.001 accuracy 86.92578 wps 19921.46 step time 0.39s\n","# Epoch 1  global step 1180 loss 0.69672 batch 1180/3281 lr 0.001 accuracy 86.46094 wps 19800.13 step time 0.42s\n","# Epoch 1  global step 1200 loss 0.66387 batch 1200/3281 lr 0.001 accuracy 87.41406 wps 18909.46 step time 0.36s\n","# Epoch 1  global step 1220 loss 0.60262 batch 1220/3281 lr 0.001 accuracy 88.76172 wps 17620.95 step time 0.31s\n","# Epoch 1  global step 1240 loss 0.67322 batch 1240/3281 lr 0.001 accuracy 86.98828 wps 18756.74 step time 0.47s\n","# Epoch 1  global step 1260 loss 0.65765 batch 1260/3281 lr 0.001 accuracy 87.68750 wps 19286.03 step time 0.36s\n","# Epoch 1  global step 1280 loss 0.61729 batch 1280/3281 lr 0.001 accuracy 88.54687 wps 18934.93 step time 0.35s\n","# Epoch 1  global step 1300 loss 0.63405 batch 1300/3281 lr 0.001 accuracy 87.93359 wps 20104.12 step time 0.39s\n","# Epoch 1  global step 1320 loss 0.62632 batch 1320/3281 lr 0.001 accuracy 88.17187 wps 19665.42 step time 0.38s\n","# Epoch 1  global step 1340 loss 0.62218 batch 1340/3281 lr 0.001 accuracy 88.46094 wps 19102.35 step time 0.42s\n","# Epoch 1  global step 1360 loss 0.60713 batch 1360/3281 lr 0.001 accuracy 88.75391 wps 18167.17 step time 0.33s\n","# Epoch 1  global step 1380 loss 0.61740 batch 1380/3281 lr 0.001 accuracy 88.49219 wps 19508.79 step time 0.38s\n","# Epoch 1  global step 1400 loss 0.60674 batch 1400/3281 lr 0.001 accuracy 88.86328 wps 19836.89 step time 0.38s\n","# Epoch 1  global step 1420 loss 0.57039 batch 1420/3281 lr 0.001 accuracy 89.45703 wps 18374.90 step time 0.34s\n","# Epoch 1  global step 1440 loss 0.60652 batch 1440/3281 lr 0.001 accuracy 88.92187 wps 18847.56 step time 0.34s\n","# Epoch 1  global step 1460 loss 0.57138 batch 1460/3281 lr 0.001 accuracy 89.55859 wps 18635.46 step time 0.34s\n","# Epoch 1  global step 1480 loss 0.59761 batch 1480/3281 lr 0.001 accuracy 89.12109 wps 19122.19 step time 0.36s\n","# Epoch 1  global step 1500 loss 0.57661 batch 1500/3281 lr 0.001 accuracy 89.50000 wps 18525.35 step time 0.38s\n","# Epoch 1  global step 1520 loss 0.57004 batch 1520/3281 lr 0.001 accuracy 89.70312 wps 18970.20 step time 0.35s\n","# Epoch 1  global step 1540 loss 0.54346 batch 1540/3281 lr 0.001 accuracy 90.21094 wps 18069.12 step time 0.33s\n","# Epoch 1  global step 1560 loss 0.55455 batch 1560/3281 lr 0.001 accuracy 89.97656 wps 18085.92 step time 0.32s\n","# Epoch 1  global step 1580 loss 0.58261 batch 1580/3281 lr 0.001 accuracy 89.34766 wps 19467.72 step time 0.36s\n","# Epoch 1  global step 1600 loss 0.52731 batch 1600/3281 lr 0.001 accuracy 90.63281 wps 18411.97 step time 0.33s\n","# Epoch 1  global step 1620 loss 0.55734 batch 1620/3281 lr 0.001 accuracy 89.60938 wps 19513.85 step time 0.44s\n","# Epoch 1  global step 1640 loss 0.53079 batch 1640/3281 lr 0.001 accuracy 90.26172 wps 19516.54 step time 0.38s\n","# Epoch 1  global step 1660 loss 0.52686 batch 1660/3281 lr 0.001 accuracy 90.39844 wps 17863.33 step time 0.32s\n","# Epoch 1  global step 1680 loss 0.55971 batch 1680/3281 lr 0.001 accuracy 89.71484 wps 19951.65 step time 0.39s\n","# Epoch 1  global step 1700 loss 0.55513 batch 1700/3281 lr 0.001 accuracy 89.92187 wps 19690.30 step time 0.37s\n","# Epoch 1  global step 1720 loss 0.55760 batch 1720/3281 lr 0.001 accuracy 89.74609 wps 19760.64 step time 0.45s\n","# Epoch 1  global step 1740 loss 0.52984 batch 1740/3281 lr 0.001 accuracy 90.26562 wps 19186.35 step time 0.42s\n","# Epoch 1  global step 1760 loss 0.52564 batch 1760/3281 lr 0.001 accuracy 90.21094 wps 19840.83 step time 0.45s\n","# Epoch 1  global step 1780 loss 0.53044 batch 1780/3281 lr 0.001 accuracy 90.18359 wps 19171.46 step time 0.40s\n","# Epoch 1  global step 1800 loss 0.51765 batch 1800/3281 lr 0.001 accuracy 90.67578 wps 19212.31 step time 0.36s\n","# Epoch 1  global step 1820 loss 0.51757 batch 1820/3281 lr 0.001 accuracy 90.66406 wps 19201.52 step time 0.36s\n","# Epoch 1  global step 1840 loss 0.52765 batch 1840/3281 lr 0.001 accuracy 90.60938 wps 19152.08 step time 0.35s\n","# Epoch 1  global step 1860 loss 0.51572 batch 1860/3281 lr 0.001 accuracy 90.46875 wps 19511.53 step time 0.37s\n","# Epoch 1  global step 1880 loss 0.49968 batch 1880/3281 lr 0.001 accuracy 91.21094 wps 18537.03 step time 0.34s\n","# Epoch 1  global step 1900 loss 0.50386 batch 1900/3281 lr 0.001 accuracy 90.77734 wps 19502.22 step time 0.43s\n","# Epoch 1  global step 1920 loss 0.49729 batch 1920/3281 lr 0.001 accuracy 90.91797 wps 19346.00 step time 0.36s\n","# Epoch 1  global step 1940 loss 0.49097 batch 1940/3281 lr 0.001 accuracy 91.00391 wps 20112.48 step time 0.39s\n","# Epoch 1  global step 1960 loss 0.49097 batch 1960/3281 lr 0.001 accuracy 91.14063 wps 18810.80 step time 0.34s\n","# Epoch 1  global step 1980 loss 0.48881 batch 1980/3281 lr 0.001 accuracy 91.09375 wps 19089.51 step time 0.39s\n","# Epoch 1  global step 2000 loss 0.47689 batch 2000/3281 lr 0.001 accuracy 91.39844 wps 17922.50 step time 0.36s\n","# global step 2000, eval model at Fri May 22 10:26:45 2020\n","2020-05-22 10:26:48.163416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:26:48.164041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 10:26:48.164146: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 10:26:48.164187: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 10:26:48.164237: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 10:26:48.164279: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 10:26:48.164319: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 10:26:48.164360: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 10:26:48.164402: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 10:26:48.164521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:26:48.165089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:26:48.165548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 10:26:48.165600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 10:26:48.165619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 10:26:48.165632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 10:26:48.165792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:26:48.166318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:26:48.166875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","# location_traffic_convenience - 0.21969951788317077\n","# location_distance_from_business_district - 0.22255105060668837\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14272103658536586\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.02399086062452399\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.20089730807577266\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 0.97156, f1 0.19357\n","# current result -0.1935690861082896, previous best result 1000000000\n","# Epoch 1  global step 2020 loss 0.50105 batch 2020/3281 lr 0.001 accuracy 91.22266 wps 18838.24 step time 0.42s\n","# Epoch 1  global step 2040 loss 0.48354 batch 2040/3281 lr 0.001 accuracy 91.07031 wps 19660.48 step time 0.38s\n","# Epoch 1  global step 2060 loss 0.48050 batch 2060/3281 lr 0.001 accuracy 91.39063 wps 19432.67 step time 0.37s\n","# Epoch 1  global step 2080 loss 0.48005 batch 2080/3281 lr 0.001 accuracy 91.26953 wps 19260.97 step time 0.36s\n","# Epoch 1  global step 2100 loss 0.49327 batch 2100/3281 lr 0.001 accuracy 91.11328 wps 18570.16 step time 0.38s\n","# Epoch 1  global step 2120 loss 0.48234 batch 2120/3281 lr 0.001 accuracy 91.22656 wps 18209.39 step time 0.38s\n","# Epoch 1  global step 2140 loss 0.49356 batch 2140/3281 lr 0.001 accuracy 91.07031 wps 20494.69 step time 0.46s\n","# Epoch 1  global step 2160 loss 0.48936 batch 2160/3281 lr 0.001 accuracy 91.32422 wps 19282.23 step time 0.40s\n","# Epoch 1  global step 2180 loss 0.48228 batch 2180/3281 lr 0.001 accuracy 91.23828 wps 18673.08 step time 0.34s\n","# Epoch 1  global step 2200 loss 0.48328 batch 2200/3281 lr 0.001 accuracy 91.28906 wps 19730.88 step time 0.44s\n","# Epoch 1  global step 2220 loss 0.44980 batch 2220/3281 lr 0.001 accuracy 91.67578 wps 19049.01 step time 0.35s\n","# Epoch 1  global step 2240 loss 0.46452 batch 2240/3281 lr 0.001 accuracy 91.69531 wps 18777.28 step time 0.35s\n","# Epoch 1  global step 2260 loss 0.48018 batch 2260/3281 lr 0.001 accuracy 91.30078 wps 19774.61 step time 0.44s\n","# Epoch 1  global step 2280 loss 0.43148 batch 2280/3281 lr 0.001 accuracy 92.31250 wps 17827.04 step time 0.32s\n","# Epoch 1  global step 2300 loss 0.46359 batch 2300/3281 lr 0.001 accuracy 91.58984 wps 18541.29 step time 0.34s\n","# Epoch 1  global step 2320 loss 0.47041 batch 2320/3281 lr 0.001 accuracy 91.71875 wps 18491.80 step time 0.37s\n","# Epoch 1  global step 2340 loss 0.46545 batch 2340/3281 lr 0.001 accuracy 91.68359 wps 16806.74 step time 0.44s\n","# Epoch 1  global step 2360 loss 0.45091 batch 2360/3281 lr 0.001 accuracy 91.94531 wps 16248.12 step time 0.51s\n","# Epoch 1  global step 2380 loss 0.48637 batch 2380/3281 lr 0.001 accuracy 91.28906 wps 17507.21 step time 0.42s\n","# Epoch 1  global step 2400 loss 0.46211 batch 2400/3281 lr 0.001 accuracy 91.78906 wps 17124.58 step time 0.41s\n","# Epoch 1  global step 2420 loss 0.45072 batch 2420/3281 lr 0.001 accuracy 91.82031 wps 16521.73 step time 0.44s\n","# Epoch 1  global step 2440 loss 0.47234 batch 2440/3281 lr 0.001 accuracy 91.58984 wps 16763.35 step time 0.45s\n","# Epoch 1  global step 2460 loss 0.46645 batch 2460/3281 lr 0.001 accuracy 91.75781 wps 17011.91 step time 0.39s\n","# Epoch 1  global step 2480 loss 0.45287 batch 2480/3281 lr 0.001 accuracy 91.75391 wps 17879.30 step time 0.41s\n","# Epoch 1  global step 2500 loss 0.46653 batch 2500/3281 lr 0.001 accuracy 91.68750 wps 17029.48 step time 0.39s\n","# Epoch 1  global step 2520 loss 0.47484 batch 2520/3281 lr 0.001 accuracy 91.29687 wps 16813.38 step time 0.49s\n","# Epoch 1  global step 2540 loss 0.43946 batch 2540/3281 lr 0.001 accuracy 92.16016 wps 16634.63 step time 0.39s\n","# Epoch 1  global step 2560 loss 0.43886 batch 2560/3281 lr 0.001 accuracy 92.39844 wps 17196.59 step time 0.34s\n","# Epoch 1  global step 2580 loss 0.45223 batch 2580/3281 lr 0.001 accuracy 91.91016 wps 16606.33 step time 0.41s\n","# Epoch 1  global step 2600 loss 0.44084 batch 2600/3281 lr 0.001 accuracy 92.03125 wps 16655.87 step time 0.36s\n","# Epoch 1  global step 2620 loss 0.45378 batch 2620/3281 lr 0.001 accuracy 91.82031 wps 17497.93 step time 0.44s\n","# Epoch 1  global step 2640 loss 0.48144 batch 2640/3281 lr 0.001 accuracy 91.30469 wps 16964.17 step time 0.44s\n","# Epoch 1  global step 2660 loss 0.43628 batch 2660/3281 lr 0.001 accuracy 92.41406 wps 16691.63 step time 0.37s\n","# Epoch 1  global step 2680 loss 0.46596 batch 2680/3281 lr 0.001 accuracy 91.29297 wps 16995.29 step time 0.50s\n","# Epoch 1  global step 2700 loss 0.46810 batch 2700/3281 lr 0.001 accuracy 91.60547 wps 16605.41 step time 0.46s\n","# Epoch 1  global step 2720 loss 0.43919 batch 2720/3281 lr 0.001 accuracy 92.10156 wps 16623.04 step time 0.40s\n","# Epoch 1  global step 2740 loss 0.45979 batch 2740/3281 lr 0.001 accuracy 91.88281 wps 16396.19 step time 0.41s\n","# Epoch 1  global step 2760 loss 0.44897 batch 2760/3281 lr 0.001 accuracy 91.96875 wps 16996.42 step time 0.43s\n","# Epoch 1  global step 2780 loss 0.44896 batch 2780/3281 lr 0.001 accuracy 92.09375 wps 17430.31 step time 0.40s\n","# Epoch 1  global step 2800 loss 0.45054 batch 2800/3281 lr 0.001 accuracy 91.93359 wps 17359.82 step time 0.37s\n","# Epoch 1  global step 2820 loss 0.44827 batch 2820/3281 lr 0.001 accuracy 91.75781 wps 17353.28 step time 0.43s\n","# Epoch 1  global step 2840 loss 0.45755 batch 2840/3281 lr 0.001 accuracy 91.82031 wps 17212.26 step time 0.44s\n","# Epoch 1  global step 2860 loss 0.43253 batch 2860/3281 lr 0.001 accuracy 92.19141 wps 17927.55 step time 0.40s\n","# Epoch 1  global step 2880 loss 0.48576 batch 2880/3281 lr 0.001 accuracy 91.13281 wps 17390.36 step time 0.53s\n","# Epoch 1  global step 2900 loss 0.44048 batch 2900/3281 lr 0.001 accuracy 92.28125 wps 17469.08 step time 0.37s\n","# Epoch 1  global step 2920 loss 0.43807 batch 2920/3281 lr 0.001 accuracy 92.08594 wps 17183.25 step time 0.42s\n","# Epoch 1  global step 2940 loss 0.44121 batch 2940/3281 lr 0.001 accuracy 92.11328 wps 17115.40 step time 0.38s\n","# Epoch 1  global step 2960 loss 0.43062 batch 2960/3281 lr 0.001 accuracy 92.41406 wps 17550.90 step time 0.39s\n","# Epoch 1  global step 2980 loss 0.44579 batch 2980/3281 lr 0.001 accuracy 91.83984 wps 17726.77 step time 0.41s\n","# Epoch 1  global step 3000 loss 0.45423 batch 3000/3281 lr 0.001 accuracy 91.71094 wps 17709.83 step time 0.45s\n","# Epoch 1  global step 3020 loss 0.43634 batch 3020/3281 lr 0.001 accuracy 92.24609 wps 16588.13 step time 0.43s\n","# Epoch 1  global step 3040 loss 0.44995 batch 3040/3281 lr 0.001 accuracy 92.00781 wps 17359.80 step time 0.41s\n","# Epoch 1  global step 3060 loss 0.43817 batch 3060/3281 lr 0.001 accuracy 91.92578 wps 16929.16 step time 0.46s\n","# Epoch 1  global step 3080 loss 0.44360 batch 3080/3281 lr 0.001 accuracy 92.06641 wps 15928.27 step time 0.47s\n","# Epoch 1  global step 3100 loss 0.41163 batch 3100/3281 lr 0.001 accuracy 92.82422 wps 15765.21 step time 0.39s\n","# Epoch 1  global step 3120 loss 0.42964 batch 3120/3281 lr 0.001 accuracy 92.12891 wps 16920.98 step time 0.37s\n","# Epoch 1  global step 3140 loss 0.44109 batch 3140/3281 lr 0.001 accuracy 92.18750 wps 16622.15 step time 0.46s\n","# Epoch 1  global step 3160 loss 0.41088 batch 3160/3281 lr 0.001 accuracy 92.71094 wps 16518.23 step time 0.35s\n","# Epoch 1  global step 3180 loss 0.43085 batch 3180/3281 lr 0.001 accuracy 92.40625 wps 17071.43 step time 0.42s\n","# Epoch 1  global step 3200 loss 0.44232 batch 3200/3281 lr 0.001 accuracy 92.07812 wps 16395.39 step time 0.54s\n","# Epoch 1  global step 3220 loss 0.43606 batch 3220/3281 lr 0.001 accuracy 92.02734 wps 16783.03 step time 0.49s\n","# Epoch 1  global step 3240 loss 0.42327 batch 3240/3281 lr 0.001 accuracy 92.44922 wps 16954.45 step time 0.35s\n","# Epoch 1  global step 3260 loss 0.44796 batch 3260/3281 lr 0.001 accuracy 91.94922 wps 17309.13 step time 0.46s\n","# Epoch 1  global step 3280 loss 0.43714 batch 3280/3281 lr 0.001 accuracy 92.07031 wps 16843.38 step time 0.46s\n","# Finsh epoch 1, global step 3282\n","# Epoch 2  global step 3300 loss 0.39837 batch 18/3281 lr 0.001 accuracy 82.54687 wps 19317.92 step time 0.43s\n","# Epoch 2  global step 3320 loss 0.41204 batch 38/3281 lr 0.001 accuracy 92.65234 wps 18521.51 step time 0.33s\n","# Epoch 2  global step 3340 loss 0.43381 batch 58/3281 lr 0.001 accuracy 92.17969 wps 19281.62 step time 0.36s\n","# Epoch 2  global step 3360 loss 0.42683 batch 78/3281 lr 0.001 accuracy 92.23047 wps 19390.88 step time 0.42s\n","# Epoch 2  global step 3380 loss 0.42990 batch 98/3281 lr 0.001 accuracy 92.23828 wps 18765.52 step time 0.50s\n","# Epoch 2  global step 3400 loss 0.37622 batch 118/3281 lr 0.001 accuracy 93.30078 wps 17771.61 step time 0.31s\n","# Epoch 2  global step 3420 loss 0.41228 batch 138/3281 lr 0.001 accuracy 92.60547 wps 19020.59 step time 0.35s\n","# Epoch 2  global step 3440 loss 0.45156 batch 158/3281 lr 0.001 accuracy 91.53516 wps 20013.18 step time 0.39s\n","# Epoch 2  global step 3460 loss 0.43443 batch 178/3281 lr 0.001 accuracy 92.28125 wps 19662.07 step time 0.37s\n","# Epoch 2  global step 3480 loss 0.46235 batch 198/3281 lr 0.001 accuracy 91.62500 wps 19331.40 step time 0.53s\n","# Epoch 2  global step 3500 loss 0.41199 batch 218/3281 lr 0.001 accuracy 92.66406 wps 18683.24 step time 0.34s\n","# Epoch 2  global step 3520 loss 0.39386 batch 238/3281 lr 0.001 accuracy 93.16016 wps 18096.88 step time 0.32s\n","# Epoch 2  global step 3540 loss 0.42197 batch 258/3281 lr 0.001 accuracy 92.27344 wps 18594.10 step time 0.33s\n","# Epoch 2  global step 3560 loss 0.43245 batch 278/3281 lr 0.001 accuracy 92.14453 wps 20222.14 step time 0.40s\n","# Epoch 2  global step 3580 loss 0.44603 batch 298/3281 lr 0.001 accuracy 92.16797 wps 18588.85 step time 0.47s\n","# Epoch 2  global step 3600 loss 0.44294 batch 318/3281 lr 0.001 accuracy 91.95312 wps 16502.47 step time 0.51s\n","# Epoch 2  global step 3620 loss 0.41706 batch 338/3281 lr 0.001 accuracy 92.58203 wps 16619.07 step time 0.37s\n","# Epoch 2  global step 3640 loss 0.42268 batch 358/3281 lr 0.001 accuracy 92.41016 wps 17734.76 step time 0.43s\n","# Epoch 2  global step 3660 loss 0.43648 batch 378/3281 lr 0.001 accuracy 91.93359 wps 17152.77 step time 0.46s\n","# Epoch 2  global step 3680 loss 0.41449 batch 398/3281 lr 0.001 accuracy 92.60547 wps 15991.32 step time 0.47s\n","# Epoch 2  global step 3700 loss 0.40055 batch 418/3281 lr 0.001 accuracy 93.08984 wps 17070.75 step time 0.36s\n","# Epoch 2  global step 3720 loss 0.42123 batch 438/3281 lr 0.001 accuracy 92.55859 wps 17312.25 step time 0.47s\n","# Epoch 2  global step 3740 loss 0.43558 batch 458/3281 lr 0.001 accuracy 92.10156 wps 16774.66 step time 0.48s\n","# Epoch 2  global step 3760 loss 0.41962 batch 478/3281 lr 0.001 accuracy 92.63281 wps 16393.52 step time 0.43s\n","# Epoch 2  global step 3780 loss 0.41957 batch 498/3281 lr 0.001 accuracy 92.35156 wps 16409.89 step time 0.38s\n","# Epoch 2  global step 3800 loss 0.43312 batch 518/3281 lr 0.001 accuracy 92.17578 wps 16593.39 step time 0.51s\n","# Epoch 2  global step 3820 loss 0.41851 batch 538/3281 lr 0.001 accuracy 92.58984 wps 16872.64 step time 0.35s\n","# Epoch 2  global step 3840 loss 0.41569 batch 558/3281 lr 0.001 accuracy 92.60156 wps 16642.33 step time 0.39s\n","# Epoch 2  global step 3860 loss 0.41809 batch 578/3281 lr 0.001 accuracy 92.48828 wps 16678.22 step time 0.42s\n","# Epoch 2  global step 3880 loss 0.42575 batch 598/3281 lr 0.001 accuracy 92.43750 wps 16040.80 step time 0.49s\n","# Epoch 2  global step 3900 loss 0.43100 batch 618/3281 lr 0.001 accuracy 92.27344 wps 17370.01 step time 0.43s\n","# Epoch 2  global step 3920 loss 0.40115 batch 638/3281 lr 0.001 accuracy 92.79297 wps 16815.92 step time 0.35s\n","# Epoch 2  global step 3940 loss 0.41747 batch 658/3281 lr 0.001 accuracy 92.37891 wps 17118.55 step time 0.44s\n","# Epoch 2  global step 3960 loss 0.41628 batch 678/3281 lr 0.001 accuracy 92.44141 wps 16997.71 step time 0.39s\n","# Epoch 2  global step 3980 loss 0.40303 batch 698/3281 lr 0.001 accuracy 92.65625 wps 16870.78 step time 0.41s\n","# Epoch 2  global step 4000 loss 0.41074 batch 718/3281 lr 0.001 accuracy 92.82422 wps 17271.86 step time 0.38s\n","# global step 4000, eval model at Fri May 22 10:41:39 2020\n","2020-05-22 10:41:42.388522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:41:42.389374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 10:41:42.389543: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 10:41:42.389586: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 10:41:42.389627: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 10:41:42.389681: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 10:41:42.389723: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 10:41:42.389763: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 10:41:42.389803: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 10:41:42.389923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:41:42.390527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:41:42.391017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 10:41:42.391113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 10:41:42.391173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 10:41:42.391188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 10:41:42.391428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:41:42.391992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:41:42.392482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.22219629340953675\n","# location_distance_from_business_district - 0.22255105060668837\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14272103658536586\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.0711406234398256\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.20645735098963772\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 0.84219, f1 0.19633\n","# current result -0.19632941517106622, previous best result -0.1935690861082896\n","# Epoch 2  global step 4020 loss 0.41355 batch 738/3281 lr 0.001 accuracy 92.67969 wps 17302.62 step time 0.39s\n","# Epoch 2  global step 4040 loss 0.43140 batch 758/3281 lr 0.001 accuracy 92.49219 wps 17083.29 step time 0.39s\n","# Epoch 2  global step 4060 loss 0.42899 batch 778/3281 lr 0.001 accuracy 92.20312 wps 17486.91 step time 0.39s\n","# Epoch 2  global step 4080 loss 0.43263 batch 798/3281 lr 0.001 accuracy 91.96484 wps 17380.12 step time 0.50s\n","# Epoch 2  global step 4100 loss 0.42386 batch 818/3281 lr 0.001 accuracy 92.25000 wps 16746.28 step time 0.49s\n","# Epoch 2  global step 4120 loss 0.43326 batch 838/3281 lr 0.001 accuracy 92.23438 wps 16649.57 step time 0.44s\n","# Epoch 2  global step 4140 loss 0.42236 batch 858/3281 lr 0.001 accuracy 92.26953 wps 16577.12 step time 0.50s\n","# Epoch 2  global step 4160 loss 0.41345 batch 878/3281 lr 0.001 accuracy 92.45703 wps 17491.43 step time 0.40s\n","# Epoch 2  global step 4180 loss 0.39377 batch 898/3281 lr 0.001 accuracy 92.84766 wps 16257.31 step time 0.36s\n","# Epoch 2  global step 4200 loss 0.38327 batch 918/3281 lr 0.001 accuracy 92.97656 wps 17379.65 step time 0.38s\n","# Epoch 2  global step 4220 loss 0.41312 batch 938/3281 lr 0.001 accuracy 92.30469 wps 18258.95 step time 0.46s\n","# Epoch 2  global step 4240 loss 0.41056 batch 958/3281 lr 0.001 accuracy 92.50391 wps 16618.26 step time 0.44s\n","# Epoch 2  global step 4260 loss 0.39280 batch 978/3281 lr 0.001 accuracy 93.08203 wps 16569.91 step time 0.34s\n","# Epoch 2  global step 4280 loss 0.41576 batch 998/3281 lr 0.001 accuracy 92.34766 wps 17997.50 step time 0.42s\n","# Epoch 2  global step 4300 loss 0.43941 batch 1018/3281 lr 0.001 accuracy 91.90625 wps 17348.92 step time 0.51s\n","# Epoch 2  global step 4320 loss 0.40097 batch 1038/3281 lr 0.001 accuracy 92.76562 wps 16013.33 step time 0.47s\n","# Epoch 2  global step 4340 loss 0.43010 batch 1058/3281 lr 0.001 accuracy 92.30078 wps 17292.65 step time 0.42s\n","# Epoch 2  global step 4360 loss 0.42493 batch 1078/3281 lr 0.001 accuracy 92.21484 wps 17663.96 step time 0.45s\n","# Epoch 2  global step 4380 loss 0.41294 batch 1098/3281 lr 0.001 accuracy 92.75391 wps 17384.88 step time 0.41s\n","# Epoch 2  global step 4400 loss 0.41820 batch 1118/3281 lr 0.001 accuracy 92.37109 wps 16180.62 step time 0.52s\n","# Epoch 2  global step 4420 loss 0.41150 batch 1138/3281 lr 0.001 accuracy 92.59375 wps 16452.47 step time 0.43s\n","# Epoch 2  global step 4440 loss 0.42362 batch 1158/3281 lr 0.001 accuracy 92.50391 wps 17292.24 step time 0.40s\n","# Epoch 2  global step 4460 loss 0.39664 batch 1178/3281 lr 0.001 accuracy 92.94141 wps 17057.05 step time 0.35s\n","# Epoch 2  global step 4480 loss 0.41016 batch 1198/3281 lr 0.001 accuracy 92.66406 wps 17525.36 step time 0.41s\n","# Epoch 2  global step 4500 loss 0.38504 batch 1218/3281 lr 0.001 accuracy 92.97266 wps 16964.28 step time 0.37s\n","# Epoch 2  global step 4520 loss 0.40371 batch 1238/3281 lr 0.001 accuracy 92.66016 wps 15943.97 step time 0.45s\n","# Epoch 2  global step 4540 loss 0.41812 batch 1258/3281 lr 0.001 accuracy 92.49609 wps 17257.54 step time 0.36s\n","# Epoch 2  global step 4560 loss 0.41262 batch 1278/3281 lr 0.001 accuracy 92.45703 wps 16353.63 step time 0.49s\n","# Epoch 2  global step 4580 loss 0.41655 batch 1298/3281 lr 0.001 accuracy 92.49219 wps 16438.62 step time 0.43s\n","# Epoch 2  global step 4600 loss 0.39491 batch 1318/3281 lr 0.001 accuracy 92.99219 wps 16684.54 step time 0.37s\n","# Epoch 2  global step 4620 loss 0.41208 batch 1338/3281 lr 0.001 accuracy 92.58203 wps 17342.96 step time 0.38s\n","# Epoch 2  global step 4640 loss 0.41032 batch 1358/3281 lr 0.001 accuracy 92.62891 wps 17392.04 step time 0.40s\n","# Epoch 2  global step 4660 loss 0.41297 batch 1378/3281 lr 0.001 accuracy 92.45312 wps 16486.85 step time 0.47s\n","# Epoch 2  global step 4680 loss 0.42853 batch 1398/3281 lr 0.001 accuracy 92.17578 wps 16732.05 step time 0.46s\n","# Epoch 2  global step 4700 loss 0.44614 batch 1418/3281 lr 0.001 accuracy 91.87891 wps 17245.01 step time 0.54s\n","# Epoch 2  global step 4720 loss 0.40607 batch 1438/3281 lr 0.001 accuracy 92.63672 wps 16781.54 step time 0.42s\n","# Epoch 2  global step 4740 loss 0.41851 batch 1458/3281 lr 0.001 accuracy 92.40625 wps 16956.04 step time 0.41s\n","# Epoch 2  global step 4760 loss 0.40449 batch 1478/3281 lr 0.001 accuracy 92.72656 wps 17852.47 step time 0.43s\n","# Epoch 2  global step 4780 loss 0.40419 batch 1498/3281 lr 0.001 accuracy 92.55469 wps 16583.44 step time 0.42s\n","# Epoch 2  global step 4800 loss 0.40595 batch 1518/3281 lr 0.001 accuracy 92.80469 wps 16694.04 step time 0.41s\n","# Epoch 2  global step 4820 loss 0.40222 batch 1538/3281 lr 0.001 accuracy 92.54687 wps 17146.60 step time 0.43s\n","# Epoch 2  global step 4840 loss 0.40485 batch 1558/3281 lr 0.001 accuracy 92.85547 wps 17043.24 step time 0.41s\n","# Epoch 2  global step 4860 loss 0.42020 batch 1578/3281 lr 0.001 accuracy 92.48047 wps 16706.07 step time 0.47s\n","# Epoch 2  global step 4880 loss 0.42038 batch 1598/3281 lr 0.001 accuracy 92.40625 wps 16645.47 step time 0.43s\n","# Epoch 2  global step 4900 loss 0.41115 batch 1618/3281 lr 0.001 accuracy 92.57422 wps 16931.03 step time 0.50s\n","# Epoch 2  global step 4920 loss 0.42394 batch 1638/3281 lr 0.001 accuracy 92.51953 wps 17374.43 step time 0.44s\n","# Epoch 2  global step 4940 loss 0.37716 batch 1658/3281 lr 0.001 accuracy 93.22266 wps 15927.54 step time 0.38s\n","# Epoch 2  global step 4960 loss 0.40875 batch 1678/3281 lr 0.001 accuracy 92.47266 wps 16587.44 step time 0.50s\n","# Epoch 2  global step 4980 loss 0.42803 batch 1698/3281 lr 0.001 accuracy 92.07031 wps 16953.96 step time 0.49s\n","# Epoch 2  global step 5000 loss 0.41248 batch 1718/3281 lr 0.001 accuracy 92.62500 wps 16881.15 step time 0.40s\n","# Epoch 2  global step 5020 loss 0.41456 batch 1738/3281 lr 0.001 accuracy 92.33984 wps 17510.34 step time 0.43s\n","# Epoch 2  global step 5040 loss 0.39668 batch 1758/3281 lr 0.001 accuracy 92.50781 wps 16311.82 step time 0.49s\n","# Epoch 2  global step 5060 loss 0.41939 batch 1778/3281 lr 0.001 accuracy 92.31250 wps 16106.92 step time 0.47s\n","# Epoch 2  global step 5080 loss 0.39954 batch 1798/3281 lr 0.001 accuracy 92.80859 wps 16455.45 step time 0.38s\n","# Epoch 2  global step 5100 loss 0.40248 batch 1818/3281 lr 0.001 accuracy 92.76563 wps 17849.04 step time 0.41s\n","# Epoch 2  global step 5120 loss 0.41513 batch 1838/3281 lr 0.001 accuracy 92.55859 wps 17090.43 step time 0.38s\n","# Epoch 2  global step 5140 loss 0.40792 batch 1858/3281 lr 0.001 accuracy 92.80078 wps 17429.36 step time 0.35s\n","# Epoch 2  global step 5160 loss 0.40108 batch 1878/3281 lr 0.001 accuracy 92.76172 wps 16369.12 step time 0.43s\n","# Epoch 2  global step 5180 loss 0.41702 batch 1898/3281 lr 0.001 accuracy 92.47266 wps 17448.89 step time 0.43s\n","# Epoch 2  global step 5200 loss 0.41224 batch 1918/3281 lr 0.001 accuracy 92.44531 wps 16906.65 step time 0.36s\n","# Epoch 2  global step 5220 loss 0.41132 batch 1938/3281 lr 0.001 accuracy 92.53125 wps 16770.33 step time 0.42s\n","# Epoch 2  global step 5240 loss 0.40048 batch 1958/3281 lr 0.001 accuracy 92.83984 wps 17511.58 step time 0.38s\n","# Epoch 2  global step 5260 loss 0.38747 batch 1978/3281 lr 0.001 accuracy 92.92188 wps 16651.65 step time 0.41s\n","# Epoch 2  global step 5280 loss 0.40084 batch 1998/3281 lr 0.001 accuracy 92.67578 wps 16747.71 step time 0.39s\n","# Epoch 2  global step 5300 loss 0.40750 batch 2018/3281 lr 0.001 accuracy 92.75000 wps 16330.79 step time 0.35s\n","# Epoch 2  global step 5320 loss 0.40098 batch 2038/3281 lr 0.001 accuracy 92.77344 wps 16539.71 step time 0.48s\n","# Epoch 2  global step 5340 loss 0.39579 batch 2058/3281 lr 0.001 accuracy 92.93359 wps 15674.29 step time 0.44s\n","# Epoch 2  global step 5360 loss 0.37496 batch 2078/3281 lr 0.001 accuracy 93.27344 wps 16195.48 step time 0.35s\n","# Epoch 2  global step 5380 loss 0.43185 batch 2098/3281 lr 0.001 accuracy 92.29688 wps 17940.55 step time 0.45s\n","# Epoch 2  global step 5400 loss 0.42326 batch 2118/3281 lr 0.001 accuracy 92.41797 wps 17976.70 step time 0.44s\n","# Epoch 2  global step 5420 loss 0.41640 batch 2138/3281 lr 0.001 accuracy 92.57422 wps 15894.30 step time 0.48s\n","# Epoch 2  global step 5440 loss 0.40547 batch 2158/3281 lr 0.001 accuracy 92.59766 wps 16186.21 step time 0.46s\n","# Epoch 2  global step 5460 loss 0.41079 batch 2178/3281 lr 0.001 accuracy 92.47266 wps 17166.72 step time 0.54s\n","# Epoch 2  global step 5480 loss 0.39755 batch 2198/3281 lr 0.001 accuracy 92.77344 wps 17206.07 step time 0.47s\n","# Epoch 2  global step 5500 loss 0.43004 batch 2218/3281 lr 0.001 accuracy 92.10547 wps 16362.40 step time 0.51s\n","# Epoch 2  global step 5520 loss 0.42332 batch 2238/3281 lr 0.001 accuracy 92.08594 wps 17400.68 step time 0.52s\n","# Epoch 2  global step 5540 loss 0.40499 batch 2258/3281 lr 0.001 accuracy 92.64062 wps 16759.27 step time 0.39s\n","# Epoch 2  global step 5560 loss 0.39973 batch 2278/3281 lr 0.001 accuracy 92.79687 wps 17111.97 step time 0.40s\n","# Epoch 2  global step 5580 loss 0.39562 batch 2298/3281 lr 0.001 accuracy 92.99219 wps 16799.05 step time 0.40s\n","# Epoch 2  global step 5600 loss 0.39538 batch 2318/3281 lr 0.001 accuracy 93.25781 wps 16853.73 step time 0.40s\n","# Epoch 2  global step 5620 loss 0.40772 batch 2338/3281 lr 0.001 accuracy 92.71094 wps 16854.14 step time 0.42s\n","# Epoch 2  global step 5640 loss 0.38923 batch 2358/3281 lr 0.001 accuracy 92.99609 wps 16575.90 step time 0.36s\n","# Epoch 2  global step 5660 loss 0.42541 batch 2378/3281 lr 0.001 accuracy 92.12109 wps 17572.52 step time 0.43s\n","# Epoch 2  global step 5680 loss 0.37269 batch 2398/3281 lr 0.001 accuracy 93.32812 wps 16127.32 step time 0.35s\n","# Epoch 2  global step 5700 loss 0.43484 batch 2418/3281 lr 0.001 accuracy 92.22266 wps 17042.91 step time 0.53s\n","# Epoch 2  global step 5720 loss 0.41963 batch 2438/3281 lr 0.001 accuracy 92.39844 wps 16611.74 step time 0.53s\n","# Epoch 2  global step 5740 loss 0.41185 batch 2458/3281 lr 0.001 accuracy 92.68750 wps 16924.99 step time 0.38s\n","# Epoch 2  global step 5760 loss 0.39900 batch 2478/3281 lr 0.001 accuracy 92.76953 wps 16869.08 step time 0.44s\n","# Epoch 2  global step 5780 loss 0.39915 batch 2498/3281 lr 0.001 accuracy 92.60156 wps 16906.64 step time 0.42s\n","# Epoch 2  global step 5800 loss 0.40074 batch 2518/3281 lr 0.001 accuracy 92.86719 wps 17702.26 step time 0.42s\n","# Epoch 2  global step 5820 loss 0.39988 batch 2538/3281 lr 0.001 accuracy 92.87500 wps 16919.72 step time 0.40s\n","# Epoch 2  global step 5840 loss 0.39240 batch 2558/3281 lr 0.001 accuracy 92.86719 wps 16498.53 step time 0.45s\n","# Epoch 2  global step 5860 loss 0.41374 batch 2578/3281 lr 0.001 accuracy 92.45703 wps 17081.09 step time 0.49s\n","# Epoch 2  global step 5880 loss 0.41261 batch 2598/3281 lr 0.001 accuracy 92.46094 wps 17224.23 step time 0.46s\n","# Epoch 2  global step 5900 loss 0.39471 batch 2618/3281 lr 0.001 accuracy 92.87109 wps 17911.50 step time 0.39s\n","# Epoch 2  global step 5920 loss 0.40079 batch 2638/3281 lr 0.001 accuracy 92.71094 wps 16026.25 step time 0.46s\n","# Epoch 2  global step 5940 loss 0.41313 batch 2658/3281 lr 0.001 accuracy 92.57422 wps 17468.71 step time 0.45s\n","# Epoch 2  global step 5960 loss 0.39312 batch 2678/3281 lr 0.001 accuracy 92.79687 wps 17023.08 step time 0.44s\n","# Epoch 2  global step 5980 loss 0.41263 batch 2698/3281 lr 0.001 accuracy 92.59766 wps 16837.93 step time 0.52s\n","# Epoch 2  global step 6000 loss 0.42740 batch 2718/3281 lr 0.001 accuracy 92.20703 wps 16876.63 step time 0.54s\n","# global step 6000, eval model at Fri May 22 10:57:11 2020\n","2020-05-22 10:57:14.243914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:57:14.244655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 10:57:14.244769: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 10:57:14.244815: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 10:57:14.244862: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 10:57:14.244911: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 10:57:14.244957: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 10:57:14.244999: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 10:57:14.245041: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 10:57:14.245176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:57:14.245855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:57:14.246389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 10:57:14.246506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 10:57:14.246530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 10:57:14.246545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 10:57:14.246738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:57:14.247367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 10:57:14.247945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.2375590963462205\n","# location_distance_from_business_district - 0.22348412412629323\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14272103658536586\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.22849458345037457\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.36538938764024576\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 0.73536, f1 0.21296\n","# current result -0.2129585088269385, previous best result -0.19632941517106622\n","# Epoch 2  global step 6020 loss 0.38797 batch 2738/3281 lr 0.001 accuracy 92.79687 wps 16101.26 step time 0.42s\n","# Epoch 2  global step 6040 loss 0.40864 batch 2758/3281 lr 0.001 accuracy 92.61719 wps 15560.26 step time 0.48s\n","# Epoch 2  global step 6060 loss 0.41735 batch 2778/3281 lr 0.001 accuracy 92.34766 wps 16943.25 step time 0.49s\n","# Epoch 2  global step 6080 loss 0.38462 batch 2798/3281 lr 0.001 accuracy 93.01563 wps 17353.73 step time 0.38s\n","# Epoch 2  global step 6100 loss 0.39213 batch 2818/3281 lr 0.001 accuracy 92.95313 wps 16687.31 step time 0.38s\n","# Epoch 2  global step 6120 loss 0.41901 batch 2838/3281 lr 0.001 accuracy 92.32031 wps 16655.64 step time 0.43s\n","# Epoch 2  global step 6140 loss 0.38495 batch 2858/3281 lr 0.001 accuracy 93.05859 wps 16584.12 step time 0.38s\n","# Epoch 2  global step 6160 loss 0.38929 batch 2878/3281 lr 0.001 accuracy 92.87500 wps 16159.54 step time 0.39s\n","# Epoch 2  global step 6180 loss 0.41531 batch 2898/3281 lr 0.001 accuracy 92.54688 wps 18037.65 step time 0.43s\n","# Epoch 2  global step 6200 loss 0.42145 batch 2918/3281 lr 0.001 accuracy 92.35938 wps 18064.06 step time 0.44s\n","# Epoch 2  global step 6220 loss 0.38724 batch 2938/3281 lr 0.001 accuracy 93.03516 wps 16362.71 step time 0.41s\n","# Epoch 2  global step 6240 loss 0.38843 batch 2958/3281 lr 0.001 accuracy 92.90625 wps 17298.90 step time 0.37s\n","# Epoch 2  global step 6260 loss 0.39021 batch 2978/3281 lr 0.001 accuracy 92.98438 wps 16723.60 step time 0.38s\n","# Epoch 2  global step 6280 loss 0.39163 batch 2998/3281 lr 0.001 accuracy 92.85547 wps 16463.29 step time 0.43s\n","# Epoch 2  global step 6300 loss 0.40108 batch 3018/3281 lr 0.001 accuracy 92.69531 wps 16543.79 step time 0.43s\n","# Epoch 2  global step 6320 loss 0.41006 batch 3038/3281 lr 0.001 accuracy 92.59375 wps 15996.96 step time 0.49s\n","# Epoch 2  global step 6340 loss 0.39406 batch 3058/3281 lr 0.001 accuracy 92.90625 wps 16694.49 step time 0.38s\n","# Epoch 2  global step 6360 loss 0.39393 batch 3078/3281 lr 0.001 accuracy 93.01172 wps 17453.63 step time 0.39s\n","# Epoch 2  global step 6380 loss 0.38009 batch 3098/3281 lr 0.001 accuracy 93.11328 wps 16510.71 step time 0.43s\n","# Epoch 2  global step 6400 loss 0.40508 batch 3118/3281 lr 0.001 accuracy 92.60937 wps 17098.75 step time 0.44s\n","# Epoch 2  global step 6420 loss 0.39815 batch 3138/3281 lr 0.001 accuracy 92.79687 wps 16439.82 step time 0.48s\n","# Epoch 2  global step 6440 loss 0.38506 batch 3158/3281 lr 0.001 accuracy 93.08203 wps 16786.23 step time 0.39s\n","# Epoch 2  global step 6460 loss 0.41809 batch 3178/3281 lr 0.001 accuracy 92.19141 wps 16335.54 step time 0.48s\n","# Epoch 2  global step 6480 loss 0.38773 batch 3198/3281 lr 0.001 accuracy 92.94922 wps 16913.64 step time 0.37s\n","# Epoch 2  global step 6500 loss 0.36630 batch 3218/3281 lr 0.001 accuracy 93.48437 wps 16826.64 step time 0.36s\n","# Epoch 2  global step 6520 loss 0.37842 batch 3238/3281 lr 0.001 accuracy 93.12109 wps 16963.31 step time 0.41s\n","# Epoch 2  global step 6540 loss 0.38788 batch 3258/3281 lr 0.001 accuracy 92.95313 wps 16828.48 step time 0.42s\n","# Epoch 2  global step 6560 loss 0.40184 batch 3278/3281 lr 0.001 accuracy 92.67187 wps 16515.85 step time 0.39s\n","# Finsh epoch 2, global step 6564\n","# Epoch 3  global step 6580 loss 0.31097 batch 16/3281 lr 0.001 accuracy 74.37109 wps 18655.74 step time 0.38s\n","# Epoch 3  global step 6600 loss 0.40617 batch 36/3281 lr 0.001 accuracy 92.49609 wps 19909.22 step time 0.40s\n","# Epoch 3  global step 6620 loss 0.39584 batch 56/3281 lr 0.001 accuracy 92.78906 wps 18482.71 step time 0.41s\n","# Epoch 3  global step 6640 loss 0.37592 batch 76/3281 lr 0.001 accuracy 93.11719 wps 18770.13 step time 0.36s\n","# Epoch 3  global step 6660 loss 0.38123 batch 96/3281 lr 0.001 accuracy 93.17578 wps 18806.85 step time 0.36s\n","# Epoch 3  global step 6680 loss 0.38204 batch 116/3281 lr 0.001 accuracy 93.13672 wps 18840.15 step time 0.36s\n","# Epoch 3  global step 6700 loss 0.38868 batch 136/3281 lr 0.001 accuracy 92.90625 wps 19051.49 step time 0.38s\n","# Epoch 3  global step 6720 loss 0.37473 batch 156/3281 lr 0.001 accuracy 93.52344 wps 18400.70 step time 0.34s\n","# Epoch 3  global step 6740 loss 0.39081 batch 176/3281 lr 0.001 accuracy 92.99219 wps 19050.82 step time 0.41s\n","# Epoch 3  global step 6760 loss 0.38816 batch 196/3281 lr 0.001 accuracy 92.98828 wps 19151.64 step time 0.41s\n","# Epoch 3  global step 6780 loss 0.39836 batch 216/3281 lr 0.001 accuracy 92.91406 wps 19623.39 step time 0.39s\n","# Epoch 3  global step 6800 loss 0.39829 batch 236/3281 lr 0.001 accuracy 92.78906 wps 19931.00 step time 0.40s\n","# Epoch 3  global step 6820 loss 0.38209 batch 256/3281 lr 0.001 accuracy 93.18359 wps 19130.74 step time 0.37s\n","# Epoch 3  global step 6840 loss 0.37993 batch 276/3281 lr 0.001 accuracy 93.08594 wps 18818.95 step time 0.35s\n","# Epoch 3  global step 6860 loss 0.36155 batch 296/3281 lr 0.001 accuracy 93.32812 wps 18239.96 step time 0.35s\n","# Epoch 3  global step 6880 loss 0.37576 batch 316/3281 lr 0.001 accuracy 93.19531 wps 18695.90 step time 0.36s\n","# Epoch 3  global step 6900 loss 0.38434 batch 336/3281 lr 0.001 accuracy 93.32813 wps 17476.32 step time 0.37s\n","# Epoch 3  global step 6920 loss 0.37586 batch 356/3281 lr 0.001 accuracy 93.16797 wps 19050.47 step time 0.37s\n","# Epoch 3  global step 6940 loss 0.40991 batch 376/3281 lr 0.001 accuracy 92.45313 wps 19981.47 step time 0.40s\n","# Epoch 3  global step 6960 loss 0.39768 batch 396/3281 lr 0.001 accuracy 92.67969 wps 18655.41 step time 0.50s\n","# Epoch 3  global step 6980 loss 0.40887 batch 416/3281 lr 0.001 accuracy 92.39844 wps 19671.40 step time 0.45s\n","# Epoch 3  global step 7000 loss 0.36873 batch 436/3281 lr 0.001 accuracy 93.29688 wps 18224.89 step time 0.34s\n","# Epoch 3  global step 7020 loss 0.40824 batch 456/3281 lr 0.001 accuracy 92.59375 wps 19445.06 step time 0.39s\n","# Epoch 3  global step 7040 loss 0.40372 batch 476/3281 lr 0.001 accuracy 92.56250 wps 19434.88 step time 0.39s\n","# Epoch 3  global step 7060 loss 0.39408 batch 496/3281 lr 0.001 accuracy 92.82812 wps 19232.26 step time 0.42s\n","# Epoch 3  global step 7080 loss 0.39583 batch 516/3281 lr 0.001 accuracy 92.80469 wps 19655.02 step time 0.38s\n","# Epoch 3  global step 7100 loss 0.39104 batch 536/3281 lr 0.001 accuracy 92.85937 wps 18664.51 step time 0.39s\n","# Epoch 3  global step 7120 loss 0.39484 batch 556/3281 lr 0.001 accuracy 92.83594 wps 18766.61 step time 0.39s\n","# Epoch 3  global step 7140 loss 0.38255 batch 576/3281 lr 0.001 accuracy 93.03125 wps 18882.76 step time 0.36s\n","# Epoch 3  global step 7160 loss 0.39506 batch 596/3281 lr 0.001 accuracy 92.91797 wps 19015.17 step time 0.37s\n","# Epoch 3  global step 7180 loss 0.40009 batch 616/3281 lr 0.001 accuracy 92.50391 wps 19765.05 step time 0.40s\n","# Epoch 3  global step 7200 loss 0.35347 batch 636/3281 lr 0.001 accuracy 93.62891 wps 17293.18 step time 0.31s\n","# Epoch 3  global step 7220 loss 0.38854 batch 656/3281 lr 0.001 accuracy 92.80469 wps 19170.31 step time 0.38s\n","# Epoch 3  global step 7240 loss 0.40116 batch 676/3281 lr 0.001 accuracy 92.69141 wps 19565.34 step time 0.39s\n","# Epoch 3  global step 7260 loss 0.33825 batch 696/3281 lr 0.001 accuracy 93.91406 wps 17176.81 step time 0.32s\n","# Epoch 3  global step 7280 loss 0.37003 batch 716/3281 lr 0.001 accuracy 93.10937 wps 18732.64 step time 0.35s\n","# Epoch 3  global step 7300 loss 0.37867 batch 736/3281 lr 0.001 accuracy 92.93750 wps 18663.03 step time 0.40s\n","# Epoch 3  global step 7320 loss 0.38617 batch 756/3281 lr 0.001 accuracy 93.19922 wps 19056.58 step time 0.37s\n","# Epoch 3  global step 7340 loss 0.39753 batch 776/3281 lr 0.001 accuracy 92.89844 wps 18952.01 step time 0.41s\n","# Epoch 3  global step 7360 loss 0.39027 batch 796/3281 lr 0.001 accuracy 93.03125 wps 19807.00 step time 0.40s\n","# Epoch 3  global step 7380 loss 0.39910 batch 816/3281 lr 0.001 accuracy 92.88672 wps 19234.71 step time 0.37s\n","# Epoch 3  global step 7400 loss 0.40453 batch 836/3281 lr 0.001 accuracy 92.57812 wps 19007.19 step time 0.52s\n","# Epoch 3  global step 7420 loss 0.37984 batch 856/3281 lr 0.001 accuracy 93.10156 wps 18458.59 step time 0.39s\n","# Epoch 3  global step 7440 loss 0.37421 batch 876/3281 lr 0.001 accuracy 93.14844 wps 18478.36 step time 0.40s\n","# Epoch 3  global step 7460 loss 0.41597 batch 896/3281 lr 0.001 accuracy 92.20703 wps 19827.19 step time 0.49s\n","# Epoch 3  global step 7480 loss 0.39748 batch 916/3281 lr 0.001 accuracy 92.76172 wps 18960.02 step time 0.36s\n","# Epoch 3  global step 7500 loss 0.37738 batch 936/3281 lr 0.001 accuracy 93.26172 wps 17706.98 step time 0.33s\n","# Epoch 3  global step 7520 loss 0.39074 batch 956/3281 lr 0.001 accuracy 92.93359 wps 18600.33 step time 0.35s\n","# Epoch 3  global step 7540 loss 0.38786 batch 976/3281 lr 0.001 accuracy 92.79297 wps 18956.91 step time 0.41s\n","# Epoch 3  global step 7560 loss 0.38631 batch 996/3281 lr 0.001 accuracy 92.82031 wps 19634.66 step time 0.39s\n","# Epoch 3  global step 7580 loss 0.38738 batch 1016/3281 lr 0.001 accuracy 93.03125 wps 18218.12 step time 0.39s\n","# Epoch 3  global step 7600 loss 0.38455 batch 1036/3281 lr 0.001 accuracy 93.05859 wps 18966.75 step time 0.36s\n","# Epoch 3  global step 7620 loss 0.39309 batch 1056/3281 lr 0.001 accuracy 92.89453 wps 18830.16 step time 0.36s\n","# Epoch 3  global step 7640 loss 0.39071 batch 1076/3281 lr 0.001 accuracy 92.80859 wps 18624.70 step time 0.35s\n","# Epoch 3  global step 7660 loss 0.37719 batch 1096/3281 lr 0.001 accuracy 93.23437 wps 18236.47 step time 0.38s\n","# Epoch 3  global step 7680 loss 0.40441 batch 1116/3281 lr 0.001 accuracy 92.73437 wps 19649.76 step time 0.39s\n","# Epoch 3  global step 7700 loss 0.36278 batch 1136/3281 lr 0.001 accuracy 93.47266 wps 18101.90 step time 0.38s\n","# Epoch 3  global step 7720 loss 0.39215 batch 1156/3281 lr 0.001 accuracy 92.84766 wps 20159.19 step time 0.41s\n","# Epoch 3  global step 7740 loss 0.39672 batch 1176/3281 lr 0.001 accuracy 92.68750 wps 17277.24 step time 0.44s\n","# Epoch 3  global step 7760 loss 0.39053 batch 1196/3281 lr 0.001 accuracy 92.80859 wps 17340.62 step time 0.43s\n","# Epoch 3  global step 7780 loss 0.39839 batch 1216/3281 lr 0.001 accuracy 92.63672 wps 16696.29 step time 0.50s\n","# Epoch 3  global step 7800 loss 0.38266 batch 1236/3281 lr 0.001 accuracy 92.95703 wps 17824.36 step time 0.45s\n","# Epoch 3  global step 7820 loss 0.39489 batch 1256/3281 lr 0.001 accuracy 92.75781 wps 16230.79 step time 0.56s\n","# Epoch 3  global step 7840 loss 0.38332 batch 1276/3281 lr 0.001 accuracy 92.92578 wps 16493.74 step time 0.43s\n","# Epoch 3  global step 7860 loss 0.38395 batch 1296/3281 lr 0.001 accuracy 93.12500 wps 17141.93 step time 0.42s\n","# Epoch 3  global step 7880 loss 0.38825 batch 1316/3281 lr 0.001 accuracy 92.82422 wps 15731.80 step time 0.52s\n","# Epoch 3  global step 7900 loss 0.38282 batch 1336/3281 lr 0.001 accuracy 93.08984 wps 16657.16 step time 0.43s\n","# Epoch 3  global step 7920 loss 0.39492 batch 1356/3281 lr 0.001 accuracy 92.78125 wps 17336.70 step time 0.42s\n","# Epoch 3  global step 7940 loss 0.39777 batch 1376/3281 lr 0.001 accuracy 92.77344 wps 16730.50 step time 0.45s\n","# Epoch 3  global step 7960 loss 0.39478 batch 1396/3281 lr 0.001 accuracy 92.88672 wps 16814.68 step time 0.42s\n","# Epoch 3  global step 7980 loss 0.37925 batch 1416/3281 lr 0.001 accuracy 93.12109 wps 17263.68 step time 0.38s\n","# Epoch 3  global step 8000 loss 0.37841 batch 1436/3281 lr 0.001 accuracy 93.40234 wps 16406.97 step time 0.36s\n","# global step 8000, eval model at Fri May 22 11:11:45 2020\n","2020-05-22 11:11:48.272437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:11:48.273351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 11:11:48.273452: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 11:11:48.273502: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 11:11:48.273542: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 11:11:48.273591: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 11:11:48.273635: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 11:11:48.273697: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 11:11:48.273748: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 11:11:48.273921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:11:48.274525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:11:48.275116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 11:11:48.275207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 11:11:48.275228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 11:11:48.275256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 11:11:48.275433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:11:48.276066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:11:48.276608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.2652264389777673\n","# location_distance_from_business_district - 0.22422830617009526\n","# location_easy_to_find - 0.21735211034648969\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14689422357933526\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.35566007397550614\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.506017682210657\n","# others_willing_to_consume_again - 0.19215721261982022\n","# Eval loss 0.65651, f1 0.22799\n","# current result -0.22799318565094726, previous best result -0.2129585088269385\n","# Epoch 3  global step 8020 loss 0.39745 batch 1456/3281 lr 0.001 accuracy 92.68750 wps 16922.55 step time 0.45s\n","# Epoch 3  global step 8040 loss 0.40680 batch 1476/3281 lr 0.001 accuracy 92.60156 wps 16051.40 step time 0.56s\n","# Epoch 3  global step 8060 loss 0.36793 batch 1496/3281 lr 0.001 accuracy 93.42969 wps 16744.71 step time 0.36s\n","# Epoch 3  global step 8080 loss 0.36457 batch 1516/3281 lr 0.001 accuracy 93.52734 wps 17047.29 step time 0.35s\n","# Epoch 3  global step 8100 loss 0.37925 batch 1536/3281 lr 0.001 accuracy 93.22266 wps 16758.18 step time 0.37s\n","# Epoch 3  global step 8120 loss 0.38593 batch 1556/3281 lr 0.001 accuracy 93.11719 wps 16986.38 step time 0.45s\n","# Epoch 3  global step 8140 loss 0.36977 batch 1576/3281 lr 0.001 accuracy 93.34766 wps 16017.37 step time 0.41s\n","# Epoch 3  global step 8160 loss 0.38404 batch 1596/3281 lr 0.001 accuracy 92.93750 wps 17031.63 step time 0.49s\n","# Epoch 3  global step 8180 loss 0.37584 batch 1616/3281 lr 0.001 accuracy 93.14844 wps 16451.50 step time 0.39s\n","# Epoch 3  global step 8200 loss 0.38241 batch 1636/3281 lr 0.001 accuracy 93.15625 wps 17232.94 step time 0.38s\n","# Epoch 3  global step 8220 loss 0.37939 batch 1656/3281 lr 0.001 accuracy 93.30078 wps 16775.93 step time 0.39s\n","# Epoch 3  global step 8240 loss 0.38826 batch 1676/3281 lr 0.001 accuracy 93.07813 wps 17578.05 step time 0.43s\n","# Epoch 3  global step 8260 loss 0.39284 batch 1696/3281 lr 0.001 accuracy 92.91016 wps 16344.11 step time 0.40s\n","# Epoch 3  global step 8280 loss 0.39469 batch 1716/3281 lr 0.001 accuracy 92.85156 wps 17043.29 step time 0.46s\n","# Epoch 3  global step 8300 loss 0.40552 batch 1736/3281 lr 0.001 accuracy 92.54297 wps 17274.94 step time 0.48s\n","# Epoch 3  global step 8320 loss 0.37807 batch 1756/3281 lr 0.001 accuracy 93.20703 wps 16593.89 step time 0.41s\n","# Epoch 3  global step 8340 loss 0.38930 batch 1776/3281 lr 0.001 accuracy 92.76172 wps 17672.82 step time 0.42s\n","# Epoch 3  global step 8360 loss 0.37755 batch 1796/3281 lr 0.001 accuracy 93.05859 wps 16693.90 step time 0.46s\n","# Epoch 3  global step 8380 loss 0.37582 batch 1816/3281 lr 0.001 accuracy 92.99609 wps 17265.45 step time 0.40s\n","# Epoch 3  global step 8400 loss 0.36925 batch 1836/3281 lr 0.001 accuracy 93.08984 wps 15998.16 step time 0.51s\n","# Epoch 3  global step 8420 loss 0.39045 batch 1856/3281 lr 0.001 accuracy 92.86328 wps 16677.89 step time 0.44s\n","# Epoch 3  global step 8440 loss 0.37710 batch 1876/3281 lr 0.001 accuracy 93.09766 wps 16434.49 step time 0.39s\n","# Epoch 3  global step 8460 loss 0.38764 batch 1896/3281 lr 0.001 accuracy 93.10547 wps 16534.03 step time 0.45s\n","# Epoch 3  global step 8480 loss 0.37415 batch 1916/3281 lr 0.001 accuracy 93.34375 wps 17431.41 step time 0.38s\n","# Epoch 3  global step 8500 loss 0.40626 batch 1936/3281 lr 0.001 accuracy 92.86719 wps 16743.23 step time 0.48s\n","# Epoch 3  global step 8520 loss 0.37163 batch 1956/3281 lr 0.001 accuracy 93.48437 wps 16501.61 step time 0.37s\n","# Epoch 3  global step 8540 loss 0.41851 batch 1976/3281 lr 0.001 accuracy 92.05469 wps 17034.57 step time 0.55s\n","# Epoch 3  global step 8560 loss 0.37702 batch 1996/3281 lr 0.001 accuracy 93.24219 wps 16856.93 step time 0.37s\n","# Epoch 3  global step 8580 loss 0.35768 batch 2016/3281 lr 0.001 accuracy 93.50000 wps 16826.40 step time 0.42s\n","# Epoch 3  global step 8600 loss 0.37646 batch 2036/3281 lr 0.001 accuracy 93.16406 wps 16516.97 step time 0.39s\n","# Epoch 3  global step 8620 loss 0.37341 batch 2056/3281 lr 0.001 accuracy 93.44531 wps 16870.51 step time 0.38s\n","# Epoch 3  global step 8640 loss 0.38979 batch 2076/3281 lr 0.001 accuracy 92.98828 wps 17678.11 step time 0.45s\n","# Epoch 3  global step 8660 loss 0.37968 batch 2096/3281 lr 0.001 accuracy 93.27344 wps 16700.04 step time 0.45s\n","# Epoch 3  global step 8680 loss 0.38645 batch 2116/3281 lr 0.001 accuracy 92.98828 wps 17717.49 step time 0.42s\n","# Epoch 3  global step 8700 loss 0.38307 batch 2136/3281 lr 0.001 accuracy 93.10156 wps 17006.45 step time 0.39s\n","# Epoch 3  global step 8720 loss 0.39848 batch 2156/3281 lr 0.001 accuracy 92.75781 wps 16915.36 step time 0.43s\n","# Epoch 3  global step 8740 loss 0.39534 batch 2176/3281 lr 0.001 accuracy 92.78516 wps 15913.66 step time 0.52s\n","# Epoch 3  global step 8760 loss 0.35394 batch 2196/3281 lr 0.001 accuracy 93.46875 wps 16412.01 step time 0.38s\n","# Epoch 3  global step 8780 loss 0.38316 batch 2216/3281 lr 0.001 accuracy 92.83984 wps 16714.32 step time 0.38s\n","# Epoch 3  global step 8800 loss 0.36378 batch 2236/3281 lr 0.001 accuracy 93.39844 wps 16451.97 step time 0.42s\n","# Epoch 3  global step 8820 loss 0.38948 batch 2256/3281 lr 0.001 accuracy 92.96094 wps 16792.70 step time 0.48s\n","# Epoch 3  global step 8840 loss 0.38256 batch 2276/3281 lr 0.001 accuracy 93.11328 wps 16898.09 step time 0.47s\n","# Epoch 3  global step 8860 loss 0.37940 batch 2296/3281 lr 0.001 accuracy 93.01562 wps 17423.23 step time 0.39s\n","# Epoch 3  global step 8880 loss 0.38465 batch 2316/3281 lr 0.001 accuracy 93.04688 wps 17425.22 step time 0.44s\n","# Epoch 3  global step 8900 loss 0.36981 batch 2336/3281 lr 0.001 accuracy 93.23047 wps 17173.29 step time 0.47s\n","# Epoch 3  global step 8920 loss 0.39871 batch 2356/3281 lr 0.001 accuracy 92.65625 wps 16417.76 step time 0.47s\n","# Epoch 3  global step 8940 loss 0.39614 batch 2376/3281 lr 0.001 accuracy 92.82422 wps 16414.84 step time 0.44s\n","# Epoch 3  global step 8960 loss 0.41108 batch 2396/3281 lr 0.001 accuracy 92.63281 wps 17741.67 step time 0.44s\n","# Epoch 3  global step 8980 loss 0.38480 batch 2416/3281 lr 0.001 accuracy 92.97656 wps 16171.67 step time 0.43s\n","# Epoch 3  global step 9000 loss 0.40205 batch 2436/3281 lr 0.001 accuracy 92.83984 wps 17477.23 step time 0.43s\n","# Epoch 3  global step 9020 loss 0.37927 batch 2456/3281 lr 0.001 accuracy 93.17969 wps 16265.79 step time 0.46s\n","# Epoch 3  global step 9040 loss 0.39288 batch 2476/3281 lr 0.001 accuracy 92.84375 wps 17673.46 step time 0.39s\n","# Epoch 3  global step 9060 loss 0.38010 batch 2496/3281 lr 0.001 accuracy 93.19922 wps 16813.89 step time 0.38s\n","# Epoch 3  global step 9080 loss 0.38895 batch 2516/3281 lr 0.001 accuracy 93.02344 wps 17756.80 step time 0.41s\n","# Epoch 3  global step 9100 loss 0.38603 batch 2536/3281 lr 0.001 accuracy 93.21094 wps 16676.73 step time 0.46s\n","# Epoch 3  global step 9120 loss 0.37280 batch 2556/3281 lr 0.001 accuracy 93.30078 wps 17136.84 step time 0.40s\n","# Epoch 3  global step 9140 loss 0.38154 batch 2576/3281 lr 0.001 accuracy 93.01172 wps 16348.06 step time 0.44s\n","# Epoch 3  global step 9160 loss 0.36950 batch 2596/3281 lr 0.001 accuracy 93.41016 wps 16778.74 step time 0.39s\n","# Epoch 3  global step 9180 loss 0.38355 batch 2616/3281 lr 0.001 accuracy 92.94141 wps 16041.14 step time 0.48s\n","# Epoch 3  global step 9200 loss 0.37444 batch 2636/3281 lr 0.001 accuracy 93.23437 wps 16350.36 step time 0.41s\n","# Epoch 3  global step 9220 loss 0.38562 batch 2656/3281 lr 0.001 accuracy 93.06250 wps 16624.86 step time 0.35s\n","# Epoch 3  global step 9240 loss 0.36583 batch 2676/3281 lr 0.001 accuracy 93.40234 wps 16824.09 step time 0.37s\n","# Epoch 3  global step 9260 loss 0.38613 batch 2696/3281 lr 0.001 accuracy 92.96094 wps 16445.90 step time 0.58s\n","# Epoch 3  global step 9280 loss 0.37585 batch 2716/3281 lr 0.001 accuracy 93.18750 wps 15828.42 step time 0.50s\n","# Epoch 3  global step 9300 loss 0.37469 batch 2736/3281 lr 0.001 accuracy 93.33984 wps 16348.73 step time 0.38s\n","# Epoch 3  global step 9320 loss 0.38402 batch 2756/3281 lr 0.001 accuracy 93.03906 wps 16906.33 step time 0.51s\n","# Epoch 3  global step 9340 loss 0.39023 batch 2776/3281 lr 0.001 accuracy 93.03516 wps 17370.88 step time 0.41s\n","# Epoch 3  global step 9360 loss 0.37952 batch 2796/3281 lr 0.001 accuracy 93.26562 wps 17090.09 step time 0.39s\n","# Epoch 3  global step 9380 loss 0.36533 batch 2816/3281 lr 0.001 accuracy 93.55078 wps 16500.96 step time 0.40s\n","# Epoch 3  global step 9400 loss 0.37381 batch 2836/3281 lr 0.001 accuracy 93.14844 wps 17968.54 step time 0.40s\n","# Epoch 3  global step 9420 loss 0.38802 batch 2856/3281 lr 0.001 accuracy 92.87891 wps 18056.22 step time 0.45s\n","# Epoch 3  global step 9440 loss 0.37187 batch 2876/3281 lr 0.001 accuracy 93.19141 wps 16726.73 step time 0.42s\n","# Epoch 3  global step 9460 loss 0.37190 batch 2896/3281 lr 0.001 accuracy 93.35547 wps 17856.27 step time 0.37s\n","# Epoch 3  global step 9480 loss 0.35855 batch 2916/3281 lr 0.001 accuracy 93.55469 wps 16446.02 step time 0.38s\n","# Epoch 3  global step 9500 loss 0.37961 batch 2936/3281 lr 0.001 accuracy 93.11719 wps 16182.92 step time 0.46s\n","# Epoch 3  global step 9520 loss 0.37529 batch 2956/3281 lr 0.001 accuracy 93.11719 wps 16729.25 step time 0.46s\n","# Epoch 3  global step 9540 loss 0.38978 batch 2976/3281 lr 0.001 accuracy 92.87109 wps 17337.24 step time 0.42s\n","# Epoch 3  global step 9560 loss 0.38337 batch 2996/3281 lr 0.001 accuracy 93.07031 wps 16906.42 step time 0.38s\n","# Epoch 3  global step 9580 loss 0.38992 batch 3016/3281 lr 0.001 accuracy 92.79297 wps 17171.53 step time 0.39s\n","# Epoch 3  global step 9600 loss 0.39041 batch 3036/3281 lr 0.001 accuracy 92.95703 wps 16579.57 step time 0.53s\n","# Epoch 3  global step 9620 loss 0.40434 batch 3056/3281 lr 0.001 accuracy 92.59766 wps 17846.55 step time 0.43s\n","# Epoch 3  global step 9640 loss 0.38725 batch 3076/3281 lr 0.001 accuracy 92.80469 wps 17583.90 step time 0.46s\n","# Epoch 3  global step 9660 loss 0.37421 batch 3096/3281 lr 0.001 accuracy 93.22656 wps 16694.82 step time 0.38s\n","# Epoch 3  global step 9680 loss 0.36532 batch 3116/3281 lr 0.001 accuracy 93.41016 wps 16037.41 step time 0.35s\n","# Epoch 3  global step 9700 loss 0.38567 batch 3136/3281 lr 0.001 accuracy 93.06250 wps 17068.26 step time 0.41s\n","# Epoch 3  global step 9720 loss 0.37123 batch 3156/3281 lr 0.001 accuracy 93.42969 wps 16284.86 step time 0.44s\n","# Epoch 3  global step 9740 loss 0.38725 batch 3176/3281 lr 0.001 accuracy 93.17578 wps 17708.50 step time 0.41s\n","# Epoch 3  global step 9760 loss 0.38626 batch 3196/3281 lr 0.001 accuracy 93.14063 wps 16794.99 step time 0.43s\n","# Epoch 3  global step 9780 loss 0.36812 batch 3216/3281 lr 0.001 accuracy 93.49219 wps 16384.45 step time 0.36s\n","# Epoch 3  global step 9800 loss 0.39568 batch 3236/3281 lr 0.001 accuracy 92.83594 wps 17192.57 step time 0.42s\n","# Epoch 3  global step 9820 loss 0.38393 batch 3256/3281 lr 0.001 accuracy 92.82422 wps 17785.65 step time 0.40s\n","# Epoch 3  global step 9840 loss 0.36121 batch 3276/3281 lr 0.001 accuracy 93.63672 wps 16472.61 step time 0.36s\n","# Finsh epoch 3, global step 9846\n","# Epoch 4  global step 9860 loss 0.27312 batch 14/3281 lr 0.001 accuracy 64.78516 wps 19828.05 step time 0.33s\n","# Epoch 4  global step 9880 loss 0.38160 batch 34/3281 lr 0.001 accuracy 93.16406 wps 19434.62 step time 0.38s\n","# Epoch 4  global step 9900 loss 0.36480 batch 54/3281 lr 0.001 accuracy 93.23438 wps 19073.86 step time 0.43s\n","# Epoch 4  global step 9920 loss 0.36085 batch 74/3281 lr 0.001 accuracy 93.60547 wps 18730.23 step time 0.36s\n","# Epoch 4  global step 9940 loss 0.35620 batch 94/3281 lr 0.001 accuracy 93.46094 wps 19169.72 step time 0.37s\n","# Epoch 4  global step 9960 loss 0.37402 batch 114/3281 lr 0.001 accuracy 93.25391 wps 19264.60 step time 0.37s\n","# Epoch 4  global step 9980 loss 0.36380 batch 134/3281 lr 0.001 accuracy 93.43750 wps 18342.74 step time 0.35s\n","# Epoch 4  global step 10000 loss 0.37357 batch 154/3281 lr 0.001 accuracy 93.06250 wps 20091.20 step time 0.41s\n","# global step 10000, eval model at Fri May 22 11:26:59 2020\n","2020-05-22 11:27:01.647041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:27:01.647865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 11:27:01.648003: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 11:27:01.648051: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 11:27:01.648089: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 11:27:01.648140: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 11:27:01.648184: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 11:27:01.648231: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 11:27:01.648269: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 11:27:01.648407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:27:01.649042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:27:01.649564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 11:27:01.649722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 11:27:01.649749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 11:27:01.649765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 11:27:01.649972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:27:01.650750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:27:01.651399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.36513461718077067\n","# location_distance_from_business_district - 0.22900432568854395\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.2004286197179621\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.1667791970704892\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.1950764020592185\n","# environment_decoration - 0.17154965719430865\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.46665940523322585\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.5363484669591314\n","# others_willing_to_consume_again - 0.20208960572902973\n","# Eval loss 0.59252, f1 0.24370\n","# current result -0.24369929883752645, previous best result -0.22799318565094726\n","# Epoch 4  global step 10020 loss 0.35788 batch 174/3281 lr 0.001 accuracy 93.43359 wps 17827.97 step time 0.40s\n","# Epoch 4  global step 10040 loss 0.37759 batch 194/3281 lr 0.001 accuracy 92.99219 wps 19104.11 step time 0.42s\n","# Epoch 4  global step 10060 loss 0.37253 batch 214/3281 lr 0.001 accuracy 93.20703 wps 19309.91 step time 0.38s\n","# Epoch 4  global step 10080 loss 0.35109 batch 234/3281 lr 0.001 accuracy 93.48438 wps 19093.64 step time 0.37s\n","# Epoch 4  global step 10100 loss 0.37512 batch 254/3281 lr 0.001 accuracy 92.84375 wps 17402.80 step time 0.45s\n","# Epoch 4  global step 10120 loss 0.38618 batch 274/3281 lr 0.001 accuracy 92.98047 wps 17108.97 step time 0.44s\n","# Epoch 4  global step 10140 loss 0.36208 batch 294/3281 lr 0.001 accuracy 93.42578 wps 16727.86 step time 0.39s\n","# Epoch 4  global step 10160 loss 0.36334 batch 314/3281 lr 0.001 accuracy 93.41797 wps 16581.75 step time 0.39s\n","# Epoch 4  global step 10180 loss 0.39899 batch 334/3281 lr 0.001 accuracy 92.77734 wps 17896.60 step time 0.46s\n","# Epoch 4  global step 10200 loss 0.37772 batch 354/3281 lr 0.001 accuracy 93.17187 wps 17009.91 step time 0.42s\n","# Epoch 4  global step 10220 loss 0.36818 batch 374/3281 lr 0.001 accuracy 93.15234 wps 17200.67 step time 0.45s\n","# Epoch 4  global step 10240 loss 0.34860 batch 394/3281 lr 0.001 accuracy 93.77734 wps 16885.86 step time 0.37s\n","# Epoch 4  global step 10260 loss 0.38472 batch 414/3281 lr 0.001 accuracy 92.99219 wps 16755.70 step time 0.52s\n","# Epoch 4  global step 10280 loss 0.38169 batch 434/3281 lr 0.001 accuracy 93.06250 wps 17087.55 step time 0.44s\n","# Epoch 4  global step 10300 loss 0.37448 batch 454/3281 lr 0.001 accuracy 93.10937 wps 17048.39 step time 0.46s\n","# Epoch 4  global step 10320 loss 0.37945 batch 474/3281 lr 0.001 accuracy 93.13672 wps 17237.99 step time 0.42s\n","# Epoch 4  global step 10340 loss 0.37461 batch 494/3281 lr 0.001 accuracy 93.18359 wps 16667.80 step time 0.50s\n","# Epoch 4  global step 10360 loss 0.39147 batch 514/3281 lr 0.001 accuracy 92.70703 wps 17764.00 step time 0.50s\n","# Epoch 4  global step 10380 loss 0.39361 batch 534/3281 lr 0.001 accuracy 92.96094 wps 16692.59 step time 0.50s\n","# Epoch 4  global step 10400 loss 0.36568 batch 554/3281 lr 0.001 accuracy 93.38281 wps 16856.40 step time 0.38s\n","# Epoch 4  global step 10420 loss 0.38143 batch 574/3281 lr 0.001 accuracy 93.03906 wps 16373.59 step time 0.47s\n","# Epoch 4  global step 10440 loss 0.39002 batch 594/3281 lr 0.001 accuracy 92.83594 wps 17483.42 step time 0.40s\n","# Epoch 4  global step 10460 loss 0.35056 batch 614/3281 lr 0.001 accuracy 93.60937 wps 17383.79 step time 0.39s\n","# Epoch 4  global step 10480 loss 0.36865 batch 634/3281 lr 0.001 accuracy 93.29297 wps 16299.62 step time 0.42s\n","# Epoch 4  global step 10500 loss 0.36502 batch 654/3281 lr 0.001 accuracy 93.46094 wps 17065.74 step time 0.45s\n","# Epoch 4  global step 10520 loss 0.36192 batch 674/3281 lr 0.001 accuracy 93.30469 wps 17049.22 step time 0.42s\n","# Epoch 4  global step 10540 loss 0.39024 batch 694/3281 lr 0.001 accuracy 93.04687 wps 17644.26 step time 0.46s\n","# Epoch 4  global step 10560 loss 0.37699 batch 714/3281 lr 0.001 accuracy 93.17969 wps 17757.02 step time 0.39s\n","# Epoch 4  global step 10580 loss 0.38858 batch 734/3281 lr 0.001 accuracy 92.91797 wps 16963.17 step time 0.46s\n","# Epoch 4  global step 10600 loss 0.35107 batch 754/3281 lr 0.001 accuracy 93.68359 wps 16037.53 step time 0.37s\n","# Epoch 4  global step 10620 loss 0.34938 batch 774/3281 lr 0.001 accuracy 93.69141 wps 16520.37 step time 0.38s\n","# Epoch 4  global step 10640 loss 0.35155 batch 794/3281 lr 0.001 accuracy 93.51562 wps 16030.30 step time 0.35s\n","# Epoch 4  global step 10660 loss 0.33657 batch 814/3281 lr 0.001 accuracy 93.92578 wps 15989.32 step time 0.35s\n","# Epoch 4  global step 10680 loss 0.37502 batch 834/3281 lr 0.001 accuracy 93.02344 wps 17060.82 step time 0.42s\n","# Epoch 4  global step 10700 loss 0.37311 batch 854/3281 lr 0.001 accuracy 93.29297 wps 16387.40 step time 0.47s\n","# Epoch 4  global step 10720 loss 0.36297 batch 874/3281 lr 0.001 accuracy 93.46094 wps 16538.23 step time 0.43s\n","# Epoch 4  global step 10740 loss 0.36020 batch 894/3281 lr 0.001 accuracy 93.50391 wps 16675.60 step time 0.37s\n","# Epoch 4  global step 10760 loss 0.35820 batch 914/3281 lr 0.001 accuracy 93.50000 wps 16395.11 step time 0.36s\n","# Epoch 4  global step 10780 loss 0.40091 batch 934/3281 lr 0.001 accuracy 92.60938 wps 16407.66 step time 0.50s\n","# Epoch 4  global step 10800 loss 0.36136 batch 954/3281 lr 0.001 accuracy 93.50391 wps 17045.85 step time 0.40s\n","# Epoch 4  global step 10820 loss 0.39503 batch 974/3281 lr 0.001 accuracy 92.75391 wps 16378.58 step time 0.54s\n","# Epoch 4  global step 10840 loss 0.37670 batch 994/3281 lr 0.001 accuracy 93.10547 wps 17005.72 step time 0.46s\n","# Epoch 4  global step 10860 loss 0.36609 batch 1014/3281 lr 0.001 accuracy 93.35547 wps 16931.01 step time 0.47s\n","# Epoch 4  global step 10880 loss 0.37184 batch 1034/3281 lr 0.001 accuracy 93.10938 wps 16140.85 step time 0.43s\n","# Epoch 4  global step 10900 loss 0.38491 batch 1054/3281 lr 0.001 accuracy 93.08984 wps 16466.34 step time 0.52s\n","# Epoch 4  global step 10920 loss 0.37934 batch 1074/3281 lr 0.001 accuracy 93.05078 wps 17521.62 step time 0.41s\n","# Epoch 4  global step 10940 loss 0.37810 batch 1094/3281 lr 0.001 accuracy 92.96094 wps 17777.87 step time 0.45s\n","# Epoch 4  global step 10960 loss 0.34871 batch 1114/3281 lr 0.001 accuracy 93.74609 wps 16391.43 step time 0.36s\n","# Epoch 4  global step 10980 loss 0.38925 batch 1134/3281 lr 0.001 accuracy 92.89453 wps 17302.62 step time 0.43s\n","# Epoch 4  global step 11000 loss 0.41199 batch 1154/3281 lr 0.001 accuracy 92.37891 wps 18277.59 step time 0.53s\n","# Epoch 4  global step 11020 loss 0.35456 batch 1174/3281 lr 0.001 accuracy 93.62500 wps 16385.09 step time 0.40s\n","# Epoch 4  global step 11040 loss 0.37839 batch 1194/3281 lr 0.001 accuracy 93.16016 wps 15861.59 step time 0.52s\n","# Epoch 4  global step 11060 loss 0.36607 batch 1214/3281 lr 0.001 accuracy 93.53906 wps 16898.55 step time 0.45s\n","# Epoch 4  global step 11080 loss 0.36185 batch 1234/3281 lr 0.001 accuracy 93.47656 wps 16826.57 step time 0.39s\n","# Epoch 4  global step 11100 loss 0.36583 batch 1254/3281 lr 0.001 accuracy 93.51953 wps 16109.28 step time 0.45s\n","# Epoch 4  global step 11120 loss 0.36945 batch 1274/3281 lr 0.001 accuracy 93.17187 wps 17085.29 step time 0.43s\n","# Epoch 4  global step 11140 loss 0.35664 batch 1294/3281 lr 0.001 accuracy 93.52344 wps 15783.62 step time 0.42s\n","# Epoch 4  global step 11160 loss 0.39915 batch 1314/3281 lr 0.001 accuracy 92.66016 wps 17566.55 step time 0.47s\n","# Epoch 4  global step 11180 loss 0.33489 batch 1334/3281 lr 0.001 accuracy 93.89453 wps 16261.51 step time 0.35s\n","# Epoch 4  global step 11200 loss 0.38698 batch 1354/3281 lr 0.001 accuracy 93.04297 wps 17235.17 step time 0.42s\n","# Epoch 4  global step 11220 loss 0.37364 batch 1374/3281 lr 0.001 accuracy 93.28516 wps 16990.21 step time 0.46s\n","# Epoch 4  global step 11240 loss 0.38142 batch 1394/3281 lr 0.001 accuracy 93.09766 wps 16460.82 step time 0.43s\n","# Epoch 4  global step 11260 loss 0.38472 batch 1414/3281 lr 0.001 accuracy 92.99609 wps 17242.33 step time 0.44s\n","# Epoch 4  global step 11280 loss 0.35885 batch 1434/3281 lr 0.001 accuracy 93.43359 wps 16322.59 step time 0.38s\n","# Epoch 4  global step 11300 loss 0.37299 batch 1454/3281 lr 0.001 accuracy 93.19141 wps 17423.02 step time 0.40s\n","# Epoch 4  global step 11320 loss 0.36009 batch 1474/3281 lr 0.001 accuracy 93.49609 wps 16155.93 step time 0.45s\n","# Epoch 4  global step 11340 loss 0.37298 batch 1494/3281 lr 0.001 accuracy 93.32813 wps 16523.95 step time 0.45s\n","# Epoch 4  global step 11360 loss 0.35432 batch 1514/3281 lr 0.001 accuracy 93.51953 wps 17629.56 step time 0.41s\n","# Epoch 4  global step 11380 loss 0.36252 batch 1534/3281 lr 0.001 accuracy 93.43359 wps 16899.12 step time 0.37s\n","# Epoch 4  global step 11400 loss 0.36361 batch 1554/3281 lr 0.001 accuracy 93.26953 wps 16619.53 step time 0.39s\n","# Epoch 4  global step 11420 loss 0.34960 batch 1574/3281 lr 0.001 accuracy 93.76562 wps 15903.68 step time 0.38s\n","# Epoch 4  global step 11440 loss 0.38432 batch 1594/3281 lr 0.001 accuracy 93.07812 wps 17211.20 step time 0.49s\n","# Epoch 4  global step 11460 loss 0.36898 batch 1614/3281 lr 0.001 accuracy 93.38672 wps 15641.74 step time 0.48s\n","# Epoch 4  global step 11480 loss 0.36293 batch 1634/3281 lr 0.001 accuracy 93.43750 wps 17076.64 step time 0.38s\n","# Epoch 4  global step 11500 loss 0.34986 batch 1654/3281 lr 0.001 accuracy 93.61328 wps 16590.85 step time 0.40s\n","# Epoch 4  global step 11520 loss 0.38360 batch 1674/3281 lr 0.001 accuracy 93.04688 wps 16815.84 step time 0.53s\n","# Epoch 4  global step 11540 loss 0.36846 batch 1694/3281 lr 0.001 accuracy 93.20312 wps 16634.37 step time 0.42s\n","# Epoch 4  global step 11560 loss 0.37888 batch 1714/3281 lr 0.001 accuracy 92.98438 wps 16195.94 step time 0.49s\n","# Epoch 4  global step 11580 loss 0.38201 batch 1734/3281 lr 0.001 accuracy 93.15234 wps 16638.96 step time 0.44s\n","# Epoch 4  global step 11600 loss 0.34995 batch 1754/3281 lr 0.001 accuracy 93.50391 wps 17486.16 step time 0.44s\n","# Epoch 4  global step 11620 loss 0.38357 batch 1774/3281 lr 0.001 accuracy 92.90234 wps 16722.43 step time 0.46s\n","# Epoch 4  global step 11640 loss 0.38337 batch 1794/3281 lr 0.001 accuracy 92.98047 wps 17210.91 step time 0.49s\n","# Epoch 4  global step 11660 loss 0.34676 batch 1814/3281 lr 0.001 accuracy 93.59766 wps 16827.90 step time 0.37s\n","# Epoch 4  global step 11680 loss 0.36964 batch 1834/3281 lr 0.001 accuracy 93.33203 wps 16740.91 step time 0.39s\n","# Epoch 4  global step 11700 loss 0.35220 batch 1854/3281 lr 0.001 accuracy 93.62891 wps 15908.13 step time 0.41s\n","# Epoch 4  global step 11720 loss 0.37436 batch 1874/3281 lr 0.001 accuracy 93.38281 wps 16652.32 step time 0.41s\n","# Epoch 4  global step 11740 loss 0.35229 batch 1894/3281 lr 0.001 accuracy 93.66016 wps 16278.12 step time 0.34s\n","# Epoch 4  global step 11760 loss 0.38126 batch 1914/3281 lr 0.001 accuracy 92.92578 wps 16467.83 step time 0.37s\n","# Epoch 4  global step 11780 loss 0.39199 batch 1934/3281 lr 0.001 accuracy 92.86328 wps 15849.03 step time 0.42s\n","# Epoch 4  global step 11800 loss 0.40449 batch 1954/3281 lr 0.001 accuracy 92.70703 wps 16701.66 step time 0.52s\n","# Epoch 4  global step 11820 loss 0.35294 batch 1974/3281 lr 0.001 accuracy 93.57422 wps 17156.35 step time 0.41s\n","# Epoch 4  global step 11840 loss 0.37101 batch 1994/3281 lr 0.001 accuracy 93.29687 wps 16416.39 step time 0.43s\n","# Epoch 4  global step 11860 loss 0.37524 batch 2014/3281 lr 0.001 accuracy 93.17188 wps 16033.28 step time 0.53s\n","# Epoch 4  global step 11880 loss 0.35583 batch 2034/3281 lr 0.001 accuracy 93.48047 wps 16175.67 step time 0.38s\n","# Epoch 4  global step 11900 loss 0.36786 batch 2054/3281 lr 0.001 accuracy 93.24609 wps 16776.23 step time 0.40s\n","# Epoch 4  global step 11920 loss 0.36002 batch 2074/3281 lr 0.001 accuracy 93.57813 wps 15981.84 step time 0.42s\n","# Epoch 4  global step 11940 loss 0.37127 batch 2094/3281 lr 0.001 accuracy 93.17578 wps 15795.06 step time 0.51s\n","# Epoch 4  global step 11960 loss 0.35701 batch 2114/3281 lr 0.001 accuracy 93.46875 wps 16787.17 step time 0.38s\n","# Epoch 4  global step 11980 loss 0.35355 batch 2134/3281 lr 0.001 accuracy 93.57812 wps 16799.15 step time 0.37s\n","# Epoch 4  global step 12000 loss 0.38703 batch 2154/3281 lr 0.001 accuracy 92.80078 wps 16386.85 step time 0.56s\n","# global step 12000, eval model at Fri May 22 11:42:29 2020\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","2020-05-22 11:42:32.152097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:42:32.152966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 11:42:32.153115: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 11:42:32.153165: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 11:42:32.153215: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 11:42:32.153258: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 11:42:32.153299: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 11:42:32.153350: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 11:42:32.153399: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 11:42:32.153536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:42:32.154191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:42:32.154753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 11:42:32.154863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 11:42:32.154884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 11:42:32.154899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 11:42:32.155093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:42:32.155736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:42:32.156289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.4612730081322429\n","# location_distance_from_business_district - 0.27305352150544615\n","# location_easy_to_find - 0.2584283767211186\n","# service_wait_time - 0.23547230972301517\n","# service_waiters_attitude - 0.340877841436544\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.23089813543398136\n","# price_level - 0.1885261759532353\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.28804395440765507\n","# environment_decoration - 0.2250314234084585\n","# environment_noise - 0.2064403728227191\n","# environment_space - 0.20054162269556006\n","# environment_cleaness - 0.20507025160105716\n","# dish_portion - 0.17685683351626239\n","# dish_taste - 0.5547814787682934\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.5513064462104474\n","# others_willing_to_consume_again - 0.3138749961158186\n","# Eval loss 0.50950, f1 0.28002\n","# current result -0.2800188777597481, previous best result -0.24369929883752645\n","# Epoch 4  global step 12020 loss 0.36186 batch 2174/3281 lr 0.001 accuracy 93.32422 wps 16627.03 step time 0.40s\n","# Epoch 4  global step 12040 loss 0.37777 batch 2194/3281 lr 0.001 accuracy 92.96875 wps 16787.72 step time 0.47s\n","# Epoch 4  global step 12060 loss 0.36121 batch 2214/3281 lr 0.001 accuracy 93.39844 wps 16856.47 step time 0.41s\n","# Epoch 4  global step 12080 loss 0.35015 batch 2234/3281 lr 0.001 accuracy 93.76563 wps 16323.02 step time 0.37s\n","# Epoch 4  global step 12100 loss 0.34599 batch 2254/3281 lr 0.001 accuracy 93.82813 wps 16640.38 step time 0.37s\n","# Epoch 4  global step 12120 loss 0.36485 batch 2274/3281 lr 0.001 accuracy 93.52344 wps 16697.66 step time 0.41s\n","# Epoch 4  global step 12140 loss 0.37924 batch 2294/3281 lr 0.001 accuracy 93.14453 wps 16618.72 step time 0.38s\n","# Epoch 4  global step 12160 loss 0.40432 batch 2314/3281 lr 0.001 accuracy 92.57813 wps 16036.62 step time 0.61s\n","# Epoch 4  global step 12180 loss 0.37654 batch 2334/3281 lr 0.001 accuracy 93.19141 wps 16299.44 step time 0.41s\n","# Epoch 4  global step 12200 loss 0.35461 batch 2354/3281 lr 0.001 accuracy 93.66797 wps 16542.26 step time 0.38s\n","# Epoch 4  global step 12220 loss 0.37048 batch 2374/3281 lr 0.001 accuracy 93.37891 wps 16641.37 step time 0.42s\n","# Epoch 4  global step 12240 loss 0.37045 batch 2394/3281 lr 0.001 accuracy 93.46875 wps 16673.83 step time 0.44s\n","# Epoch 4  global step 12260 loss 0.37552 batch 2414/3281 lr 0.001 accuracy 93.23437 wps 16526.57 step time 0.39s\n","# Epoch 4  global step 12280 loss 0.37517 batch 2434/3281 lr 0.001 accuracy 93.28906 wps 17106.48 step time 0.44s\n","# Epoch 4  global step 12300 loss 0.36343 batch 2454/3281 lr 0.001 accuracy 93.33594 wps 17183.15 step time 0.45s\n","# Epoch 4  global step 12320 loss 0.37137 batch 2474/3281 lr 0.001 accuracy 93.41016 wps 16253.98 step time 0.38s\n","# Epoch 4  global step 12340 loss 0.37971 batch 2494/3281 lr 0.001 accuracy 92.97266 wps 16611.56 step time 0.48s\n","# Epoch 4  global step 12360 loss 0.34895 batch 2514/3281 lr 0.001 accuracy 93.56250 wps 17053.36 step time 0.37s\n","# Epoch 4  global step 12380 loss 0.35964 batch 2534/3281 lr 0.001 accuracy 93.33984 wps 16874.99 step time 0.40s\n","# Epoch 4  global step 12400 loss 0.38394 batch 2554/3281 lr 0.001 accuracy 92.95313 wps 16176.81 step time 0.47s\n","# Epoch 4  global step 12420 loss 0.35977 batch 2574/3281 lr 0.001 accuracy 93.43359 wps 16191.48 step time 0.44s\n","# Epoch 4  global step 12440 loss 0.37872 batch 2594/3281 lr 0.001 accuracy 93.10937 wps 17037.33 step time 0.47s\n","# Epoch 4  global step 12460 loss 0.36989 batch 2614/3281 lr 0.001 accuracy 93.37891 wps 16991.09 step time 0.38s\n","# Epoch 4  global step 12480 loss 0.34375 batch 2634/3281 lr 0.001 accuracy 93.75000 wps 16890.84 step time 0.39s\n","# Epoch 4  global step 12500 loss 0.36713 batch 2654/3281 lr 0.001 accuracy 93.39844 wps 16924.59 step time 0.39s\n","# Epoch 4  global step 12520 loss 0.37783 batch 2674/3281 lr 0.001 accuracy 93.24609 wps 16788.85 step time 0.40s\n","# Epoch 4  global step 12540 loss 0.37867 batch 2694/3281 lr 0.001 accuracy 93.12891 wps 16716.50 step time 0.45s\n","# Epoch 4  global step 12560 loss 0.37922 batch 2714/3281 lr 0.001 accuracy 93.11719 wps 17605.70 step time 0.40s\n","# Epoch 4  global step 12580 loss 0.37619 batch 2734/3281 lr 0.001 accuracy 93.28906 wps 16156.20 step time 0.43s\n","# Epoch 4  global step 12600 loss 0.37519 batch 2754/3281 lr 0.001 accuracy 93.30859 wps 15534.31 step time 0.42s\n","# Epoch 4  global step 12620 loss 0.37883 batch 2774/3281 lr 0.001 accuracy 93.16406 wps 17009.61 step time 0.40s\n","# Epoch 4  global step 12640 loss 0.35000 batch 2794/3281 lr 0.001 accuracy 93.55078 wps 16626.47 step time 0.38s\n","# Epoch 4  global step 12660 loss 0.37539 batch 2814/3281 lr 0.001 accuracy 93.02344 wps 16453.47 step time 0.48s\n","# Epoch 4  global step 12680 loss 0.35644 batch 2834/3281 lr 0.001 accuracy 93.61328 wps 16210.77 step time 0.35s\n","# Epoch 4  global step 12700 loss 0.35503 batch 2854/3281 lr 0.001 accuracy 93.57422 wps 16575.33 step time 0.39s\n","# Epoch 4  global step 12720 loss 0.37584 batch 2874/3281 lr 0.001 accuracy 93.12500 wps 17213.08 step time 0.39s\n","# Epoch 4  global step 12740 loss 0.35108 batch 2894/3281 lr 0.001 accuracy 93.68750 wps 16333.13 step time 0.37s\n","# Epoch 4  global step 12760 loss 0.36497 batch 2914/3281 lr 0.001 accuracy 93.21094 wps 16610.43 step time 0.43s\n","# Epoch 4  global step 12780 loss 0.38411 batch 2934/3281 lr 0.001 accuracy 92.98047 wps 16849.02 step time 0.51s\n","# Epoch 4  global step 12800 loss 0.38699 batch 2954/3281 lr 0.001 accuracy 93.10156 wps 17810.33 step time 0.43s\n","# Epoch 4  global step 12820 loss 0.37656 batch 2974/3281 lr 0.001 accuracy 93.12109 wps 16090.40 step time 0.48s\n","# Epoch 4  global step 12840 loss 0.38257 batch 2994/3281 lr 0.001 accuracy 93.19922 wps 16508.56 step time 0.40s\n","# Epoch 4  global step 12860 loss 0.39311 batch 3014/3281 lr 0.001 accuracy 92.71484 wps 17998.66 step time 0.48s\n","# Epoch 4  global step 12880 loss 0.37122 batch 3034/3281 lr 0.001 accuracy 93.25391 wps 16474.98 step time 0.48s\n","# Epoch 4  global step 12900 loss 0.38757 batch 3054/3281 lr 0.001 accuracy 92.96094 wps 17066.24 step time 0.47s\n","# Epoch 4  global step 12920 loss 0.35732 batch 3074/3281 lr 0.001 accuracy 93.57031 wps 17098.65 step time 0.38s\n","# Epoch 4  global step 12940 loss 0.39097 batch 3094/3281 lr 0.001 accuracy 92.75000 wps 16877.83 step time 0.49s\n","# Epoch 4  global step 12960 loss 0.35858 batch 3114/3281 lr 0.001 accuracy 93.56250 wps 16239.61 step time 0.40s\n","# Epoch 4  global step 12980 loss 0.38677 batch 3134/3281 lr 0.001 accuracy 93.02344 wps 16230.18 step time 0.42s\n","# Epoch 4  global step 13000 loss 0.40301 batch 3154/3281 lr 0.001 accuracy 92.63281 wps 16807.74 step time 0.56s\n","# Epoch 4  global step 13020 loss 0.38036 batch 3174/3281 lr 0.001 accuracy 93.14063 wps 16977.59 step time 0.50s\n","# Epoch 4  global step 13040 loss 0.38291 batch 3194/3281 lr 0.001 accuracy 93.00781 wps 17438.78 step time 0.45s\n","# Epoch 4  global step 13060 loss 0.38654 batch 3214/3281 lr 0.001 accuracy 92.85547 wps 17035.40 step time 0.57s\n","# Epoch 4  global step 13080 loss 0.38244 batch 3234/3281 lr 0.001 accuracy 93.06641 wps 16721.12 step time 0.49s\n","# Epoch 4  global step 13100 loss 0.36785 batch 3254/3281 lr 0.001 accuracy 93.48438 wps 17112.62 step time 0.39s\n","# Epoch 4  global step 13120 loss 0.38475 batch 3274/3281 lr 0.001 accuracy 92.98438 wps 16641.71 step time 0.43s\n","# Finsh epoch 4, global step 13128\n","# Epoch 5  global step 13140 loss 0.21899 batch 12/3281 lr 0.001 accuracy 55.93359 wps 19403.77 step time 0.27s\n","# Epoch 5  global step 13160 loss 0.34021 batch 32/3281 lr 0.001 accuracy 93.74219 wps 18590.43 step time 0.35s\n","# Epoch 5  global step 13180 loss 0.35633 batch 52/3281 lr 0.001 accuracy 93.33984 wps 19817.82 step time 0.40s\n","# Epoch 5  global step 13200 loss 0.35916 batch 72/3281 lr 0.001 accuracy 93.51562 wps 19494.83 step time 0.40s\n","# Epoch 5  global step 13220 loss 0.36036 batch 92/3281 lr 0.001 accuracy 93.44141 wps 19669.42 step time 0.40s\n","# Epoch 5  global step 13240 loss 0.35013 batch 112/3281 lr 0.001 accuracy 93.61328 wps 18510.91 step time 0.36s\n","# Epoch 5  global step 13260 loss 0.37110 batch 132/3281 lr 0.001 accuracy 93.36328 wps 18636.60 step time 0.40s\n","# Epoch 5  global step 13280 loss 0.34137 batch 152/3281 lr 0.001 accuracy 93.85938 wps 19320.25 step time 0.38s\n","# Epoch 5  global step 13300 loss 0.35717 batch 172/3281 lr 0.001 accuracy 93.51563 wps 18475.30 step time 0.35s\n","# Epoch 5  global step 13320 loss 0.35847 batch 192/3281 lr 0.001 accuracy 93.57422 wps 18787.47 step time 0.40s\n","# Epoch 5  global step 13340 loss 0.37470 batch 212/3281 lr 0.001 accuracy 93.22266 wps 19534.88 step time 0.38s\n","# Epoch 5  global step 13360 loss 0.35481 batch 232/3281 lr 0.001 accuracy 93.46875 wps 18141.23 step time 0.34s\n","# Epoch 5  global step 13380 loss 0.36277 batch 252/3281 lr 0.001 accuracy 93.20312 wps 19167.06 step time 0.37s\n","# Epoch 5  global step 13400 loss 0.35179 batch 272/3281 lr 0.001 accuracy 93.74219 wps 18344.91 step time 0.35s\n","# Epoch 5  global step 13420 loss 0.36694 batch 292/3281 lr 0.001 accuracy 93.39453 wps 18819.00 step time 0.41s\n","# Epoch 5  global step 13440 loss 0.36065 batch 312/3281 lr 0.001 accuracy 93.33594 wps 19340.01 step time 0.38s\n","# Epoch 5  global step 13460 loss 0.34766 batch 332/3281 lr 0.001 accuracy 93.69141 wps 18365.70 step time 0.34s\n","# Epoch 5  global step 13480 loss 0.35658 batch 352/3281 lr 0.001 accuracy 93.48828 wps 19515.28 step time 0.39s\n","# Epoch 5  global step 13500 loss 0.36117 batch 372/3281 lr 0.001 accuracy 93.28906 wps 18905.09 step time 0.37s\n","# Epoch 5  global step 13520 loss 0.36599 batch 392/3281 lr 0.001 accuracy 93.27734 wps 19268.65 step time 0.38s\n","# Epoch 5  global step 13540 loss 0.37119 batch 412/3281 lr 0.001 accuracy 93.09375 wps 18550.79 step time 0.40s\n","# Epoch 5  global step 13560 loss 0.36426 batch 432/3281 lr 0.001 accuracy 93.48437 wps 18667.49 step time 0.35s\n","# Epoch 5  global step 13580 loss 0.35409 batch 452/3281 lr 0.001 accuracy 93.68359 wps 17678.03 step time 0.33s\n","# Epoch 5  global step 13600 loss 0.37083 batch 472/3281 lr 0.001 accuracy 93.32031 wps 19261.49 step time 0.37s\n","# Epoch 5  global step 13620 loss 0.34796 batch 492/3281 lr 0.001 accuracy 93.66016 wps 19123.18 step time 0.38s\n","# Epoch 5  global step 13640 loss 0.37407 batch 512/3281 lr 0.001 accuracy 93.21094 wps 18983.29 step time 0.42s\n","# Epoch 5  global step 13660 loss 0.35096 batch 532/3281 lr 0.001 accuracy 93.64453 wps 18544.95 step time 0.35s\n","# Epoch 5  global step 13680 loss 0.34356 batch 552/3281 lr 0.001 accuracy 93.67187 wps 18710.52 step time 0.36s\n","# Epoch 5  global step 13700 loss 0.33586 batch 572/3281 lr 0.001 accuracy 93.82031 wps 18017.86 step time 0.33s\n","# Epoch 5  global step 13720 loss 0.34375 batch 592/3281 lr 0.001 accuracy 93.95313 wps 17592.47 step time 0.38s\n","# Epoch 5  global step 13740 loss 0.36532 batch 612/3281 lr 0.001 accuracy 93.36328 wps 19022.59 step time 0.36s\n","# Epoch 5  global step 13760 loss 0.34519 batch 632/3281 lr 0.001 accuracy 93.71484 wps 17763.31 step time 0.33s\n","# Epoch 5  global step 13780 loss 0.35703 batch 652/3281 lr 0.001 accuracy 93.50000 wps 18470.01 step time 0.40s\n","# Epoch 5  global step 13800 loss 0.35640 batch 672/3281 lr 0.001 accuracy 93.50781 wps 19214.87 step time 0.38s\n","# Epoch 5  global step 13820 loss 0.33690 batch 692/3281 lr 0.001 accuracy 93.98828 wps 17555.66 step time 0.32s\n","# Epoch 5  global step 13840 loss 0.34545 batch 712/3281 lr 0.001 accuracy 93.71875 wps 19171.31 step time 0.41s\n","# Epoch 5  global step 13860 loss 0.36658 batch 732/3281 lr 0.001 accuracy 93.48828 wps 18845.51 step time 0.42s\n","# Epoch 5  global step 13880 loss 0.38193 batch 752/3281 lr 0.001 accuracy 92.91406 wps 19008.08 step time 0.36s\n","# Epoch 5  global step 13900 loss 0.36412 batch 772/3281 lr 0.001 accuracy 93.15234 wps 20036.69 step time 0.40s\n","# Epoch 5  global step 13920 loss 0.35237 batch 792/3281 lr 0.001 accuracy 93.48047 wps 18810.03 step time 0.36s\n","# Epoch 5  global step 13940 loss 0.32728 batch 812/3281 lr 0.001 accuracy 94.05469 wps 17235.82 step time 0.37s\n","# Epoch 5  global step 13960 loss 0.36426 batch 832/3281 lr 0.001 accuracy 93.41016 wps 19096.54 step time 0.37s\n","# Epoch 5  global step 13980 loss 0.36974 batch 852/3281 lr 0.001 accuracy 93.22656 wps 19503.48 step time 0.39s\n","# Epoch 5  global step 14000 loss 0.36783 batch 872/3281 lr 0.001 accuracy 93.17578 wps 18648.75 step time 0.45s\n","# global step 14000, eval model at Fri May 22 11:57:13 2020\n","2020-05-22 11:57:15.986154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:57:15.986973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 11:57:15.987069: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 11:57:15.987125: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 11:57:15.987183: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 11:57:15.987246: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 11:57:15.987286: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 11:57:15.987328: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 11:57:15.987369: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 11:57:15.987503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:57:15.988145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:57:15.988698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 11:57:15.988828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 11:57:15.988851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 11:57:15.988866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 11:57:15.989041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:57:15.989686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 11:57:15.990255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.5587469531807279\n","# location_distance_from_business_district - 0.3569082642782937\n","# location_easy_to_find - 0.4934480270961231\n","# service_wait_time - 0.32606544998429005\n","# service_waiters_attitude - 0.6099025351717131\n","# service_parking_convenience - 0.3129810450520154\n","# service_serving_speed - 0.3514872628169236\n","# price_level - 0.4234742670332486\n","# price_cost_effective - 0.32594046683845157\n","# price_discount - 0.44261130287908856\n","# environment_decoration - 0.40988426145422174\n","# environment_noise - 0.33677721223184437\n","# environment_space - 0.41466731046041183\n","# environment_cleaness - 0.4106425430852723\n","# dish_portion - 0.27392537612118795\n","# dish_taste - 0.6275355816757112\n","# dish_look - 0.21686259832574303\n","# dish_recommendation - 0.2264438863929037\n","# others_overall_experience - 0.5638042869505338\n","# others_willing_to_consume_again - 0.4962236127054693\n","# Eval loss 0.41907, f1 0.40892\n","# current result -0.40891661218670866, previous best result -0.2800188777597481\n","# Epoch 5  global step 14020 loss 0.35845 batch 892/3281 lr 0.001 accuracy 93.38281 wps 18697.56 step time 0.36s\n","# Epoch 5  global step 14040 loss 0.37193 batch 912/3281 lr 0.001 accuracy 93.32813 wps 17830.62 step time 0.42s\n","# Epoch 5  global step 14060 loss 0.36895 batch 932/3281 lr 0.001 accuracy 93.14844 wps 18824.91 step time 0.36s\n","# Epoch 5  global step 14080 loss 0.36672 batch 952/3281 lr 0.001 accuracy 93.12500 wps 19592.09 step time 0.39s\n","# Epoch 5  global step 14100 loss 0.37054 batch 972/3281 lr 0.001 accuracy 93.20703 wps 20234.72 step time 0.41s\n","# Epoch 5  global step 14120 loss 0.37634 batch 992/3281 lr 0.001 accuracy 93.29297 wps 18574.47 step time 0.40s\n","# Epoch 5  global step 14140 loss 0.36130 batch 1012/3281 lr 0.001 accuracy 93.32031 wps 19726.03 step time 0.39s\n","# Epoch 5  global step 14160 loss 0.38401 batch 1032/3281 lr 0.001 accuracy 93.21094 wps 19006.50 step time 0.41s\n","# Epoch 5  global step 14180 loss 0.36700 batch 1052/3281 lr 0.001 accuracy 93.35938 wps 19401.36 step time 0.37s\n","# Epoch 5  global step 14200 loss 0.34717 batch 1072/3281 lr 0.001 accuracy 93.74219 wps 18315.02 step time 0.39s\n","# Epoch 5  global step 14220 loss 0.33946 batch 1092/3281 lr 0.001 accuracy 93.92187 wps 18058.16 step time 0.34s\n","# Epoch 5  global step 14240 loss 0.35296 batch 1112/3281 lr 0.001 accuracy 93.44922 wps 18919.11 step time 0.36s\n","# Epoch 5  global step 14260 loss 0.36848 batch 1132/3281 lr 0.001 accuracy 93.24219 wps 19537.87 step time 0.39s\n","# Epoch 5  global step 14280 loss 0.37129 batch 1152/3281 lr 0.001 accuracy 93.27734 wps 18989.13 step time 0.46s\n","# Epoch 5  global step 14300 loss 0.36065 batch 1172/3281 lr 0.001 accuracy 93.54297 wps 19438.36 step time 0.38s\n","# Epoch 5  global step 14320 loss 0.37104 batch 1192/3281 lr 0.001 accuracy 93.08984 wps 19114.24 step time 0.38s\n","# Epoch 5  global step 14340 loss 0.35363 batch 1212/3281 lr 0.001 accuracy 93.60547 wps 18277.92 step time 0.39s\n","# Epoch 5  global step 14360 loss 0.35659 batch 1232/3281 lr 0.001 accuracy 93.42187 wps 18823.09 step time 0.41s\n","# Epoch 5  global step 14380 loss 0.35771 batch 1252/3281 lr 0.001 accuracy 93.44531 wps 18972.44 step time 0.37s\n","# Epoch 5  global step 14400 loss 0.35960 batch 1272/3281 lr 0.001 accuracy 93.39844 wps 18529.59 step time 0.36s\n","# Epoch 5  global step 14420 loss 0.35861 batch 1292/3281 lr 0.001 accuracy 93.57422 wps 18310.71 step time 0.35s\n","# Epoch 5  global step 14440 loss 0.35609 batch 1312/3281 lr 0.001 accuracy 93.49219 wps 19045.74 step time 0.41s\n","# Epoch 5  global step 14460 loss 0.36858 batch 1332/3281 lr 0.001 accuracy 93.32031 wps 20106.06 step time 0.44s\n","# Epoch 5  global step 14480 loss 0.38421 batch 1352/3281 lr 0.001 accuracy 92.94922 wps 19772.20 step time 0.40s\n","# Epoch 5  global step 14500 loss 0.34731 batch 1372/3281 lr 0.001 accuracy 93.65625 wps 18641.39 step time 0.39s\n","# Epoch 5  global step 14520 loss 0.35508 batch 1392/3281 lr 0.001 accuracy 93.37109 wps 18252.95 step time 0.35s\n","# Epoch 5  global step 14540 loss 0.36327 batch 1412/3281 lr 0.001 accuracy 93.28516 wps 19207.07 step time 0.37s\n","# Epoch 5  global step 14560 loss 0.35574 batch 1432/3281 lr 0.001 accuracy 93.59375 wps 18508.15 step time 0.34s\n","# Epoch 5  global step 14580 loss 0.35594 batch 1452/3281 lr 0.001 accuracy 93.60156 wps 18664.56 step time 0.35s\n","# Epoch 5  global step 14600 loss 0.36204 batch 1472/3281 lr 0.001 accuracy 93.56641 wps 18938.17 step time 0.36s\n","# Epoch 5  global step 14620 loss 0.36884 batch 1492/3281 lr 0.001 accuracy 93.21094 wps 19464.52 step time 0.37s\n","# Epoch 5  global step 14640 loss 0.35387 batch 1512/3281 lr 0.001 accuracy 93.61328 wps 18646.00 step time 0.35s\n","# Epoch 5  global step 14660 loss 0.37149 batch 1532/3281 lr 0.001 accuracy 93.31641 wps 19394.26 step time 0.37s\n","# Epoch 5  global step 14680 loss 0.36822 batch 1552/3281 lr 0.001 accuracy 93.05469 wps 18907.66 step time 0.46s\n","# Epoch 5  global step 14700 loss 0.34921 batch 1572/3281 lr 0.001 accuracy 93.75000 wps 18347.96 step time 0.35s\n","# Epoch 5  global step 14720 loss 0.36117 batch 1592/3281 lr 0.001 accuracy 93.50000 wps 18579.72 step time 0.35s\n","# Epoch 5  global step 14740 loss 0.34046 batch 1612/3281 lr 0.001 accuracy 93.74219 wps 18309.73 step time 0.34s\n","# Epoch 5  global step 14760 loss 0.35880 batch 1632/3281 lr 0.001 accuracy 93.39453 wps 19357.87 step time 0.37s\n","# Epoch 5  global step 14780 loss 0.38240 batch 1652/3281 lr 0.001 accuracy 92.98047 wps 19173.68 step time 0.62s\n","# Epoch 5  global step 14800 loss 0.36211 batch 1672/3281 lr 0.001 accuracy 93.48828 wps 18762.37 step time 0.39s\n","# Epoch 5  global step 14820 loss 0.35765 batch 1692/3281 lr 0.001 accuracy 93.47656 wps 19759.21 step time 0.39s\n","# Epoch 5  global step 14840 loss 0.36847 batch 1712/3281 lr 0.001 accuracy 93.30469 wps 18573.75 step time 0.39s\n","# Epoch 5  global step 14860 loss 0.36431 batch 1732/3281 lr 0.001 accuracy 93.41406 wps 18272.37 step time 0.34s\n","# Epoch 5  global step 14880 loss 0.35431 batch 1752/3281 lr 0.001 accuracy 93.55859 wps 18679.89 step time 0.40s\n","# Epoch 5  global step 14900 loss 0.36329 batch 1772/3281 lr 0.001 accuracy 93.56250 wps 19127.21 step time 0.37s\n","# Epoch 5  global step 14920 loss 0.36120 batch 1792/3281 lr 0.001 accuracy 93.33594 wps 19235.17 step time 0.38s\n","# Epoch 5  global step 14940 loss 0.36293 batch 1812/3281 lr 0.001 accuracy 93.40234 wps 19822.30 step time 0.39s\n","# Epoch 5  global step 14960 loss 0.34746 batch 1832/3281 lr 0.001 accuracy 93.69531 wps 17787.90 step time 0.33s\n","# Epoch 5  global step 14980 loss 0.34215 batch 1852/3281 lr 0.001 accuracy 93.90234 wps 17652.87 step time 0.33s\n","# Epoch 5  global step 15000 loss 0.35078 batch 1872/3281 lr 0.001 accuracy 93.57812 wps 18449.19 step time 0.40s\n","# Epoch 5  global step 15020 loss 0.36141 batch 1892/3281 lr 0.001 accuracy 93.38281 wps 19358.26 step time 0.38s\n","# Epoch 5  global step 15040 loss 0.36428 batch 1912/3281 lr 0.001 accuracy 93.40625 wps 19401.13 step time 0.38s\n","# Epoch 5  global step 15060 loss 0.34920 batch 1932/3281 lr 0.001 accuracy 93.45312 wps 18706.15 step time 0.36s\n","# Epoch 5  global step 15080 loss 0.35838 batch 1952/3281 lr 0.001 accuracy 93.48047 wps 18786.59 step time 0.36s\n","# Epoch 5  global step 15100 loss 0.35535 batch 1972/3281 lr 0.001 accuracy 93.50000 wps 19270.34 step time 0.38s\n","# Epoch 5  global step 15120 loss 0.35944 batch 1992/3281 lr 0.001 accuracy 93.66016 wps 18412.13 step time 0.34s\n","# Epoch 5  global step 15140 loss 0.37273 batch 2012/3281 lr 0.001 accuracy 93.26562 wps 18983.61 step time 0.43s\n","# Epoch 5  global step 15160 loss 0.35191 batch 2032/3281 lr 0.001 accuracy 93.74219 wps 18965.83 step time 0.38s\n","# Epoch 5  global step 15180 loss 0.35934 batch 2052/3281 lr 0.001 accuracy 93.56250 wps 18379.46 step time 0.35s\n","# Epoch 5  global step 15200 loss 0.37760 batch 2072/3281 lr 0.001 accuracy 93.00391 wps 19366.20 step time 0.37s\n","# Epoch 5  global step 15220 loss 0.36523 batch 2092/3281 lr 0.001 accuracy 93.29297 wps 19011.31 step time 0.36s\n","# Epoch 5  global step 15240 loss 0.36026 batch 2112/3281 lr 0.001 accuracy 93.36328 wps 18778.95 step time 0.40s\n","# Epoch 5  global step 15260 loss 0.33416 batch 2132/3281 lr 0.001 accuracy 94.01563 wps 17755.57 step time 0.33s\n","# Epoch 5  global step 15280 loss 0.37004 batch 2152/3281 lr 0.001 accuracy 93.42187 wps 20053.97 step time 0.40s\n","# Epoch 5  global step 15300 loss 0.38436 batch 2172/3281 lr 0.001 accuracy 93.13672 wps 19587.03 step time 0.40s\n","# Epoch 5  global step 15320 loss 0.36694 batch 2192/3281 lr 0.001 accuracy 93.10156 wps 19648.22 step time 0.39s\n","# Epoch 5  global step 15340 loss 0.35804 batch 2212/3281 lr 0.001 accuracy 93.36328 wps 19563.21 step time 0.39s\n","# Epoch 5  global step 15360 loss 0.37654 batch 2232/3281 lr 0.001 accuracy 93.07422 wps 19979.04 step time 0.41s\n","# Epoch 5  global step 15380 loss 0.36290 batch 2252/3281 lr 0.001 accuracy 93.25000 wps 19308.09 step time 0.38s\n","# Epoch 5  global step 15400 loss 0.32356 batch 2272/3281 lr 0.001 accuracy 94.18359 wps 18262.57 step time 0.37s\n","# Epoch 5  global step 15420 loss 0.37548 batch 2292/3281 lr 0.001 accuracy 93.08984 wps 19177.09 step time 0.40s\n","# Epoch 5  global step 15440 loss 0.37579 batch 2312/3281 lr 0.001 accuracy 93.15234 wps 19295.19 step time 0.42s\n","# Epoch 5  global step 15460 loss 0.35068 batch 2332/3281 lr 0.001 accuracy 93.67578 wps 18117.89 step time 0.33s\n","# Epoch 5  global step 15480 loss 0.32811 batch 2352/3281 lr 0.001 accuracy 94.20312 wps 17625.94 step time 0.32s\n","# Epoch 5  global step 15500 loss 0.33950 batch 2372/3281 lr 0.001 accuracy 93.83594 wps 18447.07 step time 0.35s\n","# Epoch 5  global step 15520 loss 0.37155 batch 2392/3281 lr 0.001 accuracy 93.24609 wps 19726.49 step time 0.40s\n","# Epoch 5  global step 15540 loss 0.40759 batch 2412/3281 lr 0.001 accuracy 92.63672 wps 19321.28 step time 0.45s\n","# Epoch 5  global step 15560 loss 0.38104 batch 2432/3281 lr 0.001 accuracy 93.27344 wps 19013.04 step time 0.42s\n","# Epoch 5  global step 15580 loss 0.36799 batch 2452/3281 lr 0.001 accuracy 93.31641 wps 20010.01 step time 0.40s\n","# Epoch 5  global step 15600 loss 0.36089 batch 2472/3281 lr 0.001 accuracy 93.33984 wps 18540.36 step time 0.39s\n","# Epoch 5  global step 15620 loss 0.34721 batch 2492/3281 lr 0.001 accuracy 93.78125 wps 17967.49 step time 0.39s\n","# Epoch 5  global step 15640 loss 0.37222 batch 2512/3281 lr 0.001 accuracy 93.33594 wps 18508.63 step time 0.35s\n","# Epoch 5  global step 15660 loss 0.35419 batch 2532/3281 lr 0.001 accuracy 93.50000 wps 18963.19 step time 0.36s\n","# Epoch 5  global step 15680 loss 0.35985 batch 2552/3281 lr 0.001 accuracy 93.48047 wps 17985.51 step time 0.39s\n","# Epoch 5  global step 15700 loss 0.39270 batch 2572/3281 lr 0.001 accuracy 92.86328 wps 19256.01 step time 0.46s\n","# Epoch 5  global step 15720 loss 0.37331 batch 2592/3281 lr 0.001 accuracy 93.18359 wps 19632.20 step time 0.43s\n","# Epoch 5  global step 15740 loss 0.37756 batch 2612/3281 lr 0.001 accuracy 93.19141 wps 20625.18 step time 0.43s\n","# Epoch 5  global step 15760 loss 0.38060 batch 2632/3281 lr 0.001 accuracy 93.07031 wps 18951.86 step time 0.45s\n","# Epoch 5  global step 15780 loss 0.36933 batch 2652/3281 lr 0.001 accuracy 93.24609 wps 19388.64 step time 0.37s\n","# Epoch 5  global step 15800 loss 0.37743 batch 2672/3281 lr 0.001 accuracy 93.05078 wps 18980.84 step time 0.42s\n","# Epoch 5  global step 15820 loss 0.37830 batch 2692/3281 lr 0.001 accuracy 93.07422 wps 19029.67 step time 0.42s\n","# Epoch 5  global step 15840 loss 0.38913 batch 2712/3281 lr 0.001 accuracy 92.83594 wps 19092.82 step time 0.57s\n","# Epoch 5  global step 15860 loss 0.36186 batch 2732/3281 lr 0.001 accuracy 93.37891 wps 19454.83 step time 0.39s\n","# Epoch 5  global step 15880 loss 0.33519 batch 2752/3281 lr 0.001 accuracy 93.83203 wps 17410.36 step time 0.32s\n","# Epoch 5  global step 15900 loss 0.37298 batch 2772/3281 lr 0.001 accuracy 93.39844 wps 18404.61 step time 0.45s\n","# Epoch 5  global step 15920 loss 0.38393 batch 2792/3281 lr 0.001 accuracy 93.04297 wps 19998.59 step time 0.44s\n","# Epoch 5  global step 15940 loss 0.36264 batch 2812/3281 lr 0.001 accuracy 93.28125 wps 18622.63 step time 0.39s\n","# Epoch 5  global step 15960 loss 0.33270 batch 2832/3281 lr 0.001 accuracy 93.96484 wps 17255.91 step time 0.32s\n","# Epoch 5  global step 15980 loss 0.34816 batch 2852/3281 lr 0.001 accuracy 93.77734 wps 19176.91 step time 0.38s\n","# Epoch 5  global step 16000 loss 0.36006 batch 2872/3281 lr 0.001 accuracy 93.31250 wps 19357.29 step time 0.42s\n","# global step 16000, eval model at Fri May 22 12:11:17 2020\n","2020-05-22 12:11:20.052713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:11:20.053414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 12:11:20.053528: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 12:11:20.053576: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 12:11:20.053623: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 12:11:20.053698: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 12:11:20.053747: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 12:11:20.053792: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 12:11:20.053834: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 12:11:20.053960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:11:20.054594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:11:20.055232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 12:11:20.055299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 12:11:20.055329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 12:11:20.055344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 12:11:20.055488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:11:20.056104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:11:20.056643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6114762711157818\n","# location_distance_from_business_district - 0.39477074349700925\n","# location_easy_to_find - 0.6196435084795197\n","# service_wait_time - 0.47042009524702866\n","# service_waiters_attitude - 0.7237548412913225\n","# service_parking_convenience - 0.632825439457064\n","# service_serving_speed - 0.575898757376546\n","# price_level - 0.6204994337092836\n","# price_cost_effective - 0.5758241602740632\n","# price_discount - 0.5389712235624308\n","# environment_decoration - 0.5637830918147579\n","# environment_noise - 0.627708970887875\n","# environment_space - 0.6470304636432155\n","# environment_cleaness - 0.6185617276745375\n","# dish_portion - 0.5002733202586654\n","# dish_taste - 0.6730579664268542\n","# dish_look - 0.30060950555898336\n","# dish_recommendation - 0.3519891411737761\n","# others_overall_experience - 0.57029680763659\n","# others_willing_to_consume_again - 0.6107970924492994\n","# Eval loss 0.35212, f1 0.56141\n","# current result -0.5614096280767302, previous best result -0.40891661218670866\n","# Epoch 5  global step 16020 loss 0.36726 batch 2892/3281 lr 0.001 accuracy 93.46094 wps 18607.41 step time 0.41s\n","# Epoch 5  global step 16040 loss 0.36845 batch 2912/3281 lr 0.001 accuracy 93.27734 wps 19482.31 step time 0.39s\n","# Epoch 5  global step 16060 loss 0.35415 batch 2932/3281 lr 0.001 accuracy 93.54297 wps 18866.94 step time 0.36s\n","# Epoch 5  global step 16080 loss 0.35661 batch 2952/3281 lr 0.001 accuracy 93.58203 wps 18263.68 step time 0.39s\n","# Epoch 5  global step 16100 loss 0.35557 batch 2972/3281 lr 0.001 accuracy 93.54688 wps 18899.07 step time 0.36s\n","# Epoch 5  global step 16120 loss 0.35097 batch 2992/3281 lr 0.001 accuracy 93.60156 wps 19076.41 step time 0.43s\n","# Epoch 5  global step 16140 loss 0.35620 batch 3012/3281 lr 0.001 accuracy 93.50391 wps 18306.73 step time 0.39s\n","# Epoch 5  global step 16160 loss 0.36237 batch 3032/3281 lr 0.001 accuracy 93.37891 wps 19307.13 step time 0.37s\n","# Epoch 5  global step 16180 loss 0.35164 batch 3052/3281 lr 0.001 accuracy 93.65625 wps 18472.88 step time 0.35s\n","# Epoch 5  global step 16200 loss 0.37212 batch 3072/3281 lr 0.001 accuracy 93.21875 wps 18866.01 step time 0.42s\n","# Epoch 5  global step 16220 loss 0.37464 batch 3092/3281 lr 0.001 accuracy 93.27344 wps 19252.77 step time 0.37s\n","# Epoch 5  global step 16240 loss 0.36326 batch 3112/3281 lr 0.001 accuracy 93.38672 wps 18704.92 step time 0.41s\n","# Epoch 5  global step 16260 loss 0.35315 batch 3132/3281 lr 0.001 accuracy 93.62109 wps 18641.22 step time 0.35s\n","# Epoch 5  global step 16280 loss 0.34919 batch 3152/3281 lr 0.001 accuracy 93.71484 wps 17647.70 step time 0.32s\n","# Epoch 5  global step 16300 loss 0.37032 batch 3172/3281 lr 0.001 accuracy 93.24219 wps 18761.14 step time 0.37s\n","# Epoch 5  global step 16320 loss 0.34985 batch 3192/3281 lr 0.001 accuracy 93.61719 wps 19315.83 step time 0.38s\n","# Epoch 5  global step 16340 loss 0.34614 batch 3212/3281 lr 0.001 accuracy 93.75781 wps 17715.32 step time 0.35s\n","# Epoch 5  global step 16360 loss 0.37771 batch 3232/3281 lr 0.001 accuracy 93.17578 wps 17379.87 step time 0.42s\n","# Epoch 5  global step 16380 loss 0.35887 batch 3252/3281 lr 0.001 accuracy 93.59766 wps 16874.21 step time 0.41s\n","# Epoch 5  global step 16400 loss 0.34846 batch 3272/3281 lr 0.001 accuracy 93.68750 wps 16771.53 step time 0.40s\n","# Finsh epoch 5, global step 16410\n","# Epoch 6  global step 16420 loss 0.18443 batch 10/3281 lr 0.001 accuracy 46.68359 wps 18728.74 step time 0.22s\n","# Epoch 6  global step 16440 loss 0.36045 batch 30/3281 lr 0.001 accuracy 93.65625 wps 19536.37 step time 0.39s\n","# Epoch 6  global step 16460 loss 0.32324 batch 50/3281 lr 0.001 accuracy 94.04297 wps 18520.90 step time 0.34s\n","# Epoch 6  global step 16480 loss 0.34745 batch 70/3281 lr 0.001 accuracy 93.57031 wps 18404.64 step time 0.39s\n","# Epoch 6  global step 16500 loss 0.34483 batch 90/3281 lr 0.001 accuracy 93.69922 wps 18603.89 step time 0.39s\n","# Epoch 6  global step 16520 loss 0.33191 batch 110/3281 lr 0.001 accuracy 94.15234 wps 18174.80 step time 0.34s\n","# Epoch 6  global step 16540 loss 0.34139 batch 130/3281 lr 0.001 accuracy 93.74609 wps 19041.89 step time 0.36s\n","# Epoch 6  global step 16560 loss 0.32724 batch 150/3281 lr 0.001 accuracy 94.12500 wps 18384.14 step time 0.34s\n","# Epoch 6  global step 16580 loss 0.33991 batch 170/3281 lr 0.001 accuracy 93.75000 wps 19795.10 step time 0.37s\n","# Epoch 6  global step 16600 loss 0.33905 batch 190/3281 lr 0.001 accuracy 93.95312 wps 18231.87 step time 0.34s\n","# Epoch 6  global step 16620 loss 0.36501 batch 210/3281 lr 0.001 accuracy 93.43750 wps 19717.87 step time 0.45s\n","# Epoch 6  global step 16640 loss 0.35120 batch 230/3281 lr 0.001 accuracy 93.49609 wps 18355.55 step time 0.39s\n","# Epoch 6  global step 16660 loss 0.33822 batch 250/3281 lr 0.001 accuracy 93.80078 wps 19102.95 step time 0.41s\n","# Epoch 6  global step 16680 loss 0.32975 batch 270/3281 lr 0.001 accuracy 94.00391 wps 18632.79 step time 0.34s\n","# Epoch 6  global step 16700 loss 0.33705 batch 290/3281 lr 0.001 accuracy 93.92578 wps 19867.01 step time 0.39s\n","# Epoch 6  global step 16720 loss 0.33815 batch 310/3281 lr 0.001 accuracy 93.84375 wps 19548.37 step time 0.38s\n","# Epoch 6  global step 16740 loss 0.33201 batch 330/3281 lr 0.001 accuracy 93.86328 wps 18636.04 step time 0.34s\n","# Epoch 6  global step 16760 loss 0.33164 batch 350/3281 lr 0.001 accuracy 94.03906 wps 18995.48 step time 0.37s\n","# Epoch 6  global step 16780 loss 0.35437 batch 370/3281 lr 0.001 accuracy 93.61328 wps 19169.60 step time 0.37s\n","# Epoch 6  global step 16800 loss 0.34056 batch 390/3281 lr 0.001 accuracy 93.73438 wps 18564.41 step time 0.35s\n","# Epoch 6  global step 16820 loss 0.33933 batch 410/3281 lr 0.001 accuracy 93.87891 wps 19170.73 step time 0.37s\n","# Epoch 6  global step 16840 loss 0.35878 batch 430/3281 lr 0.001 accuracy 93.48047 wps 18596.78 step time 0.35s\n","# Epoch 6  global step 16860 loss 0.35277 batch 450/3281 lr 0.001 accuracy 93.56250 wps 18965.67 step time 0.36s\n","# Epoch 6  global step 16880 loss 0.35011 batch 470/3281 lr 0.001 accuracy 93.74609 wps 19657.17 step time 0.39s\n","# Epoch 6  global step 16900 loss 0.33895 batch 490/3281 lr 0.001 accuracy 93.99609 wps 18438.07 step time 0.34s\n","# Epoch 6  global step 16920 loss 0.35609 batch 510/3281 lr 0.001 accuracy 93.51172 wps 19517.84 step time 0.39s\n","# Epoch 6  global step 16940 loss 0.33760 batch 530/3281 lr 0.001 accuracy 93.96484 wps 19399.79 step time 0.37s\n","# Epoch 6  global step 16960 loss 0.37221 batch 550/3281 lr 0.001 accuracy 93.23047 wps 19445.77 step time 0.43s\n","# Epoch 6  global step 16980 loss 0.36944 batch 570/3281 lr 0.001 accuracy 93.30078 wps 19082.51 step time 0.46s\n","# Epoch 6  global step 17000 loss 0.33661 batch 590/3281 lr 0.001 accuracy 93.76953 wps 19177.36 step time 0.36s\n","# Epoch 6  global step 17020 loss 0.35078 batch 610/3281 lr 0.001 accuracy 93.67188 wps 19578.69 step time 0.38s\n","# Epoch 6  global step 17040 loss 0.33163 batch 630/3281 lr 0.001 accuracy 93.95703 wps 18924.48 step time 0.36s\n","# Epoch 6  global step 17060 loss 0.35118 batch 650/3281 lr 0.001 accuracy 93.39844 wps 19705.34 step time 0.41s\n","# Epoch 6  global step 17080 loss 0.34220 batch 670/3281 lr 0.001 accuracy 93.78906 wps 19683.77 step time 0.38s\n","# Epoch 6  global step 17100 loss 0.34143 batch 690/3281 lr 0.001 accuracy 93.67188 wps 18868.03 step time 0.41s\n","# Epoch 6  global step 17120 loss 0.34158 batch 710/3281 lr 0.001 accuracy 93.88672 wps 18721.45 step time 0.34s\n","# Epoch 6  global step 17140 loss 0.34233 batch 730/3281 lr 0.001 accuracy 93.78125 wps 18602.96 step time 0.34s\n","# Epoch 6  global step 17160 loss 0.33188 batch 750/3281 lr 0.001 accuracy 93.93359 wps 18871.68 step time 0.35s\n","# Epoch 6  global step 17180 loss 0.35106 batch 770/3281 lr 0.001 accuracy 93.45703 wps 18820.67 step time 0.35s\n","# Epoch 6  global step 17200 loss 0.33634 batch 790/3281 lr 0.001 accuracy 93.81250 wps 18036.65 step time 0.32s\n","# Epoch 6  global step 17220 loss 0.35173 batch 810/3281 lr 0.001 accuracy 93.61328 wps 19201.96 step time 0.40s\n","# Epoch 6  global step 17240 loss 0.35696 batch 830/3281 lr 0.001 accuracy 93.45313 wps 18842.04 step time 0.41s\n","# Epoch 6  global step 17260 loss 0.35949 batch 850/3281 lr 0.001 accuracy 93.40625 wps 19998.97 step time 0.40s\n","# Epoch 6  global step 17280 loss 0.34184 batch 870/3281 lr 0.001 accuracy 93.83203 wps 19084.15 step time 0.36s\n","# Epoch 6  global step 17300 loss 0.35471 batch 890/3281 lr 0.001 accuracy 93.37109 wps 18989.30 step time 0.41s\n","# Epoch 6  global step 17320 loss 0.36569 batch 910/3281 lr 0.001 accuracy 93.41016 wps 18542.64 step time 0.39s\n","# Epoch 6  global step 17340 loss 0.37479 batch 930/3281 lr 0.001 accuracy 93.23438 wps 19020.69 step time 0.43s\n","# Epoch 6  global step 17360 loss 0.35160 batch 950/3281 lr 0.001 accuracy 93.62891 wps 19422.99 step time 0.36s\n","# Epoch 6  global step 17380 loss 0.36480 batch 970/3281 lr 0.001 accuracy 93.41016 wps 19398.70 step time 0.37s\n","# Epoch 6  global step 17400 loss 0.34598 batch 990/3281 lr 0.001 accuracy 93.78516 wps 18797.87 step time 0.35s\n","# Epoch 6  global step 17420 loss 0.34259 batch 1010/3281 lr 0.001 accuracy 93.71875 wps 17899.77 step time 0.38s\n","# Epoch 6  global step 17440 loss 0.34750 batch 1030/3281 lr 0.001 accuracy 93.54297 wps 18920.00 step time 0.41s\n","# Epoch 6  global step 17460 loss 0.33099 batch 1050/3281 lr 0.001 accuracy 93.85547 wps 18060.92 step time 0.33s\n","# Epoch 6  global step 17480 loss 0.35026 batch 1070/3281 lr 0.001 accuracy 93.75781 wps 18562.30 step time 0.40s\n","# Epoch 6  global step 17500 loss 0.33610 batch 1090/3281 lr 0.001 accuracy 93.91797 wps 18421.59 step time 0.35s\n","# Epoch 6  global step 17520 loss 0.34945 batch 1110/3281 lr 0.001 accuracy 93.38672 wps 19434.89 step time 0.43s\n","# Epoch 6  global step 17540 loss 0.37890 batch 1130/3281 lr 0.001 accuracy 93.14453 wps 19692.23 step time 0.44s\n","# Epoch 6  global step 17560 loss 0.35288 batch 1150/3281 lr 0.001 accuracy 93.73437 wps 18475.92 step time 0.35s\n","# Epoch 6  global step 17580 loss 0.35155 batch 1170/3281 lr 0.001 accuracy 93.64844 wps 17950.75 step time 0.38s\n","# Epoch 6  global step 17600 loss 0.35078 batch 1190/3281 lr 0.001 accuracy 93.60938 wps 17523.09 step time 0.37s\n","# Epoch 6  global step 17620 loss 0.34743 batch 1210/3281 lr 0.001 accuracy 93.68750 wps 17053.28 step time 0.40s\n","# Epoch 6  global step 17640 loss 0.34307 batch 1230/3281 lr 0.001 accuracy 93.74219 wps 16595.31 step time 0.44s\n","# Epoch 6  global step 17660 loss 0.33847 batch 1250/3281 lr 0.001 accuracy 93.91797 wps 17511.11 step time 0.37s\n","# Epoch 6  global step 17680 loss 0.35331 batch 1270/3281 lr 0.001 accuracy 93.59375 wps 16695.05 step time 0.39s\n","# Epoch 6  global step 17700 loss 0.34993 batch 1290/3281 lr 0.001 accuracy 93.78125 wps 16185.14 step time 0.46s\n","# Epoch 6  global step 17720 loss 0.33069 batch 1310/3281 lr 0.001 accuracy 94.01172 wps 16713.68 step time 0.40s\n","# Epoch 6  global step 17740 loss 0.37566 batch 1330/3281 lr 0.001 accuracy 93.02344 wps 17173.36 step time 0.49s\n","# Epoch 6  global step 17760 loss 0.35359 batch 1350/3281 lr 0.001 accuracy 93.44531 wps 16664.72 step time 0.42s\n","# Epoch 6  global step 17780 loss 0.38903 batch 1370/3281 lr 0.001 accuracy 92.74219 wps 16366.47 step time 0.63s\n","# Epoch 6  global step 17800 loss 0.36988 batch 1390/3281 lr 0.001 accuracy 93.18750 wps 16983.40 step time 0.44s\n","# Epoch 6  global step 17820 loss 0.35571 batch 1410/3281 lr 0.001 accuracy 93.44922 wps 17217.58 step time 0.42s\n","# Epoch 6  global step 17840 loss 0.35210 batch 1430/3281 lr 0.001 accuracy 93.66797 wps 16581.24 step time 0.46s\n","# Epoch 6  global step 17860 loss 0.35287 batch 1450/3281 lr 0.001 accuracy 93.44531 wps 16772.00 step time 0.37s\n","# Epoch 6  global step 17880 loss 0.35552 batch 1470/3281 lr 0.001 accuracy 93.74609 wps 17388.37 step time 0.40s\n","# Epoch 6  global step 17900 loss 0.35428 batch 1490/3281 lr 0.001 accuracy 93.52734 wps 17267.86 step time 0.44s\n","# Epoch 6  global step 17920 loss 0.33664 batch 1510/3281 lr 0.001 accuracy 93.84766 wps 15773.28 step time 0.48s\n","# Epoch 6  global step 17940 loss 0.35301 batch 1530/3281 lr 0.001 accuracy 93.46484 wps 16579.08 step time 0.43s\n","# Epoch 6  global step 17960 loss 0.36230 batch 1550/3281 lr 0.001 accuracy 93.34766 wps 16979.27 step time 0.42s\n","# Epoch 6  global step 17980 loss 0.35980 batch 1570/3281 lr 0.001 accuracy 93.42187 wps 16636.73 step time 0.40s\n","# Epoch 6  global step 18000 loss 0.34973 batch 1590/3281 lr 0.001 accuracy 93.84375 wps 16360.23 step time 0.43s\n","# global step 18000, eval model at Fri May 22 12:25:26 2020\n","2020-05-22 12:25:28.984421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:25:28.985278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 12:25:28.985393: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 12:25:28.985439: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 12:25:28.985484: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 12:25:28.985541: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 12:25:28.985596: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 12:25:28.985635: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 12:25:28.985691: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 12:25:28.985817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:25:28.986410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:25:28.987052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 12:25:28.987169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 12:25:28.987198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 12:25:28.987227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 12:25:28.987431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:25:28.988075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:25:28.988740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.637543919283178\n","# location_distance_from_business_district - 0.439268446462059\n","# location_easy_to_find - 0.6676399309989143\n","# service_wait_time - 0.5787391580315416\n","# service_waiters_attitude - 0.7572181904917914\n","# service_parking_convenience - 0.6962274675961385\n","# service_serving_speed - 0.6732732536959474\n","# price_level - 0.7129014473569953\n","# price_cost_effective - 0.6553827783896948\n","# price_discount - 0.5852550502458489\n","# environment_decoration - 0.6566469300908946\n","# environment_noise - 0.7231735931629013\n","# environment_space - 0.7226723853107717\n","# environment_cleaness - 0.7047431927608134\n","# dish_portion - 0.6171146946892285\n","# dish_taste - 0.7055500269638861\n","# dish_look - 0.40191827320635326\n","# dish_recommendation - 0.6353285074414572\n","# others_overall_experience - 0.5770402548061553\n","# others_willing_to_consume_again - 0.6590381832576648\n","# Eval loss 0.31223, f1 0.64033\n","# current result -0.6403337842121118, previous best result -0.5614096280767302\n","# Epoch 6  global step 18020 loss 0.36027 batch 1610/3281 lr 0.001 accuracy 93.25391 wps 17248.03 step time 0.45s\n","# Epoch 6  global step 18040 loss 0.35590 batch 1630/3281 lr 0.001 accuracy 93.60547 wps 16527.16 step time 0.46s\n","# Epoch 6  global step 18060 loss 0.33832 batch 1650/3281 lr 0.001 accuracy 93.80078 wps 16407.85 step time 0.38s\n","# Epoch 6  global step 18080 loss 0.34336 batch 1670/3281 lr 0.001 accuracy 93.79297 wps 16350.32 step time 0.40s\n","# Epoch 6  global step 18100 loss 0.35235 batch 1690/3281 lr 0.001 accuracy 93.73828 wps 17023.35 step time 0.37s\n","# Epoch 6  global step 18120 loss 0.34332 batch 1710/3281 lr 0.001 accuracy 93.73828 wps 16836.58 step time 0.38s\n","# Epoch 6  global step 18140 loss 0.33787 batch 1730/3281 lr 0.001 accuracy 93.86328 wps 16235.25 step time 0.35s\n","# Epoch 6  global step 18160 loss 0.36022 batch 1750/3281 lr 0.001 accuracy 93.53125 wps 16495.71 step time 0.38s\n","# Epoch 6  global step 18180 loss 0.35663 batch 1770/3281 lr 0.001 accuracy 93.52344 wps 16376.48 step time 0.40s\n","# Epoch 6  global step 18200 loss 0.35162 batch 1790/3281 lr 0.001 accuracy 93.73047 wps 17477.78 step time 0.42s\n","# Epoch 6  global step 18220 loss 0.35536 batch 1810/3281 lr 0.001 accuracy 93.48047 wps 17200.50 step time 0.46s\n","# Epoch 6  global step 18240 loss 0.35732 batch 1830/3281 lr 0.001 accuracy 93.60547 wps 17716.50 step time 0.42s\n","# Epoch 6  global step 18260 loss 0.35277 batch 1850/3281 lr 0.001 accuracy 93.66406 wps 17953.47 step time 0.42s\n","# Epoch 6  global step 18280 loss 0.34382 batch 1870/3281 lr 0.001 accuracy 93.90234 wps 17234.95 step time 0.41s\n","# Epoch 6  global step 18300 loss 0.35335 batch 1890/3281 lr 0.001 accuracy 93.54297 wps 17253.56 step time 0.39s\n","# Epoch 6  global step 18320 loss 0.37272 batch 1910/3281 lr 0.001 accuracy 93.21094 wps 17549.08 step time 0.39s\n","# Epoch 6  global step 18340 loss 0.32231 batch 1930/3281 lr 0.001 accuracy 94.17188 wps 16668.86 step time 0.34s\n","# Epoch 6  global step 18360 loss 0.35348 batch 1950/3281 lr 0.001 accuracy 93.56641 wps 16183.96 step time 0.54s\n","# Epoch 6  global step 18380 loss 0.34956 batch 1970/3281 lr 0.001 accuracy 93.75781 wps 16860.09 step time 0.47s\n","# Epoch 6  global step 18400 loss 0.36205 batch 1990/3281 lr 0.001 accuracy 93.49219 wps 17905.25 step time 0.43s\n","# Epoch 6  global step 18420 loss 0.35828 batch 2010/3281 lr 0.001 accuracy 93.44531 wps 16593.40 step time 0.49s\n","# Epoch 6  global step 18440 loss 0.33949 batch 2030/3281 lr 0.001 accuracy 93.90234 wps 16190.24 step time 0.44s\n","# Epoch 6  global step 18460 loss 0.36766 batch 2050/3281 lr 0.001 accuracy 93.42578 wps 17055.56 step time 0.51s\n","# Epoch 6  global step 18480 loss 0.36725 batch 2070/3281 lr 0.001 accuracy 93.30078 wps 16734.33 step time 0.47s\n","# Epoch 6  global step 18500 loss 0.36703 batch 2090/3281 lr 0.001 accuracy 93.50000 wps 17536.18 step time 0.44s\n","# Epoch 6  global step 18520 loss 0.37271 batch 2110/3281 lr 0.001 accuracy 93.30859 wps 17686.83 step time 0.43s\n","# Epoch 6  global step 18540 loss 0.35109 batch 2130/3281 lr 0.001 accuracy 93.51953 wps 16945.85 step time 0.40s\n","# Epoch 6  global step 18560 loss 0.37069 batch 2150/3281 lr 0.001 accuracy 93.23047 wps 17399.26 step time 0.51s\n","# Epoch 6  global step 18580 loss 0.34137 batch 2170/3281 lr 0.001 accuracy 93.96875 wps 16752.72 step time 0.49s\n","# Epoch 6  global step 18600 loss 0.37368 batch 2190/3281 lr 0.001 accuracy 93.42578 wps 16722.75 step time 0.46s\n","# Epoch 6  global step 18620 loss 0.36639 batch 2210/3281 lr 0.001 accuracy 93.39844 wps 16933.93 step time 0.44s\n","# Epoch 6  global step 18640 loss 0.34614 batch 2230/3281 lr 0.001 accuracy 93.60938 wps 16710.76 step time 0.34s\n","# Epoch 6  global step 18660 loss 0.33976 batch 2250/3281 lr 0.001 accuracy 93.87500 wps 16926.21 step time 0.37s\n","# Epoch 6  global step 18680 loss 0.36847 batch 2270/3281 lr 0.001 accuracy 93.47656 wps 17668.56 step time 0.44s\n","# Epoch 6  global step 18700 loss 0.37369 batch 2290/3281 lr 0.001 accuracy 92.99609 wps 17315.28 step time 0.46s\n","# Epoch 6  global step 18720 loss 0.35043 batch 2310/3281 lr 0.001 accuracy 93.63672 wps 17217.48 step time 0.41s\n","# Epoch 6  global step 18740 loss 0.36565 batch 2330/3281 lr 0.001 accuracy 93.43750 wps 16892.73 step time 0.45s\n","# Epoch 6  global step 18760 loss 0.35191 batch 2350/3281 lr 0.001 accuracy 93.64453 wps 17071.86 step time 0.38s\n","# Epoch 6  global step 18780 loss 0.35593 batch 2370/3281 lr 0.001 accuracy 93.46875 wps 17718.69 step time 0.43s\n","# Epoch 6  global step 18800 loss 0.35739 batch 2390/3281 lr 0.001 accuracy 93.48828 wps 17376.33 step time 0.51s\n","# Epoch 6  global step 18820 loss 0.35460 batch 2410/3281 lr 0.001 accuracy 93.34766 wps 17324.11 step time 0.41s\n","# Epoch 6  global step 18840 loss 0.36481 batch 2430/3281 lr 0.001 accuracy 93.45313 wps 16507.10 step time 0.50s\n","# Epoch 6  global step 18860 loss 0.37249 batch 2450/3281 lr 0.001 accuracy 93.30469 wps 16409.00 step time 0.57s\n","# Epoch 6  global step 18880 loss 0.34711 batch 2470/3281 lr 0.001 accuracy 93.74219 wps 16514.17 step time 0.42s\n","# Epoch 6  global step 18900 loss 0.36665 batch 2490/3281 lr 0.001 accuracy 93.20703 wps 16553.62 step time 0.50s\n","# Epoch 6  global step 18920 loss 0.36943 batch 2510/3281 lr 0.001 accuracy 93.29687 wps 15905.60 step time 0.59s\n","# Epoch 6  global step 18940 loss 0.34720 batch 2530/3281 lr 0.001 accuracy 93.85938 wps 16822.17 step time 0.41s\n","# Epoch 6  global step 18960 loss 0.36080 batch 2550/3281 lr 0.001 accuracy 93.32031 wps 17658.18 step time 0.43s\n","# Epoch 6  global step 18980 loss 0.36953 batch 2570/3281 lr 0.001 accuracy 93.36719 wps 17331.43 step time 0.46s\n","# Epoch 6  global step 19000 loss 0.36167 batch 2590/3281 lr 0.001 accuracy 93.49609 wps 17292.73 step time 0.44s\n","# Epoch 6  global step 19020 loss 0.34928 batch 2610/3281 lr 0.001 accuracy 93.78125 wps 16109.81 step time 0.43s\n","# Epoch 6  global step 19040 loss 0.34482 batch 2630/3281 lr 0.001 accuracy 93.85547 wps 17111.84 step time 0.39s\n","# Epoch 6  global step 19060 loss 0.37834 batch 2650/3281 lr 0.001 accuracy 93.12891 wps 17207.51 step time 0.42s\n","# Epoch 6  global step 19080 loss 0.33840 batch 2670/3281 lr 0.001 accuracy 93.97656 wps 16591.12 step time 0.41s\n","# Epoch 6  global step 19100 loss 0.34691 batch 2690/3281 lr 0.001 accuracy 93.77734 wps 16430.34 step time 0.37s\n","# Epoch 6  global step 19120 loss 0.36644 batch 2710/3281 lr 0.001 accuracy 93.47266 wps 16943.39 step time 0.44s\n","# Epoch 6  global step 19140 loss 0.33041 batch 2730/3281 lr 0.001 accuracy 94.00781 wps 15990.93 step time 0.39s\n","# Epoch 6  global step 19160 loss 0.38142 batch 2750/3281 lr 0.001 accuracy 93.02734 wps 17892.79 step time 0.43s\n","# Epoch 6  global step 19180 loss 0.36956 batch 2770/3281 lr 0.001 accuracy 93.28906 wps 17256.02 step time 0.42s\n","# Epoch 6  global step 19200 loss 0.36250 batch 2790/3281 lr 0.001 accuracy 93.52734 wps 16240.86 step time 0.53s\n","# Epoch 6  global step 19220 loss 0.31498 batch 2810/3281 lr 0.001 accuracy 94.20703 wps 17418.30 step time 0.35s\n","# Epoch 6  global step 19240 loss 0.33977 batch 2830/3281 lr 0.001 accuracy 94.03906 wps 16551.14 step time 0.37s\n","# Epoch 6  global step 19260 loss 0.34177 batch 2850/3281 lr 0.001 accuracy 93.80078 wps 16024.93 step time 0.44s\n","# Epoch 6  global step 19280 loss 0.35380 batch 2870/3281 lr 0.001 accuracy 93.55078 wps 17786.30 step time 0.44s\n","# Epoch 6  global step 19300 loss 0.36330 batch 2890/3281 lr 0.001 accuracy 93.41406 wps 17679.54 step time 0.42s\n","# Epoch 6  global step 19320 loss 0.34592 batch 2910/3281 lr 0.001 accuracy 93.69531 wps 17888.52 step time 0.40s\n","# Epoch 6  global step 19340 loss 0.36130 batch 2930/3281 lr 0.001 accuracy 93.48047 wps 16279.57 step time 0.47s\n","# Epoch 6  global step 19360 loss 0.35478 batch 2950/3281 lr 0.001 accuracy 93.56250 wps 16122.61 step time 0.52s\n","# Epoch 6  global step 19380 loss 0.37598 batch 2970/3281 lr 0.001 accuracy 93.36328 wps 17078.94 step time 0.40s\n","# Epoch 6  global step 19400 loss 0.36267 batch 2990/3281 lr 0.001 accuracy 93.37891 wps 16794.92 step time 0.45s\n","# Epoch 6  global step 19420 loss 0.37052 batch 3010/3281 lr 0.001 accuracy 93.08203 wps 16186.86 step time 0.62s\n","# Epoch 6  global step 19440 loss 0.33633 batch 3030/3281 lr 0.001 accuracy 93.96484 wps 16784.41 step time 0.35s\n","# Epoch 6  global step 19460 loss 0.36131 batch 3050/3281 lr 0.001 accuracy 93.45703 wps 16585.90 step time 0.49s\n","# Epoch 6  global step 19480 loss 0.33379 batch 3070/3281 lr 0.001 accuracy 93.79297 wps 16930.36 step time 0.39s\n","# Epoch 6  global step 19500 loss 0.35443 batch 3090/3281 lr 0.001 accuracy 93.49219 wps 16283.27 step time 0.46s\n","# Epoch 6  global step 19520 loss 0.36524 batch 3110/3281 lr 0.001 accuracy 93.15625 wps 17523.40 step time 0.49s\n","# Epoch 6  global step 19540 loss 0.35718 batch 3130/3281 lr 0.001 accuracy 93.47266 wps 16873.55 step time 0.37s\n","# Epoch 6  global step 19560 loss 0.36481 batch 3150/3281 lr 0.001 accuracy 93.34766 wps 17325.68 step time 0.40s\n","# Epoch 6  global step 19580 loss 0.33231 batch 3170/3281 lr 0.001 accuracy 94.18359 wps 16839.04 step time 0.38s\n","# Epoch 6  global step 19600 loss 0.33032 batch 3190/3281 lr 0.001 accuracy 94.00000 wps 16962.64 step time 0.34s\n","# Epoch 6  global step 19620 loss 0.34882 batch 3210/3281 lr 0.001 accuracy 93.51953 wps 16946.33 step time 0.39s\n","# Epoch 6  global step 19640 loss 0.34611 batch 3230/3281 lr 0.001 accuracy 93.80078 wps 15989.91 step time 0.42s\n","# Epoch 6  global step 19660 loss 0.34310 batch 3250/3281 lr 0.001 accuracy 93.73047 wps 16130.92 step time 0.44s\n","# Epoch 6  global step 19680 loss 0.35975 batch 3270/3281 lr 0.001 accuracy 93.46875 wps 16285.92 step time 0.50s\n","# Finsh epoch 6, global step 19692\n","# Epoch 7  global step 19700 loss 0.13708 batch 8/3281 lr 0.001 accuracy 37.46875 wps 19286.30 step time 0.18s\n","# Epoch 7  global step 19720 loss 0.34100 batch 28/3281 lr 0.001 accuracy 93.70703 wps 19705.14 step time 0.39s\n","# Epoch 7  global step 19740 loss 0.34821 batch 48/3281 lr 0.001 accuracy 93.55078 wps 19297.51 step time 0.41s\n","# Epoch 7  global step 19760 loss 0.33835 batch 68/3281 lr 0.001 accuracy 93.85938 wps 18716.81 step time 0.34s\n","# Epoch 7  global step 19780 loss 0.33511 batch 88/3281 lr 0.001 accuracy 94.06641 wps 18979.87 step time 0.35s\n","# Epoch 7  global step 19800 loss 0.33685 batch 108/3281 lr 0.001 accuracy 93.80078 wps 18466.97 step time 0.36s\n","# Epoch 7  global step 19820 loss 0.33760 batch 128/3281 lr 0.001 accuracy 93.85937 wps 18646.26 step time 0.39s\n","# Epoch 7  global step 19840 loss 0.31865 batch 148/3281 lr 0.001 accuracy 94.12891 wps 18050.91 step time 0.32s\n","# Epoch 7  global step 19860 loss 0.33519 batch 168/3281 lr 0.001 accuracy 93.95313 wps 19014.58 step time 0.35s\n","# Epoch 7  global step 19880 loss 0.33605 batch 188/3281 lr 0.001 accuracy 94.14062 wps 19348.34 step time 0.37s\n","# Epoch 7  global step 19900 loss 0.32029 batch 208/3281 lr 0.001 accuracy 94.07422 wps 17589.81 step time 0.31s\n","# Epoch 7  global step 19920 loss 0.35132 batch 228/3281 lr 0.001 accuracy 93.67969 wps 19157.99 step time 0.37s\n","# Epoch 7  global step 19940 loss 0.33397 batch 248/3281 lr 0.001 accuracy 93.95703 wps 18999.92 step time 0.35s\n","# Epoch 7  global step 19960 loss 0.33725 batch 268/3281 lr 0.001 accuracy 93.79297 wps 18548.20 step time 0.38s\n","# Epoch 7  global step 19980 loss 0.35670 batch 288/3281 lr 0.001 accuracy 93.42187 wps 19886.92 step time 0.39s\n","# Epoch 7  global step 20000 loss 0.32138 batch 308/3281 lr 0.001 accuracy 94.15625 wps 18643.75 step time 0.35s\n","# global step 20000, eval model at Fri May 22 12:40:39 2020\n","2020-05-22 12:40:41.744786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:40:41.745641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 12:40:41.745762: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 12:40:41.745806: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 12:40:41.745864: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 12:40:41.745903: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 12:40:41.745998: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 12:40:41.746036: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 12:40:41.746074: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 12:40:41.746194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:40:41.746937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:40:41.747430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 12:40:41.747521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 12:40:41.747542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 12:40:41.747556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 12:40:41.747734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:40:41.748327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:40:41.748881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6485067491028794\n","# location_distance_from_business_district - 0.48391662601954266\n","# location_easy_to_find - 0.6827350056623946\n","# service_wait_time - 0.6229155910217997\n","# service_waiters_attitude - 0.7776045390920172\n","# service_parking_convenience - 0.7104226820930954\n","# service_serving_speed - 0.7127539707610029\n","# price_level - 0.7511850622134062\n","# price_cost_effective - 0.6844460035716898\n","# price_discount - 0.6199396252461608\n","# environment_decoration - 0.6870908372944012\n","# environment_noise - 0.7461988189241344\n","# environment_space - 0.745967509386541\n","# environment_cleaness - 0.7336675516125147\n","# dish_portion - 0.6687574184780765\n","# dish_taste - 0.7213931294665201\n","# dish_look - 0.46695787051992965\n","# dish_recommendation - 0.7088207251437909\n","# others_overall_experience - 0.580417838593764\n","# others_willing_to_consume_again - 0.6867152872508863\n","# Eval loss 0.28995, f1 0.67202\n","# current result -0.6720206420727273, previous best result -0.6403337842121118\n","# Epoch 7  global step 20020 loss 0.32474 batch 328/3281 lr 0.001 accuracy 93.98828 wps 18887.64 step time 0.37s\n","# Epoch 7  global step 20040 loss 0.34969 batch 348/3281 lr 0.001 accuracy 93.58984 wps 19131.24 step time 0.45s\n","# Epoch 7  global step 20060 loss 0.34017 batch 368/3281 lr 0.001 accuracy 93.90234 wps 19007.00 step time 0.40s\n","# Epoch 7  global step 20080 loss 0.34008 batch 388/3281 lr 0.001 accuracy 93.68359 wps 18707.92 step time 0.40s\n","# Epoch 7  global step 20100 loss 0.34853 batch 408/3281 lr 0.001 accuracy 93.68750 wps 19127.88 step time 0.43s\n","# Epoch 7  global step 20120 loss 0.35004 batch 428/3281 lr 0.001 accuracy 93.60547 wps 19422.38 step time 0.42s\n","# Epoch 7  global step 20140 loss 0.34276 batch 448/3281 lr 0.001 accuracy 93.77734 wps 19018.30 step time 0.40s\n","# Epoch 7  global step 20160 loss 0.35735 batch 468/3281 lr 0.001 accuracy 93.47656 wps 19977.00 step time 0.39s\n","# Epoch 7  global step 20180 loss 0.35243 batch 488/3281 lr 0.001 accuracy 93.61328 wps 19591.95 step time 0.39s\n","# Epoch 7  global step 20200 loss 0.32515 batch 508/3281 lr 0.001 accuracy 93.96094 wps 18702.56 step time 0.36s\n","# Epoch 7  global step 20220 loss 0.36662 batch 528/3281 lr 0.001 accuracy 93.39844 wps 19546.05 step time 0.43s\n","# Epoch 7  global step 20240 loss 0.33826 batch 548/3281 lr 0.001 accuracy 93.91406 wps 19878.06 step time 0.39s\n","# Epoch 7  global step 20260 loss 0.36528 batch 568/3281 lr 0.001 accuracy 93.47656 wps 19430.37 step time 0.38s\n","# Epoch 7  global step 20280 loss 0.34916 batch 588/3281 lr 0.001 accuracy 93.64062 wps 19353.39 step time 0.37s\n","# Epoch 7  global step 20300 loss 0.33207 batch 608/3281 lr 0.001 accuracy 93.88281 wps 19000.92 step time 0.36s\n","# Epoch 7  global step 20320 loss 0.32639 batch 628/3281 lr 0.001 accuracy 94.24219 wps 17778.27 step time 0.32s\n","# Epoch 7  global step 20340 loss 0.34857 batch 648/3281 lr 0.001 accuracy 93.75391 wps 19432.75 step time 0.42s\n","# Epoch 7  global step 20360 loss 0.33973 batch 668/3281 lr 0.001 accuracy 93.78906 wps 19232.81 step time 0.37s\n","# Epoch 7  global step 20380 loss 0.33162 batch 688/3281 lr 0.001 accuracy 93.81641 wps 19839.38 step time 0.40s\n","# Epoch 7  global step 20400 loss 0.33082 batch 708/3281 lr 0.001 accuracy 93.83984 wps 18194.31 step time 0.34s\n","# Epoch 7  global step 20420 loss 0.34408 batch 728/3281 lr 0.001 accuracy 93.91016 wps 19006.61 step time 0.36s\n","# Epoch 7  global step 20440 loss 0.35882 batch 748/3281 lr 0.001 accuracy 93.43359 wps 18357.54 step time 0.39s\n","# Epoch 7  global step 20460 loss 0.36938 batch 768/3281 lr 0.001 accuracy 93.23437 wps 19938.95 step time 0.40s\n","# Epoch 7  global step 20480 loss 0.33163 batch 788/3281 lr 0.001 accuracy 94.00000 wps 18929.80 step time 0.35s\n","# Epoch 7  global step 20500 loss 0.35591 batch 808/3281 lr 0.001 accuracy 93.48438 wps 19758.18 step time 0.39s\n","# Epoch 7  global step 20520 loss 0.34166 batch 828/3281 lr 0.001 accuracy 93.80469 wps 19074.08 step time 0.42s\n","# Epoch 7  global step 20540 loss 0.34764 batch 848/3281 lr 0.001 accuracy 93.94141 wps 19104.12 step time 0.36s\n","# Epoch 7  global step 20560 loss 0.34247 batch 868/3281 lr 0.001 accuracy 93.73828 wps 18584.51 step time 0.34s\n","# Epoch 7  global step 20580 loss 0.34543 batch 888/3281 lr 0.001 accuracy 93.68359 wps 19888.25 step time 0.38s\n","# Epoch 7  global step 20600 loss 0.32682 batch 908/3281 lr 0.001 accuracy 94.16016 wps 18312.86 step time 0.33s\n","# Epoch 7  global step 20620 loss 0.37214 batch 928/3281 lr 0.001 accuracy 93.31641 wps 19616.19 step time 0.42s\n","# Epoch 7  global step 20640 loss 0.32792 batch 948/3281 lr 0.001 accuracy 94.10156 wps 18577.24 step time 0.39s\n","# Epoch 7  global step 20660 loss 0.32066 batch 968/3281 lr 0.001 accuracy 94.09766 wps 18242.58 step time 0.34s\n","# Epoch 7  global step 20680 loss 0.33561 batch 988/3281 lr 0.001 accuracy 93.96484 wps 18321.03 step time 0.34s\n","# Epoch 7  global step 20700 loss 0.31847 batch 1008/3281 lr 0.001 accuracy 94.30078 wps 19187.61 step time 0.36s\n","# Epoch 7  global step 20720 loss 0.33823 batch 1028/3281 lr 0.001 accuracy 93.93359 wps 19234.69 step time 0.37s\n","# Epoch 7  global step 20740 loss 0.34061 batch 1048/3281 lr 0.001 accuracy 93.72656 wps 18548.80 step time 0.40s\n","# Epoch 7  global step 20760 loss 0.35471 batch 1068/3281 lr 0.001 accuracy 93.41406 wps 18680.07 step time 0.40s\n","# Epoch 7  global step 20780 loss 0.32832 batch 1088/3281 lr 0.001 accuracy 93.91797 wps 18318.91 step time 0.38s\n","# Epoch 7  global step 20800 loss 0.34092 batch 1108/3281 lr 0.001 accuracy 93.80469 wps 18493.27 step time 0.35s\n","# Epoch 7  global step 20820 loss 0.34591 batch 1128/3281 lr 0.001 accuracy 93.63281 wps 19265.52 step time 0.36s\n","# Epoch 7  global step 20840 loss 0.33676 batch 1148/3281 lr 0.001 accuracy 93.94922 wps 18411.62 step time 0.33s\n","# Epoch 7  global step 20860 loss 0.33423 batch 1168/3281 lr 0.001 accuracy 94.01953 wps 18620.86 step time 0.35s\n","# Epoch 7  global step 20880 loss 0.36259 batch 1188/3281 lr 0.001 accuracy 93.17187 wps 19384.45 step time 0.42s\n","# Epoch 7  global step 20900 loss 0.34577 batch 1208/3281 lr 0.001 accuracy 93.56250 wps 20175.96 step time 0.42s\n","# Epoch 7  global step 20920 loss 0.32690 batch 1228/3281 lr 0.001 accuracy 94.26953 wps 17492.28 step time 0.31s\n","# Epoch 7  global step 20940 loss 0.35188 batch 1248/3281 lr 0.001 accuracy 93.56250 wps 20243.63 step time 0.40s\n","# Epoch 7  global step 20960 loss 0.33718 batch 1268/3281 lr 0.001 accuracy 93.85156 wps 18735.68 step time 0.39s\n","# Epoch 7  global step 20980 loss 0.32966 batch 1288/3281 lr 0.001 accuracy 93.78906 wps 18426.57 step time 0.44s\n","# Epoch 7  global step 21000 loss 0.35349 batch 1308/3281 lr 0.001 accuracy 93.58594 wps 18737.63 step time 0.35s\n","# Epoch 7  global step 21020 loss 0.31861 batch 1328/3281 lr 0.001 accuracy 94.19531 wps 17833.50 step time 0.32s\n","# Epoch 7  global step 21040 loss 0.34159 batch 1348/3281 lr 0.001 accuracy 93.80469 wps 19514.07 step time 0.37s\n","# Epoch 7  global step 21060 loss 0.33535 batch 1368/3281 lr 0.001 accuracy 93.90625 wps 18800.37 step time 0.36s\n","# Epoch 7  global step 21080 loss 0.32578 batch 1388/3281 lr 0.001 accuracy 94.08203 wps 18489.41 step time 0.35s\n","# Epoch 7  global step 21100 loss 0.31954 batch 1408/3281 lr 0.001 accuracy 94.42578 wps 18078.75 step time 0.32s\n","# Epoch 7  global step 21120 loss 0.34133 batch 1428/3281 lr 0.001 accuracy 93.89844 wps 19468.38 step time 0.42s\n","# Epoch 7  global step 21140 loss 0.35272 batch 1448/3281 lr 0.001 accuracy 93.60938 wps 19237.22 step time 0.37s\n","# Epoch 7  global step 21160 loss 0.33512 batch 1468/3281 lr 0.001 accuracy 93.85156 wps 18123.35 step time 0.37s\n","# Epoch 7  global step 21180 loss 0.33691 batch 1488/3281 lr 0.001 accuracy 94.00781 wps 18902.16 step time 0.35s\n","# Epoch 7  global step 21200 loss 0.32410 batch 1508/3281 lr 0.001 accuracy 94.14453 wps 17837.20 step time 0.37s\n","# Epoch 7  global step 21220 loss 0.34812 batch 1528/3281 lr 0.001 accuracy 93.78516 wps 17882.04 step time 0.42s\n","# Epoch 7  global step 21240 loss 0.35076 batch 1548/3281 lr 0.001 accuracy 93.63672 wps 17487.75 step time 0.40s\n","# Epoch 7  global step 21260 loss 0.34697 batch 1568/3281 lr 0.001 accuracy 93.62109 wps 16079.24 step time 0.45s\n","# Epoch 7  global step 21280 loss 0.33084 batch 1588/3281 lr 0.001 accuracy 94.02344 wps 16540.24 step time 0.37s\n","# Epoch 7  global step 21300 loss 0.33920 batch 1608/3281 lr 0.001 accuracy 93.80078 wps 17273.39 step time 0.36s\n","# Epoch 7  global step 21320 loss 0.37986 batch 1628/3281 lr 0.001 accuracy 93.05469 wps 18148.54 step time 0.49s\n","# Epoch 7  global step 21340 loss 0.31351 batch 1648/3281 lr 0.001 accuracy 94.16797 wps 16183.95 step time 0.34s\n","# Epoch 7  global step 21360 loss 0.35077 batch 1668/3281 lr 0.001 accuracy 93.49219 wps 17108.01 step time 0.47s\n","# Epoch 7  global step 21380 loss 0.35340 batch 1688/3281 lr 0.001 accuracy 93.50000 wps 18122.61 step time 0.42s\n","# Epoch 7  global step 21400 loss 0.34260 batch 1708/3281 lr 0.001 accuracy 93.85156 wps 17156.53 step time 0.43s\n","# Epoch 7  global step 21420 loss 0.36210 batch 1728/3281 lr 0.001 accuracy 93.42578 wps 17021.73 step time 0.52s\n","# Epoch 7  global step 21440 loss 0.33337 batch 1748/3281 lr 0.001 accuracy 93.93750 wps 17364.53 step time 0.41s\n","# Epoch 7  global step 21460 loss 0.34927 batch 1768/3281 lr 0.001 accuracy 93.60937 wps 17800.43 step time 0.46s\n","# Epoch 7  global step 21480 loss 0.35709 batch 1788/3281 lr 0.001 accuracy 93.44141 wps 17057.75 step time 0.50s\n","# Epoch 7  global step 21500 loss 0.33894 batch 1808/3281 lr 0.001 accuracy 93.64453 wps 16585.12 step time 0.49s\n","# Epoch 7  global step 21520 loss 0.35912 batch 1828/3281 lr 0.001 accuracy 93.45312 wps 15661.12 step time 0.54s\n","# Epoch 7  global step 21540 loss 0.33414 batch 1848/3281 lr 0.001 accuracy 94.06250 wps 17253.97 step time 0.37s\n","# Epoch 7  global step 21560 loss 0.34778 batch 1868/3281 lr 0.001 accuracy 93.75000 wps 17432.23 step time 0.37s\n","# Epoch 7  global step 21580 loss 0.33927 batch 1888/3281 lr 0.001 accuracy 94.02344 wps 17325.59 step time 0.38s\n","# Epoch 7  global step 21600 loss 0.34521 batch 1908/3281 lr 0.001 accuracy 93.64844 wps 17522.87 step time 0.45s\n","# Epoch 7  global step 21620 loss 0.33269 batch 1928/3281 lr 0.001 accuracy 94.00781 wps 16210.89 step time 0.41s\n","# Epoch 7  global step 21640 loss 0.36325 batch 1948/3281 lr 0.001 accuracy 93.28516 wps 16497.08 step time 0.53s\n","# Epoch 7  global step 21660 loss 0.35945 batch 1968/3281 lr 0.001 accuracy 93.59766 wps 16606.62 step time 0.43s\n","# Epoch 7  global step 21680 loss 0.36452 batch 1988/3281 lr 0.001 accuracy 93.41016 wps 16383.03 step time 0.47s\n","# Epoch 7  global step 21700 loss 0.35690 batch 2008/3281 lr 0.001 accuracy 93.39844 wps 17744.30 step time 0.48s\n","# Epoch 7  global step 21720 loss 0.34286 batch 2028/3281 lr 0.001 accuracy 93.59375 wps 16730.81 step time 0.43s\n","# Epoch 7  global step 21740 loss 0.33373 batch 2048/3281 lr 0.001 accuracy 94.00781 wps 16142.59 step time 0.41s\n","# Epoch 7  global step 21760 loss 0.33941 batch 2068/3281 lr 0.001 accuracy 93.85156 wps 16772.89 step time 0.43s\n","# Epoch 7  global step 21780 loss 0.34360 batch 2088/3281 lr 0.001 accuracy 93.65625 wps 16973.56 step time 0.45s\n","# Epoch 7  global step 21800 loss 0.34961 batch 2108/3281 lr 0.001 accuracy 93.68359 wps 16314.51 step time 0.45s\n","# Epoch 7  global step 21820 loss 0.35011 batch 2128/3281 lr 0.001 accuracy 93.66016 wps 17053.18 step time 0.43s\n","# Epoch 7  global step 21840 loss 0.34096 batch 2148/3281 lr 0.001 accuracy 93.85937 wps 16894.66 step time 0.43s\n","# Epoch 7  global step 21860 loss 0.33581 batch 2168/3281 lr 0.001 accuracy 93.87500 wps 16840.70 step time 0.38s\n","# Epoch 7  global step 21880 loss 0.33428 batch 2188/3281 lr 0.001 accuracy 93.87891 wps 16958.70 step time 0.39s\n","# Epoch 7  global step 21900 loss 0.34675 batch 2208/3281 lr 0.001 accuracy 93.71875 wps 15851.13 step time 0.43s\n","# Epoch 7  global step 21920 loss 0.35896 batch 2228/3281 lr 0.001 accuracy 93.56641 wps 17303.54 step time 0.47s\n","# Epoch 7  global step 21940 loss 0.34003 batch 2248/3281 lr 0.001 accuracy 93.82422 wps 17544.85 step time 0.41s\n","# Epoch 7  global step 21960 loss 0.34166 batch 2268/3281 lr 0.001 accuracy 93.74219 wps 16674.87 step time 0.38s\n","# Epoch 7  global step 21980 loss 0.34215 batch 2288/3281 lr 0.001 accuracy 93.82813 wps 16648.87 step time 0.49s\n","# Epoch 7  global step 22000 loss 0.36282 batch 2308/3281 lr 0.001 accuracy 93.40625 wps 16540.73 step time 0.51s\n","# global step 22000, eval model at Fri May 22 12:55:09 2020\n","2020-05-22 12:55:11.455885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:55:11.456514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 12:55:11.456640: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 12:55:11.456718: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 12:55:11.456762: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 12:55:11.456808: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 12:55:11.456855: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 12:55:11.456895: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 12:55:11.456963: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 12:55:11.457124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:55:11.457814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:55:11.458378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 12:55:11.458443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 12:55:11.458465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 12:55:11.458479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 12:55:11.458682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:55:11.459269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 12:55:11.459808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6453089917204209\n","# location_distance_from_business_district - 0.50126573075487\n","# location_easy_to_find - 0.6893906517827173\n","# service_wait_time - 0.6467214058018682\n","# service_waiters_attitude - 0.7852116052450755\n","# service_parking_convenience - 0.7245015274085871\n","# service_serving_speed - 0.7358159703158814\n","# price_level - 0.7690011295836691\n","# price_cost_effective - 0.7001861456888929\n","# price_discount - 0.6328952851788712\n","# environment_decoration - 0.7044427074555112\n","# environment_noise - 0.7514027377970247\n","# environment_space - 0.7521568125479945\n","# environment_cleaness - 0.7425171796718506\n","# dish_portion - 0.6922795232746191\n","# dish_taste - 0.7277486063120414\n","# dish_look - 0.5083764534458601\n","# dish_recommendation - 0.7261903515064247\n","# others_overall_experience - 0.5826829956999664\n","# others_willing_to_consume_again - 0.6963699177718601\n","# Eval loss 0.27685, f1 0.68572\n","# current result -0.6857232864482004, previous best result -0.6720206420727273\n","# Epoch 7  global step 22020 loss 0.34708 batch 2328/3281 lr 0.001 accuracy 93.48047 wps 17367.26 step time 0.48s\n","# Epoch 7  global step 22040 loss 0.35735 batch 2348/3281 lr 0.001 accuracy 93.64062 wps 17063.51 step time 0.40s\n","# Epoch 7  global step 22060 loss 0.34232 batch 2368/3281 lr 0.001 accuracy 93.83203 wps 17831.45 step time 0.42s\n","# Epoch 7  global step 22080 loss 0.36073 batch 2388/3281 lr 0.001 accuracy 93.43359 wps 16347.42 step time 0.44s\n","# Epoch 7  global step 22100 loss 0.34247 batch 2408/3281 lr 0.001 accuracy 93.96094 wps 16984.01 step time 0.41s\n","# Epoch 7  global step 22120 loss 0.35487 batch 2428/3281 lr 0.001 accuracy 93.42578 wps 17447.03 step time 0.45s\n","# Epoch 7  global step 22140 loss 0.34865 batch 2448/3281 lr 0.001 accuracy 93.62500 wps 17050.35 step time 0.43s\n","# Epoch 7  global step 22160 loss 0.32671 batch 2468/3281 lr 0.001 accuracy 94.14062 wps 16881.29 step time 0.35s\n","# Epoch 7  global step 22180 loss 0.32800 batch 2488/3281 lr 0.001 accuracy 94.14453 wps 16703.32 step time 0.40s\n","# Epoch 7  global step 22200 loss 0.34639 batch 2508/3281 lr 0.001 accuracy 93.67188 wps 16322.31 step time 0.46s\n","# Epoch 7  global step 22220 loss 0.35612 batch 2528/3281 lr 0.001 accuracy 93.28125 wps 17371.41 step time 0.46s\n","# Epoch 7  global step 22240 loss 0.32329 batch 2548/3281 lr 0.001 accuracy 94.07422 wps 16332.67 step time 0.39s\n","# Epoch 7  global step 22260 loss 0.36196 batch 2568/3281 lr 0.001 accuracy 93.40234 wps 16975.48 step time 0.45s\n","# Epoch 7  global step 22280 loss 0.33635 batch 2588/3281 lr 0.001 accuracy 94.12109 wps 15960.48 step time 0.40s\n","# Epoch 7  global step 22300 loss 0.33385 batch 2608/3281 lr 0.001 accuracy 93.83984 wps 16827.65 step time 0.34s\n","# Epoch 7  global step 22320 loss 0.32407 batch 2628/3281 lr 0.001 accuracy 94.13672 wps 16417.27 step time 0.45s\n","# Epoch 7  global step 22340 loss 0.35581 batch 2648/3281 lr 0.001 accuracy 93.42578 wps 16432.30 step time 0.52s\n","# Epoch 7  global step 22360 loss 0.34913 batch 2668/3281 lr 0.001 accuracy 93.71484 wps 17548.52 step time 0.42s\n","# Epoch 7  global step 22380 loss 0.35934 batch 2688/3281 lr 0.001 accuracy 93.37891 wps 16747.28 step time 0.48s\n","# Epoch 7  global step 22400 loss 0.34377 batch 2708/3281 lr 0.001 accuracy 93.71094 wps 16529.03 step time 0.46s\n","# Epoch 7  global step 22420 loss 0.34743 batch 2728/3281 lr 0.001 accuracy 93.84766 wps 16090.63 step time 0.38s\n","# Epoch 7  global step 22440 loss 0.34488 batch 2748/3281 lr 0.001 accuracy 93.73047 wps 16787.94 step time 0.44s\n","# Epoch 7  global step 22460 loss 0.34056 batch 2768/3281 lr 0.001 accuracy 93.79687 wps 16173.69 step time 0.50s\n","# Epoch 7  global step 22480 loss 0.34930 batch 2788/3281 lr 0.001 accuracy 93.62891 wps 17250.16 step time 0.37s\n","# Epoch 7  global step 22500 loss 0.35124 batch 2808/3281 lr 0.001 accuracy 93.74219 wps 16310.31 step time 0.50s\n","# Epoch 7  global step 22520 loss 0.36403 batch 2828/3281 lr 0.001 accuracy 93.33984 wps 17182.04 step time 0.46s\n","# Epoch 7  global step 22540 loss 0.32151 batch 2848/3281 lr 0.001 accuracy 94.31641 wps 16673.33 step time 0.36s\n","# Epoch 7  global step 22560 loss 0.36164 batch 2868/3281 lr 0.001 accuracy 93.32031 wps 16962.01 step time 0.55s\n","# Epoch 7  global step 22580 loss 0.34020 batch 2888/3281 lr 0.001 accuracy 93.66406 wps 17733.67 step time 0.44s\n","# Epoch 7  global step 22600 loss 0.35561 batch 2908/3281 lr 0.001 accuracy 93.47266 wps 17172.79 step time 0.40s\n","# Epoch 7  global step 22620 loss 0.34345 batch 2928/3281 lr 0.001 accuracy 93.63672 wps 16802.98 step time 0.44s\n","# Epoch 7  global step 22640 loss 0.33956 batch 2948/3281 lr 0.001 accuracy 93.99609 wps 15965.35 step time 0.45s\n","# Epoch 7  global step 22660 loss 0.34682 batch 2968/3281 lr 0.001 accuracy 93.67187 wps 17402.62 step time 0.46s\n","# Epoch 7  global step 22680 loss 0.34946 batch 2988/3281 lr 0.001 accuracy 93.89062 wps 16734.74 step time 0.39s\n","# Epoch 7  global step 22700 loss 0.35531 batch 3008/3281 lr 0.001 accuracy 93.44922 wps 18273.74 step time 0.40s\n","# Epoch 7  global step 22720 loss 0.36016 batch 3028/3281 lr 0.001 accuracy 93.33984 wps 17827.59 step time 0.42s\n","# Epoch 7  global step 22740 loss 0.35186 batch 3048/3281 lr 0.001 accuracy 93.65625 wps 17104.21 step time 0.40s\n","# Epoch 7  global step 22760 loss 0.32311 batch 3068/3281 lr 0.001 accuracy 94.27344 wps 15908.77 step time 0.40s\n","# Epoch 7  global step 22780 loss 0.33401 batch 3088/3281 lr 0.001 accuracy 94.05859 wps 17117.21 step time 0.38s\n","# Epoch 7  global step 22800 loss 0.33279 batch 3108/3281 lr 0.001 accuracy 93.83984 wps 16956.92 step time 0.40s\n","# Epoch 7  global step 22820 loss 0.33114 batch 3128/3281 lr 0.001 accuracy 94.01172 wps 17178.71 step time 0.38s\n","# Epoch 7  global step 22840 loss 0.35911 batch 3148/3281 lr 0.001 accuracy 93.41016 wps 16694.35 step time 0.51s\n","# Epoch 7  global step 22860 loss 0.35396 batch 3168/3281 lr 0.001 accuracy 93.41406 wps 16247.89 step time 0.51s\n","# Epoch 7  global step 22880 loss 0.34736 batch 3188/3281 lr 0.001 accuracy 93.66016 wps 15852.82 step time 0.59s\n","# Epoch 7  global step 22900 loss 0.37805 batch 3208/3281 lr 0.001 accuracy 93.21484 wps 17105.78 step time 0.44s\n","# Epoch 7  global step 22920 loss 0.34878 batch 3228/3281 lr 0.001 accuracy 93.68359 wps 16720.70 step time 0.44s\n","# Epoch 7  global step 22940 loss 0.34380 batch 3248/3281 lr 0.001 accuracy 93.69141 wps 18337.21 step time 0.44s\n","# Epoch 7  global step 22960 loss 0.34648 batch 3268/3281 lr 0.001 accuracy 93.66406 wps 16382.69 step time 0.44s\n","# Finsh epoch 7, global step 22974\n","# Epoch 8  global step 22980 loss 0.10712 batch 6/3281 lr 0.001 accuracy 28.11719 wps 17529.08 step time 0.14s\n","# Epoch 8  global step 23000 loss 0.32572 batch 26/3281 lr 0.001 accuracy 94.14453 wps 18831.16 step time 0.35s\n","# Epoch 8  global step 23020 loss 0.32840 batch 46/3281 lr 0.001 accuracy 94.03125 wps 18839.10 step time 0.41s\n","# Epoch 8  global step 23040 loss 0.32999 batch 66/3281 lr 0.001 accuracy 94.01172 wps 19276.55 step time 0.36s\n","# Epoch 8  global step 23060 loss 0.33533 batch 86/3281 lr 0.001 accuracy 93.63672 wps 18769.90 step time 0.36s\n","# Epoch 8  global step 23080 loss 0.33748 batch 106/3281 lr 0.001 accuracy 93.94141 wps 18823.49 step time 0.40s\n","# Epoch 8  global step 23100 loss 0.33514 batch 126/3281 lr 0.001 accuracy 93.75391 wps 18584.32 step time 0.54s\n","# Epoch 8  global step 23120 loss 0.33902 batch 146/3281 lr 0.001 accuracy 93.81641 wps 19003.66 step time 0.46s\n","# Epoch 8  global step 23140 loss 0.33305 batch 166/3281 lr 0.001 accuracy 94.03906 wps 18869.63 step time 0.35s\n","# Epoch 8  global step 23160 loss 0.32263 batch 186/3281 lr 0.001 accuracy 94.06250 wps 19238.80 step time 0.36s\n","# Epoch 8  global step 23180 loss 0.32883 batch 206/3281 lr 0.001 accuracy 93.98438 wps 18861.56 step time 0.35s\n","# Epoch 8  global step 23200 loss 0.32838 batch 226/3281 lr 0.001 accuracy 94.01953 wps 17956.32 step time 0.38s\n","# Epoch 8  global step 23220 loss 0.33566 batch 246/3281 lr 0.001 accuracy 93.85937 wps 18494.38 step time 0.33s\n","# Epoch 8  global step 23240 loss 0.32455 batch 266/3281 lr 0.001 accuracy 94.15234 wps 19241.77 step time 0.37s\n","# Epoch 8  global step 23260 loss 0.35054 batch 286/3281 lr 0.001 accuracy 93.56250 wps 19816.10 step time 0.40s\n","# Epoch 8  global step 23280 loss 0.33779 batch 306/3281 lr 0.001 accuracy 93.83594 wps 19695.55 step time 0.38s\n","# Epoch 8  global step 23300 loss 0.34028 batch 326/3281 lr 0.001 accuracy 93.63281 wps 19251.90 step time 0.37s\n","# Epoch 8  global step 23320 loss 0.34415 batch 346/3281 lr 0.001 accuracy 93.76172 wps 18321.31 step time 0.44s\n","# Epoch 8  global step 23340 loss 0.33184 batch 366/3281 lr 0.001 accuracy 94.06250 wps 18867.58 step time 0.35s\n","# Epoch 8  global step 23360 loss 0.33947 batch 386/3281 lr 0.001 accuracy 93.72266 wps 18722.07 step time 0.35s\n","# Epoch 8  global step 23380 loss 0.33748 batch 406/3281 lr 0.001 accuracy 93.69141 wps 19520.44 step time 0.44s\n","# Epoch 8  global step 23400 loss 0.33588 batch 426/3281 lr 0.001 accuracy 93.80469 wps 19651.71 step time 0.38s\n","# Epoch 8  global step 23420 loss 0.32368 batch 446/3281 lr 0.001 accuracy 94.17578 wps 17653.75 step time 0.32s\n","# Epoch 8  global step 23440 loss 0.31133 batch 466/3281 lr 0.001 accuracy 94.40234 wps 18528.66 step time 0.34s\n","# Epoch 8  global step 23460 loss 0.32722 batch 486/3281 lr 0.001 accuracy 93.92187 wps 19145.63 step time 0.36s\n","# Epoch 8  global step 23480 loss 0.30878 batch 506/3281 lr 0.001 accuracy 94.36328 wps 18310.77 step time 0.33s\n","# Epoch 8  global step 23500 loss 0.34067 batch 526/3281 lr 0.001 accuracy 93.78516 wps 19644.94 step time 0.42s\n","# Epoch 8  global step 23520 loss 0.30345 batch 546/3281 lr 0.001 accuracy 94.36328 wps 19303.85 step time 0.36s\n","# Epoch 8  global step 23540 loss 0.31359 batch 566/3281 lr 0.001 accuracy 94.34766 wps 19023.62 step time 0.40s\n","# Epoch 8  global step 23560 loss 0.32420 batch 586/3281 lr 0.001 accuracy 94.10156 wps 18095.49 step time 0.32s\n","# Epoch 8  global step 23580 loss 0.35874 batch 606/3281 lr 0.001 accuracy 93.47656 wps 19825.14 step time 0.40s\n","# Epoch 8  global step 23600 loss 0.32502 batch 626/3281 lr 0.001 accuracy 94.11328 wps 18873.60 step time 0.36s\n","# Epoch 8  global step 23620 loss 0.34152 batch 646/3281 lr 0.001 accuracy 93.86719 wps 16218.45 step time 0.42s\n","# Epoch 8  global step 23640 loss 0.33279 batch 666/3281 lr 0.001 accuracy 93.79297 wps 17356.53 step time 0.43s\n","# Epoch 8  global step 23660 loss 0.33711 batch 686/3281 lr 0.001 accuracy 93.84375 wps 16861.04 step time 0.43s\n","# Epoch 8  global step 23680 loss 0.34343 batch 706/3281 lr 0.001 accuracy 93.80469 wps 17597.01 step time 0.40s\n","# Epoch 8  global step 23700 loss 0.34204 batch 726/3281 lr 0.001 accuracy 93.79687 wps 17395.60 step time 0.52s\n","# Epoch 8  global step 23720 loss 0.33837 batch 746/3281 lr 0.001 accuracy 93.80078 wps 17412.92 step time 0.49s\n","# Epoch 8  global step 23740 loss 0.34238 batch 766/3281 lr 0.001 accuracy 93.70313 wps 17513.17 step time 0.41s\n","# Epoch 8  global step 23760 loss 0.32493 batch 786/3281 lr 0.001 accuracy 94.08203 wps 16797.94 step time 0.48s\n","# Epoch 8  global step 23780 loss 0.33607 batch 806/3281 lr 0.001 accuracy 93.90234 wps 17068.22 step time 0.38s\n","# Epoch 8  global step 23800 loss 0.32075 batch 826/3281 lr 0.001 accuracy 94.03516 wps 16556.25 step time 0.37s\n","# Epoch 8  global step 23820 loss 0.32303 batch 846/3281 lr 0.001 accuracy 94.06641 wps 16985.62 step time 0.38s\n","# Epoch 8  global step 23840 loss 0.32625 batch 866/3281 lr 0.001 accuracy 94.07031 wps 16945.56 step time 0.34s\n","# Epoch 8  global step 23860 loss 0.33761 batch 886/3281 lr 0.001 accuracy 93.92969 wps 16930.29 step time 0.44s\n","# Epoch 8  global step 23880 loss 0.34272 batch 906/3281 lr 0.001 accuracy 93.58203 wps 16938.97 step time 0.47s\n","# Epoch 8  global step 23900 loss 0.35077 batch 926/3281 lr 0.001 accuracy 93.76563 wps 15809.24 step time 0.51s\n","# Epoch 8  global step 23920 loss 0.31598 batch 946/3281 lr 0.001 accuracy 94.20312 wps 16060.25 step time 0.38s\n","# Epoch 8  global step 23940 loss 0.35599 batch 966/3281 lr 0.001 accuracy 93.52344 wps 16237.26 step time 0.52s\n","# Epoch 8  global step 23960 loss 0.34506 batch 986/3281 lr 0.001 accuracy 93.68359 wps 16814.62 step time 0.43s\n","# Epoch 8  global step 23980 loss 0.34575 batch 1006/3281 lr 0.001 accuracy 93.67969 wps 16911.01 step time 0.54s\n","# Epoch 8  global step 24000 loss 0.32936 batch 1026/3281 lr 0.001 accuracy 94.05469 wps 17134.28 step time 0.38s\n","# global step 24000, eval model at Fri May 22 13:10:13 2020\n","2020-05-22 13:10:16.086454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:10:16.087174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 13:10:16.090449: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 13:10:16.090506: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 13:10:16.090553: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 13:10:16.090596: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 13:10:16.090639: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 13:10:16.090695: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 13:10:16.090739: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 13:10:16.090875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:10:16.091514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:10:16.092053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 13:10:16.092183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 13:10:16.092205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 13:10:16.092220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 13:10:16.092413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:10:16.093068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:10:16.093699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6527856767509552\n","# location_distance_from_business_district - 0.5026741959618538\n","# location_easy_to_find - 0.6973803251726778\n","# service_wait_time - 0.6572797299123662\n","# service_waiters_attitude - 0.7896231949211587\n","# service_parking_convenience - 0.7312593907019254\n","# service_serving_speed - 0.7493063384945431\n","# price_level - 0.7737183957886096\n","# price_cost_effective - 0.7047756248177927\n","# price_discount - 0.644269622017211\n","# environment_decoration - 0.7126845379580928\n","# environment_noise - 0.7574321799271896\n","# environment_space - 0.7544680711720877\n","# environment_cleaness - 0.7475601844767381\n","# dish_portion - 0.7068549132775931\n","# dish_taste - 0.732591148793619\n","# dish_look - 0.5337066056138792\n","# dish_recommendation - 0.7362746670981458\n","# others_overall_experience - 0.5834273606803076\n","# others_willing_to_consume_again - 0.7001772483380926\n","# Eval loss 0.26943, f1 0.69341\n","# current result -0.693412470593742, previous best result -0.6857232864482004\n","# Epoch 8  global step 24020 loss 0.35772 batch 1046/3281 lr 0.001 accuracy 93.53516 wps 16617.73 step time 0.45s\n","# Epoch 8  global step 24040 loss 0.35027 batch 1066/3281 lr 0.001 accuracy 93.67188 wps 16961.93 step time 0.56s\n","# Epoch 8  global step 24060 loss 0.33461 batch 1086/3281 lr 0.001 accuracy 94.06250 wps 17299.07 step time 0.39s\n","# Epoch 8  global step 24080 loss 0.33722 batch 1106/3281 lr 0.001 accuracy 93.94141 wps 15781.40 step time 0.59s\n","# Epoch 8  global step 24100 loss 0.33444 batch 1126/3281 lr 0.001 accuracy 93.92969 wps 16800.11 step time 0.39s\n","# Epoch 8  global step 24120 loss 0.32619 batch 1146/3281 lr 0.001 accuracy 94.09766 wps 17174.02 step time 0.39s\n","# Epoch 8  global step 24140 loss 0.33510 batch 1166/3281 lr 0.001 accuracy 93.79688 wps 17596.43 step time 0.43s\n","# Epoch 8  global step 24160 loss 0.34137 batch 1186/3281 lr 0.001 accuracy 93.75781 wps 15971.36 step time 0.49s\n","# Epoch 8  global step 24180 loss 0.34601 batch 1206/3281 lr 0.001 accuracy 93.65234 wps 17920.71 step time 0.44s\n","# Epoch 8  global step 24200 loss 0.31880 batch 1226/3281 lr 0.001 accuracy 94.22656 wps 16961.37 step time 0.38s\n","# Epoch 8  global step 24220 loss 0.32349 batch 1246/3281 lr 0.001 accuracy 94.16797 wps 16725.76 step time 0.37s\n","# Epoch 8  global step 24240 loss 0.33123 batch 1266/3281 lr 0.001 accuracy 94.11719 wps 17236.13 step time 0.40s\n","# Epoch 8  global step 24260 loss 0.31317 batch 1286/3281 lr 0.001 accuracy 94.25391 wps 17027.06 step time 0.38s\n","# Epoch 8  global step 24280 loss 0.34317 batch 1306/3281 lr 0.001 accuracy 93.78906 wps 16471.04 step time 0.44s\n","# Epoch 8  global step 24300 loss 0.32939 batch 1326/3281 lr 0.001 accuracy 94.00391 wps 17286.51 step time 0.36s\n","# Epoch 8  global step 24320 loss 0.34987 batch 1346/3281 lr 0.001 accuracy 93.74609 wps 16573.34 step time 0.50s\n","# Epoch 8  global step 24340 loss 0.33541 batch 1366/3281 lr 0.001 accuracy 93.94141 wps 16701.52 step time 0.46s\n","# Epoch 8  global step 24360 loss 0.34532 batch 1386/3281 lr 0.001 accuracy 93.81641 wps 16821.55 step time 0.46s\n","# Epoch 8  global step 24380 loss 0.35078 batch 1406/3281 lr 0.001 accuracy 93.59766 wps 16852.64 step time 0.50s\n","# Epoch 8  global step 24400 loss 0.35526 batch 1426/3281 lr 0.001 accuracy 93.48828 wps 15872.13 step time 0.56s\n","# Epoch 8  global step 24420 loss 0.36520 batch 1446/3281 lr 0.001 accuracy 93.31250 wps 16965.36 step time 0.46s\n","# Epoch 8  global step 24440 loss 0.32217 batch 1466/3281 lr 0.001 accuracy 94.08203 wps 16238.08 step time 0.41s\n","# Epoch 8  global step 24460 loss 0.35259 batch 1486/3281 lr 0.001 accuracy 93.73047 wps 16081.02 step time 0.58s\n","# Epoch 8  global step 24480 loss 0.33415 batch 1506/3281 lr 0.001 accuracy 93.90234 wps 17327.21 step time 0.40s\n","# Epoch 8  global step 24500 loss 0.34771 batch 1526/3281 lr 0.001 accuracy 93.67578 wps 17569.96 step time 0.43s\n","# Epoch 8  global step 24520 loss 0.32933 batch 1546/3281 lr 0.001 accuracy 93.99219 wps 16030.51 step time 0.43s\n","# Epoch 8  global step 24540 loss 0.34864 batch 1566/3281 lr 0.001 accuracy 93.42578 wps 16373.33 step time 0.44s\n","# Epoch 8  global step 24560 loss 0.33540 batch 1586/3281 lr 0.001 accuracy 93.79687 wps 18033.28 step time 0.43s\n","# Epoch 8  global step 24580 loss 0.30429 batch 1606/3281 lr 0.001 accuracy 94.54297 wps 16661.77 step time 0.36s\n","# Epoch 8  global step 24600 loss 0.34133 batch 1626/3281 lr 0.001 accuracy 93.85937 wps 16392.92 step time 0.44s\n","# Epoch 8  global step 24620 loss 0.33505 batch 1646/3281 lr 0.001 accuracy 93.85937 wps 15962.45 step time 0.46s\n","# Epoch 8  global step 24640 loss 0.32910 batch 1666/3281 lr 0.001 accuracy 93.88672 wps 17014.63 step time 0.44s\n","# Epoch 8  global step 24660 loss 0.33173 batch 1686/3281 lr 0.001 accuracy 93.99609 wps 17230.28 step time 0.40s\n","# Epoch 8  global step 24680 loss 0.35182 batch 1706/3281 lr 0.001 accuracy 93.46484 wps 17890.19 step time 0.42s\n","# Epoch 8  global step 24700 loss 0.30846 batch 1726/3281 lr 0.001 accuracy 94.55469 wps 16511.85 step time 0.36s\n","# Epoch 8  global step 24720 loss 0.32860 batch 1746/3281 lr 0.001 accuracy 93.93359 wps 16080.37 step time 0.45s\n","# Epoch 8  global step 24740 loss 0.33267 batch 1766/3281 lr 0.001 accuracy 94.04687 wps 16828.31 step time 0.35s\n","# Epoch 8  global step 24760 loss 0.31153 batch 1786/3281 lr 0.001 accuracy 94.46875 wps 16250.81 step time 0.36s\n","# Epoch 8  global step 24780 loss 0.34308 batch 1806/3281 lr 0.001 accuracy 93.56250 wps 17336.60 step time 0.43s\n","# Epoch 8  global step 24800 loss 0.33054 batch 1826/3281 lr 0.001 accuracy 94.05078 wps 16931.28 step time 0.35s\n","# Epoch 8  global step 24820 loss 0.34487 batch 1846/3281 lr 0.001 accuracy 93.67969 wps 18164.24 step time 0.43s\n","# Epoch 8  global step 24840 loss 0.32135 batch 1866/3281 lr 0.001 accuracy 94.30469 wps 17493.93 step time 0.38s\n","# Epoch 8  global step 24860 loss 0.33644 batch 1886/3281 lr 0.001 accuracy 93.85547 wps 17567.38 step time 0.37s\n","# Epoch 8  global step 24880 loss 0.33465 batch 1906/3281 lr 0.001 accuracy 93.89453 wps 16776.44 step time 0.40s\n","# Epoch 8  global step 24900 loss 0.34774 batch 1926/3281 lr 0.001 accuracy 93.58594 wps 16991.02 step time 0.51s\n","# Epoch 8  global step 24920 loss 0.33474 batch 1946/3281 lr 0.001 accuracy 93.64063 wps 17390.63 step time 0.42s\n","# Epoch 8  global step 24940 loss 0.35288 batch 1966/3281 lr 0.001 accuracy 93.36328 wps 17021.61 step time 0.52s\n","# Epoch 8  global step 24960 loss 0.36259 batch 1986/3281 lr 0.001 accuracy 93.40234 wps 16411.15 step time 0.51s\n","# Epoch 8  global step 24980 loss 0.32026 batch 2006/3281 lr 0.001 accuracy 94.12891 wps 16553.33 step time 0.36s\n","# Epoch 8  global step 25000 loss 0.35428 batch 2026/3281 lr 0.001 accuracy 93.29687 wps 17699.67 step time 0.49s\n","# Epoch 8  global step 25020 loss 0.34238 batch 2046/3281 lr 0.001 accuracy 93.56641 wps 15986.65 step time 0.46s\n","# Epoch 8  global step 25040 loss 0.34037 batch 2066/3281 lr 0.001 accuracy 93.70313 wps 16171.57 step time 0.44s\n","# Epoch 8  global step 25060 loss 0.34888 batch 2086/3281 lr 0.001 accuracy 93.74219 wps 17539.85 step time 0.47s\n","# Epoch 8  global step 25080 loss 0.34277 batch 2106/3281 lr 0.001 accuracy 93.83984 wps 17122.21 step time 0.37s\n","# Epoch 8  global step 25100 loss 0.32552 batch 2126/3281 lr 0.001 accuracy 94.16797 wps 16268.69 step time 0.40s\n","# Epoch 8  global step 25120 loss 0.35558 batch 2146/3281 lr 0.001 accuracy 93.58984 wps 17450.76 step time 0.46s\n","# Epoch 8  global step 25140 loss 0.33509 batch 2166/3281 lr 0.001 accuracy 93.98828 wps 16535.37 step time 0.44s\n","# Epoch 8  global step 25160 loss 0.35774 batch 2186/3281 lr 0.001 accuracy 93.48047 wps 17073.51 step time 0.45s\n","# Epoch 8  global step 25180 loss 0.32929 batch 2206/3281 lr 0.001 accuracy 93.92187 wps 16277.09 step time 0.37s\n","# Epoch 8  global step 25200 loss 0.33482 batch 2226/3281 lr 0.001 accuracy 93.86719 wps 16761.32 step time 0.45s\n","# Epoch 8  global step 25220 loss 0.34781 batch 2246/3281 lr 0.001 accuracy 93.50391 wps 16950.47 step time 0.43s\n","# Epoch 8  global step 25240 loss 0.32158 batch 2266/3281 lr 0.001 accuracy 94.19141 wps 16805.55 step time 0.34s\n","# Epoch 8  global step 25260 loss 0.31427 batch 2286/3281 lr 0.001 accuracy 94.32422 wps 17495.26 step time 0.39s\n","# Epoch 8  global step 25280 loss 0.35178 batch 2306/3281 lr 0.001 accuracy 93.67187 wps 16407.04 step time 0.57s\n","# Epoch 8  global step 25300 loss 0.35662 batch 2326/3281 lr 0.001 accuracy 93.49219 wps 17008.44 step time 0.49s\n","# Epoch 8  global step 25320 loss 0.32139 batch 2346/3281 lr 0.001 accuracy 94.11719 wps 16679.12 step time 0.38s\n","# Epoch 8  global step 25340 loss 0.34562 batch 2366/3281 lr 0.001 accuracy 93.73438 wps 17323.15 step time 0.36s\n","# Epoch 8  global step 25360 loss 0.34035 batch 2386/3281 lr 0.001 accuracy 93.75000 wps 17698.11 step time 0.44s\n","# Epoch 8  global step 25380 loss 0.31930 batch 2406/3281 lr 0.001 accuracy 94.29688 wps 16738.90 step time 0.38s\n","# Epoch 8  global step 25400 loss 0.34449 batch 2426/3281 lr 0.001 accuracy 93.71094 wps 17108.71 step time 0.38s\n","# Epoch 8  global step 25420 loss 0.33085 batch 2446/3281 lr 0.001 accuracy 94.03906 wps 16293.85 step time 0.37s\n","# Epoch 8  global step 25440 loss 0.33089 batch 2466/3281 lr 0.001 accuracy 93.95703 wps 16520.17 step time 0.46s\n","# Epoch 8  global step 25460 loss 0.33108 batch 2486/3281 lr 0.001 accuracy 93.82422 wps 17566.66 step time 0.44s\n","# Epoch 8  global step 25480 loss 0.37680 batch 2506/3281 lr 0.001 accuracy 93.16406 wps 16451.12 step time 0.52s\n","# Epoch 8  global step 25500 loss 0.34254 batch 2526/3281 lr 0.001 accuracy 93.77344 wps 17364.73 step time 0.42s\n","# Epoch 8  global step 25520 loss 0.37418 batch 2546/3281 lr 0.001 accuracy 93.14844 wps 15776.41 step time 0.59s\n","# Epoch 8  global step 25540 loss 0.32334 batch 2566/3281 lr 0.001 accuracy 94.16797 wps 17350.93 step time 0.36s\n","# Epoch 8  global step 25560 loss 0.35135 batch 2586/3281 lr 0.001 accuracy 93.50000 wps 17009.31 step time 0.48s\n","# Epoch 8  global step 25580 loss 0.32122 batch 2606/3281 lr 0.001 accuracy 94.17578 wps 16312.27 step time 0.44s\n","# Epoch 8  global step 25600 loss 0.34082 batch 2626/3281 lr 0.001 accuracy 93.79688 wps 18019.11 step time 0.41s\n","# Epoch 8  global step 25620 loss 0.33727 batch 2646/3281 lr 0.001 accuracy 93.87891 wps 17617.64 step time 0.42s\n","# Epoch 8  global step 25640 loss 0.31751 batch 2666/3281 lr 0.001 accuracy 94.28906 wps 17278.14 step time 0.39s\n","# Epoch 8  global step 25660 loss 0.36357 batch 2686/3281 lr 0.001 accuracy 93.16406 wps 17214.14 step time 0.59s\n","# Epoch 8  global step 25680 loss 0.32301 batch 2706/3281 lr 0.001 accuracy 94.18359 wps 16375.56 step time 0.42s\n","# Epoch 8  global step 25700 loss 0.33034 batch 2726/3281 lr 0.001 accuracy 93.96875 wps 17159.47 step time 0.38s\n","# Epoch 8  global step 25720 loss 0.31989 batch 2746/3281 lr 0.001 accuracy 94.19531 wps 17086.89 step time 0.42s\n","# Epoch 8  global step 25740 loss 0.33531 batch 2766/3281 lr 0.001 accuracy 93.92187 wps 16707.55 step time 0.47s\n","# Epoch 8  global step 25760 loss 0.33557 batch 2786/3281 lr 0.001 accuracy 93.96875 wps 16117.27 step time 0.38s\n","# Epoch 8  global step 25780 loss 0.34242 batch 2806/3281 lr 0.001 accuracy 93.60937 wps 17687.04 step time 0.41s\n","# Epoch 8  global step 25800 loss 0.33200 batch 2826/3281 lr 0.001 accuracy 94.08203 wps 17396.96 step time 0.39s\n","# Epoch 8  global step 25820 loss 0.33563 batch 2846/3281 lr 0.001 accuracy 93.97656 wps 16967.30 step time 0.44s\n","# Epoch 8  global step 25840 loss 0.33049 batch 2866/3281 lr 0.001 accuracy 94.01953 wps 17550.28 step time 0.39s\n","# Epoch 8  global step 25860 loss 0.35668 batch 2886/3281 lr 0.001 accuracy 93.39453 wps 17693.52 step time 0.41s\n","# Epoch 8  global step 25880 loss 0.33552 batch 2906/3281 lr 0.001 accuracy 93.86719 wps 17278.33 step time 0.37s\n","# Epoch 8  global step 25900 loss 0.36036 batch 2926/3281 lr 0.001 accuracy 93.43359 wps 18061.46 step time 0.40s\n","# Epoch 8  global step 25920 loss 0.32936 batch 2946/3281 lr 0.001 accuracy 94.16016 wps 17277.55 step time 0.38s\n","# Epoch 8  global step 25940 loss 0.33798 batch 2966/3281 lr 0.001 accuracy 93.72656 wps 17018.25 step time 0.40s\n","# Epoch 8  global step 25960 loss 0.32931 batch 2986/3281 lr 0.001 accuracy 94.03906 wps 16695.30 step time 0.36s\n","# Epoch 8  global step 25980 loss 0.33545 batch 3006/3281 lr 0.001 accuracy 93.93359 wps 17122.27 step time 0.43s\n","# Epoch 8  global step 26000 loss 0.33527 batch 3026/3281 lr 0.001 accuracy 94.06641 wps 16504.47 step time 0.39s\n","# global step 26000, eval model at Fri May 22 13:25:40 2020\n","2020-05-22 13:25:43.354002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:25:43.354746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 13:25:43.354844: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 13:25:43.354901: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 13:25:43.354996: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 13:25:43.355062: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 13:25:43.355133: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 13:25:43.355172: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 13:25:43.355210: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 13:25:43.355350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:25:43.355967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:25:43.356474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 13:25:43.356524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 13:25:43.356559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 13:25:43.356586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 13:25:43.356764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:25:43.357378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:25:43.357838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6639848811292173\n","# location_distance_from_business_district - 0.5424959383508543\n","# location_easy_to_find - 0.7048204320676484\n","# service_wait_time - 0.6655791891560524\n","# service_waiters_attitude - 0.7903992568233731\n","# service_parking_convenience - 0.7279345704283147\n","# service_serving_speed - 0.7574579447099387\n","# price_level - 0.7773664132943197\n","# price_cost_effective - 0.7077930043352427\n","# price_discount - 0.6543113355767034\n","# environment_decoration - 0.7201679752284725\n","# environment_noise - 0.7626491151114287\n","# environment_space - 0.7568107301485265\n","# environment_cleaness - 0.7535924711990547\n","# dish_portion - 0.7169317858648387\n","# dish_taste - 0.7346620635344173\n","# dish_look - 0.5516142080038218\n","# dish_recommendation - 0.740664129833048\n","# others_overall_experience - 0.5863178054284034\n","# others_willing_to_consume_again - 0.7061658194369063\n","# Eval loss 0.26546, f1 0.70109\n","# current result -0.701085953483029, previous best result -0.693412470593742\n","# Epoch 8  global step 26020 loss 0.31294 batch 3046/3281 lr 0.001 accuracy 94.45703 wps 16422.96 step time 0.35s\n","# Epoch 8  global step 26040 loss 0.34829 batch 3066/3281 lr 0.001 accuracy 93.60547 wps 16759.94 step time 0.45s\n","# Epoch 8  global step 26060 loss 0.33807 batch 3086/3281 lr 0.001 accuracy 93.89453 wps 17307.57 step time 0.40s\n","# Epoch 8  global step 26080 loss 0.36259 batch 3106/3281 lr 0.001 accuracy 93.40234 wps 16153.17 step time 0.53s\n","# Epoch 8  global step 26100 loss 0.34356 batch 3126/3281 lr 0.001 accuracy 93.66797 wps 17586.81 step time 0.42s\n","# Epoch 8  global step 26120 loss 0.33877 batch 3146/3281 lr 0.001 accuracy 94.05469 wps 17434.88 step time 0.41s\n","# Epoch 8  global step 26140 loss 0.33609 batch 3166/3281 lr 0.001 accuracy 93.93750 wps 17238.96 step time 0.41s\n","# Epoch 8  global step 26160 loss 0.34232 batch 3186/3281 lr 0.001 accuracy 93.81641 wps 17234.70 step time 0.43s\n","# Epoch 8  global step 26180 loss 0.35370 batch 3206/3281 lr 0.001 accuracy 93.60156 wps 16666.16 step time 0.47s\n","# Epoch 8  global step 26200 loss 0.33511 batch 3226/3281 lr 0.001 accuracy 93.91406 wps 16727.77 step time 0.36s\n","# Epoch 8  global step 26220 loss 0.35071 batch 3246/3281 lr 0.001 accuracy 93.60547 wps 18578.48 step time 0.43s\n","# Epoch 8  global step 26240 loss 0.33873 batch 3266/3281 lr 0.001 accuracy 94.07813 wps 16730.19 step time 0.36s\n","# Finsh epoch 8, global step 26256\n","# Epoch 9  global step 26260 loss 0.06173 batch 4/3281 lr 0.001 accuracy 18.90234 wps 18594.36 step time 0.11s\n","# Epoch 9  global step 26280 loss 0.31639 batch 24/3281 lr 0.001 accuracy 94.21875 wps 17915.19 step time 0.40s\n","# Epoch 9  global step 26300 loss 0.31965 batch 44/3281 lr 0.001 accuracy 94.23437 wps 18982.77 step time 0.36s\n","# Epoch 9  global step 26320 loss 0.33735 batch 64/3281 lr 0.001 accuracy 93.89062 wps 19749.58 step time 0.40s\n","# Epoch 9  global step 26340 loss 0.32450 batch 84/3281 lr 0.001 accuracy 94.12891 wps 18153.10 step time 0.45s\n","# Epoch 9  global step 26360 loss 0.31109 batch 104/3281 lr 0.001 accuracy 94.34375 wps 18789.27 step time 0.36s\n","# Epoch 9  global step 26380 loss 0.32741 batch 124/3281 lr 0.001 accuracy 94.08594 wps 18298.91 step time 0.44s\n","# Epoch 9  global step 26400 loss 0.32493 batch 144/3281 lr 0.001 accuracy 94.01563 wps 19324.29 step time 0.38s\n","# Epoch 9  global step 26420 loss 0.32651 batch 164/3281 lr 0.001 accuracy 94.11719 wps 19679.66 step time 0.39s\n","# Epoch 9  global step 26440 loss 0.32529 batch 184/3281 lr 0.001 accuracy 94.10547 wps 19812.10 step time 0.40s\n","# Epoch 9  global step 26460 loss 0.31051 batch 204/3281 lr 0.001 accuracy 94.42969 wps 18278.67 step time 0.33s\n","# Epoch 9  global step 26480 loss 0.32430 batch 224/3281 lr 0.001 accuracy 94.19531 wps 19101.33 step time 0.37s\n","# Epoch 9  global step 26500 loss 0.30910 batch 244/3281 lr 0.001 accuracy 94.44531 wps 18814.37 step time 0.35s\n","# Epoch 9  global step 26520 loss 0.30966 batch 264/3281 lr 0.001 accuracy 94.34375 wps 18273.49 step time 0.35s\n","# Epoch 9  global step 26540 loss 0.35570 batch 284/3281 lr 0.001 accuracy 93.43359 wps 19900.64 step time 0.45s\n","# Epoch 9  global step 26560 loss 0.31771 batch 304/3281 lr 0.001 accuracy 94.30469 wps 18580.65 step time 0.35s\n","# Epoch 9  global step 26580 loss 0.31407 batch 324/3281 lr 0.001 accuracy 94.42969 wps 18513.47 step time 0.35s\n","# Epoch 9  global step 26600 loss 0.32285 batch 344/3281 lr 0.001 accuracy 93.98828 wps 19183.01 step time 0.36s\n","# Epoch 9  global step 26620 loss 0.35100 batch 364/3281 lr 0.001 accuracy 93.51172 wps 19789.72 step time 0.44s\n","# Epoch 9  global step 26640 loss 0.31217 batch 384/3281 lr 0.001 accuracy 94.45312 wps 18596.27 step time 0.38s\n","# Epoch 9  global step 26660 loss 0.32300 batch 404/3281 lr 0.001 accuracy 94.13672 wps 19454.18 step time 0.38s\n","# Epoch 9  global step 26680 loss 0.32375 batch 424/3281 lr 0.001 accuracy 94.19531 wps 18587.48 step time 0.35s\n","# Epoch 9  global step 26700 loss 0.31116 batch 444/3281 lr 0.001 accuracy 94.32422 wps 18614.82 step time 0.39s\n","# Epoch 9  global step 26720 loss 0.33663 batch 464/3281 lr 0.001 accuracy 94.02734 wps 18808.14 step time 0.35s\n","# Epoch 9  global step 26740 loss 0.31814 batch 484/3281 lr 0.001 accuracy 94.10937 wps 18535.02 step time 0.34s\n","# Epoch 9  global step 26760 loss 0.31491 batch 504/3281 lr 0.001 accuracy 94.30078 wps 18219.70 step time 0.33s\n","# Epoch 9  global step 26780 loss 0.34727 batch 524/3281 lr 0.001 accuracy 93.64453 wps 18841.65 step time 0.37s\n","# Epoch 9  global step 26800 loss 0.31611 batch 544/3281 lr 0.001 accuracy 94.26172 wps 18620.10 step time 0.36s\n","# Epoch 9  global step 26820 loss 0.33915 batch 564/3281 lr 0.001 accuracy 93.85547 wps 18691.54 step time 0.36s\n","# Epoch 9  global step 26840 loss 0.35103 batch 584/3281 lr 0.001 accuracy 93.52734 wps 19649.06 step time 0.44s\n","# Epoch 9  global step 26860 loss 0.32955 batch 604/3281 lr 0.001 accuracy 93.82031 wps 18867.34 step time 0.45s\n","# Epoch 9  global step 26880 loss 0.32630 batch 624/3281 lr 0.001 accuracy 94.01172 wps 19394.55 step time 0.42s\n","# Epoch 9  global step 26900 loss 0.33654 batch 644/3281 lr 0.001 accuracy 93.87891 wps 19738.25 step time 0.40s\n","# Epoch 9  global step 26920 loss 0.30315 batch 664/3281 lr 0.001 accuracy 94.48437 wps 18928.60 step time 0.36s\n","# Epoch 9  global step 26940 loss 0.33414 batch 684/3281 lr 0.001 accuracy 93.79297 wps 18963.44 step time 0.35s\n","# Epoch 9  global step 26960 loss 0.30507 batch 704/3281 lr 0.001 accuracy 94.47266 wps 18160.58 step time 0.33s\n","# Epoch 9  global step 26980 loss 0.31521 batch 724/3281 lr 0.001 accuracy 94.33203 wps 18438.33 step time 0.34s\n","# Epoch 9  global step 27000 loss 0.33479 batch 744/3281 lr 0.001 accuracy 94.08594 wps 18809.20 step time 0.41s\n","# Epoch 9  global step 27020 loss 0.33592 batch 764/3281 lr 0.001 accuracy 94.00000 wps 19110.36 step time 0.43s\n","# Epoch 9  global step 27040 loss 0.31945 batch 784/3281 lr 0.001 accuracy 94.29297 wps 18921.60 step time 0.36s\n","# Epoch 9  global step 27060 loss 0.31887 batch 804/3281 lr 0.001 accuracy 94.38672 wps 18955.03 step time 0.40s\n","# Epoch 9  global step 27080 loss 0.31168 batch 824/3281 lr 0.001 accuracy 94.33984 wps 18206.20 step time 0.33s\n","# Epoch 9  global step 27100 loss 0.33001 batch 844/3281 lr 0.001 accuracy 93.77734 wps 20128.74 step time 0.42s\n","# Epoch 9  global step 27120 loss 0.33557 batch 864/3281 lr 0.001 accuracy 93.95703 wps 19255.98 step time 0.38s\n","# Epoch 9  global step 27140 loss 0.33719 batch 884/3281 lr 0.001 accuracy 93.78906 wps 19844.58 step time 0.44s\n","# Epoch 9  global step 27160 loss 0.32891 batch 904/3281 lr 0.001 accuracy 94.08594 wps 18633.42 step time 0.36s\n","# Epoch 9  global step 27180 loss 0.32048 batch 924/3281 lr 0.001 accuracy 94.11719 wps 19261.38 step time 0.38s\n","# Epoch 9  global step 27200 loss 0.31457 batch 944/3281 lr 0.001 accuracy 94.34375 wps 18967.34 step time 0.36s\n","# Epoch 9  global step 27220 loss 0.32225 batch 964/3281 lr 0.001 accuracy 94.08203 wps 19166.15 step time 0.37s\n","# Epoch 9  global step 27240 loss 0.34324 batch 984/3281 lr 0.001 accuracy 93.63281 wps 19514.25 step time 0.48s\n","# Epoch 9  global step 27260 loss 0.32225 batch 1004/3281 lr 0.001 accuracy 94.10547 wps 18645.60 step time 0.34s\n","# Epoch 9  global step 27280 loss 0.34668 batch 1024/3281 lr 0.001 accuracy 93.67969 wps 20047.31 step time 0.39s\n","# Epoch 9  global step 27300 loss 0.30479 batch 1044/3281 lr 0.001 accuracy 94.57031 wps 17649.23 step time 0.32s\n","# Epoch 9  global step 27320 loss 0.33219 batch 1064/3281 lr 0.001 accuracy 93.96094 wps 19337.97 step time 0.38s\n","# Epoch 9  global step 27340 loss 0.30200 batch 1084/3281 lr 0.001 accuracy 94.58594 wps 17880.55 step time 0.33s\n","# Epoch 9  global step 27360 loss 0.32279 batch 1104/3281 lr 0.001 accuracy 94.18359 wps 18347.74 step time 0.34s\n","# Epoch 9  global step 27380 loss 0.34237 batch 1124/3281 lr 0.001 accuracy 93.72656 wps 19396.47 step time 0.45s\n","# Epoch 9  global step 27400 loss 0.34261 batch 1144/3281 lr 0.001 accuracy 93.76563 wps 19191.57 step time 0.42s\n","# Epoch 9  global step 27420 loss 0.36046 batch 1164/3281 lr 0.001 accuracy 93.44141 wps 20327.69 step time 0.42s\n","# Epoch 9  global step 27440 loss 0.31700 batch 1184/3281 lr 0.001 accuracy 94.30078 wps 18753.89 step time 0.35s\n","# Epoch 9  global step 27460 loss 0.32169 batch 1204/3281 lr 0.001 accuracy 94.02734 wps 19309.24 step time 0.38s\n","# Epoch 9  global step 27480 loss 0.34440 batch 1224/3281 lr 0.001 accuracy 93.75000 wps 19588.36 step time 0.45s\n","# Epoch 9  global step 27500 loss 0.31859 batch 1244/3281 lr 0.001 accuracy 94.36719 wps 18805.24 step time 0.35s\n","# Epoch 9  global step 27520 loss 0.30635 batch 1264/3281 lr 0.001 accuracy 94.35547 wps 18648.12 step time 0.34s\n","# Epoch 9  global step 27540 loss 0.33919 batch 1284/3281 lr 0.001 accuracy 93.80469 wps 18830.24 step time 0.40s\n","# Epoch 9  global step 27560 loss 0.34248 batch 1304/3281 lr 0.001 accuracy 93.68750 wps 19627.81 step time 0.39s\n","# Epoch 9  global step 27580 loss 0.29660 batch 1324/3281 lr 0.001 accuracy 94.56250 wps 17697.50 step time 0.32s\n","# Epoch 9  global step 27600 loss 0.33745 batch 1344/3281 lr 0.001 accuracy 93.87109 wps 19093.57 step time 0.37s\n","# Epoch 9  global step 27620 loss 0.32783 batch 1364/3281 lr 0.001 accuracy 93.98438 wps 18340.24 step time 0.44s\n","# Epoch 9  global step 27640 loss 0.32062 batch 1384/3281 lr 0.001 accuracy 94.20703 wps 19025.65 step time 0.38s\n","# Epoch 9  global step 27660 loss 0.36143 batch 1404/3281 lr 0.001 accuracy 93.40625 wps 19982.43 step time 0.50s\n","# Epoch 9  global step 27680 loss 0.33924 batch 1424/3281 lr 0.001 accuracy 93.73437 wps 19267.34 step time 0.37s\n","# Epoch 9  global step 27700 loss 0.32950 batch 1444/3281 lr 0.001 accuracy 94.07422 wps 19442.60 step time 0.37s\n","# Epoch 9  global step 27720 loss 0.33650 batch 1464/3281 lr 0.001 accuracy 93.92969 wps 19475.23 step time 0.44s\n","# Epoch 9  global step 27740 loss 0.32864 batch 1484/3281 lr 0.001 accuracy 94.16406 wps 17932.63 step time 0.33s\n","# Epoch 9  global step 27760 loss 0.33441 batch 1504/3281 lr 0.001 accuracy 93.85547 wps 19394.34 step time 0.37s\n","# Epoch 9  global step 27780 loss 0.34115 batch 1524/3281 lr 0.001 accuracy 93.76953 wps 18655.58 step time 0.45s\n","# Epoch 9  global step 27800 loss 0.33042 batch 1544/3281 lr 0.001 accuracy 93.89453 wps 16161.94 step time 0.37s\n","# Epoch 9  global step 27820 loss 0.31834 batch 1564/3281 lr 0.001 accuracy 94.12500 wps 15970.45 step time 0.48s\n","# Epoch 9  global step 27840 loss 0.33770 batch 1584/3281 lr 0.001 accuracy 93.79297 wps 17021.04 step time 0.42s\n","# Epoch 9  global step 27860 loss 0.33385 batch 1604/3281 lr 0.001 accuracy 93.71484 wps 17410.62 step time 0.43s\n","# Epoch 9  global step 27880 loss 0.34700 batch 1624/3281 lr 0.001 accuracy 93.51562 wps 17420.95 step time 0.44s\n","# Epoch 9  global step 27900 loss 0.32681 batch 1644/3281 lr 0.001 accuracy 94.16406 wps 16924.88 step time 0.41s\n","# Epoch 9  global step 27920 loss 0.31055 batch 1664/3281 lr 0.001 accuracy 94.40234 wps 17234.92 step time 0.37s\n","# Epoch 9  global step 27940 loss 0.34088 batch 1684/3281 lr 0.001 accuracy 93.83984 wps 16899.76 step time 0.49s\n","# Epoch 9  global step 27960 loss 0.31222 batch 1704/3281 lr 0.001 accuracy 94.38281 wps 16532.17 step time 0.40s\n","# Epoch 9  global step 27980 loss 0.31626 batch 1724/3281 lr 0.001 accuracy 94.16016 wps 16709.75 step time 0.41s\n","# Epoch 9  global step 28000 loss 0.31627 batch 1744/3281 lr 0.001 accuracy 94.41797 wps 16899.00 step time 0.39s\n","# global step 28000, eval model at Fri May 22 13:39:53 2020\n","2020-05-22 13:39:56.608240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:39:56.608851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 13:39:56.608956: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 13:39:56.608999: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 13:39:56.609041: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 13:39:56.609082: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 13:39:56.609136: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 13:39:56.609173: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 13:39:56.609211: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 13:39:56.609331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:39:56.609894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:39:56.610361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 13:39:56.610451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 13:39:56.610472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 13:39:56.610486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 13:39:56.610648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:39:56.611226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:39:56.611699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.666319301148042\n","# location_distance_from_business_district - 0.5453187327781901\n","# location_easy_to_find - 0.704905727273687\n","# service_wait_time - 0.668203053219801\n","# service_waiters_attitude - 0.7942115345743104\n","# service_parking_convenience - 0.725597501089672\n","# service_serving_speed - 0.7594759483637792\n","# price_level - 0.7811075866301995\n","# price_cost_effective - 0.7090623367620563\n","# price_discount - 0.6629153901564908\n","# environment_decoration - 0.7272737126871193\n","# environment_noise - 0.7596068425340452\n","# environment_space - 0.7603548407070385\n","# environment_cleaness - 0.7538267395154559\n","# dish_portion - 0.721720387067533\n","# dish_taste - 0.7335903772070601\n","# dish_look - 0.5637938988794831\n","# dish_recommendation - 0.7443209141303577\n","# others_overall_experience - 0.5897651240805211\n","# others_willing_to_consume_again - 0.7071417735700819\n","# Eval loss 0.26385, f1 0.70393\n","# current result -0.7039255861187461, previous best result -0.701085953483029\n","# Epoch 9  global step 28020 loss 0.33041 batch 1764/3281 lr 0.001 accuracy 93.99219 wps 16617.37 step time 0.46s\n","# Epoch 9  global step 28040 loss 0.33971 batch 1784/3281 lr 0.001 accuracy 93.85547 wps 16210.88 step time 0.53s\n","# Epoch 9  global step 28060 loss 0.33980 batch 1804/3281 lr 0.001 accuracy 93.88672 wps 17085.03 step time 0.45s\n","# Epoch 9  global step 28080 loss 0.31654 batch 1824/3281 lr 0.001 accuracy 94.29688 wps 16200.27 step time 0.42s\n","# Epoch 9  global step 28100 loss 0.32680 batch 1844/3281 lr 0.001 accuracy 94.00781 wps 16341.40 step time 0.46s\n","# Epoch 9  global step 28120 loss 0.33345 batch 1864/3281 lr 0.001 accuracy 93.89062 wps 15807.50 step time 0.50s\n","# Epoch 9  global step 28140 loss 0.31717 batch 1884/3281 lr 0.001 accuracy 94.19141 wps 16882.12 step time 0.40s\n","# Epoch 9  global step 28160 loss 0.33423 batch 1904/3281 lr 0.001 accuracy 93.90625 wps 16467.61 step time 0.45s\n","# Epoch 9  global step 28180 loss 0.35369 batch 1924/3281 lr 0.001 accuracy 93.51172 wps 16203.93 step time 0.52s\n","# Epoch 9  global step 28200 loss 0.31885 batch 1944/3281 lr 0.001 accuracy 94.21875 wps 16555.15 step time 0.39s\n","# Epoch 9  global step 28220 loss 0.33578 batch 1964/3281 lr 0.001 accuracy 93.78906 wps 16188.39 step time 0.45s\n","# Epoch 9  global step 28240 loss 0.34773 batch 1984/3281 lr 0.001 accuracy 93.80469 wps 16942.24 step time 0.42s\n","# Epoch 9  global step 28260 loss 0.30692 batch 2004/3281 lr 0.001 accuracy 94.41016 wps 15690.65 step time 0.41s\n","# Epoch 9  global step 28280 loss 0.31407 batch 2024/3281 lr 0.001 accuracy 94.29688 wps 16677.20 step time 0.43s\n","# Epoch 9  global step 28300 loss 0.33541 batch 2044/3281 lr 0.001 accuracy 93.93750 wps 16522.92 step time 0.43s\n","# Epoch 9  global step 28320 loss 0.33408 batch 2064/3281 lr 0.001 accuracy 93.75781 wps 17633.27 step time 0.41s\n","# Epoch 9  global step 28340 loss 0.33438 batch 2084/3281 lr 0.001 accuracy 93.80859 wps 17231.36 step time 0.41s\n","# Epoch 9  global step 28360 loss 0.37156 batch 2104/3281 lr 0.001 accuracy 93.18359 wps 16823.44 step time 0.61s\n","# Epoch 9  global step 28380 loss 0.33255 batch 2124/3281 lr 0.001 accuracy 93.85547 wps 16726.51 step time 0.41s\n","# Epoch 9  global step 28400 loss 0.32681 batch 2144/3281 lr 0.001 accuracy 94.30469 wps 17542.24 step time 0.39s\n","# Epoch 9  global step 28420 loss 0.34951 batch 2164/3281 lr 0.001 accuracy 93.71094 wps 17508.33 step time 0.49s\n","# Epoch 9  global step 28440 loss 0.32257 batch 2184/3281 lr 0.001 accuracy 94.05469 wps 16969.53 step time 0.40s\n","# Epoch 9  global step 28460 loss 0.36495 batch 2204/3281 lr 0.001 accuracy 93.29297 wps 16593.10 step time 0.57s\n","# Epoch 9  global step 28480 loss 0.33825 batch 2224/3281 lr 0.001 accuracy 93.97266 wps 16246.54 step time 0.44s\n","# Epoch 9  global step 28500 loss 0.33488 batch 2244/3281 lr 0.001 accuracy 93.91797 wps 17145.58 step time 0.45s\n","# Epoch 9  global step 28520 loss 0.33449 batch 2264/3281 lr 0.001 accuracy 93.82812 wps 17412.25 step time 0.37s\n","# Epoch 9  global step 28540 loss 0.31290 batch 2284/3281 lr 0.001 accuracy 94.29688 wps 16641.57 step time 0.33s\n","# Epoch 9  global step 28560 loss 0.33949 batch 2304/3281 lr 0.001 accuracy 93.86719 wps 18017.69 step time 0.40s\n","# Epoch 9  global step 28580 loss 0.31980 batch 2324/3281 lr 0.001 accuracy 94.15625 wps 16558.60 step time 0.37s\n","# Epoch 9  global step 28600 loss 0.33477 batch 2344/3281 lr 0.001 accuracy 93.95313 wps 16635.29 step time 0.42s\n","# Epoch 9  global step 28620 loss 0.29971 batch 2364/3281 lr 0.001 accuracy 94.41016 wps 16789.70 step time 0.35s\n","# Epoch 9  global step 28640 loss 0.32334 batch 2384/3281 lr 0.001 accuracy 94.24219 wps 16782.20 step time 0.48s\n","# Epoch 9  global step 28660 loss 0.31375 batch 2404/3281 lr 0.001 accuracy 94.21094 wps 16937.19 step time 0.39s\n","# Epoch 9  global step 28680 loss 0.32936 batch 2424/3281 lr 0.001 accuracy 93.94141 wps 17104.62 step time 0.40s\n","# Epoch 9  global step 28700 loss 0.33719 batch 2444/3281 lr 0.001 accuracy 93.79688 wps 16404.89 step time 0.42s\n","# Epoch 9  global step 28720 loss 0.33396 batch 2464/3281 lr 0.001 accuracy 93.84766 wps 16863.69 step time 0.48s\n","# Epoch 9  global step 28740 loss 0.33158 batch 2484/3281 lr 0.001 accuracy 94.01172 wps 16948.63 step time 0.40s\n","# Epoch 9  global step 28760 loss 0.35528 batch 2504/3281 lr 0.001 accuracy 93.57031 wps 17606.60 step time 0.43s\n","# Epoch 9  global step 28780 loss 0.34744 batch 2524/3281 lr 0.001 accuracy 93.69531 wps 15763.78 step time 0.54s\n","# Epoch 9  global step 28800 loss 0.33601 batch 2544/3281 lr 0.001 accuracy 93.98828 wps 17270.55 step time 0.39s\n","# Epoch 9  global step 28820 loss 0.34056 batch 2564/3281 lr 0.001 accuracy 93.85156 wps 17127.15 step time 0.42s\n","# Epoch 9  global step 28840 loss 0.34361 batch 2584/3281 lr 0.001 accuracy 93.78906 wps 17179.11 step time 0.44s\n","# Epoch 9  global step 28860 loss 0.34249 batch 2604/3281 lr 0.001 accuracy 93.61719 wps 16617.41 step time 0.52s\n","# Epoch 9  global step 28880 loss 0.31893 batch 2624/3281 lr 0.001 accuracy 94.12500 wps 16854.64 step time 0.39s\n","# Epoch 9  global step 28900 loss 0.31578 batch 2644/3281 lr 0.001 accuracy 94.28516 wps 16887.58 step time 0.38s\n","# Epoch 9  global step 28920 loss 0.32886 batch 2664/3281 lr 0.001 accuracy 93.95703 wps 17083.04 step time 0.46s\n","# Epoch 9  global step 28940 loss 0.31816 batch 2684/3281 lr 0.001 accuracy 94.13281 wps 17714.85 step time 0.37s\n","# Epoch 9  global step 28960 loss 0.32925 batch 2704/3281 lr 0.001 accuracy 93.98437 wps 17324.07 step time 0.39s\n","# Epoch 9  global step 28980 loss 0.32586 batch 2724/3281 lr 0.001 accuracy 94.17578 wps 16361.30 step time 0.35s\n","# Epoch 9  global step 29000 loss 0.32836 batch 2744/3281 lr 0.001 accuracy 94.09375 wps 17508.06 step time 0.40s\n","# Epoch 9  global step 29020 loss 0.35161 batch 2764/3281 lr 0.001 accuracy 93.53516 wps 15875.73 step time 0.60s\n","# Epoch 9  global step 29040 loss 0.31625 batch 2784/3281 lr 0.001 accuracy 94.20703 wps 16169.60 step time 0.42s\n","# Epoch 9  global step 29060 loss 0.32113 batch 2804/3281 lr 0.001 accuracy 94.34766 wps 16931.78 step time 0.35s\n","# Epoch 9  global step 29080 loss 0.33551 batch 2824/3281 lr 0.001 accuracy 94.05469 wps 17362.95 step time 0.39s\n","# Epoch 9  global step 29100 loss 0.32369 batch 2844/3281 lr 0.001 accuracy 94.08594 wps 16014.46 step time 0.45s\n","# Epoch 9  global step 29120 loss 0.32174 batch 2864/3281 lr 0.001 accuracy 93.99609 wps 16853.12 step time 0.41s\n","# Epoch 9  global step 29140 loss 0.33712 batch 2884/3281 lr 0.001 accuracy 93.97656 wps 17189.03 step time 0.40s\n","# Epoch 9  global step 29160 loss 0.32811 batch 2904/3281 lr 0.001 accuracy 94.14844 wps 16574.45 step time 0.42s\n","# Epoch 9  global step 29180 loss 0.31885 batch 2924/3281 lr 0.001 accuracy 94.10156 wps 16512.75 step time 0.36s\n","# Epoch 9  global step 29200 loss 0.32714 batch 2944/3281 lr 0.001 accuracy 94.18359 wps 17431.01 step time 0.40s\n","# Epoch 9  global step 29220 loss 0.35006 batch 2964/3281 lr 0.001 accuracy 93.60547 wps 16593.37 step time 0.41s\n","# Epoch 9  global step 29240 loss 0.32842 batch 2984/3281 lr 0.001 accuracy 93.96094 wps 17136.98 step time 0.38s\n","# Epoch 9  global step 29260 loss 0.33746 batch 3004/3281 lr 0.001 accuracy 93.72266 wps 17126.06 step time 0.41s\n","# Epoch 9  global step 29280 loss 0.32458 batch 3024/3281 lr 0.001 accuracy 94.07031 wps 16625.84 step time 0.37s\n","# Epoch 9  global step 29300 loss 0.31582 batch 3044/3281 lr 0.001 accuracy 94.53516 wps 16909.86 step time 0.36s\n","# Epoch 9  global step 29320 loss 0.34927 batch 3064/3281 lr 0.001 accuracy 93.64453 wps 16916.73 step time 0.41s\n","# Epoch 9  global step 29340 loss 0.32699 batch 3084/3281 lr 0.001 accuracy 94.02344 wps 18336.39 step time 0.44s\n","# Epoch 9  global step 29360 loss 0.34030 batch 3104/3281 lr 0.001 accuracy 93.69531 wps 16946.32 step time 0.40s\n","# Epoch 9  global step 29380 loss 0.35872 batch 3124/3281 lr 0.001 accuracy 93.41406 wps 16913.14 step time 0.46s\n","# Epoch 9  global step 29400 loss 0.33194 batch 3144/3281 lr 0.001 accuracy 94.01172 wps 17031.73 step time 0.45s\n","# Epoch 9  global step 29420 loss 0.32979 batch 3164/3281 lr 0.001 accuracy 94.09375 wps 16241.38 step time 0.48s\n","# Epoch 9  global step 29440 loss 0.32716 batch 3184/3281 lr 0.001 accuracy 94.03516 wps 16445.28 step time 0.46s\n","# Epoch 9  global step 29460 loss 0.31244 batch 3204/3281 lr 0.001 accuracy 94.29297 wps 17170.57 step time 0.35s\n","# Epoch 9  global step 29480 loss 0.33641 batch 3224/3281 lr 0.001 accuracy 94.00000 wps 16985.05 step time 0.44s\n","# Epoch 9  global step 29500 loss 0.35422 batch 3244/3281 lr 0.001 accuracy 93.57812 wps 16687.21 step time 0.54s\n","# Epoch 9  global step 29520 loss 0.33945 batch 3264/3281 lr 0.001 accuracy 93.71484 wps 15904.89 step time 0.47s\n","# Finsh epoch 9, global step 29538\n","# Epoch 10  global step 29540 loss 0.03246 batch 2/3281 lr 0.001 accuracy 9.39844 wps 16734.88 step time 0.08s\n","# Epoch 10  global step 29560 loss 0.31410 batch 22/3281 lr 0.001 accuracy 94.35547 wps 19080.32 step time 0.41s\n","# Epoch 10  global step 29580 loss 0.30686 batch 42/3281 lr 0.001 accuracy 94.48047 wps 19419.70 step time 0.37s\n","# Epoch 10  global step 29600 loss 0.31122 batch 62/3281 lr 0.001 accuracy 94.23828 wps 19365.29 step time 0.38s\n","# Epoch 10  global step 29620 loss 0.30303 batch 82/3281 lr 0.001 accuracy 94.53906 wps 18846.32 step time 0.35s\n","# Epoch 10  global step 29640 loss 0.32692 batch 102/3281 lr 0.001 accuracy 94.14844 wps 19657.02 step time 0.44s\n","# Epoch 10  global step 29660 loss 0.31114 batch 122/3281 lr 0.001 accuracy 94.33203 wps 18533.15 step time 0.34s\n","# Epoch 10  global step 29680 loss 0.33353 batch 142/3281 lr 0.001 accuracy 93.78125 wps 20846.47 step time 0.43s\n","# Epoch 10  global step 29700 loss 0.32227 batch 162/3281 lr 0.001 accuracy 94.06250 wps 18997.31 step time 0.35s\n","# Epoch 10  global step 29720 loss 0.30185 batch 182/3281 lr 0.001 accuracy 94.55859 wps 18563.11 step time 0.34s\n","# Epoch 10  global step 29740 loss 0.32267 batch 202/3281 lr 0.001 accuracy 94.03125 wps 18786.72 step time 0.42s\n","# Epoch 10  global step 29760 loss 0.31626 batch 222/3281 lr 0.001 accuracy 94.14844 wps 18855.85 step time 0.36s\n","# Epoch 10  global step 29780 loss 0.32596 batch 242/3281 lr 0.001 accuracy 94.05469 wps 19164.42 step time 0.42s\n","# Epoch 10  global step 29800 loss 0.32321 batch 262/3281 lr 0.001 accuracy 94.25781 wps 18943.48 step time 0.40s\n","# Epoch 10  global step 29820 loss 0.30791 batch 282/3281 lr 0.001 accuracy 94.58984 wps 19041.36 step time 0.36s\n","# Epoch 10  global step 29840 loss 0.34073 batch 302/3281 lr 0.001 accuracy 93.57031 wps 19025.23 step time 0.41s\n","# Epoch 10  global step 29860 loss 0.33113 batch 322/3281 lr 0.001 accuracy 93.88672 wps 19578.95 step time 0.38s\n","# Epoch 10  global step 29880 loss 0.29530 batch 342/3281 lr 0.001 accuracy 94.71094 wps 18831.62 step time 0.35s\n","# Epoch 10  global step 29900 loss 0.32064 batch 362/3281 lr 0.001 accuracy 94.07813 wps 18869.05 step time 0.46s\n","# Epoch 10  global step 29920 loss 0.31383 batch 382/3281 lr 0.001 accuracy 94.19531 wps 18571.70 step time 0.34s\n","# Epoch 10  global step 29940 loss 0.32136 batch 402/3281 lr 0.001 accuracy 94.16797 wps 19378.98 step time 0.37s\n","# Epoch 10  global step 29960 loss 0.30601 batch 422/3281 lr 0.001 accuracy 94.47266 wps 18287.15 step time 0.34s\n","# Epoch 10  global step 29980 loss 0.30734 batch 442/3281 lr 0.001 accuracy 94.37500 wps 19847.52 step time 0.40s\n","# Epoch 10  global step 30000 loss 0.30568 batch 462/3281 lr 0.001 accuracy 94.42969 wps 19020.90 step time 0.35s\n","# global step 30000, eval model at Fri May 22 13:55:01 2020\n","2020-05-22 13:55:04.080845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:55:04.081535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 13:55:04.081699: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 13:55:04.081762: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 13:55:04.081838: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 13:55:04.081898: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 13:55:04.081955: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 13:55:04.081999: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 13:55:04.082046: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 13:55:04.082206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:55:04.082899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:55:04.083411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 13:55:04.083462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 13:55:04.083482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 13:55:04.083496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 13:55:04.083656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:55:04.084411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 13:55:04.084997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6633722765744849\n","# location_distance_from_business_district - 0.5440615361655672\n","# location_easy_to_find - 0.7121450917360639\n","# service_wait_time - 0.6680737406102428\n","# service_waiters_attitude - 0.7961250011173686\n","# service_parking_convenience - 0.7287709095060169\n","# service_serving_speed - 0.7625654025733215\n","# price_level - 0.7800718114210194\n","# price_cost_effective - 0.710951469117505\n","# price_discount - 0.6646812445003657\n","# environment_decoration - 0.7300137898120878\n","# environment_noise - 0.7572167266999144\n","# environment_space - 0.7600456186879603\n","# environment_cleaness - 0.755500336446014\n","# dish_portion - 0.7236458623421118\n","# dish_taste - 0.7324319467745385\n","# dish_look - 0.5707365206298256\n","# dish_recommendation - 0.7457888564358509\n","# others_overall_experience - 0.5893384805595293\n","# others_willing_to_consume_again - 0.7089832276657237\n","# Eval loss 0.26389, f1 0.70523\n","# current result -0.7052259924687757, previous best result -0.7039255861187461\n","# Epoch 10  global step 30020 loss 0.31445 batch 482/3281 lr 0.001 accuracy 94.27734 wps 18967.29 step time 0.35s\n","# Epoch 10  global step 30040 loss 0.30736 batch 502/3281 lr 0.001 accuracy 94.45703 wps 19177.92 step time 0.37s\n","# Epoch 10  global step 30060 loss 0.30512 batch 522/3281 lr 0.001 accuracy 94.47266 wps 18625.41 step time 0.34s\n","# Epoch 10  global step 30080 loss 0.32350 batch 542/3281 lr 0.001 accuracy 94.16797 wps 18750.84 step time 0.40s\n","# Epoch 10  global step 30100 loss 0.33196 batch 562/3281 lr 0.001 accuracy 93.96875 wps 19233.55 step time 0.37s\n","# Epoch 10  global step 30120 loss 0.32792 batch 582/3281 lr 0.001 accuracy 94.00781 wps 20226.99 step time 0.41s\n","# Epoch 10  global step 30140 loss 0.33326 batch 602/3281 lr 0.001 accuracy 93.97266 wps 19580.04 step time 0.38s\n","# Epoch 10  global step 30160 loss 0.32985 batch 622/3281 lr 0.001 accuracy 93.88281 wps 18348.45 step time 0.45s\n","# Epoch 10  global step 30180 loss 0.31260 batch 642/3281 lr 0.001 accuracy 94.31250 wps 19034.44 step time 0.40s\n","# Epoch 10  global step 30200 loss 0.30506 batch 662/3281 lr 0.001 accuracy 94.34766 wps 18114.51 step time 0.36s\n","# Epoch 10  global step 30220 loss 0.32138 batch 682/3281 lr 0.001 accuracy 94.12891 wps 18233.05 step time 0.40s\n","# Epoch 10  global step 30240 loss 0.32205 batch 702/3281 lr 0.001 accuracy 94.15234 wps 19013.51 step time 0.35s\n","# Epoch 10  global step 30260 loss 0.30224 batch 722/3281 lr 0.001 accuracy 94.46484 wps 18358.06 step time 0.34s\n","# Epoch 10  global step 30280 loss 0.32194 batch 742/3281 lr 0.001 accuracy 94.12109 wps 19601.81 step time 0.37s\n","# Epoch 10  global step 30300 loss 0.30611 batch 762/3281 lr 0.001 accuracy 94.40625 wps 18675.65 step time 0.35s\n","# Epoch 10  global step 30320 loss 0.31868 batch 782/3281 lr 0.001 accuracy 94.21875 wps 18431.43 step time 0.34s\n","# Epoch 10  global step 30340 loss 0.32165 batch 802/3281 lr 0.001 accuracy 94.15234 wps 18592.86 step time 0.44s\n","# Epoch 10  global step 30360 loss 0.31398 batch 822/3281 lr 0.001 accuracy 94.46875 wps 18699.39 step time 0.35s\n","# Epoch 10  global step 30380 loss 0.32523 batch 842/3281 lr 0.001 accuracy 93.98828 wps 19509.98 step time 0.38s\n","# Epoch 10  global step 30400 loss 0.33908 batch 862/3281 lr 0.001 accuracy 93.83203 wps 19630.00 step time 0.43s\n","# Epoch 10  global step 30420 loss 0.32589 batch 882/3281 lr 0.001 accuracy 94.10547 wps 19410.76 step time 0.42s\n","# Epoch 10  global step 30440 loss 0.32987 batch 902/3281 lr 0.001 accuracy 93.90625 wps 20052.86 step time 0.40s\n","# Epoch 10  global step 30460 loss 0.35077 batch 922/3281 lr 0.001 accuracy 93.67969 wps 19081.80 step time 0.41s\n","# Epoch 10  global step 30480 loss 0.31164 batch 942/3281 lr 0.001 accuracy 94.38281 wps 19171.46 step time 0.42s\n","# Epoch 10  global step 30500 loss 0.32794 batch 962/3281 lr 0.001 accuracy 93.94922 wps 20098.86 step time 0.39s\n","# Epoch 10  global step 30520 loss 0.34329 batch 982/3281 lr 0.001 accuracy 93.86328 wps 17742.17 step time 0.52s\n","# Epoch 10  global step 30540 loss 0.31812 batch 1002/3281 lr 0.001 accuracy 94.21094 wps 19506.42 step time 0.38s\n","# Epoch 10  global step 30560 loss 0.31564 batch 1022/3281 lr 0.001 accuracy 94.33594 wps 18932.03 step time 0.35s\n","# Epoch 10  global step 30580 loss 0.32373 batch 1042/3281 lr 0.001 accuracy 94.11719 wps 19161.12 step time 0.36s\n","# Epoch 10  global step 30600 loss 0.32365 batch 1062/3281 lr 0.001 accuracy 94.14844 wps 18571.19 step time 0.34s\n","# Epoch 10  global step 30620 loss 0.32523 batch 1082/3281 lr 0.001 accuracy 94.17187 wps 18995.08 step time 0.35s\n","# Epoch 10  global step 30640 loss 0.34105 batch 1102/3281 lr 0.001 accuracy 93.64062 wps 19656.59 step time 0.38s\n","# Epoch 10  global step 30660 loss 0.32872 batch 1122/3281 lr 0.001 accuracy 94.19922 wps 19534.84 step time 0.38s\n","# Epoch 10  global step 30680 loss 0.31834 batch 1142/3281 lr 0.001 accuracy 94.23047 wps 19254.04 step time 0.41s\n","# Epoch 10  global step 30700 loss 0.32936 batch 1162/3281 lr 0.001 accuracy 94.01562 wps 18766.72 step time 0.35s\n","# Epoch 10  global step 30720 loss 0.31112 batch 1182/3281 lr 0.001 accuracy 94.45703 wps 18429.37 step time 0.39s\n","# Epoch 10  global step 30740 loss 0.32185 batch 1202/3281 lr 0.001 accuracy 94.14844 wps 19999.14 step time 0.39s\n","# Epoch 10  global step 30760 loss 0.32392 batch 1222/3281 lr 0.001 accuracy 94.21484 wps 19015.70 step time 0.36s\n","# Epoch 10  global step 30780 loss 0.31805 batch 1242/3281 lr 0.001 accuracy 94.16406 wps 18311.28 step time 0.39s\n","# Epoch 10  global step 30800 loss 0.32814 batch 1262/3281 lr 0.001 accuracy 93.99219 wps 18887.96 step time 0.42s\n","# Epoch 10  global step 30820 loss 0.32170 batch 1282/3281 lr 0.001 accuracy 94.14062 wps 17959.83 step time 0.37s\n","# Epoch 10  global step 30840 loss 0.32982 batch 1302/3281 lr 0.001 accuracy 93.89062 wps 18834.01 step time 0.41s\n","# Epoch 10  global step 30860 loss 0.32429 batch 1322/3281 lr 0.001 accuracy 94.14063 wps 19070.76 step time 0.37s\n","# Epoch 10  global step 30880 loss 0.32963 batch 1342/3281 lr 0.001 accuracy 94.01172 wps 18147.02 step time 0.46s\n","# Epoch 10  global step 30900 loss 0.34773 batch 1362/3281 lr 0.001 accuracy 93.61719 wps 18746.36 step time 0.50s\n","# Epoch 10  global step 30920 loss 0.32899 batch 1382/3281 lr 0.001 accuracy 93.97656 wps 19076.65 step time 0.40s\n","# Epoch 10  global step 30940 loss 0.31609 batch 1402/3281 lr 0.001 accuracy 94.33984 wps 18711.33 step time 0.40s\n","# Epoch 10  global step 30960 loss 0.32447 batch 1422/3281 lr 0.001 accuracy 94.03125 wps 18838.01 step time 0.40s\n","# Epoch 10  global step 30980 loss 0.32408 batch 1442/3281 lr 0.001 accuracy 94.01172 wps 19620.28 step time 0.39s\n","# Epoch 10  global step 31000 loss 0.32117 batch 1462/3281 lr 0.001 accuracy 94.18750 wps 18824.23 step time 0.36s\n","# Epoch 10  global step 31020 loss 0.33168 batch 1482/3281 lr 0.001 accuracy 94.10938 wps 19470.05 step time 0.37s\n","# Epoch 10  global step 31040 loss 0.32077 batch 1502/3281 lr 0.001 accuracy 94.21094 wps 18943.98 step time 0.36s\n","# Epoch 10  global step 31060 loss 0.33482 batch 1522/3281 lr 0.001 accuracy 93.94922 wps 19888.84 step time 0.40s\n","# Epoch 10  global step 31080 loss 0.30269 batch 1542/3281 lr 0.001 accuracy 94.49219 wps 19597.68 step time 0.37s\n","# Epoch 10  global step 31100 loss 0.33032 batch 1562/3281 lr 0.001 accuracy 94.02344 wps 19391.10 step time 0.37s\n","# Epoch 10  global step 31120 loss 0.32418 batch 1582/3281 lr 0.001 accuracy 94.12109 wps 19416.99 step time 0.36s\n","# Epoch 10  global step 31140 loss 0.32438 batch 1602/3281 lr 0.001 accuracy 94.24609 wps 18424.10 step time 0.43s\n","# Epoch 10  global step 31160 loss 0.31339 batch 1622/3281 lr 0.001 accuracy 94.28906 wps 17673.70 step time 0.32s\n","# Epoch 10  global step 31180 loss 0.33983 batch 1642/3281 lr 0.001 accuracy 93.85547 wps 19446.89 step time 0.43s\n","# Epoch 10  global step 31200 loss 0.30046 batch 1662/3281 lr 0.001 accuracy 94.57812 wps 18400.41 step time 0.33s\n","# Epoch 10  global step 31220 loss 0.31392 batch 1682/3281 lr 0.001 accuracy 94.28516 wps 18764.06 step time 0.35s\n","# Epoch 10  global step 31240 loss 0.32738 batch 1702/3281 lr 0.001 accuracy 94.07813 wps 19523.95 step time 0.37s\n","# Epoch 10  global step 31260 loss 0.30432 batch 1722/3281 lr 0.001 accuracy 94.50781 wps 17159.55 step time 0.32s\n","# Epoch 10  global step 31280 loss 0.31786 batch 1742/3281 lr 0.001 accuracy 94.24219 wps 19417.62 step time 0.38s\n","# Epoch 10  global step 31300 loss 0.32895 batch 1762/3281 lr 0.001 accuracy 93.84766 wps 19384.29 step time 0.37s\n","# Epoch 10  global step 31320 loss 0.33077 batch 1782/3281 lr 0.001 accuracy 94.02344 wps 19689.61 step time 0.38s\n","# Epoch 10  global step 31340 loss 0.32948 batch 1802/3281 lr 0.001 accuracy 93.98438 wps 18435.39 step time 0.39s\n","# Epoch 10  global step 31360 loss 0.31411 batch 1822/3281 lr 0.001 accuracy 94.33984 wps 18957.17 step time 0.35s\n","# Epoch 10  global step 31380 loss 0.33148 batch 1842/3281 lr 0.001 accuracy 93.96094 wps 19339.23 step time 0.44s\n","# Epoch 10  global step 31400 loss 0.31703 batch 1862/3281 lr 0.001 accuracy 94.11719 wps 19667.13 step time 0.38s\n","# Epoch 10  global step 31420 loss 0.32314 batch 1882/3281 lr 0.001 accuracy 94.32422 wps 18190.59 step time 0.33s\n","# Epoch 10  global step 31440 loss 0.31227 batch 1902/3281 lr 0.001 accuracy 94.41016 wps 18380.23 step time 0.34s\n","# Epoch 10  global step 31460 loss 0.32722 batch 1922/3281 lr 0.001 accuracy 94.20703 wps 18705.12 step time 0.34s\n","# Epoch 10  global step 31480 loss 0.30032 batch 1942/3281 lr 0.001 accuracy 94.55859 wps 18151.81 step time 0.33s\n","# Epoch 10  global step 31500 loss 0.33432 batch 1962/3281 lr 0.001 accuracy 94.00000 wps 19277.01 step time 0.42s\n","# Epoch 10  global step 31520 loss 0.32336 batch 1982/3281 lr 0.001 accuracy 93.99609 wps 19115.91 step time 0.40s\n","# Epoch 10  global step 31560 loss 0.34126 batch 2022/3281 lr 0.001 accuracy 93.86328 wps 19413.82 step time 0.38s\n","# Epoch 10  global step 31580 loss 0.33125 batch 2042/3281 lr 0.001 accuracy 94.01953 wps 18637.92 step time 0.39s\n","# Epoch 10  global step 31600 loss 0.34429 batch 2062/3281 lr 0.001 accuracy 93.89062 wps 19687.00 step time 0.38s\n","# Epoch 10  global step 31620 loss 0.31019 batch 2082/3281 lr 0.001 accuracy 94.43359 wps 18310.80 step time 0.33s\n","# Epoch 10  global step 31640 loss 0.31702 batch 2102/3281 lr 0.001 accuracy 94.31250 wps 18890.09 step time 0.41s\n","# Epoch 10  global step 31660 loss 0.32413 batch 2122/3281 lr 0.001 accuracy 94.17188 wps 18537.23 step time 0.35s\n","# Epoch 10  global step 31680 loss 0.32042 batch 2142/3281 lr 0.001 accuracy 94.27344 wps 18353.19 step time 0.38s\n","# Epoch 10  global step 31700 loss 0.31404 batch 2162/3281 lr 0.001 accuracy 94.18359 wps 19126.40 step time 0.36s\n","# Epoch 10  global step 31720 loss 0.31589 batch 2182/3281 lr 0.001 accuracy 94.44531 wps 18937.50 step time 0.35s\n","# Epoch 10  global step 31740 loss 0.29330 batch 2202/3281 lr 0.001 accuracy 94.55078 wps 17909.81 step time 0.32s\n","# Epoch 10  global step 31760 loss 0.33377 batch 2222/3281 lr 0.001 accuracy 93.87109 wps 19917.67 step time 0.45s\n","# Epoch 10  global step 31780 loss 0.33430 batch 2242/3281 lr 0.001 accuracy 93.75000 wps 19278.43 step time 0.48s\n","# Epoch 10  global step 31800 loss 0.33056 batch 2262/3281 lr 0.001 accuracy 94.03516 wps 19083.67 step time 0.42s\n","# Epoch 10  global step 31820 loss 0.31650 batch 2282/3281 lr 0.001 accuracy 94.32813 wps 18329.62 step time 0.33s\n","# Epoch 10  global step 31840 loss 0.31908 batch 2302/3281 lr 0.001 accuracy 94.17187 wps 18442.51 step time 0.34s\n","# Epoch 10  global step 31860 loss 0.33831 batch 2322/3281 lr 0.001 accuracy 93.56250 wps 19275.67 step time 0.36s\n","# Epoch 10  global step 31880 loss 0.33436 batch 2342/3281 lr 0.001 accuracy 94.06250 wps 19428.93 step time 0.38s\n","# Epoch 10  global step 31900 loss 0.33411 batch 2362/3281 lr 0.001 accuracy 93.75000 wps 16604.90 step time 0.42s\n","# Epoch 10  global step 31920 loss 0.31282 batch 2382/3281 lr 0.001 accuracy 94.41016 wps 16331.80 step time 0.42s\n","# Epoch 10  global step 31940 loss 0.30724 batch 2402/3281 lr 0.001 accuracy 94.40625 wps 16857.74 step time 0.36s\n","# Epoch 10  global step 31960 loss 0.32790 batch 2422/3281 lr 0.001 accuracy 94.10547 wps 17201.48 step time 0.46s\n","# Epoch 10  global step 31980 loss 0.31692 batch 2442/3281 lr 0.001 accuracy 94.29687 wps 16913.23 step time 0.37s\n","# Epoch 10  global step 32000 loss 0.31431 batch 2462/3281 lr 0.001 accuracy 94.35156 wps 16775.83 step time 0.42s\n","# global step 32000, eval model at Fri May 22 14:08:55 2020\n","2020-05-22 14:08:58.664956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:08:58.665693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 14:08:58.665871: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 14:08:58.665948: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 14:08:58.665998: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 14:08:58.666047: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 14:08:58.666090: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 14:08:58.666157: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 14:08:58.666207: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 14:08:58.666347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:08:58.666995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:08:58.667535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 14:08:58.667642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 14:08:58.667679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 14:08:58.667701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 14:08:58.667882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:08:58.668494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:08:58.669034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6683597859432266\n","# location_distance_from_business_district - 0.553723555257658\n","# location_easy_to_find - 0.7152718815132229\n","# service_wait_time - 0.6727203758561865\n","# service_waiters_attitude - 0.7964699549775238\n","# service_parking_convenience - 0.7328162249318478\n","# service_serving_speed - 0.7670066302263996\n","# price_level - 0.7809229794264303\n","# price_cost_effective - 0.7109316295659092\n","# price_discount - 0.6730283353770461\n","# environment_decoration - 0.7313679064316593\n","# environment_noise - 0.756081990057299\n","# environment_space - 0.7596572787252194\n","# environment_cleaness - 0.7522586939278729\n","# dish_portion - 0.7247307201517849\n","# dish_taste - 0.7310932934995963\n","# dish_look - 0.5736962588907675\n","# dish_recommendation - 0.7443949304806042\n","# others_overall_experience - 0.5912020512525009\n","# others_willing_to_consume_again - 0.7095074899167864\n","# Eval loss 0.26473, f1 0.70726\n","# current result -0.7072620983204769, previous best result -0.7052259924687757\n","# Epoch 10  global step 32020 loss 0.32455 batch 2482/3281 lr 0.001 accuracy 94.16406 wps 16241.77 step time 0.48s\n","# Epoch 10  global step 32040 loss 0.33814 batch 2502/3281 lr 0.001 accuracy 93.99609 wps 17146.41 step time 0.39s\n","# Epoch 10  global step 32060 loss 0.31716 batch 2522/3281 lr 0.001 accuracy 94.28125 wps 16535.37 step time 0.46s\n","# Epoch 10  global step 32080 loss 0.32853 batch 2542/3281 lr 0.001 accuracy 94.03516 wps 17593.26 step time 0.39s\n","# Epoch 10  global step 32100 loss 0.34836 batch 2562/3281 lr 0.001 accuracy 93.55859 wps 17080.12 step time 0.51s\n","# Epoch 10  global step 32120 loss 0.33252 batch 2582/3281 lr 0.001 accuracy 94.01563 wps 17263.84 step time 0.47s\n","# Epoch 10  global step 32140 loss 0.33633 batch 2602/3281 lr 0.001 accuracy 93.84375 wps 17671.92 step time 0.48s\n","# Epoch 10  global step 32160 loss 0.34974 batch 2622/3281 lr 0.001 accuracy 93.40625 wps 16480.94 step time 0.49s\n","# Epoch 10  global step 32180 loss 0.35135 batch 2642/3281 lr 0.001 accuracy 93.53125 wps 16184.86 step time 0.61s\n","# Epoch 10  global step 32200 loss 0.32867 batch 2662/3281 lr 0.001 accuracy 94.02734 wps 16016.03 step time 0.47s\n","# Epoch 10  global step 32220 loss 0.31990 batch 2682/3281 lr 0.001 accuracy 94.09766 wps 15956.71 step time 0.47s\n","# Epoch 10  global step 32240 loss 0.33957 batch 2702/3281 lr 0.001 accuracy 93.84766 wps 17392.54 step time 0.45s\n","# Epoch 10  global step 32260 loss 0.33681 batch 2722/3281 lr 0.001 accuracy 93.95312 wps 14660.54 step time 0.42s\n","# Epoch 10  global step 32280 loss 0.33541 batch 2742/3281 lr 0.001 accuracy 93.84766 wps 16577.52 step time 0.44s\n","# Epoch 10  global step 32300 loss 0.33703 batch 2762/3281 lr 0.001 accuracy 93.85937 wps 17329.57 step time 0.40s\n","# Epoch 10  global step 32320 loss 0.31502 batch 2782/3281 lr 0.001 accuracy 94.23047 wps 16758.52 step time 0.38s\n","# Epoch 10  global step 32340 loss 0.32421 batch 2802/3281 lr 0.001 accuracy 94.06641 wps 17433.51 step time 0.41s\n","# Epoch 10  global step 32360 loss 0.30457 batch 2822/3281 lr 0.001 accuracy 94.57031 wps 16650.03 step time 0.34s\n","# Epoch 10  global step 32380 loss 0.30307 batch 2842/3281 lr 0.001 accuracy 94.54688 wps 16403.93 step time 0.35s\n","# Epoch 10  global step 32400 loss 0.32939 batch 2862/3281 lr 0.001 accuracy 94.19922 wps 17138.45 step time 0.43s\n","# Epoch 10  global step 32420 loss 0.31934 batch 2882/3281 lr 0.001 accuracy 94.20703 wps 16839.87 step time 0.39s\n","# Epoch 10  global step 32440 loss 0.32859 batch 2902/3281 lr 0.001 accuracy 93.97266 wps 15499.88 step time 0.51s\n","# Epoch 10  global step 32460 loss 0.31866 batch 2922/3281 lr 0.001 accuracy 94.12109 wps 16646.66 step time 0.49s\n","# Epoch 10  global step 32480 loss 0.32645 batch 2942/3281 lr 0.001 accuracy 94.03125 wps 16425.93 step time 0.43s\n","# Epoch 10  global step 32500 loss 0.32954 batch 2962/3281 lr 0.001 accuracy 94.04687 wps 17253.57 step time 0.42s\n","# Epoch 10  global step 32520 loss 0.33351 batch 2982/3281 lr 0.001 accuracy 93.89453 wps 17288.61 step time 0.44s\n","# Epoch 10  global step 32540 loss 0.33367 batch 3002/3281 lr 0.001 accuracy 93.94531 wps 17614.09 step time 0.42s\n","# Epoch 10  global step 32560 loss 0.31708 batch 3022/3281 lr 0.001 accuracy 94.29297 wps 16706.53 step time 0.38s\n","# Epoch 10  global step 32580 loss 0.33698 batch 3042/3281 lr 0.001 accuracy 93.69531 wps 16852.59 step time 0.47s\n","# Epoch 10  global step 32600 loss 0.34944 batch 3062/3281 lr 0.001 accuracy 93.66406 wps 17997.19 step time 0.46s\n","# Epoch 10  global step 32620 loss 0.32359 batch 3082/3281 lr 0.001 accuracy 94.04297 wps 17047.78 step time 0.40s\n","# Epoch 10  global step 32640 loss 0.33326 batch 3102/3281 lr 0.001 accuracy 94.04297 wps 17093.86 step time 0.36s\n","# Epoch 10  global step 32660 loss 0.32852 batch 3122/3281 lr 0.001 accuracy 94.05469 wps 17529.41 step time 0.44s\n","# Epoch 10  global step 32680 loss 0.33956 batch 3142/3281 lr 0.001 accuracy 93.87500 wps 17662.61 step time 0.40s\n","# Epoch 10  global step 32700 loss 0.30458 batch 3162/3281 lr 0.001 accuracy 94.52344 wps 17250.04 step time 0.39s\n","# Epoch 10  global step 32720 loss 0.31693 batch 3182/3281 lr 0.001 accuracy 94.39453 wps 16641.22 step time 0.44s\n","# Epoch 10  global step 32740 loss 0.32191 batch 3202/3281 lr 0.001 accuracy 94.13281 wps 15879.83 step time 0.41s\n","# Epoch 10  global step 32760 loss 0.32306 batch 3222/3281 lr 0.001 accuracy 94.14453 wps 16876.85 step time 0.37s\n","# Epoch 10  global step 32780 loss 0.33551 batch 3242/3281 lr 0.001 accuracy 93.81250 wps 17186.64 step time 0.44s\n","# Epoch 10  global step 32800 loss 0.32791 batch 3262/3281 lr 0.001 accuracy 94.05469 wps 16667.64 step time 0.41s\n","# Epoch 10  global step 32820 loss 0.35247 batch 3282/3281 lr 0.001 accuracy 93.62891 wps 16894.48 step time 0.43s\n","# Finsh epoch 10, global step 32820\n","# Epoch 11  global step 32840 loss 0.32089 batch 20/3281 lr 0.001 accuracy 94.14844 wps 18487.02 step time 0.38s\n","# Epoch 11  global step 32860 loss 0.30211 batch 40/3281 lr 0.001 accuracy 94.43750 wps 18847.92 step time 0.36s\n","# Epoch 11  global step 32880 loss 0.32203 batch 60/3281 lr 0.001 accuracy 94.22266 wps 18749.93 step time 0.44s\n","# Epoch 11  global step 32900 loss 0.29714 batch 80/3281 lr 0.001 accuracy 94.57422 wps 19411.32 step time 0.37s\n","# Epoch 11  global step 32920 loss 0.30391 batch 100/3281 lr 0.001 accuracy 94.53125 wps 19360.12 step time 0.40s\n","# Epoch 11  global step 32940 loss 0.31320 batch 120/3281 lr 0.001 accuracy 94.33984 wps 18861.69 step time 0.35s\n","# Epoch 11  global step 32960 loss 0.30022 batch 140/3281 lr 0.001 accuracy 94.55469 wps 17520.46 step time 0.35s\n","# Epoch 11  global step 32980 loss 0.32328 batch 160/3281 lr 0.001 accuracy 94.17969 wps 18814.07 step time 0.36s\n","# Epoch 11  global step 33000 loss 0.32344 batch 180/3281 lr 0.001 accuracy 93.94141 wps 17497.73 step time 0.43s\n","# Epoch 11  global step 33020 loss 0.30666 batch 200/3281 lr 0.001 accuracy 94.66797 wps 18398.53 step time 0.34s\n","# Epoch 11  global step 33040 loss 0.33335 batch 220/3281 lr 0.001 accuracy 93.96094 wps 19975.14 step time 0.43s\n","# Epoch 11  global step 33060 loss 0.30626 batch 240/3281 lr 0.001 accuracy 94.42578 wps 19399.37 step time 0.37s\n","# Epoch 11  global step 33080 loss 0.31479 batch 260/3281 lr 0.001 accuracy 94.13672 wps 19127.35 step time 0.41s\n","# Epoch 11  global step 33100 loss 0.29637 batch 280/3281 lr 0.001 accuracy 94.50391 wps 19023.59 step time 0.36s\n","# Epoch 11  global step 33120 loss 0.30043 batch 300/3281 lr 0.001 accuracy 94.53125 wps 19278.35 step time 0.37s\n","# Epoch 11  global step 33140 loss 0.31865 batch 320/3281 lr 0.001 accuracy 94.22266 wps 19934.56 step time 0.38s\n","# Epoch 11  global step 33160 loss 0.31609 batch 340/3281 lr 0.001 accuracy 94.15234 wps 18725.99 step time 0.35s\n","# Epoch 11  global step 33180 loss 0.29550 batch 360/3281 lr 0.001 accuracy 94.67187 wps 17347.21 step time 0.36s\n","# Epoch 11  global step 33200 loss 0.32721 batch 380/3281 lr 0.001 accuracy 94.01562 wps 19274.09 step time 0.37s\n","# Epoch 11  global step 33220 loss 0.31351 batch 400/3281 lr 0.001 accuracy 94.19922 wps 19559.21 step time 0.44s\n","# Epoch 11  global step 33240 loss 0.30687 batch 420/3281 lr 0.001 accuracy 94.46484 wps 18535.21 step time 0.33s\n","# Epoch 11  global step 33260 loss 0.32348 batch 440/3281 lr 0.001 accuracy 94.11719 wps 20190.05 step time 0.41s\n","# Epoch 11  global step 33280 loss 0.29818 batch 460/3281 lr 0.001 accuracy 94.76172 wps 18475.38 step time 0.39s\n","# Epoch 11  global step 33300 loss 0.30231 batch 480/3281 lr 0.001 accuracy 94.48828 wps 18695.68 step time 0.34s\n","# Epoch 11  global step 33320 loss 0.31481 batch 500/3281 lr 0.001 accuracy 94.36719 wps 19339.84 step time 0.38s\n","# Epoch 11  global step 33340 loss 0.29926 batch 520/3281 lr 0.001 accuracy 94.53906 wps 18468.53 step time 0.39s\n","# Epoch 11  global step 33360 loss 0.32067 batch 540/3281 lr 0.001 accuracy 94.01172 wps 18413.38 step time 0.40s\n","# Epoch 11  global step 33380 loss 0.31287 batch 560/3281 lr 0.001 accuracy 94.35547 wps 19289.74 step time 0.36s\n","# Epoch 11  global step 33400 loss 0.31989 batch 580/3281 lr 0.001 accuracy 94.22266 wps 19155.92 step time 0.37s\n","# Epoch 11  global step 33420 loss 0.31549 batch 600/3281 lr 0.001 accuracy 94.23437 wps 19728.23 step time 0.40s\n","# Epoch 11  global step 33440 loss 0.30664 batch 620/3281 lr 0.001 accuracy 94.42969 wps 18845.88 step time 0.37s\n","# Epoch 11  global step 33460 loss 0.30944 batch 640/3281 lr 0.001 accuracy 94.33594 wps 16661.90 step time 0.43s\n","# Epoch 11  global step 33480 loss 0.31093 batch 660/3281 lr 0.001 accuracy 94.24609 wps 16221.52 step time 0.50s\n","# Epoch 11  global step 33500 loss 0.30075 batch 680/3281 lr 0.001 accuracy 94.68750 wps 16762.19 step time 0.36s\n","# Epoch 11  global step 33520 loss 0.30729 batch 700/3281 lr 0.001 accuracy 94.45703 wps 17171.91 step time 0.40s\n","# Epoch 11  global step 33540 loss 0.32347 batch 720/3281 lr 0.001 accuracy 94.01563 wps 17184.53 step time 0.44s\n","# Epoch 11  global step 33560 loss 0.30097 batch 740/3281 lr 0.001 accuracy 94.57812 wps 16858.93 step time 0.40s\n","# Epoch 11  global step 33580 loss 0.31647 batch 760/3281 lr 0.001 accuracy 94.25781 wps 17568.09 step time 0.44s\n","# Epoch 11  global step 33600 loss 0.31429 batch 780/3281 lr 0.001 accuracy 94.30469 wps 17559.05 step time 0.46s\n","# Epoch 11  global step 33620 loss 0.31164 batch 800/3281 lr 0.001 accuracy 94.22656 wps 15873.52 step time 0.44s\n","# Epoch 11  global step 33640 loss 0.30481 batch 820/3281 lr 0.001 accuracy 94.36719 wps 16990.15 step time 0.41s\n","# Epoch 11  global step 33660 loss 0.32569 batch 840/3281 lr 0.001 accuracy 94.05859 wps 16136.34 step time 0.52s\n","# Epoch 11  global step 33680 loss 0.30834 batch 860/3281 lr 0.001 accuracy 94.65234 wps 15872.09 step time 0.43s\n","# Epoch 11  global step 33700 loss 0.28899 batch 880/3281 lr 0.001 accuracy 94.66016 wps 16726.90 step time 0.39s\n","# Epoch 11  global step 33720 loss 0.30772 batch 900/3281 lr 0.001 accuracy 94.48047 wps 14467.56 step time 0.44s\n","# Epoch 11  global step 33740 loss 0.31147 batch 920/3281 lr 0.001 accuracy 94.43359 wps 16716.49 step time 0.36s\n","# Epoch 11  global step 33760 loss 0.31193 batch 940/3281 lr 0.001 accuracy 94.29687 wps 16965.24 step time 0.47s\n","# Epoch 11  global step 33780 loss 0.30858 batch 960/3281 lr 0.001 accuracy 94.44531 wps 17664.34 step time 0.41s\n","# Epoch 11  global step 33800 loss 0.32473 batch 980/3281 lr 0.001 accuracy 94.07813 wps 15867.53 step time 0.54s\n","# Epoch 11  global step 33820 loss 0.31870 batch 1000/3281 lr 0.001 accuracy 94.15625 wps 16219.66 step time 0.53s\n","# Epoch 11  global step 33840 loss 0.30969 batch 1020/3281 lr 0.001 accuracy 94.25000 wps 17286.92 step time 0.37s\n","# Epoch 11  global step 33860 loss 0.31800 batch 1040/3281 lr 0.001 accuracy 94.15625 wps 16318.94 step time 0.47s\n","# Epoch 11  global step 33880 loss 0.30528 batch 1060/3281 lr 0.001 accuracy 94.46094 wps 16872.72 step time 0.40s\n","# Epoch 11  global step 33900 loss 0.34056 batch 1080/3281 lr 0.001 accuracy 93.70313 wps 16205.54 step time 0.60s\n","# Epoch 11  global step 33920 loss 0.30070 batch 1100/3281 lr 0.001 accuracy 94.61719 wps 16167.30 step time 0.41s\n","# Epoch 11  global step 33940 loss 0.33424 batch 1120/3281 lr 0.001 accuracy 93.94531 wps 16452.64 step time 0.49s\n","# Epoch 11  global step 33960 loss 0.29805 batch 1140/3281 lr 0.001 accuracy 94.62500 wps 16346.71 step time 0.39s\n","# Epoch 11  global step 33980 loss 0.30805 batch 1160/3281 lr 0.001 accuracy 94.21875 wps 16515.23 step time 0.42s\n","# Epoch 11  global step 34000 loss 0.33328 batch 1180/3281 lr 0.001 accuracy 93.85937 wps 17300.03 step time 0.43s\n","# global step 34000, eval model at Fri May 22 14:24:01 2020\n","2020-05-22 14:24:03.342583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:24:03.343309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 14:24:03.343395: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 14:24:03.343436: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 14:24:03.343477: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 14:24:03.343549: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 14:24:03.343588: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 14:24:03.343626: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 14:24:03.343693: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 14:24:03.343817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:24:03.344550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:24:03.345186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 14:24:03.345279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 14:24:03.345293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 14:24:03.345318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 14:24:03.345475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:24:03.346308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:24:03.346927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6652069467421142\n","# location_distance_from_business_district - 0.549010208629445\n","# location_easy_to_find - 0.7184944343324086\n","# service_wait_time - 0.6745285759286299\n","# service_waiters_attitude - 0.7967008135754939\n","# service_parking_convenience - 0.7389946756951983\n","# service_serving_speed - 0.7651984091762146\n","# price_level - 0.7797206593751269\n","# price_cost_effective - 0.7101685249537915\n","# price_discount - 0.6721453449244037\n","# environment_decoration - 0.7335614962098572\n","# environment_noise - 0.7571311930302073\n","# environment_space - 0.7595253733114704\n","# environment_cleaness - 0.7551062947904145\n","# dish_portion - 0.7234818116982421\n","# dish_taste - 0.7312147792017787\n","# dish_look - 0.5756957281752342\n","# dish_recommendation - 0.7433621737804105\n","# others_overall_experience - 0.5914679304894292\n","# others_willing_to_consume_again - 0.7096209699765543\n","# Eval loss 0.26566, f1 0.70752\n","# current result -0.7075168171998213, previous best result -0.7072620983204769\n","# Epoch 11  global step 34020 loss 0.31806 batch 1200/3281 lr 0.001 accuracy 94.27344 wps 16972.52 step time 0.39s\n","# Epoch 11  global step 34040 loss 0.30658 batch 1220/3281 lr 0.001 accuracy 94.48047 wps 16714.28 step time 0.43s\n","# Epoch 11  global step 34060 loss 0.30811 batch 1240/3281 lr 0.001 accuracy 94.45312 wps 16798.04 step time 0.43s\n","# Epoch 11  global step 34080 loss 0.30349 batch 1260/3281 lr 0.001 accuracy 94.57422 wps 16013.68 step time 0.49s\n","# Epoch 11  global step 34100 loss 0.31264 batch 1280/3281 lr 0.001 accuracy 94.43750 wps 16990.15 step time 0.39s\n","# Epoch 11  global step 34120 loss 0.31423 batch 1300/3281 lr 0.001 accuracy 94.41406 wps 17230.99 step time 0.40s\n","# Epoch 11  global step 34140 loss 0.31889 batch 1320/3281 lr 0.001 accuracy 94.26953 wps 15903.24 step time 0.44s\n","# Epoch 11  global step 34160 loss 0.31678 batch 1340/3281 lr 0.001 accuracy 94.25781 wps 17132.25 step time 0.44s\n","# Epoch 11  global step 34180 loss 0.30340 batch 1360/3281 lr 0.001 accuracy 94.46875 wps 16549.02 step time 0.36s\n","# Epoch 11  global step 34200 loss 0.32586 batch 1380/3281 lr 0.001 accuracy 94.11328 wps 16107.96 step time 0.56s\n","# Epoch 11  global step 34220 loss 0.31527 batch 1400/3281 lr 0.001 accuracy 94.39844 wps 16999.51 step time 0.38s\n","# Epoch 11  global step 34240 loss 0.30018 batch 1420/3281 lr 0.001 accuracy 94.60547 wps 16721.06 step time 0.36s\n","# Epoch 11  global step 34260 loss 0.31346 batch 1440/3281 lr 0.001 accuracy 94.33594 wps 13780.48 step time 0.46s\n","# Epoch 11  global step 34280 loss 0.31571 batch 1460/3281 lr 0.001 accuracy 94.22266 wps 11829.67 step time 0.55s\n","# Epoch 11  global step 34300 loss 0.31498 batch 1480/3281 lr 0.001 accuracy 94.29297 wps 17136.37 step time 0.42s\n","# Epoch 11  global step 34320 loss 0.32480 batch 1500/3281 lr 0.001 accuracy 94.15625 wps 17457.35 step time 0.47s\n","# Epoch 11  global step 34340 loss 0.28559 batch 1520/3281 lr 0.001 accuracy 94.97266 wps 16828.93 step time 0.37s\n","# Epoch 11  global step 34360 loss 0.31594 batch 1540/3281 lr 0.001 accuracy 94.25781 wps 17143.97 step time 0.42s\n","# Epoch 11  global step 34380 loss 0.29568 batch 1560/3281 lr 0.001 accuracy 94.84766 wps 16566.58 step time 0.35s\n","# Epoch 11  global step 34400 loss 0.30426 batch 1580/3281 lr 0.001 accuracy 94.47656 wps 16447.57 step time 0.45s\n","# Epoch 11  global step 34420 loss 0.32613 batch 1600/3281 lr 0.001 accuracy 94.07813 wps 16543.36 step time 0.49s\n","# Epoch 11  global step 34440 loss 0.31091 batch 1620/3281 lr 0.001 accuracy 94.43359 wps 16745.16 step time 0.39s\n","# Epoch 11  global step 34460 loss 0.31448 batch 1640/3281 lr 0.001 accuracy 94.18359 wps 17045.70 step time 0.38s\n","# Epoch 11  global step 34480 loss 0.31775 batch 1660/3281 lr 0.001 accuracy 94.36328 wps 16841.69 step time 0.44s\n","# Epoch 11  global step 34500 loss 0.31523 batch 1680/3281 lr 0.001 accuracy 94.27734 wps 15699.33 step time 0.49s\n","# Epoch 11  global step 34520 loss 0.31917 batch 1700/3281 lr 0.001 accuracy 94.19141 wps 16096.95 step time 0.44s\n","# Epoch 11  global step 34540 loss 0.32749 batch 1720/3281 lr 0.001 accuracy 94.06641 wps 15511.86 step time 0.45s\n","# Epoch 11  global step 34560 loss 0.34111 batch 1740/3281 lr 0.001 accuracy 93.79297 wps 17444.04 step time 0.41s\n","# Epoch 11  global step 34580 loss 0.30438 batch 1760/3281 lr 0.001 accuracy 94.59375 wps 17096.32 step time 0.37s\n","# Epoch 11  global step 34600 loss 0.34279 batch 1780/3281 lr 0.001 accuracy 93.57422 wps 16499.42 step time 0.61s\n","# Epoch 11  global step 34620 loss 0.32954 batch 1800/3281 lr 0.001 accuracy 93.98047 wps 15881.49 step time 0.51s\n","# Epoch 11  global step 34640 loss 0.30114 batch 1820/3281 lr 0.001 accuracy 94.57813 wps 17811.84 step time 0.37s\n","# Epoch 11  global step 34660 loss 0.33171 batch 1840/3281 lr 0.001 accuracy 94.07031 wps 16906.62 step time 0.43s\n","# Epoch 11  global step 34680 loss 0.32041 batch 1860/3281 lr 0.001 accuracy 94.11328 wps 16302.06 step time 0.51s\n","# Epoch 11  global step 34700 loss 0.31406 batch 1880/3281 lr 0.001 accuracy 94.38672 wps 17185.60 step time 0.39s\n","# Epoch 11  global step 34720 loss 0.31694 batch 1900/3281 lr 0.001 accuracy 94.21094 wps 17380.94 step time 0.46s\n","# Epoch 11  global step 34740 loss 0.32786 batch 1920/3281 lr 0.001 accuracy 93.91797 wps 16479.48 step time 0.54s\n","# Epoch 11  global step 34760 loss 0.32263 batch 1940/3281 lr 0.001 accuracy 94.17969 wps 16406.20 step time 0.44s\n","# Epoch 11  global step 34780 loss 0.30214 batch 1960/3281 lr 0.001 accuracy 94.66797 wps 16006.11 step time 0.44s\n","# Epoch 11  global step 34800 loss 0.31782 batch 1980/3281 lr 0.001 accuracy 94.33594 wps 16100.11 step time 0.45s\n","# Epoch 11  global step 34820 loss 0.30043 batch 2000/3281 lr 0.001 accuracy 94.60156 wps 15337.12 step time 0.49s\n","# Epoch 11  global step 34840 loss 0.33985 batch 2020/3281 lr 0.001 accuracy 93.76562 wps 17104.90 step time 0.49s\n","# Epoch 11  global step 34860 loss 0.30681 batch 2040/3281 lr 0.001 accuracy 94.33594 wps 17211.56 step time 0.43s\n","# Epoch 11  global step 34880 loss 0.31243 batch 2060/3281 lr 0.001 accuracy 94.29297 wps 16973.53 step time 0.39s\n","# Epoch 11  global step 34900 loss 0.33664 batch 2080/3281 lr 0.001 accuracy 93.87500 wps 17702.10 step time 0.46s\n","# Epoch 11  global step 34920 loss 0.31430 batch 2100/3281 lr 0.001 accuracy 94.16406 wps 17139.82 step time 0.36s\n","# Epoch 11  global step 34940 loss 0.31149 batch 2120/3281 lr 0.001 accuracy 94.39062 wps 17276.70 step time 0.36s\n","# Epoch 11  global step 34960 loss 0.31225 batch 2140/3281 lr 0.001 accuracy 94.39453 wps 17235.33 step time 0.41s\n","# Epoch 11  global step 34980 loss 0.32039 batch 2160/3281 lr 0.001 accuracy 94.30078 wps 17287.80 step time 0.39s\n","# Epoch 11  global step 35000 loss 0.32373 batch 2180/3281 lr 0.001 accuracy 94.26172 wps 16593.40 step time 0.40s\n","# Epoch 11  global step 35020 loss 0.29261 batch 2200/3281 lr 0.001 accuracy 94.90625 wps 16904.72 step time 0.35s\n","# Epoch 11  global step 35040 loss 0.32363 batch 2220/3281 lr 0.001 accuracy 94.12500 wps 17830.80 step time 0.41s\n","# Epoch 11  global step 35060 loss 0.32239 batch 2240/3281 lr 0.001 accuracy 94.22656 wps 13330.98 step time 0.56s\n","# Epoch 11  global step 35080 loss 0.35561 batch 2260/3281 lr 0.001 accuracy 93.61719 wps 14471.21 step time 0.62s\n","# Epoch 11  global step 35100 loss 0.31681 batch 2280/3281 lr 0.001 accuracy 94.17188 wps 17896.32 step time 0.47s\n","# Epoch 11  global step 35120 loss 0.34451 batch 2300/3281 lr 0.001 accuracy 93.83984 wps 17587.55 step time 0.44s\n","# Epoch 11  global step 35140 loss 0.33300 batch 2320/3281 lr 0.001 accuracy 93.94531 wps 17365.17 step time 0.43s\n","# Epoch 11  global step 35160 loss 0.31187 batch 2340/3281 lr 0.001 accuracy 94.34375 wps 17067.32 step time 0.41s\n","# Epoch 11  global step 35180 loss 0.30673 batch 2360/3281 lr 0.001 accuracy 94.23437 wps 17169.16 step time 0.37s\n","# Epoch 11  global step 35200 loss 0.31855 batch 2380/3281 lr 0.001 accuracy 94.14844 wps 16814.67 step time 0.40s\n","# Epoch 11  global step 35220 loss 0.31978 batch 2400/3281 lr 0.001 accuracy 94.16406 wps 16410.09 step time 0.39s\n","# Epoch 11  global step 35240 loss 0.33157 batch 2420/3281 lr 0.001 accuracy 93.98828 wps 17236.92 step time 0.42s\n","# Epoch 11  global step 35260 loss 0.30062 batch 2440/3281 lr 0.001 accuracy 94.41016 wps 16069.42 step time 0.46s\n","# Epoch 11  global step 35280 loss 0.30732 batch 2460/3281 lr 0.001 accuracy 94.55469 wps 16614.21 step time 0.38s\n","# Epoch 11  global step 35300 loss 0.31259 batch 2480/3281 lr 0.001 accuracy 94.32422 wps 16762.41 step time 0.41s\n","# Epoch 11  global step 35320 loss 0.32158 batch 2500/3281 lr 0.001 accuracy 94.27344 wps 16858.32 step time 0.44s\n","# Epoch 11  global step 35340 loss 0.32099 batch 2520/3281 lr 0.001 accuracy 94.26953 wps 17249.33 step time 0.39s\n","# Epoch 11  global step 35360 loss 0.33599 batch 2540/3281 lr 0.001 accuracy 93.63672 wps 16288.75 step time 0.57s\n","# Epoch 11  global step 35380 loss 0.31145 batch 2560/3281 lr 0.001 accuracy 94.46094 wps 17129.14 step time 0.40s\n","# Epoch 11  global step 35400 loss 0.33584 batch 2580/3281 lr 0.001 accuracy 93.92578 wps 16753.68 step time 0.43s\n","# Epoch 11  global step 35420 loss 0.33960 batch 2600/3281 lr 0.001 accuracy 93.75000 wps 17495.58 step time 0.43s\n","# Epoch 11  global step 35440 loss 0.33543 batch 2620/3281 lr 0.001 accuracy 94.00391 wps 16836.52 step time 0.42s\n","# Epoch 11  global step 35460 loss 0.32752 batch 2640/3281 lr 0.001 accuracy 94.13281 wps 16680.48 step time 0.51s\n","# Epoch 11  global step 35480 loss 0.32854 batch 2660/3281 lr 0.001 accuracy 94.03516 wps 17682.53 step time 0.42s\n","# Epoch 11  global step 35500 loss 0.32140 batch 2680/3281 lr 0.001 accuracy 94.22656 wps 17001.62 step time 0.47s\n","# Epoch 11  global step 35520 loss 0.32341 batch 2700/3281 lr 0.001 accuracy 94.11719 wps 17210.90 step time 0.39s\n","# Epoch 11  global step 35540 loss 0.31294 batch 2720/3281 lr 0.001 accuracy 94.21875 wps 16344.09 step time 0.41s\n","# Epoch 11  global step 35560 loss 0.32132 batch 2740/3281 lr 0.001 accuracy 94.23828 wps 16430.63 step time 0.46s\n","# Epoch 11  global step 35580 loss 0.33399 batch 2760/3281 lr 0.001 accuracy 93.96484 wps 16412.47 step time 0.46s\n","# Epoch 11  global step 35600 loss 0.30132 batch 2780/3281 lr 0.001 accuracy 94.58594 wps 17767.54 step time 0.38s\n","# Epoch 11  global step 35620 loss 0.31260 batch 2800/3281 lr 0.001 accuracy 94.22656 wps 16635.03 step time 0.44s\n","# Epoch 11  global step 35640 loss 0.31025 batch 2820/3281 lr 0.001 accuracy 94.37891 wps 17511.09 step time 0.41s\n","# Epoch 11  global step 35660 loss 0.32318 batch 2840/3281 lr 0.001 accuracy 94.20703 wps 17371.30 step time 0.43s\n","# Epoch 11  global step 35680 loss 0.32410 batch 2860/3281 lr 0.001 accuracy 94.19922 wps 16950.56 step time 0.39s\n","# Epoch 11  global step 35700 loss 0.32658 batch 2880/3281 lr 0.001 accuracy 94.03516 wps 17549.21 step time 0.43s\n","# Epoch 11  global step 35720 loss 0.31897 batch 2900/3281 lr 0.001 accuracy 94.25781 wps 16445.65 step time 0.38s\n","# Epoch 11  global step 35740 loss 0.34431 batch 2920/3281 lr 0.001 accuracy 93.75000 wps 17430.38 step time 0.53s\n","# Epoch 11  global step 35760 loss 0.30810 batch 2940/3281 lr 0.001 accuracy 94.52344 wps 16036.58 step time 0.36s\n","# Epoch 11  global step 35780 loss 0.34888 batch 2960/3281 lr 0.001 accuracy 93.67969 wps 17631.47 step time 0.51s\n","# Epoch 11  global step 35800 loss 0.32469 batch 2980/3281 lr 0.001 accuracy 93.98047 wps 16968.70 step time 0.45s\n","# Epoch 11  global step 35820 loss 0.32811 batch 3000/3281 lr 0.001 accuracy 93.92187 wps 14991.70 step time 0.54s\n","# Epoch 11  global step 35840 loss 0.32619 batch 3020/3281 lr 0.001 accuracy 94.07813 wps 16490.06 step time 0.44s\n","# Epoch 11  global step 35860 loss 0.33600 batch 3040/3281 lr 0.001 accuracy 93.93750 wps 16859.80 step time 0.49s\n","# Epoch 11  global step 35880 loss 0.32944 batch 3060/3281 lr 0.001 accuracy 94.05469 wps 16670.95 step time 0.41s\n","# Epoch 11  global step 35900 loss 0.31579 batch 3080/3281 lr 0.001 accuracy 94.22656 wps 15953.20 step time 0.43s\n","# Epoch 11  global step 35920 loss 0.33182 batch 3100/3281 lr 0.001 accuracy 94.01953 wps 17285.89 step time 0.46s\n","# Epoch 11  global step 35940 loss 0.34487 batch 3120/3281 lr 0.001 accuracy 93.55078 wps 17038.03 step time 0.44s\n","# Epoch 11  global step 35960 loss 0.31272 batch 3140/3281 lr 0.001 accuracy 94.27344 wps 16679.24 step time 0.37s\n","# Epoch 11  global step 35980 loss 0.31638 batch 3160/3281 lr 0.001 accuracy 94.36328 wps 16150.50 step time 0.44s\n","# Epoch 11  global step 36000 loss 0.32406 batch 3180/3281 lr 0.001 accuracy 94.03516 wps 16990.35 step time 0.40s\n","# global step 36000, eval model at Fri May 22 14:39:41 2020\n","2020-05-22 14:39:43.354190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:39:43.354875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 14:39:43.354960: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 14:39:43.355054: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 14:39:43.355097: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 14:39:43.355177: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 14:39:43.355224: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 14:39:43.355264: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 14:39:43.355347: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 14:39:43.355479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:39:43.356027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:39:43.356496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 14:39:43.356613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 14:39:43.356680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 14:39:43.356707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 14:39:43.356921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:39:43.357480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:39:43.358041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6625677700167825\n","# location_distance_from_business_district - 0.5546694582983269\n","# location_easy_to_find - 0.7196447536969591\n","# service_wait_time - 0.6692520805238543\n","# service_waiters_attitude - 0.7971329583230652\n","# service_parking_convenience - 0.7407407022477656\n","# service_serving_speed - 0.766412349570062\n","# price_level - 0.7801954206028759\n","# price_cost_effective - 0.7098809691574447\n","# price_discount - 0.6712735102972355\n","# environment_decoration - 0.7334055574424343\n","# environment_noise - 0.7566647905408656\n","# environment_space - 0.7591317508696718\n","# environment_cleaness - 0.754987741842921\n","# dish_portion - 0.7251803477718559\n","# dish_taste - 0.7312364778067234\n","# dish_look - 0.5782701353572324\n","# dish_recommendation - 0.7459244568182317\n","# others_overall_experience - 0.5900543864356786\n","# others_willing_to_consume_again - 0.7127511108755403\n","# Eval loss 0.26696, f1 0.70797\n","# current result -0.7079688364247765, previous best result -0.7075168171998213\n","# Epoch 11  global step 36020 loss 0.33087 batch 3200/3281 lr 0.001 accuracy 93.96875 wps 17250.97 step time 0.43s\n","# Epoch 11  global step 36040 loss 0.32633 batch 3220/3281 lr 0.001 accuracy 94.02734 wps 17028.92 step time 0.46s\n","# Epoch 11  global step 36060 loss 0.31933 batch 3240/3281 lr 0.001 accuracy 94.35937 wps 17551.23 step time 0.43s\n","# Epoch 11  global step 36080 loss 0.33083 batch 3260/3281 lr 0.001 accuracy 93.85547 wps 18265.54 step time 0.41s\n","# Epoch 11  global step 36100 loss 0.32963 batch 3280/3281 lr 0.001 accuracy 94.09766 wps 16735.66 step time 0.39s\n","# Finsh epoch 11, global step 36102\n","# Epoch 12  global step 36120 loss 0.28073 batch 18/3281 lr 0.001 accuracy 84.88672 wps 19313.36 step time 0.39s\n","# Epoch 12  global step 36140 loss 0.31251 batch 38/3281 lr 0.001 accuracy 94.31250 wps 19215.83 step time 0.39s\n","# Epoch 12  global step 36160 loss 0.31141 batch 58/3281 lr 0.001 accuracy 94.29688 wps 19695.58 step time 0.39s\n","# Epoch 12  global step 36180 loss 0.33170 batch 78/3281 lr 0.001 accuracy 93.72266 wps 19651.14 step time 0.44s\n","# Epoch 12  global step 36200 loss 0.29413 batch 98/3281 lr 0.001 accuracy 94.70703 wps 19158.99 step time 0.36s\n","# Epoch 12  global step 36220 loss 0.29629 batch 118/3281 lr 0.001 accuracy 94.50781 wps 19170.36 step time 0.36s\n","# Epoch 12  global step 36240 loss 0.29729 batch 138/3281 lr 0.001 accuracy 94.69141 wps 18680.42 step time 0.35s\n","# Epoch 12  global step 36260 loss 0.30181 batch 158/3281 lr 0.001 accuracy 94.41406 wps 18404.32 step time 0.39s\n","# Epoch 12  global step 36280 loss 0.30791 batch 178/3281 lr 0.001 accuracy 94.47656 wps 19086.23 step time 0.36s\n","# Epoch 12  global step 36300 loss 0.30108 batch 198/3281 lr 0.001 accuracy 94.59766 wps 18405.37 step time 0.42s\n","# Epoch 12  global step 36320 loss 0.29960 batch 218/3281 lr 0.001 accuracy 94.56641 wps 19297.83 step time 0.37s\n","# Epoch 12  global step 36340 loss 0.30290 batch 238/3281 lr 0.001 accuracy 94.32813 wps 18902.45 step time 0.36s\n","# Epoch 12  global step 36360 loss 0.29758 batch 258/3281 lr 0.001 accuracy 94.55469 wps 19143.07 step time 0.41s\n","# Epoch 12  global step 36380 loss 0.30025 batch 278/3281 lr 0.001 accuracy 94.47266 wps 17386.39 step time 0.39s\n","# Epoch 12  global step 36400 loss 0.31895 batch 298/3281 lr 0.001 accuracy 94.14062 wps 17029.96 step time 0.50s\n","# Epoch 12  global step 36420 loss 0.29986 batch 318/3281 lr 0.001 accuracy 94.56250 wps 16893.31 step time 0.40s\n","# Epoch 12  global step 36440 loss 0.31220 batch 338/3281 lr 0.001 accuracy 94.26172 wps 13682.57 step time 0.54s\n","# Epoch 12  global step 36460 loss 0.31509 batch 358/3281 lr 0.001 accuracy 94.22656 wps 14479.18 step time 0.55s\n","# Epoch 12  global step 36480 loss 0.28338 batch 378/3281 lr 0.001 accuracy 94.77344 wps 17319.56 step time 0.37s\n","# Epoch 12  global step 36500 loss 0.32435 batch 398/3281 lr 0.001 accuracy 94.04687 wps 16711.53 step time 0.57s\n","# Epoch 12  global step 36520 loss 0.31163 batch 418/3281 lr 0.001 accuracy 94.23828 wps 17241.05 step time 0.47s\n","# Epoch 12  global step 36540 loss 0.30027 batch 438/3281 lr 0.001 accuracy 94.76172 wps 16631.46 step time 0.47s\n","# Epoch 12  global step 36560 loss 0.30862 batch 458/3281 lr 0.001 accuracy 94.34375 wps 17298.48 step time 0.40s\n","# Epoch 12  global step 36580 loss 0.32174 batch 478/3281 lr 0.001 accuracy 94.28516 wps 17702.49 step time 0.45s\n","# Epoch 12  global step 36600 loss 0.29665 batch 498/3281 lr 0.001 accuracy 94.68750 wps 15870.40 step time 0.42s\n","# Epoch 12  global step 36620 loss 0.29966 batch 518/3281 lr 0.001 accuracy 94.55469 wps 17389.16 step time 0.41s\n","# Epoch 12  global step 36640 loss 0.31542 batch 538/3281 lr 0.001 accuracy 94.36328 wps 16104.33 step time 0.47s\n","# Epoch 12  global step 36660 loss 0.29737 batch 558/3281 lr 0.001 accuracy 94.62500 wps 17690.68 step time 0.38s\n","# Epoch 12  global step 36680 loss 0.31235 batch 578/3281 lr 0.001 accuracy 94.35938 wps 18019.94 step time 0.40s\n","# Epoch 12  global step 36700 loss 0.32091 batch 598/3281 lr 0.001 accuracy 94.28516 wps 16913.30 step time 0.44s\n","# Epoch 12  global step 36720 loss 0.29793 batch 618/3281 lr 0.001 accuracy 94.61719 wps 16496.84 step time 0.40s\n","# Epoch 12  global step 36740 loss 0.31719 batch 638/3281 lr 0.001 accuracy 94.08594 wps 16215.29 step time 0.52s\n","# Epoch 12  global step 36760 loss 0.31226 batch 658/3281 lr 0.001 accuracy 94.22266 wps 16355.37 step time 0.60s\n","# Epoch 12  global step 36780 loss 0.29817 batch 678/3281 lr 0.001 accuracy 94.59375 wps 16000.77 step time 0.44s\n","# Epoch 12  global step 36800 loss 0.32315 batch 698/3281 lr 0.001 accuracy 94.10938 wps 16379.63 step time 0.51s\n","# Epoch 12  global step 36820 loss 0.31349 batch 718/3281 lr 0.001 accuracy 94.45312 wps 16042.99 step time 0.37s\n","# Epoch 12  global step 36840 loss 0.30049 batch 738/3281 lr 0.001 accuracy 94.60937 wps 16113.01 step time 0.43s\n","# Epoch 12  global step 36860 loss 0.31370 batch 758/3281 lr 0.001 accuracy 94.21094 wps 17924.36 step time 0.45s\n","# Epoch 12  global step 36880 loss 0.34013 batch 778/3281 lr 0.001 accuracy 93.83594 wps 17306.90 step time 0.46s\n","# Epoch 12  global step 36900 loss 0.29101 batch 798/3281 lr 0.001 accuracy 94.81250 wps 16622.96 step time 0.44s\n","# Epoch 12  global step 36920 loss 0.31238 batch 818/3281 lr 0.001 accuracy 94.36328 wps 17254.25 step time 0.42s\n","# Epoch 12  global step 36940 loss 0.31554 batch 838/3281 lr 0.001 accuracy 94.27344 wps 16904.40 step time 0.41s\n","# Epoch 12  global step 36960 loss 0.29689 batch 858/3281 lr 0.001 accuracy 94.51172 wps 16270.93 step time 0.41s\n","# Epoch 12  global step 36980 loss 0.30840 batch 878/3281 lr 0.001 accuracy 94.40234 wps 16209.91 step time 0.43s\n","# Epoch 12  global step 37000 loss 0.31380 batch 898/3281 lr 0.001 accuracy 94.31641 wps 17257.78 step time 0.40s\n","# Epoch 12  global step 37020 loss 0.30144 batch 918/3281 lr 0.001 accuracy 94.52344 wps 16496.35 step time 0.41s\n","# Epoch 12  global step 37040 loss 0.29484 batch 938/3281 lr 0.001 accuracy 94.74219 wps 16085.56 step time 0.51s\n","# Epoch 12  global step 37060 loss 0.32681 batch 958/3281 lr 0.001 accuracy 94.18750 wps 17835.23 step time 0.44s\n","# Epoch 12  global step 37080 loss 0.32502 batch 978/3281 lr 0.001 accuracy 94.02734 wps 17109.31 step time 0.45s\n","# Epoch 12  global step 37100 loss 0.28916 batch 998/3281 lr 0.001 accuracy 94.61719 wps 17062.79 step time 0.38s\n","# Epoch 12  global step 37120 loss 0.29471 batch 1018/3281 lr 0.001 accuracy 94.56641 wps 17424.64 step time 0.38s\n","# Epoch 12  global step 37140 loss 0.29778 batch 1038/3281 lr 0.001 accuracy 94.64453 wps 16591.86 step time 0.38s\n","# Epoch 12  global step 37160 loss 0.30875 batch 1058/3281 lr 0.001 accuracy 94.49609 wps 17373.35 step time 0.39s\n","# Epoch 12  global step 37180 loss 0.31999 batch 1078/3281 lr 0.001 accuracy 94.32031 wps 16621.55 step time 0.48s\n","# Epoch 12  global step 37200 loss 0.30777 batch 1098/3281 lr 0.001 accuracy 94.43750 wps 16552.71 step time 0.41s\n","# Epoch 12  global step 37220 loss 0.32238 batch 1118/3281 lr 0.001 accuracy 94.10547 wps 16321.70 step time 0.45s\n","# Epoch 12  global step 37240 loss 0.31463 batch 1138/3281 lr 0.001 accuracy 94.35156 wps 17604.24 step time 0.48s\n","# Epoch 12  global step 37260 loss 0.31989 batch 1158/3281 lr 0.001 accuracy 94.22656 wps 15652.61 step time 0.53s\n","# Epoch 12  global step 37280 loss 0.30663 batch 1178/3281 lr 0.001 accuracy 94.31641 wps 14203.74 step time 0.47s\n","# Epoch 12  global step 37300 loss 0.30952 batch 1198/3281 lr 0.001 accuracy 94.37891 wps 17383.94 step time 0.41s\n","# Epoch 12  global step 37320 loss 0.29092 batch 1218/3281 lr 0.001 accuracy 94.77344 wps 17034.55 step time 0.37s\n","# Epoch 12  global step 37340 loss 0.33415 batch 1238/3281 lr 0.001 accuracy 93.77344 wps 17559.71 step time 0.51s\n","# Epoch 12  global step 37360 loss 0.30618 batch 1258/3281 lr 0.001 accuracy 94.46094 wps 17254.51 step time 0.40s\n","# Epoch 12  global step 37380 loss 0.29967 batch 1278/3281 lr 0.001 accuracy 94.46484 wps 17222.76 step time 0.46s\n","# Epoch 12  global step 37400 loss 0.31434 batch 1298/3281 lr 0.001 accuracy 94.33203 wps 16594.74 step time 0.42s\n","# Epoch 12  global step 37420 loss 0.30483 batch 1318/3281 lr 0.001 accuracy 94.48438 wps 16849.32 step time 0.41s\n","# Epoch 12  global step 37440 loss 0.31308 batch 1338/3281 lr 0.001 accuracy 94.30469 wps 17169.61 step time 0.43s\n","# Epoch 12  global step 37460 loss 0.31869 batch 1358/3281 lr 0.001 accuracy 94.26562 wps 17012.57 step time 0.40s\n","# Epoch 12  global step 37480 loss 0.30634 batch 1378/3281 lr 0.001 accuracy 94.50781 wps 16222.72 step time 0.44s\n","# Epoch 12  global step 37500 loss 0.30301 batch 1398/3281 lr 0.001 accuracy 94.48438 wps 17087.15 step time 0.41s\n","# Epoch 12  global step 37520 loss 0.28645 batch 1418/3281 lr 0.001 accuracy 94.83203 wps 17125.27 step time 0.43s\n","# Epoch 12  global step 37540 loss 0.31079 batch 1438/3281 lr 0.001 accuracy 94.25781 wps 15343.16 step time 0.48s\n","# Epoch 12  global step 37560 loss 0.30194 batch 1458/3281 lr 0.001 accuracy 94.58594 wps 16774.99 step time 0.41s\n","# Epoch 12  global step 37580 loss 0.26944 batch 1478/3281 lr 0.001 accuracy 95.21875 wps 15632.29 step time 0.36s\n","# Epoch 12  global step 37600 loss 0.32207 batch 1498/3281 lr 0.001 accuracy 94.19531 wps 16561.67 step time 0.45s\n","# Epoch 12  global step 37620 loss 0.30649 batch 1518/3281 lr 0.001 accuracy 94.51172 wps 16013.24 step time 0.44s\n","# Epoch 12  global step 37640 loss 0.32311 batch 1538/3281 lr 0.001 accuracy 94.16797 wps 16191.38 step time 0.52s\n","# Epoch 12  global step 37660 loss 0.31064 batch 1558/3281 lr 0.001 accuracy 94.49219 wps 16489.88 step time 0.43s\n","# Epoch 12  global step 37680 loss 0.31601 batch 1578/3281 lr 0.001 accuracy 94.23828 wps 17318.99 step time 0.39s\n","# Epoch 12  global step 37700 loss 0.31267 batch 1598/3281 lr 0.001 accuracy 94.29688 wps 17434.37 step time 0.39s\n","# Epoch 12  global step 37720 loss 0.31157 batch 1618/3281 lr 0.001 accuracy 94.43359 wps 17452.54 step time 0.39s\n","# Epoch 12  global step 37740 loss 0.32234 batch 1638/3281 lr 0.001 accuracy 93.99609 wps 16110.22 step time 0.54s\n","# Epoch 12  global step 37760 loss 0.33129 batch 1658/3281 lr 0.001 accuracy 94.11328 wps 17191.02 step time 0.35s\n","# Epoch 12  global step 37780 loss 0.32366 batch 1678/3281 lr 0.001 accuracy 93.97656 wps 16540.35 step time 0.51s\n","# Epoch 12  global step 37800 loss 0.29341 batch 1698/3281 lr 0.001 accuracy 94.76562 wps 15285.79 step time 0.46s\n","# Epoch 12  global step 37820 loss 0.33448 batch 1718/3281 lr 0.001 accuracy 93.94141 wps 17697.05 step time 0.44s\n","# Epoch 12  global step 37840 loss 0.28825 batch 1738/3281 lr 0.001 accuracy 94.82422 wps 16881.88 step time 0.36s\n","# Epoch 12  global step 37860 loss 0.31534 batch 1758/3281 lr 0.001 accuracy 94.39453 wps 17191.10 step time 0.39s\n","# Epoch 12  global step 37880 loss 0.30949 batch 1778/3281 lr 0.001 accuracy 94.39844 wps 17567.35 step time 0.38s\n","# Epoch 12  global step 37900 loss 0.29087 batch 1798/3281 lr 0.001 accuracy 94.60938 wps 17268.32 step time 0.38s\n","# Epoch 12  global step 37920 loss 0.32359 batch 1818/3281 lr 0.001 accuracy 94.22266 wps 16581.24 step time 0.42s\n","# Epoch 12  global step 37940 loss 0.31550 batch 1838/3281 lr 0.001 accuracy 94.38281 wps 17202.26 step time 0.44s\n","# Epoch 12  global step 37960 loss 0.32782 batch 1858/3281 lr 0.001 accuracy 94.05859 wps 15833.57 step time 0.52s\n","# Epoch 12  global step 37980 loss 0.29188 batch 1878/3281 lr 0.001 accuracy 94.86719 wps 16922.45 step time 0.39s\n","# Epoch 12  global step 38000 loss 0.31201 batch 1898/3281 lr 0.001 accuracy 94.32813 wps 17329.58 step time 0.40s\n","# global step 38000, eval model at Fri May 22 14:55:08 2020\n","2020-05-22 14:55:11.062553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:55:11.063292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 14:55:11.063380: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 14:55:11.063422: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 14:55:11.063465: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 14:55:11.063505: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 14:55:11.063546: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 14:55:11.063583: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 14:55:11.063644: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 14:55:11.063789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:55:11.064699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:55:11.065224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 14:55:11.065280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 14:55:11.065301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 14:55:11.065315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 14:55:11.065495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:55:11.066111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 14:55:11.066580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6503957753064883\n","# location_distance_from_business_district - 0.5563386118667757\n","# location_easy_to_find - 0.7171170139927641\n","# service_wait_time - 0.6692250623979943\n","# service_waiters_attitude - 0.7964252314468665\n","# service_parking_convenience - 0.7389502726234393\n","# service_serving_speed - 0.7634798065125512\n","# price_level - 0.7781039036924537\n","# price_cost_effective - 0.7132039418683223\n","# price_discount - 0.6726038501863059\n","# environment_decoration - 0.7375020628894395\n","# environment_noise - 0.7543301553521047\n","# environment_space - 0.7584507756494336\n","# environment_cleaness - 0.7511251327532737\n","# dish_portion - 0.7247113228276759\n","# dish_taste - 0.7316005663912258\n","# dish_look - 0.5821373765315443\n","# dish_recommendation - 0.7434895943400273\n","# others_overall_experience - 0.5899751973898391\n","# others_willing_to_consume_again - 0.7097755982697806\n","# Eval loss 0.26918, f1 0.70695\n","# current result -0.7069470626144154, previous best result -0.7079688364247765\n","# Epoch 12  global step 38020 loss 0.29951 batch 1918/3281 lr 0.001 accuracy 94.52344 wps 16938.54 step time 0.39s\n","# Epoch 12  global step 38040 loss 0.32637 batch 1938/3281 lr 0.001 accuracy 94.05469 wps 17151.68 step time 0.47s\n","# Epoch 12  global step 38060 loss 0.32922 batch 1958/3281 lr 0.001 accuracy 94.03906 wps 17263.22 step time 0.43s\n","# Epoch 12  global step 38080 loss 0.31955 batch 1978/3281 lr 0.001 accuracy 94.11719 wps 17136.37 step time 0.42s\n","# Epoch 12  global step 38100 loss 0.29201 batch 1998/3281 lr 0.001 accuracy 94.71484 wps 16296.51 step time 0.34s\n","# Epoch 12  global step 38120 loss 0.32144 batch 2018/3281 lr 0.001 accuracy 94.12891 wps 16968.59 step time 0.47s\n","# Epoch 12  global step 38140 loss 0.31739 batch 2038/3281 lr 0.001 accuracy 94.07422 wps 16191.22 step time 0.48s\n","# Epoch 12  global step 38160 loss 0.32553 batch 2058/3281 lr 0.001 accuracy 94.17578 wps 17290.02 step time 0.39s\n","# Epoch 12  global step 38180 loss 0.30575 batch 2078/3281 lr 0.001 accuracy 94.48437 wps 16888.26 step time 0.41s\n","# Epoch 12  global step 38200 loss 0.31494 batch 2098/3281 lr 0.001 accuracy 94.21484 wps 17155.73 step time 0.44s\n","# Epoch 12  global step 38220 loss 0.34099 batch 2118/3281 lr 0.001 accuracy 93.69141 wps 14359.33 step time 0.57s\n","# Epoch 12  global step 38240 loss 0.33307 batch 2138/3281 lr 0.001 accuracy 94.06641 wps 12569.77 step time 0.52s\n","# Epoch 12  global step 38260 loss 0.31553 batch 2158/3281 lr 0.001 accuracy 94.28906 wps 16547.21 step time 0.43s\n","# Epoch 12  global step 38280 loss 0.29328 batch 2178/3281 lr 0.001 accuracy 94.59766 wps 16969.28 step time 0.41s\n","# Epoch 12  global step 38300 loss 0.32829 batch 2198/3281 lr 0.001 accuracy 93.92578 wps 16514.66 step time 0.43s\n","# Epoch 12  global step 38320 loss 0.32692 batch 2218/3281 lr 0.001 accuracy 94.19922 wps 16669.75 step time 0.43s\n","# Epoch 12  global step 38340 loss 0.29489 batch 2238/3281 lr 0.001 accuracy 94.63281 wps 16661.94 step time 0.36s\n","# Epoch 12  global step 38360 loss 0.31911 batch 2258/3281 lr 0.001 accuracy 94.11719 wps 17535.58 step time 0.43s\n","# Epoch 12  global step 38380 loss 0.29595 batch 2278/3281 lr 0.001 accuracy 94.66406 wps 16924.08 step time 0.40s\n","# Epoch 12  global step 38400 loss 0.29912 batch 2298/3281 lr 0.001 accuracy 94.50000 wps 16515.67 step time 0.34s\n","# Epoch 12  global step 38420 loss 0.30819 batch 2318/3281 lr 0.001 accuracy 94.35547 wps 17419.65 step time 0.37s\n","# Epoch 12  global step 38440 loss 0.33001 batch 2338/3281 lr 0.001 accuracy 94.23438 wps 16756.16 step time 0.45s\n","# Epoch 12  global step 38460 loss 0.30585 batch 2358/3281 lr 0.001 accuracy 94.48047 wps 16647.87 step time 0.36s\n","# Epoch 12  global step 38480 loss 0.32918 batch 2378/3281 lr 0.001 accuracy 93.97656 wps 17511.15 step time 0.46s\n","# Epoch 12  global step 38500 loss 0.33305 batch 2398/3281 lr 0.001 accuracy 93.82031 wps 17233.34 step time 0.45s\n","# Epoch 12  global step 38520 loss 0.31870 batch 2418/3281 lr 0.001 accuracy 94.20312 wps 17140.83 step time 0.41s\n","# Epoch 12  global step 38540 loss 0.32550 batch 2438/3281 lr 0.001 accuracy 94.28906 wps 16715.78 step time 0.45s\n","# Epoch 12  global step 38560 loss 0.31224 batch 2458/3281 lr 0.001 accuracy 94.23828 wps 16643.31 step time 0.44s\n","# Epoch 12  global step 38580 loss 0.30606 batch 2478/3281 lr 0.001 accuracy 94.41406 wps 16739.38 step time 0.37s\n","# Epoch 12  global step 38600 loss 0.32221 batch 2498/3281 lr 0.001 accuracy 94.11328 wps 17499.66 step time 0.41s\n","# Epoch 12  global step 38620 loss 0.32739 batch 2518/3281 lr 0.001 accuracy 93.97266 wps 16420.38 step time 0.46s\n","# Epoch 12  global step 38640 loss 0.31826 batch 2538/3281 lr 0.001 accuracy 94.21094 wps 15959.04 step time 0.48s\n","# Epoch 12  global step 38660 loss 0.31219 batch 2558/3281 lr 0.001 accuracy 94.41016 wps 16492.36 step time 0.43s\n","# Epoch 12  global step 38680 loss 0.29344 batch 2578/3281 lr 0.001 accuracy 94.74219 wps 16899.96 step time 0.35s\n","# Epoch 12  global step 38700 loss 0.32113 batch 2598/3281 lr 0.001 accuracy 94.13672 wps 16939.78 step time 0.55s\n","# Epoch 12  global step 38720 loss 0.32479 batch 2618/3281 lr 0.001 accuracy 94.16016 wps 17151.80 step time 0.43s\n","# Epoch 12  global step 38740 loss 0.31527 batch 2638/3281 lr 0.001 accuracy 94.35938 wps 15946.33 step time 0.47s\n","# Epoch 12  global step 38760 loss 0.32491 batch 2658/3281 lr 0.001 accuracy 94.01172 wps 16161.93 step time 0.46s\n","# Epoch 12  global step 38780 loss 0.31599 batch 2678/3281 lr 0.001 accuracy 94.26172 wps 17030.10 step time 0.39s\n","# Epoch 12  global step 38800 loss 0.31500 batch 2698/3281 lr 0.001 accuracy 94.26562 wps 17651.51 step time 0.39s\n","# Epoch 12  global step 38820 loss 0.33224 batch 2718/3281 lr 0.001 accuracy 94.03125 wps 16799.01 step time 0.49s\n","# Epoch 12  global step 38840 loss 0.29095 batch 2738/3281 lr 0.001 accuracy 94.81641 wps 16855.28 step time 0.38s\n","# Epoch 12  global step 38860 loss 0.29833 batch 2758/3281 lr 0.001 accuracy 94.63672 wps 17052.20 step time 0.40s\n","# Epoch 12  global step 38880 loss 0.33436 batch 2778/3281 lr 0.001 accuracy 93.81641 wps 16129.33 step time 0.52s\n","# Epoch 12  global step 38900 loss 0.32587 batch 2798/3281 lr 0.001 accuracy 94.04297 wps 17381.20 step time 0.37s\n","# Epoch 12  global step 38920 loss 0.30822 batch 2818/3281 lr 0.001 accuracy 94.16016 wps 16287.32 step time 0.43s\n","# Epoch 12  global step 38940 loss 0.32233 batch 2838/3281 lr 0.001 accuracy 94.09766 wps 16880.69 step time 0.48s\n","# Epoch 12  global step 38960 loss 0.31415 batch 2858/3281 lr 0.001 accuracy 94.36328 wps 16462.71 step time 0.39s\n","# Epoch 12  global step 38980 loss 0.31814 batch 2878/3281 lr 0.001 accuracy 94.13672 wps 16203.19 step time 0.46s\n","# Epoch 12  global step 39000 loss 0.30434 batch 2898/3281 lr 0.001 accuracy 94.60938 wps 17609.29 step time 0.40s\n","# Epoch 12  global step 39020 loss 0.31462 batch 2918/3281 lr 0.001 accuracy 94.20703 wps 16220.77 step time 0.45s\n","# Epoch 12  global step 39040 loss 0.29485 batch 2938/3281 lr 0.001 accuracy 94.86328 wps 15607.49 step time 0.38s\n","# Epoch 12  global step 39060 loss 0.31883 batch 2958/3281 lr 0.001 accuracy 94.18750 wps 15890.63 step time 0.47s\n","# Epoch 12  global step 39080 loss 0.32176 batch 2978/3281 lr 0.001 accuracy 94.09766 wps 17246.43 step time 0.37s\n","# Epoch 12  global step 39100 loss 0.29965 batch 2998/3281 lr 0.001 accuracy 94.51172 wps 17324.22 step time 0.39s\n","# Epoch 12  global step 39120 loss 0.31116 batch 3018/3281 lr 0.001 accuracy 94.32812 wps 15808.18 step time 0.39s\n","# Epoch 12  global step 39140 loss 0.32586 batch 3038/3281 lr 0.001 accuracy 93.96484 wps 15646.34 step time 0.61s\n","# Epoch 12  global step 39160 loss 0.31936 batch 3058/3281 lr 0.001 accuracy 94.37109 wps 17252.37 step time 0.39s\n","# Epoch 12  global step 39180 loss 0.31645 batch 3078/3281 lr 0.001 accuracy 94.21875 wps 16613.95 step time 0.50s\n","# Epoch 12  global step 39200 loss 0.33341 batch 3098/3281 lr 0.001 accuracy 94.08594 wps 17224.77 step time 0.49s\n","# Epoch 12  global step 39220 loss 0.31213 batch 3118/3281 lr 0.001 accuracy 94.24219 wps 16383.93 step time 0.37s\n","# Epoch 12  global step 39240 loss 0.32115 batch 3138/3281 lr 0.001 accuracy 94.09375 wps 16294.88 step time 0.41s\n","# Epoch 12  global step 39260 loss 0.34301 batch 3158/3281 lr 0.001 accuracy 93.66406 wps 17029.90 step time 0.44s\n","# Epoch 12  global step 39280 loss 0.30793 batch 3178/3281 lr 0.001 accuracy 94.36328 wps 17013.26 step time 0.39s\n","# Epoch 12  global step 39300 loss 0.31822 batch 3198/3281 lr 0.001 accuracy 94.12109 wps 17144.13 step time 0.40s\n","# Epoch 12  global step 39320 loss 0.32611 batch 3218/3281 lr 0.001 accuracy 93.92578 wps 15196.11 step time 0.56s\n","# Epoch 12  global step 39340 loss 0.31930 batch 3238/3281 lr 0.001 accuracy 94.30078 wps 11637.05 step time 0.60s\n","# Epoch 12  global step 39360 loss 0.33856 batch 3258/3281 lr 0.001 accuracy 93.72656 wps 17541.30 step time 0.45s\n","# Epoch 12  global step 39380 loss 0.32103 batch 3278/3281 lr 0.001 accuracy 94.20703 wps 17153.90 step time 0.40s\n","# Finsh epoch 12, global step 39384\n","# Epoch 13  global step 39400 loss 0.24116 batch 16/3281 lr 0.001 accuracy 75.65625 wps 19428.52 step time 0.35s\n","# Epoch 13  global step 39420 loss 0.29684 batch 36/3281 lr 0.001 accuracy 94.60547 wps 19094.88 step time 0.41s\n","# Epoch 13  global step 39440 loss 0.31461 batch 56/3281 lr 0.001 accuracy 94.19141 wps 18881.92 step time 0.47s\n","# Epoch 13  global step 39460 loss 0.29711 batch 76/3281 lr 0.001 accuracy 94.52734 wps 20048.40 step time 0.40s\n","# Epoch 13  global step 39480 loss 0.28709 batch 96/3281 lr 0.001 accuracy 94.89844 wps 19649.50 step time 0.38s\n","# Epoch 13  global step 39500 loss 0.29083 batch 116/3281 lr 0.001 accuracy 94.67578 wps 19175.21 step time 0.41s\n","# Epoch 13  global step 39520 loss 0.30411 batch 136/3281 lr 0.001 accuracy 94.55469 wps 18855.09 step time 0.35s\n","# Epoch 13  global step 39540 loss 0.29382 batch 156/3281 lr 0.001 accuracy 94.67969 wps 18971.67 step time 0.36s\n","# Epoch 13  global step 39560 loss 0.29738 batch 176/3281 lr 0.001 accuracy 94.62500 wps 18946.25 step time 0.35s\n","# Epoch 13  global step 39580 loss 0.29751 batch 196/3281 lr 0.001 accuracy 94.47266 wps 18538.76 step time 0.38s\n","# Epoch 13  global step 39600 loss 0.30637 batch 216/3281 lr 0.001 accuracy 94.53125 wps 19022.89 step time 0.36s\n","# Epoch 13  global step 39620 loss 0.29651 batch 236/3281 lr 0.001 accuracy 94.65625 wps 19486.13 step time 0.38s\n","# Epoch 13  global step 39640 loss 0.29940 batch 256/3281 lr 0.001 accuracy 94.75391 wps 19610.11 step time 0.38s\n","# Epoch 13  global step 39660 loss 0.29473 batch 276/3281 lr 0.001 accuracy 94.65234 wps 18801.52 step time 0.36s\n","# Epoch 13  global step 39680 loss 0.29569 batch 296/3281 lr 0.001 accuracy 94.61719 wps 17121.46 step time 0.40s\n","# Epoch 13  global step 39700 loss 0.30090 batch 316/3281 lr 0.001 accuracy 94.51953 wps 19013.23 step time 0.36s\n","# Epoch 13  global step 39720 loss 0.28064 batch 336/3281 lr 0.001 accuracy 94.91797 wps 18150.58 step time 0.38s\n","# Epoch 13  global step 39740 loss 0.31605 batch 356/3281 lr 0.001 accuracy 94.43359 wps 19192.21 step time 0.45s\n","# Epoch 13  global step 39760 loss 0.30717 batch 376/3281 lr 0.001 accuracy 94.36328 wps 19288.53 step time 0.42s\n","# Epoch 13  global step 39780 loss 0.29371 batch 396/3281 lr 0.001 accuracy 94.70312 wps 18894.30 step time 0.35s\n","# Epoch 13  global step 39800 loss 0.31582 batch 416/3281 lr 0.001 accuracy 94.39062 wps 19140.09 step time 0.42s\n","# Epoch 13  global step 39820 loss 0.30354 batch 436/3281 lr 0.001 accuracy 94.48047 wps 18830.88 step time 0.35s\n","# Epoch 13  global step 39840 loss 0.30223 batch 456/3281 lr 0.001 accuracy 94.40625 wps 18706.41 step time 0.34s\n","# Epoch 13  global step 39860 loss 0.32964 batch 476/3281 lr 0.001 accuracy 94.08594 wps 19106.46 step time 0.47s\n","# Epoch 13  global step 39880 loss 0.30282 batch 496/3281 lr 0.001 accuracy 94.52344 wps 18873.05 step time 0.40s\n","# Epoch 13  global step 39900 loss 0.31604 batch 516/3281 lr 0.001 accuracy 94.30469 wps 19252.22 step time 0.38s\n","# Epoch 13  global step 39920 loss 0.29483 batch 536/3281 lr 0.001 accuracy 94.66016 wps 18207.46 step time 0.39s\n","# Epoch 13  global step 39940 loss 0.30957 batch 556/3281 lr 0.001 accuracy 94.43359 wps 19766.94 step time 0.38s\n","# Epoch 13  global step 39960 loss 0.28217 batch 576/3281 lr 0.001 accuracy 94.84766 wps 18528.93 step time 0.35s\n","# Epoch 13  global step 39980 loss 0.31689 batch 596/3281 lr 0.001 accuracy 94.38281 wps 19675.83 step time 0.39s\n","# Epoch 13  global step 40000 loss 0.31604 batch 616/3281 lr 0.001 accuracy 94.41016 wps 18804.13 step time 0.35s\n","# global step 40000, eval model at Fri May 22 15:10:14 2020\n","2020-05-22 15:10:17.640601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:10:17.641455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 15:10:17.641567: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 15:10:17.641628: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 15:10:17.641691: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 15:10:17.641744: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 15:10:17.641786: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 15:10:17.641833: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 15:10:17.641880: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 15:10:17.642017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:10:17.642738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:10:17.643252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 15:10:17.643374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 15:10:17.643397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 15:10:17.643416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 15:10:17.643594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:10:17.644406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:10:17.644968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6475492921086098\n","# location_distance_from_business_district - 0.5529038158309636\n","# location_easy_to_find - 0.7145744284381428\n","# service_wait_time - 0.6690251490741295\n","# service_waiters_attitude - 0.7964291454595729\n","# service_parking_convenience - 0.7345845758566927\n","# service_serving_speed - 0.7643238591631035\n","# price_level - 0.7775690371537038\n","# price_cost_effective - 0.7136167479736103\n","# price_discount - 0.67203612917929\n","# environment_decoration - 0.7378300636438956\n","# environment_noise - 0.7552182269088142\n","# environment_space - 0.7575535774742679\n","# environment_cleaness - 0.7482080599009896\n","# dish_portion - 0.7235094072559336\n","# dish_taste - 0.7305838606520051\n","# dish_look - 0.5834170622860482\n","# dish_recommendation - 0.7416359392599974\n","# others_overall_experience - 0.5902891293856902\n","# others_willing_to_consume_again - 0.712121286154067\n","# Eval loss 0.27115, f1 0.70615\n","# current result -0.7061489396579763, previous best result -0.7079688364247765\n","# Epoch 13  global step 40020 loss 0.31285 batch 636/3281 lr 0.001 accuracy 94.30859 wps 19408.50 step time 0.38s\n","# Epoch 13  global step 40040 loss 0.28033 batch 656/3281 lr 0.001 accuracy 94.99609 wps 17691.68 step time 0.39s\n","# Epoch 13  global step 40060 loss 0.29528 batch 676/3281 lr 0.001 accuracy 94.55078 wps 16276.85 step time 0.39s\n","# Epoch 13  global step 40080 loss 0.30461 batch 696/3281 lr 0.001 accuracy 94.53125 wps 18504.25 step time 0.34s\n","# Epoch 13  global step 40100 loss 0.29807 batch 716/3281 lr 0.001 accuracy 94.75000 wps 18786.75 step time 0.35s\n","# Epoch 13  global step 40120 loss 0.29947 batch 736/3281 lr 0.001 accuracy 94.56250 wps 19504.31 step time 0.44s\n","# Epoch 13  global step 40140 loss 0.28626 batch 756/3281 lr 0.001 accuracy 94.92187 wps 18419.97 step time 0.34s\n","# Epoch 13  global step 40160 loss 0.30050 batch 776/3281 lr 0.001 accuracy 94.51953 wps 18951.45 step time 0.35s\n","# Epoch 13  global step 40180 loss 0.30490 batch 796/3281 lr 0.001 accuracy 94.58203 wps 19019.94 step time 0.36s\n","# Epoch 13  global step 40200 loss 0.30213 batch 816/3281 lr 0.001 accuracy 94.60156 wps 19018.05 step time 0.35s\n","# Epoch 13  global step 40220 loss 0.29790 batch 836/3281 lr 0.001 accuracy 94.63672 wps 19294.07 step time 0.36s\n","# Epoch 13  global step 40240 loss 0.31326 batch 856/3281 lr 0.001 accuracy 94.36328 wps 17712.00 step time 0.42s\n","# Epoch 13  global step 40260 loss 0.29890 batch 876/3281 lr 0.001 accuracy 94.54297 wps 19079.91 step time 0.35s\n","# Epoch 13  global step 40280 loss 0.30718 batch 896/3281 lr 0.001 accuracy 94.55078 wps 18928.35 step time 0.40s\n","# Epoch 13  global step 40300 loss 0.30611 batch 916/3281 lr 0.001 accuracy 94.54687 wps 18862.00 step time 0.40s\n","# Epoch 13  global step 40320 loss 0.30135 batch 936/3281 lr 0.001 accuracy 94.49219 wps 19511.80 step time 0.37s\n","# Epoch 13  global step 40340 loss 0.31115 batch 956/3281 lr 0.001 accuracy 94.41016 wps 19245.22 step time 0.41s\n","# Epoch 13  global step 40360 loss 0.30251 batch 976/3281 lr 0.001 accuracy 94.48437 wps 19848.98 step time 0.43s\n","# Epoch 13  global step 40380 loss 0.32990 batch 996/3281 lr 0.001 accuracy 93.94531 wps 18995.62 step time 0.41s\n","# Epoch 13  global step 40400 loss 0.30042 batch 1016/3281 lr 0.001 accuracy 94.57422 wps 14546.20 step time 0.44s\n","# Epoch 13  global step 40420 loss 0.29504 batch 1036/3281 lr 0.001 accuracy 94.58984 wps 19127.44 step time 0.36s\n","# Epoch 13  global step 40440 loss 0.30476 batch 1056/3281 lr 0.001 accuracy 94.50000 wps 19004.85 step time 0.36s\n","# Epoch 13  global step 40460 loss 0.29702 batch 1076/3281 lr 0.001 accuracy 94.55859 wps 19420.28 step time 0.37s\n","# Epoch 13  global step 40480 loss 0.31618 batch 1096/3281 lr 0.001 accuracy 94.24609 wps 20241.38 step time 0.40s\n","# Epoch 13  global step 40500 loss 0.31044 batch 1116/3281 lr 0.001 accuracy 94.31250 wps 18715.06 step time 0.36s\n","# Epoch 13  global step 40520 loss 0.31917 batch 1136/3281 lr 0.001 accuracy 94.23047 wps 19560.47 step time 0.38s\n","# Epoch 13  global step 40540 loss 0.29652 batch 1156/3281 lr 0.001 accuracy 94.49219 wps 19715.66 step time 0.41s\n","# Epoch 13  global step 40560 loss 0.32311 batch 1176/3281 lr 0.001 accuracy 94.24219 wps 19985.85 step time 0.39s\n","# Epoch 13  global step 40580 loss 0.31134 batch 1196/3281 lr 0.001 accuracy 94.41016 wps 19691.49 step time 0.38s\n","# Epoch 13  global step 40600 loss 0.30067 batch 1216/3281 lr 0.001 accuracy 94.71875 wps 18959.39 step time 0.36s\n","# Epoch 13  global step 40620 loss 0.28883 batch 1236/3281 lr 0.001 accuracy 94.91406 wps 18382.68 step time 0.33s\n","# Epoch 13  global step 40640 loss 0.30216 batch 1256/3281 lr 0.001 accuracy 94.62109 wps 18499.94 step time 0.38s\n","# Epoch 13  global step 40660 loss 0.30849 batch 1276/3281 lr 0.001 accuracy 94.30859 wps 19636.71 step time 0.42s\n","# Epoch 13  global step 40680 loss 0.29938 batch 1296/3281 lr 0.001 accuracy 94.65234 wps 19044.11 step time 0.37s\n","# Epoch 13  global step 40700 loss 0.29228 batch 1316/3281 lr 0.001 accuracy 94.74219 wps 18803.43 step time 0.35s\n","# Epoch 13  global step 40720 loss 0.30086 batch 1336/3281 lr 0.001 accuracy 94.61328 wps 18527.92 step time 0.33s\n","# Epoch 13  global step 40740 loss 0.32288 batch 1356/3281 lr 0.001 accuracy 94.16406 wps 19896.29 step time 0.38s\n","# Epoch 13  global step 40760 loss 0.29945 batch 1376/3281 lr 0.001 accuracy 94.50000 wps 19794.26 step time 0.38s\n","# Epoch 13  global step 40780 loss 0.30093 batch 1396/3281 lr 0.001 accuracy 94.58984 wps 18435.46 step time 0.45s\n","# Epoch 13  global step 40800 loss 0.29891 batch 1416/3281 lr 0.001 accuracy 94.44922 wps 19121.97 step time 0.36s\n","# Epoch 13  global step 40820 loss 0.32889 batch 1436/3281 lr 0.001 accuracy 93.99609 wps 18932.33 step time 0.47s\n","# Epoch 13  global step 40840 loss 0.30256 batch 1456/3281 lr 0.001 accuracy 94.55469 wps 18899.99 step time 0.35s\n","# Epoch 13  global step 40860 loss 0.29213 batch 1476/3281 lr 0.001 accuracy 94.77734 wps 18982.18 step time 0.35s\n","# Epoch 13  global step 40880 loss 0.30767 batch 1496/3281 lr 0.001 accuracy 94.30859 wps 18652.41 step time 0.39s\n","# Epoch 13  global step 40900 loss 0.31597 batch 1516/3281 lr 0.001 accuracy 94.28906 wps 18822.13 step time 0.38s\n","# Epoch 13  global step 40920 loss 0.29567 batch 1536/3281 lr 0.001 accuracy 94.73047 wps 18093.92 step time 0.34s\n","# Epoch 13  global step 40940 loss 0.32158 batch 1556/3281 lr 0.001 accuracy 94.27344 wps 18348.70 step time 0.42s\n","# Epoch 13  global step 40960 loss 0.32002 batch 1576/3281 lr 0.001 accuracy 94.14062 wps 19469.91 step time 0.41s\n","# Epoch 13  global step 40980 loss 0.32714 batch 1596/3281 lr 0.001 accuracy 94.22266 wps 18804.44 step time 0.41s\n","# Epoch 13  global step 41000 loss 0.27902 batch 1616/3281 lr 0.001 accuracy 94.90625 wps 18503.25 step time 0.34s\n","# Epoch 13  global step 41020 loss 0.31922 batch 1636/3281 lr 0.001 accuracy 94.24609 wps 19049.88 step time 0.45s\n","# Epoch 13  global step 41040 loss 0.31823 batch 1656/3281 lr 0.001 accuracy 94.14844 wps 19618.82 step time 0.37s\n","# Epoch 13  global step 41060 loss 0.30013 batch 1676/3281 lr 0.001 accuracy 94.67188 wps 18562.54 step time 0.34s\n","# Epoch 13  global step 41080 loss 0.30623 batch 1696/3281 lr 0.001 accuracy 94.45703 wps 19981.24 step time 0.40s\n","# Epoch 13  global step 41100 loss 0.29509 batch 1716/3281 lr 0.001 accuracy 94.69531 wps 18236.54 step time 0.38s\n","# Epoch 13  global step 41120 loss 0.28507 batch 1736/3281 lr 0.001 accuracy 94.89453 wps 18896.94 step time 0.35s\n","# Epoch 13  global step 41140 loss 0.28905 batch 1756/3281 lr 0.001 accuracy 94.62500 wps 17346.24 step time 0.30s\n","# Epoch 13  global step 41160 loss 0.31620 batch 1776/3281 lr 0.001 accuracy 94.21484 wps 18955.87 step time 0.46s\n","# Epoch 13  global step 41180 loss 0.32167 batch 1796/3281 lr 0.001 accuracy 94.14844 wps 19573.46 step time 0.49s\n","# Epoch 13  global step 41200 loss 0.30693 batch 1816/3281 lr 0.001 accuracy 94.52734 wps 18797.64 step time 0.34s\n","# Epoch 13  global step 41220 loss 0.34764 batch 1836/3281 lr 0.001 accuracy 93.76172 wps 19806.02 step time 0.48s\n","# Epoch 13  global step 41240 loss 0.31228 batch 1856/3281 lr 0.001 accuracy 94.26172 wps 18593.32 step time 0.42s\n","# Epoch 13  global step 41260 loss 0.30058 batch 1876/3281 lr 0.001 accuracy 94.55859 wps 16977.55 step time 0.40s\n","# Epoch 13  global step 41280 loss 0.30377 batch 1896/3281 lr 0.001 accuracy 94.51172 wps 18590.00 step time 0.35s\n","# Epoch 13  global step 41300 loss 0.32503 batch 1916/3281 lr 0.001 accuracy 94.16016 wps 19843.64 step time 0.51s\n","# Epoch 13  global step 41320 loss 0.30680 batch 1936/3281 lr 0.001 accuracy 94.54688 wps 19152.30 step time 0.37s\n","# Epoch 13  global step 41340 loss 0.30007 batch 1956/3281 lr 0.001 accuracy 94.49609 wps 18585.59 step time 0.34s\n","# Epoch 13  global step 41360 loss 0.30668 batch 1976/3281 lr 0.001 accuracy 94.59375 wps 18833.80 step time 0.36s\n","# Epoch 13  global step 41380 loss 0.31378 batch 1996/3281 lr 0.001 accuracy 94.28906 wps 18591.88 step time 0.39s\n","# Epoch 13  global step 41400 loss 0.30769 batch 2016/3281 lr 0.001 accuracy 94.52344 wps 18918.26 step time 0.36s\n","# Epoch 13  global step 41420 loss 0.32012 batch 2036/3281 lr 0.001 accuracy 94.36328 wps 19422.35 step time 0.37s\n","# Epoch 13  global step 41440 loss 0.30393 batch 2056/3281 lr 0.001 accuracy 94.43359 wps 19220.58 step time 0.36s\n","# Epoch 13  global step 41460 loss 0.30650 batch 2076/3281 lr 0.001 accuracy 94.48828 wps 18651.18 step time 0.39s\n","# Epoch 13  global step 41480 loss 0.28335 batch 2096/3281 lr 0.001 accuracy 94.85156 wps 18129.78 step time 0.32s\n","# Epoch 13  global step 41500 loss 0.32137 batch 2116/3281 lr 0.001 accuracy 94.14062 wps 19257.27 step time 0.42s\n","# Epoch 13  global step 41520 loss 0.28902 batch 2136/3281 lr 0.001 accuracy 94.67969 wps 17719.12 step time 0.32s\n","# Epoch 13  global step 41540 loss 0.33139 batch 2156/3281 lr 0.001 accuracy 94.11328 wps 20095.84 step time 0.38s\n","# Epoch 13  global step 41560 loss 0.30144 batch 2176/3281 lr 0.001 accuracy 94.50000 wps 18669.82 step time 0.34s\n","# Epoch 13  global step 41580 loss 0.30697 batch 2196/3281 lr 0.001 accuracy 94.50000 wps 19022.37 step time 0.35s\n","# Epoch 13  global step 41600 loss 0.30219 batch 2216/3281 lr 0.001 accuracy 94.62891 wps 19000.24 step time 0.36s\n","# Epoch 13  global step 41620 loss 0.30591 batch 2236/3281 lr 0.001 accuracy 94.48047 wps 19279.29 step time 0.36s\n","# Epoch 13  global step 41640 loss 0.27667 batch 2256/3281 lr 0.001 accuracy 95.00391 wps 18485.96 step time 0.33s\n","# Epoch 13  global step 41660 loss 0.32701 batch 2276/3281 lr 0.001 accuracy 94.12500 wps 19084.22 step time 0.42s\n","# Epoch 13  global step 41680 loss 0.29873 batch 2296/3281 lr 0.001 accuracy 94.57031 wps 18874.39 step time 0.35s\n","# Epoch 13  global step 41700 loss 0.32166 batch 2316/3281 lr 0.001 accuracy 94.23047 wps 19124.70 step time 0.40s\n","# Epoch 13  global step 41720 loss 0.32314 batch 2336/3281 lr 0.001 accuracy 94.13672 wps 19861.63 step time 0.40s\n","# Epoch 13  global step 41740 loss 0.32403 batch 2356/3281 lr 0.001 accuracy 94.04297 wps 18656.00 step time 0.36s\n","# Epoch 13  global step 41760 loss 0.29560 batch 2376/3281 lr 0.001 accuracy 94.60938 wps 19148.10 step time 0.36s\n","# Epoch 13  global step 41780 loss 0.29180 batch 2396/3281 lr 0.001 accuracy 94.68750 wps 16780.76 step time 0.41s\n","# Epoch 13  global step 41800 loss 0.29874 batch 2416/3281 lr 0.001 accuracy 94.54688 wps 18927.78 step time 0.35s\n","# Epoch 13  global step 41820 loss 0.32111 batch 2436/3281 lr 0.001 accuracy 94.02734 wps 19032.08 step time 0.36s\n","# Epoch 13  global step 41840 loss 0.30162 batch 2456/3281 lr 0.001 accuracy 94.34375 wps 18496.23 step time 0.33s\n","# Epoch 13  global step 41860 loss 0.31300 batch 2476/3281 lr 0.001 accuracy 94.27344 wps 19174.86 step time 0.36s\n","# Epoch 13  global step 41880 loss 0.32506 batch 2496/3281 lr 0.001 accuracy 94.09766 wps 19807.44 step time 0.39s\n","# Epoch 13  global step 41900 loss 0.29706 batch 2516/3281 lr 0.001 accuracy 94.42578 wps 18553.85 step time 0.34s\n","# Epoch 13  global step 41920 loss 0.30234 batch 2536/3281 lr 0.001 accuracy 94.57031 wps 19389.05 step time 0.37s\n","# Epoch 13  global step 41940 loss 0.32193 batch 2556/3281 lr 0.001 accuracy 94.25781 wps 19827.53 step time 0.38s\n","# Epoch 13  global step 41960 loss 0.30521 batch 2576/3281 lr 0.001 accuracy 94.38281 wps 19296.19 step time 0.37s\n","# Epoch 13  global step 41980 loss 0.31914 batch 2596/3281 lr 0.001 accuracy 94.15234 wps 19655.44 step time 0.43s\n","# Epoch 13  global step 42000 loss 0.33716 batch 2616/3281 lr 0.001 accuracy 93.98828 wps 20068.14 step time 0.39s\n","# global step 42000, eval model at Fri May 22 15:24:02 2020\n","2020-05-22 15:24:04.848184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:24:04.848954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 15:24:04.849048: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 15:24:04.849108: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 15:24:04.849152: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 15:24:04.849195: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 15:24:04.849235: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 15:24:04.849274: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 15:24:04.849313: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 15:24:04.849445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:24:04.850106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:24:04.850817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 15:24:04.850894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 15:24:04.850923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 15:24:04.850941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 15:24:04.851118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:24:04.851776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:24:04.852328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6410274953398751\n","# location_distance_from_business_district - 0.5662935717592201\n","# location_easy_to_find - 0.7182471280855194\n","# service_wait_time - 0.6666535673664107\n","# service_waiters_attitude - 0.7969108865810818\n","# service_parking_convenience - 0.7335308883224003\n","# service_serving_speed - 0.7634557287148778\n","# price_level - 0.777446598129603\n","# price_cost_effective - 0.7129546181723325\n","# price_discount - 0.6732880909419834\n","# environment_decoration - 0.7337908036324976\n","# environment_noise - 0.7565647920342726\n","# environment_space - 0.7571716419399814\n","# environment_cleaness - 0.7455136017188241\n","# dish_portion - 0.7233083558963136\n","# dish_taste - 0.7279724934003611\n","# dish_look - 0.5834753448981601\n","# dish_recommendation - 0.7389037161908426\n","# others_overall_experience - 0.5896531708563042\n","# others_willing_to_consume_again - 0.7127581680164016\n","# Eval loss 0.27352, f1 0.70595\n","# current result -0.7059460330998633, previous best result -0.7079688364247765\n","# Epoch 13  global step 42020 loss 0.31498 batch 2636/3281 lr 0.001 accuracy 94.20312 wps 19344.33 step time 0.38s\n","# Epoch 13  global step 42040 loss 0.30708 batch 2656/3281 lr 0.001 accuracy 94.47656 wps 18511.21 step time 0.39s\n","# Epoch 13  global step 42060 loss 0.30058 batch 2676/3281 lr 0.001 accuracy 94.50000 wps 18853.23 step time 0.44s\n","# Epoch 13  global step 42080 loss 0.32118 batch 2696/3281 lr 0.001 accuracy 94.18750 wps 19852.77 step time 0.40s\n","# Epoch 13  global step 42100 loss 0.31373 batch 2716/3281 lr 0.001 accuracy 94.23047 wps 19018.77 step time 0.36s\n","# Epoch 13  global step 42120 loss 0.32493 batch 2736/3281 lr 0.001 accuracy 94.04687 wps 19726.04 step time 0.46s\n","# Epoch 13  global step 42140 loss 0.30035 batch 2756/3281 lr 0.001 accuracy 94.51562 wps 19668.38 step time 0.40s\n","# Epoch 13  global step 42160 loss 0.31150 batch 2776/3281 lr 0.001 accuracy 94.26563 wps 18345.94 step time 0.39s\n","# Epoch 13  global step 42180 loss 0.29888 batch 2796/3281 lr 0.001 accuracy 94.72656 wps 18761.11 step time 0.35s\n","# Epoch 13  global step 42200 loss 0.32418 batch 2816/3281 lr 0.001 accuracy 94.04688 wps 19596.53 step time 0.40s\n","# Epoch 13  global step 42220 loss 0.31542 batch 2836/3281 lr 0.001 accuracy 94.39844 wps 18741.63 step time 0.39s\n","# Epoch 13  global step 42240 loss 0.31096 batch 2856/3281 lr 0.001 accuracy 94.37500 wps 19422.03 step time 0.37s\n","# Epoch 13  global step 42260 loss 0.31424 batch 2876/3281 lr 0.001 accuracy 94.29687 wps 18547.14 step time 0.34s\n","# Epoch 13  global step 42280 loss 0.32660 batch 2896/3281 lr 0.001 accuracy 94.19922 wps 19097.18 step time 0.43s\n","# Epoch 13  global step 42300 loss 0.31635 batch 2916/3281 lr 0.001 accuracy 94.28516 wps 19361.52 step time 0.38s\n","# Epoch 13  global step 42320 loss 0.35061 batch 2936/3281 lr 0.001 accuracy 93.64844 wps 19041.25 step time 0.51s\n","# Epoch 13  global step 42340 loss 0.31070 batch 2956/3281 lr 0.001 accuracy 94.19531 wps 19090.62 step time 0.42s\n","# Epoch 13  global step 42360 loss 0.32455 batch 2976/3281 lr 0.001 accuracy 94.19531 wps 18554.53 step time 0.38s\n","# Epoch 13  global step 42380 loss 0.31133 batch 2996/3281 lr 0.001 accuracy 94.41406 wps 18045.07 step time 0.34s\n","# Epoch 13  global step 42400 loss 0.29875 batch 3016/3281 lr 0.001 accuracy 94.62500 wps 18527.27 step time 0.34s\n","# Epoch 13  global step 42420 loss 0.28695 batch 3036/3281 lr 0.001 accuracy 94.76953 wps 18577.41 step time 0.34s\n","# Epoch 13  global step 42440 loss 0.31444 batch 3056/3281 lr 0.001 accuracy 94.29688 wps 18855.60 step time 0.35s\n","# Epoch 13  global step 42460 loss 0.32058 batch 3076/3281 lr 0.001 accuracy 94.12891 wps 19880.40 step time 0.44s\n","# Epoch 13  global step 42480 loss 0.31872 batch 3096/3281 lr 0.001 accuracy 94.20703 wps 19048.30 step time 0.37s\n","# Epoch 13  global step 42500 loss 0.29912 batch 3116/3281 lr 0.001 accuracy 94.70313 wps 18087.03 step time 0.33s\n","# Epoch 13  global step 42520 loss 0.32215 batch 3136/3281 lr 0.001 accuracy 94.08984 wps 19064.97 step time 0.40s\n","# Epoch 13  global step 42540 loss 0.30302 batch 3156/3281 lr 0.001 accuracy 94.40234 wps 18950.34 step time 0.36s\n","# Epoch 13  global step 42560 loss 0.30832 batch 3176/3281 lr 0.001 accuracy 94.50391 wps 18200.76 step time 0.33s\n","# Epoch 13  global step 42580 loss 0.31836 batch 3196/3281 lr 0.001 accuracy 94.23828 wps 18670.44 step time 0.36s\n","# Epoch 13  global step 42600 loss 0.34893 batch 3216/3281 lr 0.001 accuracy 93.71875 wps 16504.92 step time 0.57s\n","# Epoch 13  global step 42620 loss 0.32428 batch 3236/3281 lr 0.001 accuracy 94.09766 wps 16942.79 step time 0.46s\n","# Epoch 13  global step 42640 loss 0.29984 batch 3256/3281 lr 0.001 accuracy 94.61719 wps 16857.83 step time 0.37s\n","# Epoch 13  global step 42660 loss 0.30232 batch 3276/3281 lr 0.001 accuracy 94.51172 wps 15700.30 step time 0.43s\n","# Finsh epoch 13, global step 42666\n","# Epoch 14  global step 42680 loss 0.21491 batch 14/3281 lr 0.001 accuracy 66.08203 wps 19480.99 step time 0.30s\n","# Epoch 14  global step 42700 loss 0.29049 batch 34/3281 lr 0.001 accuracy 94.76172 wps 19235.73 step time 0.36s\n","# Epoch 14  global step 42720 loss 0.29435 batch 54/3281 lr 0.001 accuracy 94.55859 wps 19258.19 step time 0.37s\n","# Epoch 14  global step 42740 loss 0.29836 batch 74/3281 lr 0.001 accuracy 94.48047 wps 20222.45 step time 0.41s\n","# Epoch 14  global step 42760 loss 0.29523 batch 94/3281 lr 0.001 accuracy 94.62500 wps 18728.11 step time 0.39s\n","# Epoch 14  global step 42780 loss 0.30748 batch 114/3281 lr 0.001 accuracy 94.38672 wps 19044.51 step time 0.42s\n","# Epoch 14  global step 42800 loss 0.27048 batch 134/3281 lr 0.001 accuracy 95.19922 wps 18326.14 step time 0.34s\n","# Epoch 14  global step 42820 loss 0.29585 batch 154/3281 lr 0.001 accuracy 94.75391 wps 19020.89 step time 0.37s\n","# Epoch 14  global step 42840 loss 0.28638 batch 174/3281 lr 0.001 accuracy 94.82031 wps 16383.32 step time 0.41s\n","# Epoch 14  global step 42860 loss 0.30092 batch 194/3281 lr 0.001 accuracy 94.41406 wps 18306.82 step time 0.43s\n","# Epoch 14  global step 42880 loss 0.30760 batch 214/3281 lr 0.001 accuracy 94.42578 wps 19516.13 step time 0.43s\n","# Epoch 14  global step 42900 loss 0.27667 batch 234/3281 lr 0.001 accuracy 95.09375 wps 17981.85 step time 0.39s\n","# Epoch 14  global step 42920 loss 0.29531 batch 254/3281 lr 0.001 accuracy 94.69922 wps 16796.99 step time 0.37s\n","# Epoch 14  global step 42940 loss 0.28813 batch 274/3281 lr 0.001 accuracy 94.89453 wps 16488.84 step time 0.41s\n","# Epoch 14  global step 42960 loss 0.28482 batch 294/3281 lr 0.001 accuracy 94.83203 wps 16990.36 step time 0.41s\n","# Epoch 14  global step 42980 loss 0.28276 batch 314/3281 lr 0.001 accuracy 94.82812 wps 16153.23 step time 0.47s\n","# Epoch 14  global step 43000 loss 0.29450 batch 334/3281 lr 0.001 accuracy 94.73828 wps 16201.27 step time 0.44s\n","# Epoch 14  global step 43020 loss 0.28956 batch 354/3281 lr 0.001 accuracy 94.65625 wps 16124.81 step time 0.49s\n","# Epoch 14  global step 43040 loss 0.29790 batch 374/3281 lr 0.001 accuracy 94.51562 wps 16949.78 step time 0.39s\n","# Epoch 14  global step 43060 loss 0.29763 batch 394/3281 lr 0.001 accuracy 94.70703 wps 16617.04 step time 0.46s\n","# Epoch 14  global step 43080 loss 0.30351 batch 414/3281 lr 0.001 accuracy 94.35938 wps 16632.40 step time 0.42s\n","# Epoch 14  global step 43100 loss 0.28495 batch 434/3281 lr 0.001 accuracy 94.78516 wps 17288.71 step time 0.37s\n","# Epoch 14  global step 43120 loss 0.29864 batch 454/3281 lr 0.001 accuracy 94.53516 wps 17365.79 step time 0.44s\n","# Epoch 14  global step 43140 loss 0.30768 batch 474/3281 lr 0.001 accuracy 94.51562 wps 16044.28 step time 0.51s\n","# Epoch 14  global step 43160 loss 0.30283 batch 494/3281 lr 0.001 accuracy 94.55859 wps 15750.86 step time 0.46s\n","# Epoch 14  global step 43180 loss 0.29739 batch 514/3281 lr 0.001 accuracy 94.63281 wps 15867.51 step time 0.43s\n","# Epoch 14  global step 43200 loss 0.30117 batch 534/3281 lr 0.001 accuracy 94.62500 wps 17544.92 step time 0.46s\n","# Epoch 14  global step 43220 loss 0.27891 batch 554/3281 lr 0.001 accuracy 95.07031 wps 17159.81 step time 0.38s\n","# Epoch 14  global step 43240 loss 0.30365 batch 574/3281 lr 0.001 accuracy 94.46484 wps 17068.25 step time 0.42s\n","# Epoch 14  global step 43260 loss 0.27892 batch 594/3281 lr 0.001 accuracy 94.97656 wps 16543.38 step time 0.32s\n","# Epoch 14  global step 43280 loss 0.29426 batch 614/3281 lr 0.001 accuracy 94.81641 wps 17008.47 step time 0.47s\n","# Epoch 14  global step 43300 loss 0.28419 batch 634/3281 lr 0.001 accuracy 94.90234 wps 16710.15 step time 0.40s\n","# Epoch 14  global step 43320 loss 0.28009 batch 654/3281 lr 0.001 accuracy 94.98437 wps 16653.42 step time 0.43s\n","# Epoch 14  global step 43340 loss 0.31578 batch 674/3281 lr 0.001 accuracy 94.16797 wps 16364.37 step time 0.49s\n","# Epoch 14  global step 43360 loss 0.29729 batch 694/3281 lr 0.001 accuracy 94.52734 wps 16705.78 step time 0.45s\n","# Epoch 14  global step 43380 loss 0.31160 batch 714/3281 lr 0.001 accuracy 94.47266 wps 17136.92 step time 0.45s\n","# Epoch 14  global step 43400 loss 0.31411 batch 734/3281 lr 0.001 accuracy 94.31250 wps 15863.68 step time 0.57s\n","# Epoch 14  global step 43420 loss 0.31849 batch 754/3281 lr 0.001 accuracy 94.37891 wps 17193.23 step time 0.40s\n","# Epoch 14  global step 43440 loss 0.31511 batch 774/3281 lr 0.001 accuracy 94.20312 wps 16177.45 step time 0.58s\n","# Epoch 14  global step 43460 loss 0.28036 batch 794/3281 lr 0.001 accuracy 94.97266 wps 17117.38 step time 0.40s\n","# Epoch 14  global step 43480 loss 0.28594 batch 814/3281 lr 0.001 accuracy 94.91406 wps 16894.00 step time 0.38s\n","# Epoch 14  global step 43500 loss 0.32381 batch 834/3281 lr 0.001 accuracy 94.05469 wps 17044.42 step time 0.48s\n","# Epoch 14  global step 43520 loss 0.29471 batch 854/3281 lr 0.001 accuracy 94.63281 wps 16899.65 step time 0.48s\n","# Epoch 14  global step 43540 loss 0.31181 batch 874/3281 lr 0.001 accuracy 94.42969 wps 16714.38 step time 0.46s\n","# Epoch 14  global step 43560 loss 0.29779 batch 894/3281 lr 0.001 accuracy 94.62109 wps 17207.89 step time 0.42s\n","# Epoch 14  global step 43580 loss 0.29989 batch 914/3281 lr 0.001 accuracy 94.57031 wps 17947.54 step time 0.40s\n","# Epoch 14  global step 43600 loss 0.28356 batch 934/3281 lr 0.001 accuracy 94.92969 wps 16295.83 step time 0.36s\n","# Epoch 14  global step 43620 loss 0.29803 batch 954/3281 lr 0.001 accuracy 94.64063 wps 15794.04 step time 0.51s\n","# Epoch 14  global step 43640 loss 0.28024 batch 974/3281 lr 0.001 accuracy 95.00781 wps 14120.75 step time 0.46s\n","# Epoch 14  global step 43660 loss 0.29243 batch 994/3281 lr 0.001 accuracy 94.73047 wps 17033.12 step time 0.40s\n","# Epoch 14  global step 43680 loss 0.29833 batch 1014/3281 lr 0.001 accuracy 94.51953 wps 17124.98 step time 0.41s\n","# Epoch 14  global step 43700 loss 0.31647 batch 1034/3281 lr 0.001 accuracy 94.20703 wps 17614.75 step time 0.53s\n","# Epoch 14  global step 43720 loss 0.31291 batch 1054/3281 lr 0.001 accuracy 94.34375 wps 16928.61 step time 0.45s\n","# Epoch 14  global step 43740 loss 0.31597 batch 1074/3281 lr 0.001 accuracy 94.25391 wps 16449.14 step time 0.49s\n","# Epoch 14  global step 43760 loss 0.29997 batch 1094/3281 lr 0.001 accuracy 94.64063 wps 16313.84 step time 0.48s\n","# Epoch 14  global step 43780 loss 0.30227 batch 1114/3281 lr 0.001 accuracy 94.60937 wps 16816.18 step time 0.43s\n","# Epoch 14  global step 43800 loss 0.29701 batch 1134/3281 lr 0.001 accuracy 94.59766 wps 16785.39 step time 0.45s\n","# Epoch 14  global step 43820 loss 0.28393 batch 1154/3281 lr 0.001 accuracy 94.78125 wps 17331.96 step time 0.42s\n","# Epoch 14  global step 43840 loss 0.29808 batch 1174/3281 lr 0.001 accuracy 94.67969 wps 17612.69 step time 0.40s\n","# Epoch 14  global step 43860 loss 0.29922 batch 1194/3281 lr 0.001 accuracy 94.57422 wps 16907.96 step time 0.39s\n","# Epoch 14  global step 43880 loss 0.31029 batch 1214/3281 lr 0.001 accuracy 94.44922 wps 17184.08 step time 0.39s\n","# Epoch 14  global step 43900 loss 0.30576 batch 1234/3281 lr 0.001 accuracy 94.51172 wps 17051.33 step time 0.40s\n","# Epoch 14  global step 43920 loss 0.30345 batch 1254/3281 lr 0.001 accuracy 94.45312 wps 16953.84 step time 0.46s\n","# Epoch 14  global step 43940 loss 0.31994 batch 1274/3281 lr 0.001 accuracy 94.54687 wps 17400.19 step time 0.46s\n","# Epoch 14  global step 43960 loss 0.30007 batch 1294/3281 lr 0.001 accuracy 94.38672 wps 17166.00 step time 0.41s\n","# Epoch 14  global step 43980 loss 0.30610 batch 1314/3281 lr 0.001 accuracy 94.37891 wps 16739.14 step time 0.43s\n","# Epoch 14  global step 44000 loss 0.30415 batch 1334/3281 lr 0.001 accuracy 94.58203 wps 16200.44 step time 0.44s\n","# global step 44000, eval model at Fri May 22 15:39:05 2020\n","2020-05-22 15:39:08.603570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:39:08.604236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 15:39:08.604320: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 15:39:08.604361: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 15:39:08.604432: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 15:39:08.604501: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 15:39:08.604545: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 15:39:08.604583: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 15:39:08.604621: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 15:39:08.604813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:39:08.605374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:39:08.605905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 15:39:08.606008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 15:39:08.606043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 15:39:08.606056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 15:39:08.606233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:39:08.606916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:39:08.607468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6401948511992489\n","# location_distance_from_business_district - 0.5678366738444074\n","# location_easy_to_find - 0.7220050500608681\n","# service_wait_time - 0.6707366987465108\n","# service_waiters_attitude - 0.7956262054572991\n","# service_parking_convenience - 0.7347672471718273\n","# service_serving_speed - 0.7626637228089204\n","# price_level - 0.777277164423106\n","# price_cost_effective - 0.7172016080472086\n","# price_discount - 0.6702777388214343\n","# environment_decoration - 0.7327658202433807\n","# environment_noise - 0.7537371845487062\n","# environment_space - 0.756604071650416\n","# environment_cleaness - 0.7457577696489134\n","# dish_portion - 0.7216369428534339\n","# dish_taste - 0.7251122001182563\n","# dish_look - 0.582195245314175\n","# dish_recommendation - 0.7345965727135645\n","# others_overall_experience - 0.5901832869695085\n","# others_willing_to_consume_again - 0.7125964961157973\n","# Eval loss 0.27619, f1 0.70569\n","# current result -0.7056886275378491, previous best result -0.7079688364247765\n","# No loss decrease, restore previous best model and set learning rate to half of previous one\n","# Epoch 14  global step 36020 loss 0.29557 batch 1354/3281 lr 0.0001 accuracy 94.66016 wps 16379.80 step time 0.38s\n","# Epoch 14  global step 36040 loss 0.31045 batch 1374/3281 lr 0.0001 accuracy 94.31250 wps 16381.30 step time 0.47s\n","# Epoch 14  global step 36060 loss 0.30128 batch 1394/3281 lr 0.0001 accuracy 94.59766 wps 17157.74 step time 0.46s\n","# Epoch 14  global step 36080 loss 0.27866 batch 1414/3281 lr 0.0001 accuracy 94.81641 wps 17380.80 step time 0.39s\n","# Epoch 14  global step 36100 loss 0.29534 batch 1434/3281 lr 0.0001 accuracy 94.46094 wps 16722.56 step time 0.50s\n","# Epoch 14  global step 36120 loss 0.30860 batch 1454/3281 lr 0.0001 accuracy 94.37109 wps 16892.82 step time 0.51s\n","# Epoch 14  global step 36140 loss 0.28772 batch 1474/3281 lr 0.0001 accuracy 94.92187 wps 16663.68 step time 0.38s\n","# Epoch 14  global step 36160 loss 0.28277 batch 1494/3281 lr 0.0001 accuracy 94.91016 wps 17346.91 step time 0.38s\n","# Epoch 14  global step 36180 loss 0.30240 batch 1514/3281 lr 0.0001 accuracy 94.37891 wps 12072.40 step time 0.63s\n","# Epoch 14  global step 36200 loss 0.27523 batch 1534/3281 lr 0.0001 accuracy 95.02734 wps 15474.59 step time 0.41s\n","# Epoch 14  global step 36220 loss 0.28374 batch 1554/3281 lr 0.0001 accuracy 94.92188 wps 16858.63 step time 0.39s\n","# Epoch 14  global step 36240 loss 0.31485 batch 1574/3281 lr 0.0001 accuracy 94.44922 wps 17282.22 step time 0.41s\n","# Epoch 14  global step 36260 loss 0.30897 batch 1594/3281 lr 0.0001 accuracy 94.48438 wps 17258.41 step time 0.37s\n","# Epoch 14  global step 36280 loss 0.27920 batch 1614/3281 lr 0.0001 accuracy 94.81250 wps 17292.40 step time 0.42s\n","# Epoch 14  global step 36300 loss 0.28830 batch 1634/3281 lr 0.0001 accuracy 94.76562 wps 16853.98 step time 0.43s\n","# Epoch 14  global step 36320 loss 0.29344 batch 1654/3281 lr 0.0001 accuracy 94.64844 wps 17248.03 step time 0.39s\n","# Epoch 14  global step 36340 loss 0.26864 batch 1674/3281 lr 0.0001 accuracy 95.17969 wps 16728.06 step time 0.36s\n","# Epoch 14  global step 36360 loss 0.29456 batch 1694/3281 lr 0.0001 accuracy 94.62109 wps 17423.63 step time 0.42s\n","# Epoch 14  global step 36380 loss 0.29143 batch 1714/3281 lr 0.0001 accuracy 94.66406 wps 17033.26 step time 0.40s\n","# Epoch 14  global step 36400 loss 0.30264 batch 1734/3281 lr 0.0001 accuracy 94.42578 wps 17163.08 step time 0.45s\n","# Epoch 14  global step 36420 loss 0.29091 batch 1754/3281 lr 0.0001 accuracy 94.72266 wps 16712.66 step time 0.40s\n","# Epoch 14  global step 36440 loss 0.29437 batch 1774/3281 lr 0.0001 accuracy 94.64062 wps 16744.04 step time 0.35s\n","# Epoch 14  global step 36460 loss 0.30271 batch 1794/3281 lr 0.0001 accuracy 94.37500 wps 16942.61 step time 0.51s\n","# Epoch 14  global step 36480 loss 0.29352 batch 1814/3281 lr 0.0001 accuracy 94.63672 wps 16791.64 step time 0.52s\n","# Epoch 14  global step 36500 loss 0.29921 batch 1834/3281 lr 0.0001 accuracy 94.47266 wps 16848.17 step time 0.46s\n","# Epoch 14  global step 36520 loss 0.27778 batch 1854/3281 lr 0.0001 accuracy 94.98828 wps 16000.53 step time 0.38s\n","# Epoch 14  global step 36540 loss 0.31169 batch 1874/3281 lr 0.0001 accuracy 94.34766 wps 18178.67 step time 0.45s\n","# Epoch 14  global step 36560 loss 0.30158 batch 1894/3281 lr 0.0001 accuracy 94.47266 wps 16610.76 step time 0.42s\n","# Epoch 14  global step 36580 loss 0.28104 batch 1914/3281 lr 0.0001 accuracy 94.78125 wps 16776.78 step time 0.39s\n","# Epoch 14  global step 36600 loss 0.28684 batch 1934/3281 lr 0.0001 accuracy 94.84375 wps 17408.53 step time 0.40s\n","# Epoch 14  global step 36620 loss 0.29533 batch 1954/3281 lr 0.0001 accuracy 94.50391 wps 16335.36 step time 0.47s\n","# Epoch 14  global step 36640 loss 0.29452 batch 1974/3281 lr 0.0001 accuracy 94.69922 wps 15850.10 step time 0.45s\n","# Epoch 14  global step 36660 loss 0.26097 batch 1994/3281 lr 0.0001 accuracy 95.24609 wps 16037.25 step time 0.34s\n","# Epoch 14  global step 36680 loss 0.30972 batch 2014/3281 lr 0.0001 accuracy 94.48828 wps 15286.75 step time 0.59s\n","# Epoch 14  global step 36700 loss 0.28930 batch 2034/3281 lr 0.0001 accuracy 94.95703 wps 16935.71 step time 0.38s\n","# Epoch 14  global step 36720 loss 0.27667 batch 2054/3281 lr 0.0001 accuracy 94.95703 wps 16602.20 step time 0.41s\n","# Epoch 14  global step 36740 loss 0.27837 batch 2074/3281 lr 0.0001 accuracy 95.02734 wps 16858.86 step time 0.38s\n","# Epoch 14  global step 36760 loss 0.30020 batch 2094/3281 lr 0.0001 accuracy 94.49609 wps 16470.93 step time 0.46s\n","# Epoch 14  global step 36780 loss 0.29444 batch 2114/3281 lr 0.0001 accuracy 94.55859 wps 17060.39 step time 0.42s\n","# Epoch 14  global step 36800 loss 0.29537 batch 2134/3281 lr 0.0001 accuracy 94.73047 wps 17227.72 step time 0.40s\n","# Epoch 14  global step 36820 loss 0.27848 batch 2154/3281 lr 0.0001 accuracy 94.92969 wps 16934.46 step time 0.35s\n","# Epoch 14  global step 36840 loss 0.29651 batch 2174/3281 lr 0.0001 accuracy 94.75391 wps 17032.38 step time 0.42s\n","# Epoch 14  global step 36860 loss 0.32720 batch 2194/3281 lr 0.0001 accuracy 93.91797 wps 17439.99 step time 0.61s\n","# Epoch 14  global step 36880 loss 0.29949 batch 2214/3281 lr 0.0001 accuracy 94.53125 wps 16758.19 step time 0.43s\n","# Epoch 14  global step 36900 loss 0.29808 batch 2234/3281 lr 0.0001 accuracy 94.45312 wps 17366.20 step time 0.42s\n","# Epoch 14  global step 36920 loss 0.27105 batch 2254/3281 lr 0.0001 accuracy 94.92578 wps 12309.23 step time 0.50s\n","# Epoch 14  global step 36940 loss 0.31228 batch 2274/3281 lr 0.0001 accuracy 94.27734 wps 16460.08 step time 0.51s\n","# Epoch 14  global step 36960 loss 0.31889 batch 2294/3281 lr 0.0001 accuracy 94.16797 wps 17594.87 step time 0.45s\n","# Epoch 14  global step 36980 loss 0.27688 batch 2314/3281 lr 0.0001 accuracy 94.98828 wps 16612.89 step time 0.43s\n","# Epoch 14  global step 37000 loss 0.30164 batch 2334/3281 lr 0.0001 accuracy 94.41797 wps 17734.79 step time 0.46s\n","# Epoch 14  global step 37020 loss 0.28667 batch 2354/3281 lr 0.0001 accuracy 94.93750 wps 16983.88 step time 0.37s\n","# Epoch 14  global step 37040 loss 0.30023 batch 2374/3281 lr 0.0001 accuracy 94.52734 wps 17610.69 step time 0.41s\n","# Epoch 14  global step 37060 loss 0.30733 batch 2394/3281 lr 0.0001 accuracy 94.42578 wps 16869.36 step time 0.50s\n","# Epoch 14  global step 37080 loss 0.28184 batch 2414/3281 lr 0.0001 accuracy 95.01563 wps 16949.38 step time 0.35s\n","# Epoch 14  global step 37100 loss 0.30045 batch 2434/3281 lr 0.0001 accuracy 94.60547 wps 16865.91 step time 0.40s\n","# Epoch 14  global step 37120 loss 0.32025 batch 2454/3281 lr 0.0001 accuracy 94.25000 wps 17447.50 step time 0.55s\n","# Epoch 14  global step 37140 loss 0.29879 batch 2474/3281 lr 0.0001 accuracy 94.63672 wps 16293.98 step time 0.36s\n","# Epoch 14  global step 37160 loss 0.29719 batch 2494/3281 lr 0.0001 accuracy 94.53906 wps 16352.24 step time 0.46s\n","# Epoch 14  global step 37180 loss 0.29596 batch 2514/3281 lr 0.0001 accuracy 94.53516 wps 16720.89 step time 0.44s\n","# Epoch 14  global step 37200 loss 0.27271 batch 2534/3281 lr 0.0001 accuracy 95.03516 wps 17117.76 step time 0.37s\n","# Epoch 14  global step 37220 loss 0.28980 batch 2554/3281 lr 0.0001 accuracy 94.70312 wps 17594.70 step time 0.43s\n","# Epoch 14  global step 37240 loss 0.29793 batch 2574/3281 lr 0.0001 accuracy 94.54297 wps 16579.67 step time 0.48s\n","# Epoch 14  global step 37260 loss 0.27830 batch 2594/3281 lr 0.0001 accuracy 95.00391 wps 16558.54 step time 0.36s\n","# Epoch 14  global step 37280 loss 0.29241 batch 2614/3281 lr 0.0001 accuracy 94.67578 wps 16943.27 step time 0.49s\n","# Epoch 14  global step 37300 loss 0.29418 batch 2634/3281 lr 0.0001 accuracy 94.69922 wps 16641.88 step time 0.38s\n","# Epoch 14  global step 37320 loss 0.30515 batch 2654/3281 lr 0.0001 accuracy 94.33984 wps 16540.36 step time 0.41s\n","# Epoch 14  global step 37340 loss 0.29510 batch 2674/3281 lr 0.0001 accuracy 94.74609 wps 17665.40 step time 0.38s\n","# Epoch 14  global step 37360 loss 0.30746 batch 2694/3281 lr 0.0001 accuracy 94.42187 wps 17028.02 step time 0.41s\n","# Epoch 14  global step 37380 loss 0.30292 batch 2714/3281 lr 0.0001 accuracy 94.49219 wps 16806.03 step time 0.43s\n","# Epoch 14  global step 37400 loss 0.28051 batch 2734/3281 lr 0.0001 accuracy 94.78125 wps 16553.44 step time 0.48s\n","# Epoch 14  global step 37420 loss 0.30964 batch 2754/3281 lr 0.0001 accuracy 94.35156 wps 15616.39 step time 0.56s\n","# Epoch 14  global step 37440 loss 0.28811 batch 2774/3281 lr 0.0001 accuracy 94.75391 wps 15778.60 step time 0.41s\n","# Epoch 14  global step 37460 loss 0.28997 batch 2794/3281 lr 0.0001 accuracy 94.76562 wps 15827.15 step time 0.42s\n","# Epoch 14  global step 37480 loss 0.29358 batch 2814/3281 lr 0.0001 accuracy 94.66797 wps 17801.17 step time 0.43s\n","# Epoch 14  global step 37500 loss 0.27244 batch 2834/3281 lr 0.0001 accuracy 95.15625 wps 16625.74 step time 0.39s\n","# Epoch 14  global step 37520 loss 0.29415 batch 2854/3281 lr 0.0001 accuracy 94.53906 wps 17104.52 step time 0.44s\n","# Epoch 14  global step 37540 loss 0.28140 batch 2874/3281 lr 0.0001 accuracy 94.86328 wps 16632.38 step time 0.36s\n","# Epoch 14  global step 37560 loss 0.28317 batch 2894/3281 lr 0.0001 accuracy 94.73828 wps 17019.08 step time 0.40s\n","# Epoch 14  global step 37580 loss 0.30087 batch 2914/3281 lr 0.0001 accuracy 94.44922 wps 16712.88 step time 0.52s\n","# Epoch 14  global step 37600 loss 0.28706 batch 2934/3281 lr 0.0001 accuracy 94.81250 wps 16292.86 step time 0.39s\n","# Epoch 14  global step 37620 loss 0.30289 batch 2954/3281 lr 0.0001 accuracy 94.54688 wps 14378.65 step time 0.53s\n","# Epoch 14  global step 37640 loss 0.27680 batch 2974/3281 lr 0.0001 accuracy 94.95703 wps 13454.14 step time 0.46s\n","# Epoch 14  global step 37660 loss 0.29002 batch 2994/3281 lr 0.0001 accuracy 94.65234 wps 16759.28 step time 0.41s\n","# Epoch 14  global step 37680 loss 0.29593 batch 3014/3281 lr 0.0001 accuracy 94.62109 wps 17341.90 step time 0.43s\n","# Epoch 14  global step 37700 loss 0.30936 batch 3034/3281 lr 0.0001 accuracy 94.49219 wps 17368.86 step time 0.42s\n","# Epoch 14  global step 37720 loss 0.29667 batch 3054/3281 lr 0.0001 accuracy 94.60938 wps 17454.16 step time 0.41s\n","# Epoch 14  global step 37740 loss 0.28732 batch 3074/3281 lr 0.0001 accuracy 94.69922 wps 16922.24 step time 0.42s\n","# Epoch 14  global step 37760 loss 0.28427 batch 3094/3281 lr 0.0001 accuracy 94.73828 wps 17158.78 step time 0.43s\n","# Epoch 14  global step 37780 loss 0.27137 batch 3114/3281 lr 0.0001 accuracy 95.01563 wps 16226.93 step time 0.37s\n","# Epoch 14  global step 37800 loss 0.30675 batch 3134/3281 lr 0.0001 accuracy 94.41016 wps 17406.95 step time 0.50s\n","# Epoch 14  global step 37820 loss 0.29363 batch 3154/3281 lr 0.0001 accuracy 94.68359 wps 17548.99 step time 0.44s\n","# Epoch 14  global step 37840 loss 0.30630 batch 3174/3281 lr 0.0001 accuracy 94.44531 wps 17002.02 step time 0.42s\n","# Epoch 14  global step 37860 loss 0.29859 batch 3194/3281 lr 0.0001 accuracy 94.47266 wps 16977.80 step time 0.48s\n","# Epoch 14  global step 37880 loss 0.29863 batch 3214/3281 lr 0.0001 accuracy 94.64844 wps 17415.37 step time 0.38s\n","# Epoch 14  global step 37900 loss 0.27926 batch 3234/3281 lr 0.0001 accuracy 95.05469 wps 16578.68 step time 0.36s\n","# Epoch 14  global step 37920 loss 0.29445 batch 3254/3281 lr 0.0001 accuracy 94.52734 wps 16468.33 step time 0.48s\n","# Epoch 14  global step 37940 loss 0.28894 batch 3274/3281 lr 0.0001 accuracy 94.81641 wps 17072.93 step time 0.39s\n","# Finsh epoch 14, global step 37948\n","# Epoch 15  global step 37960 loss 0.15853 batch 12/3281 lr 0.0001 accuracy 57.07813 wps 18140.08 step time 0.25s\n","# Epoch 15  global step 37980 loss 0.27820 batch 32/3281 lr 0.0001 accuracy 94.84375 wps 19237.57 step time 0.37s\n","# Epoch 15  global step 38000 loss 0.28206 batch 52/3281 lr 0.0001 accuracy 94.85938 wps 19713.80 step time 0.38s\n","# global step 38000, eval model at Fri May 22 15:54:44 2020\n","2020-05-22 15:54:46.524128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:54:46.524945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 15:54:46.525058: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 15:54:46.525112: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 15:54:46.525165: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 15:54:46.525217: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 15:54:46.525259: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 15:54:46.525304: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 15:54:46.525350: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 15:54:46.525484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:54:46.526160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:54:46.526746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 15:54:46.526830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 15:54:46.526851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 15:54:46.526866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 15:54:46.527001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:54:46.527749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:54:46.528415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6495059938809851\n","# location_distance_from_business_district - 0.5594877417570151\n","# location_easy_to_find - 0.7174447945319358\n","# service_wait_time - 0.6693658203714179\n","# service_waiters_attitude - 0.7976291414482075\n","# service_parking_convenience - 0.7390202853444887\n","# service_serving_speed - 0.7638703608481802\n","# price_level - 0.7799959066412762\n","# price_cost_effective - 0.7136268662660662\n","# price_discount - 0.6752104411112503\n","# environment_decoration - 0.737874840011807\n","# environment_noise - 0.753164347894\n","# environment_space - 0.7584058036570934\n","# environment_cleaness - 0.7508484841120036\n","# dish_portion - 0.7260369101971451\n","# dish_taste - 0.7322027929580499\n","# dish_look - 0.5825235940412641\n","# dish_recommendation - 0.743073453285276\n","# others_overall_experience - 0.5902151763580847\n","# others_willing_to_consume_again - 0.7109126326859025\n","# Eval loss 0.26898, f1 0.70752\n","# current result -0.7075207693700725, previous best result -0.7079688364247765\n","# Epoch 15  global step 38020 loss 0.28978 batch 72/3281 lr 0.0001 accuracy 94.70703 wps 18793.21 step time 0.37s\n","# Epoch 15  global step 38040 loss 0.26522 batch 92/3281 lr 0.0001 accuracy 95.27734 wps 16773.16 step time 0.31s\n","# Epoch 15  global step 38060 loss 0.30870 batch 112/3281 lr 0.0001 accuracy 94.37891 wps 19543.98 step time 0.45s\n","# Epoch 15  global step 38080 loss 0.30023 batch 132/3281 lr 0.0001 accuracy 94.41016 wps 18787.58 step time 0.39s\n","# Epoch 15  global step 38100 loss 0.28194 batch 152/3281 lr 0.0001 accuracy 94.83594 wps 18389.75 step time 0.34s\n","# Epoch 15  global step 38120 loss 0.29153 batch 172/3281 lr 0.0001 accuracy 94.58984 wps 19016.73 step time 0.41s\n","# Epoch 15  global step 38140 loss 0.27945 batch 192/3281 lr 0.0001 accuracy 94.90234 wps 18543.56 step time 0.35s\n","# Epoch 15  global step 38160 loss 0.31090 batch 212/3281 lr 0.0001 accuracy 94.30078 wps 19639.43 step time 0.38s\n","# Epoch 15  global step 38180 loss 0.30591 batch 232/3281 lr 0.0001 accuracy 94.38281 wps 19140.98 step time 0.44s\n","# Epoch 15  global step 38200 loss 0.30601 batch 252/3281 lr 0.0001 accuracy 94.34766 wps 19491.40 step time 0.50s\n","# Epoch 15  global step 38220 loss 0.29806 batch 272/3281 lr 0.0001 accuracy 94.66016 wps 19691.59 step time 0.45s\n","# Epoch 15  global step 38240 loss 0.28396 batch 292/3281 lr 0.0001 accuracy 94.84766 wps 19122.80 step time 0.36s\n","# Epoch 15  global step 38260 loss 0.27944 batch 312/3281 lr 0.0001 accuracy 94.92578 wps 17210.05 step time 0.38s\n","# Epoch 15  global step 38280 loss 0.29803 batch 332/3281 lr 0.0001 accuracy 94.65625 wps 12703.53 step time 0.55s\n","# Epoch 15  global step 38300 loss 0.29487 batch 352/3281 lr 0.0001 accuracy 94.56250 wps 18696.24 step time 0.38s\n","# Epoch 15  global step 38320 loss 0.27885 batch 372/3281 lr 0.0001 accuracy 95.05078 wps 18464.57 step time 0.39s\n","# Epoch 15  global step 38340 loss 0.30090 batch 392/3281 lr 0.0001 accuracy 94.32813 wps 18850.08 step time 0.36s\n","# Epoch 15  global step 38360 loss 0.27908 batch 412/3281 lr 0.0001 accuracy 94.96875 wps 19475.75 step time 0.37s\n","# Epoch 15  global step 38380 loss 0.29089 batch 432/3281 lr 0.0001 accuracy 94.87891 wps 19058.10 step time 0.35s\n","# Epoch 15  global step 38400 loss 0.27538 batch 452/3281 lr 0.0001 accuracy 94.95703 wps 18226.06 step time 0.34s\n","# Epoch 15  global step 38420 loss 0.27509 batch 472/3281 lr 0.0001 accuracy 95.15625 wps 17946.75 step time 0.38s\n","# Epoch 15  global step 38440 loss 0.29283 batch 492/3281 lr 0.0001 accuracy 94.65234 wps 19047.49 step time 0.42s\n","# Epoch 15  global step 38460 loss 0.28228 batch 512/3281 lr 0.0001 accuracy 94.76953 wps 19032.12 step time 0.40s\n","# Epoch 15  global step 38480 loss 0.29460 batch 532/3281 lr 0.0001 accuracy 94.52344 wps 19262.24 step time 0.36s\n","# Epoch 15  global step 38500 loss 0.28198 batch 552/3281 lr 0.0001 accuracy 94.98438 wps 18594.98 step time 0.35s\n","# Epoch 15  global step 38520 loss 0.29454 batch 572/3281 lr 0.0001 accuracy 94.56641 wps 19657.23 step time 0.38s\n","# Epoch 15  global step 38540 loss 0.27963 batch 592/3281 lr 0.0001 accuracy 94.96094 wps 18312.91 step time 0.39s\n","# Epoch 15  global step 38560 loss 0.31039 batch 612/3281 lr 0.0001 accuracy 94.42187 wps 19313.11 step time 0.43s\n","# Epoch 15  global step 38580 loss 0.28287 batch 632/3281 lr 0.0001 accuracy 94.83984 wps 19113.60 step time 0.41s\n","# Epoch 15  global step 38600 loss 0.27089 batch 652/3281 lr 0.0001 accuracy 95.15625 wps 18053.07 step time 0.33s\n","# Epoch 15  global step 38620 loss 0.28401 batch 672/3281 lr 0.0001 accuracy 94.81641 wps 18705.20 step time 0.37s\n","# Epoch 15  global step 38640 loss 0.29078 batch 692/3281 lr 0.0001 accuracy 94.56250 wps 19262.21 step time 0.41s\n","# Epoch 15  global step 38660 loss 0.27497 batch 712/3281 lr 0.0001 accuracy 95.01953 wps 19092.21 step time 0.35s\n","# Epoch 15  global step 38680 loss 0.30478 batch 732/3281 lr 0.0001 accuracy 94.30859 wps 19491.49 step time 0.41s\n","# Epoch 15  global step 38700 loss 0.28698 batch 752/3281 lr 0.0001 accuracy 94.83594 wps 19067.07 step time 0.42s\n","# Epoch 15  global step 38720 loss 0.27745 batch 772/3281 lr 0.0001 accuracy 94.95703 wps 17944.12 step time 0.34s\n","# Epoch 15  global step 38740 loss 0.29412 batch 792/3281 lr 0.0001 accuracy 94.53906 wps 19800.48 step time 0.40s\n","# Epoch 15  global step 38760 loss 0.29532 batch 812/3281 lr 0.0001 accuracy 94.57422 wps 19585.26 step time 0.36s\n","# Epoch 15  global step 38780 loss 0.28204 batch 832/3281 lr 0.0001 accuracy 94.97266 wps 18689.91 step time 0.34s\n","# Epoch 15  global step 38800 loss 0.28223 batch 852/3281 lr 0.0001 accuracy 94.88281 wps 19503.47 step time 0.37s\n","# Epoch 15  global step 38820 loss 0.28819 batch 872/3281 lr 0.0001 accuracy 94.62500 wps 19728.99 step time 0.38s\n","# Epoch 15  global step 38840 loss 0.29690 batch 892/3281 lr 0.0001 accuracy 94.59766 wps 19134.47 step time 0.36s\n","# Epoch 15  global step 38860 loss 0.28060 batch 912/3281 lr 0.0001 accuracy 94.90234 wps 19230.46 step time 0.39s\n","# Epoch 15  global step 38880 loss 0.28040 batch 932/3281 lr 0.0001 accuracy 94.89453 wps 18790.54 step time 0.36s\n","# Epoch 15  global step 38900 loss 0.28604 batch 952/3281 lr 0.0001 accuracy 94.93359 wps 18388.82 step time 0.35s\n","# Epoch 15  global step 38920 loss 0.28267 batch 972/3281 lr 0.0001 accuracy 94.72656 wps 19546.45 step time 0.43s\n","# Epoch 15  global step 38940 loss 0.28326 batch 992/3281 lr 0.0001 accuracy 94.83984 wps 18456.41 step time 0.39s\n","# Epoch 15  global step 38960 loss 0.27339 batch 1012/3281 lr 0.0001 accuracy 94.98437 wps 18619.01 step time 0.35s\n","# Epoch 15  global step 38980 loss 0.29820 batch 1032/3281 lr 0.0001 accuracy 94.61719 wps 19556.15 step time 0.40s\n","# Epoch 15  global step 39000 loss 0.28865 batch 1052/3281 lr 0.0001 accuracy 94.74609 wps 18687.99 step time 0.34s\n","# Epoch 15  global step 39020 loss 0.27365 batch 1072/3281 lr 0.0001 accuracy 95.13672 wps 18046.42 step time 0.33s\n","# Epoch 15  global step 39040 loss 0.29811 batch 1092/3281 lr 0.0001 accuracy 94.67969 wps 19833.24 step time 0.38s\n","# Epoch 15  global step 39060 loss 0.28849 batch 1112/3281 lr 0.0001 accuracy 94.52734 wps 18867.63 step time 0.35s\n","# Epoch 15  global step 39080 loss 0.28032 batch 1132/3281 lr 0.0001 accuracy 94.91016 wps 18281.33 step time 0.38s\n","# Epoch 15  global step 39100 loss 0.28660 batch 1152/3281 lr 0.0001 accuracy 94.75781 wps 18906.63 step time 0.40s\n","# Epoch 15  global step 39120 loss 0.29944 batch 1172/3281 lr 0.0001 accuracy 94.57812 wps 19138.16 step time 0.43s\n","# Epoch 15  global step 39140 loss 0.30717 batch 1192/3281 lr 0.0001 accuracy 94.32813 wps 20418.67 step time 0.43s\n","# Epoch 15  global step 39160 loss 0.28301 batch 1212/3281 lr 0.0001 accuracy 94.88281 wps 17724.85 step time 0.38s\n","# Epoch 15  global step 39180 loss 0.28712 batch 1232/3281 lr 0.0001 accuracy 94.85547 wps 17190.35 step time 0.40s\n","# Epoch 15  global step 39200 loss 0.28021 batch 1252/3281 lr 0.0001 accuracy 94.84766 wps 18878.37 step time 0.36s\n","# Epoch 15  global step 39220 loss 0.27318 batch 1272/3281 lr 0.0001 accuracy 95.10547 wps 18926.05 step time 0.36s\n","# Epoch 15  global step 39240 loss 0.27911 batch 1292/3281 lr 0.0001 accuracy 94.98437 wps 18445.35 step time 0.34s\n","# Epoch 15  global step 39260 loss 0.28542 batch 1312/3281 lr 0.0001 accuracy 94.63672 wps 18989.91 step time 0.36s\n","# Epoch 15  global step 39280 loss 0.30382 batch 1332/3281 lr 0.0001 accuracy 94.50391 wps 18676.91 step time 0.39s\n","# Epoch 15  global step 39300 loss 0.28394 batch 1352/3281 lr 0.0001 accuracy 94.87891 wps 18580.87 step time 0.40s\n","# Epoch 15  global step 39320 loss 0.28862 batch 1372/3281 lr 0.0001 accuracy 94.77734 wps 18597.04 step time 0.45s\n","# Epoch 15  global step 39340 loss 0.29189 batch 1392/3281 lr 0.0001 accuracy 94.71094 wps 19803.85 step time 0.38s\n","# Epoch 15  global step 39360 loss 0.30037 batch 1412/3281 lr 0.0001 accuracy 94.48828 wps 20015.75 step time 0.40s\n","# Epoch 15  global step 39380 loss 0.28856 batch 1432/3281 lr 0.0001 accuracy 94.72656 wps 18818.05 step time 0.35s\n","# Epoch 15  global step 39400 loss 0.29901 batch 1452/3281 lr 0.0001 accuracy 94.53125 wps 18911.92 step time 0.36s\n","# Epoch 15  global step 39420 loss 0.27878 batch 1472/3281 lr 0.0001 accuracy 95.03516 wps 16885.52 step time 0.37s\n","# Epoch 15  global step 39440 loss 0.28261 batch 1492/3281 lr 0.0001 accuracy 95.00391 wps 16601.04 step time 0.47s\n","# Epoch 15  global step 39460 loss 0.28441 batch 1512/3281 lr 0.0001 accuracy 94.92969 wps 16859.07 step time 0.39s\n","# Epoch 15  global step 39480 loss 0.29706 batch 1532/3281 lr 0.0001 accuracy 94.78516 wps 17282.88 step time 0.43s\n","# Epoch 15  global step 39500 loss 0.27154 batch 1552/3281 lr 0.0001 accuracy 95.09375 wps 17801.33 step time 0.39s\n","# Epoch 15  global step 39520 loss 0.29986 batch 1572/3281 lr 0.0001 accuracy 94.55859 wps 16395.94 step time 0.47s\n","# Epoch 15  global step 39540 loss 0.28490 batch 1592/3281 lr 0.0001 accuracy 94.80859 wps 14939.00 step time 0.49s\n","# Epoch 15  global step 39560 loss 0.28208 batch 1612/3281 lr 0.0001 accuracy 94.66016 wps 17089.40 step time 0.43s\n","# Epoch 15  global step 39580 loss 0.27703 batch 1632/3281 lr 0.0001 accuracy 94.87500 wps 16996.23 step time 0.37s\n","# Epoch 15  global step 39600 loss 0.29142 batch 1652/3281 lr 0.0001 accuracy 94.68359 wps 16419.06 step time 0.51s\n","# Epoch 15  global step 39620 loss 0.27565 batch 1672/3281 lr 0.0001 accuracy 94.86719 wps 16409.38 step time 0.45s\n","# Epoch 15  global step 39640 loss 0.28543 batch 1692/3281 lr 0.0001 accuracy 95.08984 wps 16973.85 step time 0.44s\n","# Epoch 15  global step 39660 loss 0.27821 batch 1712/3281 lr 0.0001 accuracy 94.91016 wps 17472.87 step time 0.40s\n","# Epoch 15  global step 39680 loss 0.27391 batch 1732/3281 lr 0.0001 accuracy 95.15625 wps 17338.53 step time 0.46s\n","# Epoch 15  global step 39700 loss 0.29756 batch 1752/3281 lr 0.0001 accuracy 94.53516 wps 16746.64 step time 0.41s\n","# Epoch 15  global step 39720 loss 0.28382 batch 1772/3281 lr 0.0001 accuracy 94.93359 wps 16683.24 step time 0.46s\n","# Epoch 15  global step 39740 loss 0.30229 batch 1792/3281 lr 0.0001 accuracy 94.51172 wps 17520.17 step time 0.39s\n","# Epoch 15  global step 39760 loss 0.28293 batch 1812/3281 lr 0.0001 accuracy 94.92188 wps 16947.30 step time 0.38s\n","# Epoch 15  global step 39780 loss 0.28854 batch 1832/3281 lr 0.0001 accuracy 94.70703 wps 17328.51 step time 0.47s\n","# Epoch 15  global step 39800 loss 0.29579 batch 1852/3281 lr 0.0001 accuracy 94.55859 wps 15756.11 step time 0.56s\n","# Epoch 15  global step 39820 loss 0.28269 batch 1872/3281 lr 0.0001 accuracy 94.62109 wps 16913.97 step time 0.40s\n","# Epoch 15  global step 39840 loss 0.28798 batch 1892/3281 lr 0.0001 accuracy 94.88672 wps 16525.34 step time 0.46s\n","# Epoch 15  global step 39860 loss 0.30543 batch 1912/3281 lr 0.0001 accuracy 94.29297 wps 17026.52 step time 0.45s\n","# Epoch 15  global step 39880 loss 0.28245 batch 1932/3281 lr 0.0001 accuracy 94.84375 wps 15730.25 step time 0.49s\n","# Epoch 15  global step 39900 loss 0.27950 batch 1952/3281 lr 0.0001 accuracy 94.91406 wps 16749.96 step time 0.37s\n","# Epoch 15  global step 39920 loss 0.29903 batch 1972/3281 lr 0.0001 accuracy 94.55078 wps 16862.89 step time 0.48s\n","# Epoch 15  global step 39940 loss 0.30165 batch 1992/3281 lr 0.0001 accuracy 94.43750 wps 17802.11 step time 0.44s\n","# Epoch 15  global step 39960 loss 0.27154 batch 2012/3281 lr 0.0001 accuracy 95.00781 wps 17082.36 step time 0.37s\n","# Epoch 15  global step 39980 loss 0.29561 batch 2032/3281 lr 0.0001 accuracy 94.75391 wps 17008.34 step time 0.46s\n","# Epoch 15  global step 40000 loss 0.29278 batch 2052/3281 lr 0.0001 accuracy 94.76172 wps 15725.50 step time 0.44s\n","# global step 40000, eval model at Fri May 22 16:09:09 2020\n","2020-05-22 16:09:12.527954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:09:12.528581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 16:09:12.528688: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 16:09:12.528754: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 16:09:12.528790: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 16:09:12.528834: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 16:09:12.528873: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 16:09:12.528910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 16:09:12.528947: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 16:09:12.529103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:09:12.529721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:09:12.530268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 16:09:12.530388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 16:09:12.530409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 16:09:12.530423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 16:09:12.530599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:09:12.531142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:09:12.531616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6571159721833303\n","# location_distance_from_business_district - 0.5571617321341713\n","# location_easy_to_find - 0.7139052057949474\n","# service_wait_time - 0.6672638806145554\n","# service_waiters_attitude - 0.7957541649002269\n","# service_parking_convenience - 0.7351046287572607\n","# service_serving_speed - 0.7639044279061793\n","# price_level - 0.7799834246655578\n","# price_cost_effective - 0.7093995894527392\n","# price_discount - 0.6718263597510397\n","# environment_decoration - 0.7373880945607023\n","# environment_noise - 0.7535711364546287\n","# environment_space - 0.7579469450556883\n","# environment_cleaness - 0.7490802234203612\n","# dish_portion - 0.7249997708505089\n","# dish_taste - 0.7282190164566734\n","# dish_look - 0.5838331508539915\n","# dish_recommendation - 0.7436715318492668\n","# others_overall_experience - 0.5902737845045737\n","# others_willing_to_consume_again - 0.7113770920834743\n","# Eval loss 0.27124, f1 0.70659\n","# current result -0.7065890066124937, previous best result -0.7079688364247765\n","# Epoch 15  global step 40020 loss 0.27732 batch 2072/3281 lr 0.0001 accuracy 95.10938 wps 16076.88 step time 0.43s\n","# Epoch 15  global step 40040 loss 0.28002 batch 2092/3281 lr 0.0001 accuracy 94.99609 wps 16014.40 step time 0.37s\n","# Epoch 15  global step 40060 loss 0.28088 batch 2112/3281 lr 0.0001 accuracy 94.95703 wps 16901.87 step time 0.41s\n","# Epoch 15  global step 40080 loss 0.30113 batch 2132/3281 lr 0.0001 accuracy 94.42969 wps 16792.15 step time 0.41s\n","# Epoch 15  global step 40100 loss 0.29181 batch 2152/3281 lr 0.0001 accuracy 94.66406 wps 17042.52 step time 0.38s\n","# Epoch 15  global step 40120 loss 0.28922 batch 2172/3281 lr 0.0001 accuracy 94.59375 wps 17915.83 step time 0.46s\n","# Epoch 15  global step 40140 loss 0.29705 batch 2192/3281 lr 0.0001 accuracy 94.53906 wps 12129.21 step time 0.58s\n","# Epoch 15  global step 40160 loss 0.29831 batch 2212/3281 lr 0.0001 accuracy 94.49219 wps 15623.91 step time 0.51s\n","# Epoch 15  global step 40180 loss 0.29837 batch 2232/3281 lr 0.0001 accuracy 94.62500 wps 16835.09 step time 0.55s\n","# Epoch 15  global step 40200 loss 0.29607 batch 2252/3281 lr 0.0001 accuracy 94.69141 wps 17104.12 step time 0.41s\n","# Epoch 15  global step 40220 loss 0.28530 batch 2272/3281 lr 0.0001 accuracy 94.85156 wps 16883.83 step time 0.44s\n","# Epoch 15  global step 40240 loss 0.27992 batch 2292/3281 lr 0.0001 accuracy 94.88281 wps 17061.35 step time 0.40s\n","# Epoch 15  global step 40260 loss 0.29142 batch 2312/3281 lr 0.0001 accuracy 94.64844 wps 17061.46 step time 0.40s\n","# Epoch 15  global step 40280 loss 0.30180 batch 2332/3281 lr 0.0001 accuracy 94.57422 wps 17172.97 step time 0.48s\n","# Epoch 15  global step 40300 loss 0.27626 batch 2352/3281 lr 0.0001 accuracy 95.03516 wps 16793.67 step time 0.44s\n","# Epoch 15  global step 40320 loss 0.28073 batch 2372/3281 lr 0.0001 accuracy 94.85938 wps 16248.19 step time 0.46s\n","# Epoch 15  global step 40340 loss 0.29546 batch 2392/3281 lr 0.0001 accuracy 94.64844 wps 16451.09 step time 0.54s\n","# Epoch 15  global step 40360 loss 0.27688 batch 2412/3281 lr 0.0001 accuracy 94.83203 wps 16060.73 step time 0.42s\n","# Epoch 15  global step 40380 loss 0.28055 batch 2432/3281 lr 0.0001 accuracy 94.83203 wps 15786.73 step time 0.50s\n","# Epoch 15  global step 40400 loss 0.28650 batch 2452/3281 lr 0.0001 accuracy 94.72656 wps 16925.12 step time 0.42s\n","# Epoch 15  global step 40420 loss 0.29309 batch 2472/3281 lr 0.0001 accuracy 94.46875 wps 17228.31 step time 0.41s\n","# Epoch 15  global step 40440 loss 0.28766 batch 2492/3281 lr 0.0001 accuracy 94.80469 wps 17056.18 step time 0.38s\n","# Epoch 15  global step 40460 loss 0.28784 batch 2512/3281 lr 0.0001 accuracy 94.69141 wps 17954.36 step time 0.41s\n","# Epoch 15  global step 40480 loss 0.29308 batch 2532/3281 lr 0.0001 accuracy 94.62109 wps 16246.49 step time 0.42s\n","# Epoch 15  global step 40500 loss 0.28210 batch 2552/3281 lr 0.0001 accuracy 94.82031 wps 17779.32 step time 0.40s\n","# Epoch 15  global step 40520 loss 0.28544 batch 2572/3281 lr 0.0001 accuracy 94.93750 wps 16615.49 step time 0.43s\n","# Epoch 15  global step 40540 loss 0.29571 batch 2592/3281 lr 0.0001 accuracy 94.67187 wps 17584.78 step time 0.46s\n","# Epoch 15  global step 40560 loss 0.26646 batch 2612/3281 lr 0.0001 accuracy 95.14062 wps 16580.41 step time 0.35s\n","# Epoch 15  global step 40580 loss 0.27601 batch 2632/3281 lr 0.0001 accuracy 94.92188 wps 17242.83 step time 0.38s\n","# Epoch 15  global step 40600 loss 0.28476 batch 2652/3281 lr 0.0001 accuracy 94.72266 wps 16809.06 step time 0.47s\n","# Epoch 15  global step 40620 loss 0.28765 batch 2672/3281 lr 0.0001 accuracy 94.80078 wps 16264.85 step time 0.44s\n","# Epoch 15  global step 40640 loss 0.28556 batch 2692/3281 lr 0.0001 accuracy 94.68359 wps 15889.83 step time 0.45s\n","# Epoch 15  global step 40660 loss 0.28809 batch 2712/3281 lr 0.0001 accuracy 94.95703 wps 17118.50 step time 0.41s\n","# Epoch 15  global step 40680 loss 0.28991 batch 2732/3281 lr 0.0001 accuracy 94.71484 wps 16788.12 step time 0.41s\n","# Epoch 15  global step 40700 loss 0.28430 batch 2752/3281 lr 0.0001 accuracy 94.71094 wps 16057.70 step time 0.54s\n","# Epoch 15  global step 40720 loss 0.26001 batch 2772/3281 lr 0.0001 accuracy 95.26172 wps 16370.94 step time 0.39s\n","# Epoch 15  global step 40740 loss 0.29438 batch 2792/3281 lr 0.0001 accuracy 94.53906 wps 16628.33 step time 0.48s\n","# Epoch 15  global step 40760 loss 0.27704 batch 2812/3281 lr 0.0001 accuracy 94.95312 wps 16499.81 step time 0.41s\n","# Epoch 15  global step 40780 loss 0.29107 batch 2832/3281 lr 0.0001 accuracy 94.82422 wps 17025.90 step time 0.39s\n","# Epoch 15  global step 40800 loss 0.27827 batch 2852/3281 lr 0.0001 accuracy 94.81641 wps 16689.56 step time 0.43s\n","# Epoch 15  global step 40820 loss 0.28928 batch 2872/3281 lr 0.0001 accuracy 94.74219 wps 16558.30 step time 0.40s\n","# Epoch 15  global step 40840 loss 0.26905 batch 2892/3281 lr 0.0001 accuracy 94.98828 wps 16794.06 step time 0.34s\n","# Epoch 15  global step 40860 loss 0.28449 batch 2912/3281 lr 0.0001 accuracy 94.79297 wps 17326.47 step time 0.44s\n","# Epoch 15  global step 40880 loss 0.28218 batch 2932/3281 lr 0.0001 accuracy 94.80078 wps 16447.73 step time 0.45s\n","# Epoch 15  global step 40900 loss 0.30306 batch 2952/3281 lr 0.0001 accuracy 94.42969 wps 16866.76 step time 0.46s\n","# Epoch 15  global step 40920 loss 0.28559 batch 2972/3281 lr 0.0001 accuracy 94.82422 wps 15928.07 step time 0.49s\n","# Epoch 15  global step 40940 loss 0.28157 batch 2992/3281 lr 0.0001 accuracy 94.96875 wps 14597.72 step time 0.51s\n","# Epoch 15  global step 40960 loss 0.28069 batch 3012/3281 lr 0.0001 accuracy 94.87500 wps 16571.28 step time 0.39s\n","# Epoch 15  global step 40980 loss 0.28941 batch 3032/3281 lr 0.0001 accuracy 94.84766 wps 16996.99 step time 0.43s\n","# Epoch 15  global step 41000 loss 0.27254 batch 3052/3281 lr 0.0001 accuracy 95.06641 wps 16697.77 step time 0.35s\n","# Epoch 15  global step 41020 loss 0.30197 batch 3072/3281 lr 0.0001 accuracy 94.50000 wps 16810.44 step time 0.49s\n","# Epoch 15  global step 41040 loss 0.29570 batch 3092/3281 lr 0.0001 accuracy 94.62109 wps 16867.79 step time 0.43s\n","# Epoch 15  global step 41060 loss 0.27936 batch 3112/3281 lr 0.0001 accuracy 94.95703 wps 15456.00 step time 0.50s\n","# Epoch 15  global step 41080 loss 0.28229 batch 3132/3281 lr 0.0001 accuracy 94.94922 wps 16663.33 step time 0.41s\n","# Epoch 15  global step 41100 loss 0.29340 batch 3152/3281 lr 0.0001 accuracy 94.70703 wps 16998.87 step time 0.40s\n","# Epoch 15  global step 41120 loss 0.30220 batch 3172/3281 lr 0.0001 accuracy 94.57031 wps 16701.74 step time 0.45s\n","# Epoch 15  global step 41140 loss 0.28568 batch 3192/3281 lr 0.0001 accuracy 94.72656 wps 17025.46 step time 0.39s\n","# Epoch 15  global step 41160 loss 0.31722 batch 3212/3281 lr 0.0001 accuracy 94.17969 wps 17475.02 step time 0.55s\n","# Epoch 15  global step 41180 loss 0.26535 batch 3232/3281 lr 0.0001 accuracy 95.23828 wps 16676.83 step time 0.41s\n","# Epoch 15  global step 41200 loss 0.29049 batch 3252/3281 lr 0.0001 accuracy 94.82812 wps 16758.30 step time 0.42s\n","# Epoch 15  global step 41220 loss 0.28938 batch 3272/3281 lr 0.0001 accuracy 94.67578 wps 17226.52 step time 0.51s\n","# Finsh epoch 15, global step 41230\n","# Epoch 16  global step 41240 loss 0.15049 batch 10/3281 lr 0.0001 accuracy 47.34375 wps 19157.82 step time 0.23s\n","# Epoch 16  global step 41260 loss 0.27290 batch 30/3281 lr 0.0001 accuracy 95.04297 wps 18578.80 step time 0.44s\n","# Epoch 16  global step 41280 loss 0.28066 batch 50/3281 lr 0.0001 accuracy 94.84375 wps 19316.13 step time 0.37s\n","# Epoch 16  global step 41300 loss 0.30084 batch 70/3281 lr 0.0001 accuracy 94.58203 wps 19353.71 step time 0.37s\n","# Epoch 16  global step 41320 loss 0.27657 batch 90/3281 lr 0.0001 accuracy 95.02734 wps 17801.59 step time 0.32s\n","# Epoch 16  global step 41340 loss 0.26146 batch 110/3281 lr 0.0001 accuracy 95.45313 wps 17484.60 step time 0.30s\n","# Epoch 16  global step 41360 loss 0.28853 batch 130/3281 lr 0.0001 accuracy 94.74609 wps 18699.62 step time 0.35s\n","# Epoch 16  global step 41380 loss 0.27680 batch 150/3281 lr 0.0001 accuracy 95.09375 wps 18738.26 step time 0.35s\n","# Epoch 16  global step 41400 loss 0.26628 batch 170/3281 lr 0.0001 accuracy 95.19531 wps 18029.46 step time 0.36s\n","# Epoch 16  global step 41420 loss 0.29511 batch 190/3281 lr 0.0001 accuracy 94.58594 wps 19274.81 step time 0.36s\n","# Epoch 16  global step 41440 loss 0.29532 batch 210/3281 lr 0.0001 accuracy 94.67578 wps 18510.24 step time 0.44s\n","# Epoch 16  global step 41460 loss 0.29170 batch 230/3281 lr 0.0001 accuracy 94.71875 wps 19221.85 step time 0.38s\n","# Epoch 16  global step 41480 loss 0.28930 batch 250/3281 lr 0.0001 accuracy 94.75000 wps 18950.71 step time 0.40s\n","# Epoch 16  global step 41500 loss 0.27502 batch 270/3281 lr 0.0001 accuracy 95.10938 wps 17989.63 step time 0.32s\n","# Epoch 16  global step 41520 loss 0.28217 batch 290/3281 lr 0.0001 accuracy 95.03125 wps 18731.01 step time 0.35s\n","# Epoch 16  global step 41540 loss 0.29640 batch 310/3281 lr 0.0001 accuracy 94.52344 wps 19193.44 step time 0.41s\n","# Epoch 16  global step 41560 loss 0.27083 batch 330/3281 lr 0.0001 accuracy 95.00000 wps 18762.41 step time 0.44s\n","# Epoch 16  global step 41580 loss 0.27773 batch 350/3281 lr 0.0001 accuracy 94.90234 wps 19770.15 step time 0.39s\n","# Epoch 16  global step 41600 loss 0.27870 batch 370/3281 lr 0.0001 accuracy 94.89453 wps 19676.45 step time 0.44s\n","# Epoch 16  global step 41620 loss 0.27077 batch 390/3281 lr 0.0001 accuracy 95.01562 wps 19028.22 step time 0.37s\n","# Epoch 16  global step 41640 loss 0.26815 batch 410/3281 lr 0.0001 accuracy 95.09375 wps 14726.18 step time 0.42s\n","# Epoch 16  global step 41660 loss 0.26924 batch 430/3281 lr 0.0001 accuracy 95.00000 wps 12618.34 step time 0.57s\n","# Epoch 16  global step 41680 loss 0.27014 batch 450/3281 lr 0.0001 accuracy 95.17188 wps 18087.57 step time 0.45s\n","# Epoch 16  global step 41700 loss 0.28339 batch 470/3281 lr 0.0001 accuracy 94.84375 wps 13453.39 step time 0.55s\n","# Epoch 16  global step 41720 loss 0.26755 batch 490/3281 lr 0.0001 accuracy 95.14063 wps 17906.11 step time 0.33s\n","# Epoch 16  global step 41740 loss 0.29687 batch 510/3281 lr 0.0001 accuracy 94.60937 wps 19599.64 step time 0.38s\n","# Epoch 16  global step 41760 loss 0.29332 batch 530/3281 lr 0.0001 accuracy 94.53125 wps 18921.00 step time 0.37s\n","# Epoch 16  global step 41780 loss 0.28762 batch 550/3281 lr 0.0001 accuracy 94.72266 wps 19458.14 step time 0.37s\n","# Epoch 16  global step 41800 loss 0.27403 batch 570/3281 lr 0.0001 accuracy 95.07813 wps 18088.02 step time 0.32s\n","# Epoch 16  global step 41820 loss 0.28829 batch 590/3281 lr 0.0001 accuracy 94.77734 wps 19289.63 step time 0.43s\n","# Epoch 16  global step 41840 loss 0.30602 batch 610/3281 lr 0.0001 accuracy 94.34375 wps 19605.87 step time 0.48s\n","# Epoch 16  global step 41860 loss 0.28845 batch 630/3281 lr 0.0001 accuracy 94.67969 wps 19228.74 step time 0.36s\n","# Epoch 16  global step 41880 loss 0.29181 batch 650/3281 lr 0.0001 accuracy 94.66016 wps 19643.40 step time 0.38s\n","# Epoch 16  global step 41900 loss 0.29038 batch 670/3281 lr 0.0001 accuracy 94.69922 wps 19250.91 step time 0.37s\n","# Epoch 16  global step 41920 loss 0.27455 batch 690/3281 lr 0.0001 accuracy 95.12500 wps 19212.08 step time 0.37s\n","# Epoch 16  global step 41940 loss 0.28351 batch 710/3281 lr 0.0001 accuracy 94.88672 wps 19278.22 step time 0.37s\n","# Epoch 16  global step 41960 loss 0.28605 batch 730/3281 lr 0.0001 accuracy 94.66406 wps 18768.25 step time 0.40s\n","# Epoch 16  global step 41980 loss 0.27917 batch 750/3281 lr 0.0001 accuracy 95.01172 wps 19491.96 step time 0.37s\n","# Epoch 16  global step 42000 loss 0.27862 batch 770/3281 lr 0.0001 accuracy 94.96094 wps 18777.17 step time 0.38s\n","# global step 42000, eval model at Fri May 22 16:24:16 2020\n","2020-05-22 16:24:19.142727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:24:19.143374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 16:24:19.143464: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 16:24:19.143504: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 16:24:19.143547: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 16:24:19.143607: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 16:24:19.143700: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 16:24:19.143738: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 16:24:19.143778: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 16:24:19.143934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:24:19.144595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:24:19.145151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 16:24:19.145220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 16:24:19.145240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 16:24:19.145261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 16:24:19.145464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:24:19.146067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:24:19.146598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6457547591057404\n","# location_distance_from_business_district - 0.5509184335601962\n","# location_easy_to_find - 0.7145334133518415\n","# service_wait_time - 0.6676777302857253\n","# service_waiters_attitude - 0.7963082763326976\n","# service_parking_convenience - 0.7325105585673265\n","# service_serving_speed - 0.7645046758724019\n","# price_level - 0.7787018641986059\n","# price_cost_effective - 0.7122496777077436\n","# price_discount - 0.6698681446597752\n","# environment_decoration - 0.7382947845195524\n","# environment_noise - 0.7492554206248554\n","# environment_space - 0.7583364368986971\n","# environment_cleaness - 0.7447277939135127\n","# dish_portion - 0.7242729200882627\n","# dish_taste - 0.7293204654006081\n","# dish_look - 0.5839004144543394\n","# dish_recommendation - 0.7386139838535195\n","# others_overall_experience - 0.5919061010115066\n","# others_willing_to_consume_again - 0.7141124385658666\n","# Eval loss 0.27366, f1 0.70529\n","# current result -0.7052884146486387, previous best result -0.7079688364247765\n","# Epoch 16  global step 42020 loss 0.28191 batch 790/3281 lr 0.0001 accuracy 94.91016 wps 18710.88 step time 0.41s\n","# Epoch 16  global step 42040 loss 0.28136 batch 810/3281 lr 0.0001 accuracy 94.89453 wps 18449.19 step time 0.38s\n","# Epoch 16  global step 42060 loss 0.29693 batch 830/3281 lr 0.0001 accuracy 94.60938 wps 19077.05 step time 0.42s\n","# Epoch 16  global step 42080 loss 0.27971 batch 850/3281 lr 0.0001 accuracy 94.94141 wps 18806.31 step time 0.36s\n","# Epoch 16  global step 42100 loss 0.28967 batch 870/3281 lr 0.0001 accuracy 94.69141 wps 19135.76 step time 0.46s\n","# Epoch 16  global step 42120 loss 0.25801 batch 890/3281 lr 0.0001 accuracy 95.35547 wps 18000.44 step time 0.34s\n","# Epoch 16  global step 42140 loss 0.28245 batch 910/3281 lr 0.0001 accuracy 94.92188 wps 18987.18 step time 0.36s\n","# Epoch 16  global step 42160 loss 0.28570 batch 930/3281 lr 0.0001 accuracy 94.81641 wps 19893.60 step time 0.40s\n","# Epoch 16  global step 42180 loss 0.27825 batch 950/3281 lr 0.0001 accuracy 95.01172 wps 19106.13 step time 0.35s\n","# Epoch 16  global step 42200 loss 0.26482 batch 970/3281 lr 0.0001 accuracy 95.28906 wps 17770.34 step time 0.32s\n","# Epoch 16  global step 42220 loss 0.28263 batch 990/3281 lr 0.0001 accuracy 94.74609 wps 18884.24 step time 0.41s\n","# Epoch 16  global step 42240 loss 0.27852 batch 1010/3281 lr 0.0001 accuracy 94.92578 wps 18766.70 step time 0.41s\n","# Epoch 16  global step 42260 loss 0.29340 batch 1030/3281 lr 0.0001 accuracy 94.59766 wps 18998.66 step time 0.42s\n","# Epoch 16  global step 42280 loss 0.28734 batch 1050/3281 lr 0.0001 accuracy 94.75781 wps 19463.34 step time 0.42s\n","# Epoch 16  global step 42300 loss 0.28432 batch 1070/3281 lr 0.0001 accuracy 95.02344 wps 19103.38 step time 0.40s\n","# Epoch 16  global step 42320 loss 0.28681 batch 1090/3281 lr 0.0001 accuracy 94.65625 wps 19540.32 step time 0.38s\n","# Epoch 16  global step 42340 loss 0.30343 batch 1110/3281 lr 0.0001 accuracy 94.46484 wps 20455.81 step time 0.47s\n","# Epoch 16  global step 42360 loss 0.28674 batch 1130/3281 lr 0.0001 accuracy 94.91797 wps 19593.47 step time 0.37s\n","# Epoch 16  global step 42380 loss 0.27216 batch 1150/3281 lr 0.0001 accuracy 95.05859 wps 19152.95 step time 0.37s\n","# Epoch 16  global step 42400 loss 0.29192 batch 1170/3281 lr 0.0001 accuracy 94.67969 wps 19331.19 step time 0.37s\n","# Epoch 16  global step 42420 loss 0.29514 batch 1190/3281 lr 0.0001 accuracy 94.56250 wps 19124.42 step time 0.46s\n","# Epoch 16  global step 42440 loss 0.29704 batch 1210/3281 lr 0.0001 accuracy 94.75391 wps 19571.46 step time 0.38s\n","# Epoch 16  global step 42460 loss 0.27836 batch 1230/3281 lr 0.0001 accuracy 94.98438 wps 17000.22 step time 0.40s\n","# Epoch 16  global step 42480 loss 0.28464 batch 1250/3281 lr 0.0001 accuracy 94.93359 wps 17452.33 step time 0.39s\n","# Epoch 16  global step 42500 loss 0.27562 batch 1270/3281 lr 0.0001 accuracy 95.01563 wps 15885.70 step time 0.42s\n","# Epoch 16  global step 42520 loss 0.30491 batch 1290/3281 lr 0.0001 accuracy 94.50391 wps 17039.47 step time 0.46s\n","# Epoch 16  global step 42540 loss 0.28149 batch 1310/3281 lr 0.0001 accuracy 94.87891 wps 17583.04 step time 0.39s\n","# Epoch 16  global step 42560 loss 0.29925 batch 1330/3281 lr 0.0001 accuracy 94.46484 wps 17797.85 step time 0.40s\n","# Epoch 16  global step 42580 loss 0.29554 batch 1350/3281 lr 0.0001 accuracy 94.67969 wps 18317.98 step time 0.46s\n","# Epoch 16  global step 42600 loss 0.28149 batch 1370/3281 lr 0.0001 accuracy 94.75000 wps 16574.78 step time 0.49s\n","# Epoch 16  global step 42620 loss 0.27625 batch 1390/3281 lr 0.0001 accuracy 94.95313 wps 16351.20 step time 0.46s\n","# Epoch 16  global step 42640 loss 0.29354 batch 1410/3281 lr 0.0001 accuracy 94.59375 wps 17291.28 step time 0.46s\n","# Epoch 16  global step 42660 loss 0.29029 batch 1430/3281 lr 0.0001 accuracy 94.91797 wps 17106.25 step time 0.36s\n","# Epoch 16  global step 42680 loss 0.28098 batch 1450/3281 lr 0.0001 accuracy 94.80469 wps 17066.80 step time 0.38s\n","# Epoch 16  global step 42700 loss 0.27383 batch 1470/3281 lr 0.0001 accuracy 94.99609 wps 16362.14 step time 0.36s\n","# Epoch 16  global step 42720 loss 0.25859 batch 1490/3281 lr 0.0001 accuracy 95.35547 wps 16800.89 step time 0.39s\n","# Epoch 16  global step 42740 loss 0.27802 batch 1510/3281 lr 0.0001 accuracy 94.91406 wps 17406.26 step time 0.38s\n","# Epoch 16  global step 42760 loss 0.26985 batch 1530/3281 lr 0.0001 accuracy 95.14062 wps 16493.15 step time 0.37s\n","# Epoch 16  global step 42780 loss 0.29529 batch 1550/3281 lr 0.0001 accuracy 94.57813 wps 17493.27 step time 0.49s\n","# Epoch 16  global step 42800 loss 0.28178 batch 1570/3281 lr 0.0001 accuracy 94.85547 wps 15984.54 step time 0.48s\n","# Epoch 16  global step 42820 loss 0.27493 batch 1590/3281 lr 0.0001 accuracy 94.85156 wps 16784.25 step time 0.37s\n","# Epoch 16  global step 42840 loss 0.27718 batch 1610/3281 lr 0.0001 accuracy 94.99609 wps 16825.05 step time 0.37s\n","# Epoch 16  global step 42860 loss 0.28931 batch 1630/3281 lr 0.0001 accuracy 94.73828 wps 16558.71 step time 0.42s\n","# Epoch 16  global step 42880 loss 0.27478 batch 1650/3281 lr 0.0001 accuracy 95.05469 wps 17868.35 step time 0.39s\n","# Epoch 16  global step 42900 loss 0.26381 batch 1670/3281 lr 0.0001 accuracy 95.08984 wps 16891.06 step time 0.37s\n","# Epoch 16  global step 42920 loss 0.27184 batch 1690/3281 lr 0.0001 accuracy 95.02344 wps 16708.02 step time 0.41s\n","# Epoch 16  global step 42940 loss 0.28685 batch 1710/3281 lr 0.0001 accuracy 94.80469 wps 18016.18 step time 0.40s\n","# Epoch 16  global step 42960 loss 0.28293 batch 1730/3281 lr 0.0001 accuracy 95.05859 wps 16589.03 step time 0.42s\n","# Epoch 16  global step 42980 loss 0.28273 batch 1750/3281 lr 0.0001 accuracy 95.05859 wps 17416.41 step time 0.38s\n","# Epoch 16  global step 43000 loss 0.27891 batch 1770/3281 lr 0.0001 accuracy 94.92969 wps 17658.13 step time 0.45s\n","# Epoch 16  global step 43020 loss 0.26387 batch 1790/3281 lr 0.0001 accuracy 95.20312 wps 16907.25 step time 0.39s\n","# Epoch 16  global step 43040 loss 0.27354 batch 1810/3281 lr 0.0001 accuracy 95.01953 wps 16653.89 step time 0.38s\n","# Epoch 16  global step 43060 loss 0.27649 batch 1830/3281 lr 0.0001 accuracy 95.13281 wps 16801.43 step time 0.46s\n","# Epoch 16  global step 43080 loss 0.29012 batch 1850/3281 lr 0.0001 accuracy 94.68359 wps 16860.93 step time 0.41s\n","# Epoch 16  global step 43100 loss 0.27748 batch 1870/3281 lr 0.0001 accuracy 95.01172 wps 16480.54 step time 0.43s\n","# Epoch 16  global step 43120 loss 0.28190 batch 1890/3281 lr 0.0001 accuracy 94.81250 wps 17229.82 step time 0.39s\n","# Epoch 16  global step 43140 loss 0.28367 batch 1910/3281 lr 0.0001 accuracy 94.82031 wps 17743.59 step time 0.45s\n","# Epoch 16  global step 43160 loss 0.29649 batch 1930/3281 lr 0.0001 accuracy 94.70703 wps 16758.44 step time 0.53s\n","# Epoch 16  global step 43180 loss 0.26832 batch 1950/3281 lr 0.0001 accuracy 95.12891 wps 16312.38 step time 0.36s\n","# Epoch 16  global step 43200 loss 0.30917 batch 1970/3281 lr 0.0001 accuracy 94.21875 wps 17728.24 step time 0.51s\n","# Epoch 16  global step 43220 loss 0.28863 batch 1990/3281 lr 0.0001 accuracy 94.76172 wps 17339.76 step time 0.44s\n","# Epoch 16  global step 43240 loss 0.26965 batch 2010/3281 lr 0.0001 accuracy 95.10937 wps 16605.61 step time 0.38s\n","# Epoch 16  global step 43260 loss 0.28669 batch 2030/3281 lr 0.0001 accuracy 94.70703 wps 16537.53 step time 0.49s\n","# Epoch 16  global step 43280 loss 0.27488 batch 2050/3281 lr 0.0001 accuracy 95.07031 wps 16565.41 step time 0.46s\n","# Epoch 16  global step 43300 loss 0.28451 batch 2070/3281 lr 0.0001 accuracy 94.93750 wps 17496.74 step time 0.39s\n","# Epoch 16  global step 43320 loss 0.26636 batch 2090/3281 lr 0.0001 accuracy 95.19531 wps 16473.82 step time 0.43s\n","# Epoch 16  global step 43340 loss 0.28271 batch 2110/3281 lr 0.0001 accuracy 94.75781 wps 17550.37 step time 0.40s\n","# Epoch 16  global step 43360 loss 0.26847 batch 2130/3281 lr 0.0001 accuracy 95.17188 wps 16572.06 step time 0.36s\n","# Epoch 16  global step 43380 loss 0.28919 batch 2150/3281 lr 0.0001 accuracy 94.78125 wps 16411.56 step time 0.51s\n","# Epoch 16  global step 43400 loss 0.29088 batch 2170/3281 lr 0.0001 accuracy 94.76562 wps 15545.26 step time 0.52s\n","# Epoch 16  global step 43420 loss 0.27381 batch 2190/3281 lr 0.0001 accuracy 94.91406 wps 16293.75 step time 0.41s\n","# Epoch 16  global step 43440 loss 0.27698 batch 2210/3281 lr 0.0001 accuracy 95.07031 wps 17059.61 step time 0.38s\n","# Epoch 16  global step 43460 loss 0.27816 batch 2230/3281 lr 0.0001 accuracy 94.93750 wps 17023.37 step time 0.43s\n","# Epoch 16  global step 43480 loss 0.27605 batch 2250/3281 lr 0.0001 accuracy 94.89453 wps 17291.07 step time 0.43s\n","# Epoch 16  global step 43500 loss 0.29090 batch 2270/3281 lr 0.0001 accuracy 94.88672 wps 17263.88 step time 0.40s\n","# Epoch 16  global step 43520 loss 0.28979 batch 2290/3281 lr 0.0001 accuracy 94.74219 wps 16849.70 step time 0.48s\n","# Epoch 16  global step 43540 loss 0.28754 batch 2310/3281 lr 0.0001 accuracy 94.73438 wps 16974.61 step time 0.42s\n","# Epoch 16  global step 43560 loss 0.27076 batch 2330/3281 lr 0.0001 accuracy 94.99609 wps 16923.19 step time 0.38s\n","# Epoch 16  global step 43580 loss 0.26332 batch 2350/3281 lr 0.0001 accuracy 95.24219 wps 17079.51 step time 0.39s\n","# Epoch 16  global step 43600 loss 0.28550 batch 2370/3281 lr 0.0001 accuracy 94.88281 wps 17499.03 step time 0.44s\n","# Epoch 16  global step 43620 loss 0.26635 batch 2390/3281 lr 0.0001 accuracy 95.13672 wps 16149.77 step time 0.44s\n","# Epoch 16  global step 43640 loss 0.30136 batch 2410/3281 lr 0.0001 accuracy 94.47266 wps 16757.42 step time 0.48s\n","# Epoch 16  global step 43660 loss 0.28158 batch 2430/3281 lr 0.0001 accuracy 94.82031 wps 17242.42 step time 0.43s\n","# Epoch 16  global step 43680 loss 0.29658 batch 2450/3281 lr 0.0001 accuracy 94.64453 wps 17452.32 step time 0.45s\n","# Epoch 16  global step 43700 loss 0.27609 batch 2470/3281 lr 0.0001 accuracy 94.89844 wps 16294.85 step time 0.45s\n","# Epoch 16  global step 43720 loss 0.28245 batch 2490/3281 lr 0.0001 accuracy 94.76172 wps 16554.95 step time 0.45s\n","# Epoch 16  global step 43740 loss 0.30000 batch 2510/3281 lr 0.0001 accuracy 94.56641 wps 16627.46 step time 0.60s\n","# Epoch 16  global step 43760 loss 0.29081 batch 2530/3281 lr 0.0001 accuracy 94.56641 wps 17031.50 step time 0.48s\n","# Epoch 16  global step 43780 loss 0.26964 batch 2550/3281 lr 0.0001 accuracy 94.96875 wps 17374.89 step time 0.42s\n","# Epoch 16  global step 43800 loss 0.29080 batch 2570/3281 lr 0.0001 accuracy 94.60156 wps 17558.27 step time 0.42s\n","# Epoch 16  global step 43820 loss 0.29589 batch 2590/3281 lr 0.0001 accuracy 94.51953 wps 16233.05 step time 0.58s\n","# Epoch 16  global step 43840 loss 0.27347 batch 2610/3281 lr 0.0001 accuracy 94.87891 wps 16895.01 step time 0.39s\n","# Epoch 16  global step 43860 loss 0.27535 batch 2630/3281 lr 0.0001 accuracy 94.93359 wps 16773.28 step time 0.42s\n","# Epoch 16  global step 43880 loss 0.27782 batch 2650/3281 lr 0.0001 accuracy 94.91016 wps 17128.36 step time 0.40s\n","# Epoch 16  global step 43900 loss 0.29719 batch 2670/3281 lr 0.0001 accuracy 94.58594 wps 16875.05 step time 0.49s\n","# Epoch 16  global step 43920 loss 0.27998 batch 2690/3281 lr 0.0001 accuracy 95.07031 wps 17368.11 step time 0.39s\n","# Epoch 16  global step 43940 loss 0.30605 batch 2710/3281 lr 0.0001 accuracy 94.49609 wps 16481.80 step time 0.44s\n","# Epoch 16  global step 43960 loss 0.29090 batch 2730/3281 lr 0.0001 accuracy 94.83203 wps 16843.74 step time 0.47s\n","# Epoch 16  global step 43980 loss 0.25679 batch 2750/3281 lr 0.0001 accuracy 95.34766 wps 16949.99 step time 0.36s\n","# Epoch 16  global step 44000 loss 0.27886 batch 2770/3281 lr 0.0001 accuracy 94.85938 wps 16366.47 step time 0.42s\n","# global step 44000, eval model at Fri May 22 16:39:24 2020\n","2020-05-22 16:39:27.163948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:39:27.164533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 16:39:27.164638: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 16:39:27.164705: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 16:39:27.164790: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 16:39:27.164850: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 16:39:27.164893: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 16:39:27.164938: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 16:39:27.164982: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 16:39:27.165179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:39:27.165889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:39:27.166490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 16:39:27.166577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 16:39:27.166597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 16:39:27.166611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 16:39:27.166796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:39:27.167388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:39:27.167951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6473249134236914\n","# location_distance_from_business_district - 0.5518368315908464\n","# location_easy_to_find - 0.7155354811379147\n","# service_wait_time - 0.6714628953634939\n","# service_waiters_attitude - 0.7949056442657838\n","# service_parking_convenience - 0.729504810231524\n","# service_serving_speed - 0.7650704720945606\n","# price_level - 0.7786741157840138\n","# price_cost_effective - 0.7108793565563478\n","# price_discount - 0.6715900247541234\n","# environment_decoration - 0.736841443620928\n","# environment_noise - 0.7490130348802632\n","# environment_space - 0.7590493701504238\n","# environment_cleaness - 0.7430436141765465\n","# dish_portion - 0.7216406896866554\n","# dish_taste - 0.7290537143724278\n","# dish_look - 0.5839429023605719\n","# dish_recommendation - 0.7386746057677259\n","# others_overall_experience - 0.5909297971195117\n","# others_willing_to_consume_again - 0.7140011926220671\n","# Eval loss 0.27622, f1 0.70515\n","# current result -0.705148745497971, previous best result -0.7079688364247765\n","# No loss decrease, restore previous best model and set learning rate to half of previous one\n","# Epoch 16  global step 36020 loss 0.24035 batch 2790/3281 lr 1e-05 accuracy 95.49609 wps 17319.30 step time 0.40s\n","# Epoch 16  global step 36040 loss 0.24860 batch 2810/3281 lr 1e-05 accuracy 95.50391 wps 16887.44 step time 0.54s\n","# Epoch 16  global step 36060 loss 0.24195 batch 2830/3281 lr 1e-05 accuracy 95.60156 wps 16336.65 step time 0.39s\n","# Epoch 16  global step 36080 loss 0.24060 batch 2850/3281 lr 1e-05 accuracy 95.62109 wps 16593.45 step time 0.39s\n","# Epoch 16  global step 36100 loss 0.23389 batch 2870/3281 lr 1e-05 accuracy 95.64844 wps 16545.55 step time 0.39s\n","# Epoch 16  global step 36120 loss 0.23193 batch 2890/3281 lr 1e-05 accuracy 95.66797 wps 17462.45 step time 0.39s\n","# Epoch 16  global step 36140 loss 0.23168 batch 2910/3281 lr 1e-05 accuracy 95.66016 wps 17147.35 step time 0.41s\n","# Epoch 16  global step 36160 loss 0.24486 batch 2930/3281 lr 1e-05 accuracy 95.62891 wps 15636.31 step time 0.53s\n","# Epoch 16  global step 36180 loss 0.24836 batch 2950/3281 lr 1e-05 accuracy 95.36328 wps 16744.61 step time 0.47s\n","# Epoch 16  global step 36200 loss 0.23509 batch 2970/3281 lr 1e-05 accuracy 95.76172 wps 16578.38 step time 0.44s\n","# Epoch 16  global step 36220 loss 0.24008 batch 2990/3281 lr 1e-05 accuracy 95.65234 wps 16081.91 step time 0.41s\n","# Epoch 16  global step 36240 loss 0.23659 batch 3010/3281 lr 1e-05 accuracy 95.64062 wps 16504.35 step time 0.42s\n","# Epoch 16  global step 36260 loss 0.25083 batch 3030/3281 lr 1e-05 accuracy 95.35156 wps 17253.39 step time 0.49s\n","# Epoch 16  global step 36280 loss 0.23823 batch 3050/3281 lr 1e-05 accuracy 95.74219 wps 17250.76 step time 0.41s\n","# Epoch 16  global step 36300 loss 0.25372 batch 3070/3281 lr 1e-05 accuracy 95.32422 wps 17579.36 step time 0.43s\n","# Epoch 16  global step 36320 loss 0.22968 batch 3090/3281 lr 1e-05 accuracy 96.01563 wps 16880.50 step time 0.41s\n","# Epoch 16  global step 36340 loss 0.23426 batch 3110/3281 lr 1e-05 accuracy 95.75000 wps 16664.21 step time 0.45s\n","# Epoch 16  global step 36360 loss 0.22623 batch 3130/3281 lr 1e-05 accuracy 95.89453 wps 16341.84 step time 0.36s\n","# Epoch 16  global step 36380 loss 0.23566 batch 3150/3281 lr 1e-05 accuracy 95.63281 wps 16678.90 step time 0.38s\n","# Epoch 16  global step 36400 loss 0.24469 batch 3170/3281 lr 1e-05 accuracy 95.54687 wps 17333.40 step time 0.41s\n","# Epoch 16  global step 36420 loss 0.23416 batch 3190/3281 lr 1e-05 accuracy 95.72266 wps 16673.42 step time 0.43s\n","# Epoch 16  global step 36440 loss 0.22668 batch 3210/3281 lr 1e-05 accuracy 95.96484 wps 17074.51 step time 0.37s\n","# Epoch 16  global step 36460 loss 0.22992 batch 3230/3281 lr 1e-05 accuracy 95.76563 wps 16037.69 step time 0.45s\n","# Epoch 16  global step 36480 loss 0.24515 batch 3250/3281 lr 1e-05 accuracy 95.62891 wps 17592.85 step time 0.42s\n","# Epoch 16  global step 36500 loss 0.23720 batch 3270/3281 lr 1e-05 accuracy 95.58984 wps 15772.24 step time 0.45s\n","# Finsh epoch 16, global step 36512\n","# Epoch 17  global step 36520 loss 0.10008 batch 8/3281 lr 1e-05 accuracy 38.11328 wps 19010.67 step time 0.20s\n","# Epoch 17  global step 36540 loss 0.24464 batch 28/3281 lr 1e-05 accuracy 95.49609 wps 19353.03 step time 0.41s\n","# Epoch 17  global step 36560 loss 0.23894 batch 48/3281 lr 1e-05 accuracy 95.72656 wps 19006.07 step time 0.36s\n","# Epoch 17  global step 36580 loss 0.23954 batch 68/3281 lr 1e-05 accuracy 95.63281 wps 19203.26 step time 0.37s\n","# Epoch 17  global step 36600 loss 0.25150 batch 88/3281 lr 1e-05 accuracy 95.28516 wps 20017.10 step time 0.39s\n","# Epoch 17  global step 36620 loss 0.24502 batch 108/3281 lr 1e-05 accuracy 95.67187 wps 19547.26 step time 0.38s\n","# Epoch 17  global step 36640 loss 0.22091 batch 128/3281 lr 1e-05 accuracy 96.02734 wps 18771.46 step time 0.35s\n","# Epoch 17  global step 36660 loss 0.21829 batch 148/3281 lr 1e-05 accuracy 95.93359 wps 18765.17 step time 0.35s\n","# Epoch 17  global step 36680 loss 0.22946 batch 168/3281 lr 1e-05 accuracy 95.73828 wps 18603.72 step time 0.34s\n","# Epoch 17  global step 36700 loss 0.24102 batch 188/3281 lr 1e-05 accuracy 95.55078 wps 18724.93 step time 0.40s\n","# Epoch 17  global step 36720 loss 0.25254 batch 208/3281 lr 1e-05 accuracy 95.46094 wps 19131.15 step time 0.36s\n","# Epoch 17  global step 36740 loss 0.25241 batch 228/3281 lr 1e-05 accuracy 95.18750 wps 19770.58 step time 0.39s\n","# Epoch 17  global step 36760 loss 0.23788 batch 248/3281 lr 1e-05 accuracy 95.59766 wps 18749.55 step time 0.41s\n","# Epoch 17  global step 36780 loss 0.24219 batch 268/3281 lr 1e-05 accuracy 95.64453 wps 19164.06 step time 0.37s\n","# Epoch 17  global step 36800 loss 0.24194 batch 288/3281 lr 1e-05 accuracy 95.43750 wps 19555.68 step time 0.42s\n","# Epoch 17  global step 36820 loss 0.22887 batch 308/3281 lr 1e-05 accuracy 95.86719 wps 18517.09 step time 0.34s\n","# Epoch 17  global step 36840 loss 0.22117 batch 328/3281 lr 1e-05 accuracy 95.96484 wps 18582.01 step time 0.34s\n","# Epoch 17  global step 36860 loss 0.23691 batch 348/3281 lr 1e-05 accuracy 95.66797 wps 18494.51 step time 0.34s\n","# Epoch 17  global step 36880 loss 0.23034 batch 368/3281 lr 1e-05 accuracy 95.82813 wps 19222.75 step time 0.37s\n","# Epoch 17  global step 36900 loss 0.24472 batch 388/3281 lr 1e-05 accuracy 95.45703 wps 19398.74 step time 0.37s\n","# Epoch 17  global step 36920 loss 0.21218 batch 408/3281 lr 1e-05 accuracy 96.03906 wps 17727.68 step time 0.32s\n","# Epoch 17  global step 36940 loss 0.24179 batch 428/3281 lr 1e-05 accuracy 95.64062 wps 18789.34 step time 0.40s\n","# Epoch 17  global step 36960 loss 0.22227 batch 448/3281 lr 1e-05 accuracy 95.91406 wps 18483.83 step time 0.34s\n","# Epoch 17  global step 36980 loss 0.24468 batch 468/3281 lr 1e-05 accuracy 95.57813 wps 19549.99 step time 0.38s\n","# Epoch 17  global step 37000 loss 0.21616 batch 488/3281 lr 1e-05 accuracy 96.07422 wps 17706.11 step time 0.31s\n","# Epoch 17  global step 37020 loss 0.25756 batch 508/3281 lr 1e-05 accuracy 95.27344 wps 19606.49 step time 0.45s\n","# Epoch 17  global step 37040 loss 0.24215 batch 528/3281 lr 1e-05 accuracy 95.48828 wps 19124.24 step time 0.42s\n","# Epoch 17  global step 37060 loss 0.23249 batch 548/3281 lr 1e-05 accuracy 95.74609 wps 19279.07 step time 0.37s\n","# Epoch 17  global step 37080 loss 0.22841 batch 568/3281 lr 1e-05 accuracy 95.87109 wps 18313.65 step time 0.34s\n","# Epoch 17  global step 37100 loss 0.23999 batch 588/3281 lr 1e-05 accuracy 95.58594 wps 19441.54 step time 0.37s\n","# Epoch 17  global step 37120 loss 0.24483 batch 608/3281 lr 1e-05 accuracy 95.58594 wps 18673.29 step time 0.44s\n","# Epoch 17  global step 37140 loss 0.23396 batch 628/3281 lr 1e-05 accuracy 95.69922 wps 18709.87 step time 0.40s\n","# Epoch 17  global step 37160 loss 0.23253 batch 648/3281 lr 1e-05 accuracy 95.66797 wps 18633.39 step time 0.35s\n","# Epoch 17  global step 37180 loss 0.22539 batch 668/3281 lr 1e-05 accuracy 95.76172 wps 19276.24 step time 0.35s\n","# Epoch 17  global step 37200 loss 0.23113 batch 688/3281 lr 1e-05 accuracy 95.85938 wps 18491.34 step time 0.34s\n","# Epoch 17  global step 37220 loss 0.24856 batch 708/3281 lr 1e-05 accuracy 95.57422 wps 17737.10 step time 0.42s\n","# Epoch 17  global step 37240 loss 0.23219 batch 728/3281 lr 1e-05 accuracy 95.83203 wps 17484.46 step time 0.44s\n","# Epoch 17  global step 37260 loss 0.22987 batch 748/3281 lr 1e-05 accuracy 95.77344 wps 15979.64 step time 0.42s\n","# Epoch 17  global step 37280 loss 0.24240 batch 768/3281 lr 1e-05 accuracy 95.50000 wps 16231.45 step time 0.54s\n","# Epoch 17  global step 37300 loss 0.22863 batch 788/3281 lr 1e-05 accuracy 95.76562 wps 17638.25 step time 0.41s\n","# Epoch 17  global step 37320 loss 0.25075 batch 808/3281 lr 1e-05 accuracy 95.46094 wps 16237.11 step time 0.50s\n","# Epoch 17  global step 37340 loss 0.23563 batch 828/3281 lr 1e-05 accuracy 95.67578 wps 16189.02 step time 0.45s\n","# Epoch 17  global step 37360 loss 0.23197 batch 848/3281 lr 1e-05 accuracy 95.49609 wps 18236.25 step time 0.41s\n","# Epoch 17  global step 37380 loss 0.23321 batch 868/3281 lr 1e-05 accuracy 95.75000 wps 17488.67 step time 0.46s\n","# Epoch 17  global step 37400 loss 0.22814 batch 888/3281 lr 1e-05 accuracy 95.97266 wps 17146.36 step time 0.35s\n","# Epoch 17  global step 37420 loss 0.24847 batch 908/3281 lr 1e-05 accuracy 95.40234 wps 17473.24 step time 0.49s\n","# Epoch 17  global step 37440 loss 0.22126 batch 928/3281 lr 1e-05 accuracy 95.93750 wps 17043.29 step time 0.36s\n","# Epoch 17  global step 37460 loss 0.23135 batch 948/3281 lr 1e-05 accuracy 95.71094 wps 17338.79 step time 0.41s\n","# Epoch 17  global step 37480 loss 0.24792 batch 968/3281 lr 1e-05 accuracy 95.42187 wps 16559.78 step time 0.53s\n","# Epoch 17  global step 37500 loss 0.23757 batch 988/3281 lr 1e-05 accuracy 95.71484 wps 17384.26 step time 0.41s\n","# Epoch 17  global step 37520 loss 0.22403 batch 1008/3281 lr 1e-05 accuracy 95.98828 wps 16689.88 step time 0.45s\n","# Epoch 17  global step 37540 loss 0.22962 batch 1028/3281 lr 1e-05 accuracy 95.87500 wps 17102.48 step time 0.42s\n","# Epoch 17  global step 37560 loss 0.23584 batch 1048/3281 lr 1e-05 accuracy 95.61328 wps 17260.24 step time 0.44s\n","# Epoch 17  global step 37580 loss 0.24735 batch 1068/3281 lr 1e-05 accuracy 95.47266 wps 16677.10 step time 0.45s\n","# Epoch 17  global step 37600 loss 0.23872 batch 1088/3281 lr 1e-05 accuracy 95.62109 wps 17564.00 step time 0.44s\n","# Epoch 17  global step 37620 loss 0.22726 batch 1108/3281 lr 1e-05 accuracy 95.88281 wps 18137.14 step time 0.43s\n","# Epoch 17  global step 37640 loss 0.24937 batch 1128/3281 lr 1e-05 accuracy 95.47656 wps 17263.35 step time 0.53s\n","# Epoch 17  global step 37660 loss 0.25384 batch 1148/3281 lr 1e-05 accuracy 95.34766 wps 17521.93 step time 0.43s\n","# Epoch 17  global step 37680 loss 0.23783 batch 1168/3281 lr 1e-05 accuracy 95.62500 wps 16581.56 step time 0.43s\n","# Epoch 17  global step 37700 loss 0.21669 batch 1188/3281 lr 1e-05 accuracy 95.96484 wps 16785.97 step time 0.35s\n","# Epoch 17  global step 37720 loss 0.22814 batch 1208/3281 lr 1e-05 accuracy 95.89062 wps 16608.13 step time 0.42s\n","# Epoch 17  global step 37740 loss 0.25311 batch 1228/3281 lr 1e-05 accuracy 95.39062 wps 16875.43 step time 0.52s\n","# Epoch 17  global step 37760 loss 0.23961 batch 1248/3281 lr 1e-05 accuracy 95.70312 wps 16341.79 step time 0.47s\n","# Epoch 17  global step 37780 loss 0.25039 batch 1268/3281 lr 1e-05 accuracy 95.39844 wps 16726.84 step time 0.55s\n","# Epoch 17  global step 37800 loss 0.22764 batch 1288/3281 lr 1e-05 accuracy 95.89844 wps 15804.59 step time 0.43s\n","# Epoch 17  global step 37820 loss 0.24050 batch 1308/3281 lr 1e-05 accuracy 95.60937 wps 17324.77 step time 0.40s\n","# Epoch 17  global step 37840 loss 0.24086 batch 1328/3281 lr 1e-05 accuracy 95.58984 wps 17168.43 step time 0.40s\n","# Epoch 17  global step 37860 loss 0.23824 batch 1348/3281 lr 1e-05 accuracy 95.62891 wps 16904.35 step time 0.50s\n","# Epoch 17  global step 37880 loss 0.22326 batch 1368/3281 lr 1e-05 accuracy 95.95313 wps 17052.95 step time 0.37s\n","# Epoch 17  global step 37900 loss 0.23432 batch 1388/3281 lr 1e-05 accuracy 95.79297 wps 16657.97 step time 0.40s\n","# Epoch 17  global step 37920 loss 0.22616 batch 1408/3281 lr 1e-05 accuracy 95.92969 wps 16765.56 step time 0.37s\n","# Epoch 17  global step 37940 loss 0.23923 batch 1428/3281 lr 1e-05 accuracy 95.57031 wps 16620.04 step time 0.51s\n","# Epoch 17  global step 37960 loss 0.22608 batch 1448/3281 lr 1e-05 accuracy 95.89062 wps 17185.64 step time 0.37s\n","# Epoch 17  global step 37980 loss 0.23051 batch 1468/3281 lr 1e-05 accuracy 95.82422 wps 16923.19 step time 0.40s\n","# Epoch 17  global step 38000 loss 0.23150 batch 1488/3281 lr 1e-05 accuracy 95.80078 wps 16822.43 step time 0.43s\n","# global step 38000, eval model at Fri May 22 16:54:18 2020\n","2020-05-22 16:54:20.305245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:54:20.306061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 16:54:20.306198: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 16:54:20.306269: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 16:54:20.306320: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 16:54:20.306361: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 16:54:20.306400: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 16:54:20.306459: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 16:54:20.306502: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 16:54:20.306731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:54:20.307364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:54:20.307973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 16:54:20.308040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 16:54:20.308064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 16:54:20.308093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 16:54:20.308220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:54:20.308830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 16:54:20.309308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6477838085566648\n","# location_distance_from_business_district - 0.5612947924395619\n","# location_easy_to_find - 0.716765704151904\n","# service_wait_time - 0.6675326424780743\n","# service_waiters_attitude - 0.7972688566773931\n","# service_parking_convenience - 0.7397533437240186\n","# service_serving_speed - 0.760629377438339\n","# price_level - 0.7800900147715482\n","# price_cost_effective - 0.7121792347303706\n","# price_discount - 0.6758428409051178\n","# environment_decoration - 0.736353501639808\n","# environment_noise - 0.753708071433017\n","# environment_space - 0.7586299003009662\n","# environment_cleaness - 0.7505715375781368\n","# dish_portion - 0.7256427138763742\n","# dish_taste - 0.7325326409449826\n","# dish_look - 0.5823877845923964\n","# dish_recommendation - 0.7431820702080656\n","# others_overall_experience - 0.5889418774112379\n","# others_willing_to_consume_again - 0.7127297929278891\n","# Eval loss 0.26877, f1 0.70719\n","# current result -0.7071910253392935, previous best result -0.7079688364247765\n","# Epoch 17  global step 38020 loss 0.23615 batch 1508/3281 lr 1e-05 accuracy 95.73047 wps 16878.92 step time 0.41s\n","# Epoch 17  global step 38040 loss 0.23572 batch 1528/3281 lr 1e-05 accuracy 95.53906 wps 16310.34 step time 0.45s\n","# Epoch 17  global step 38060 loss 0.23775 batch 1548/3281 lr 1e-05 accuracy 95.67188 wps 17137.87 step time 0.41s\n","# Epoch 17  global step 38080 loss 0.23149 batch 1568/3281 lr 1e-05 accuracy 95.78516 wps 16641.08 step time 0.43s\n","# Epoch 17  global step 38100 loss 0.22184 batch 1588/3281 lr 1e-05 accuracy 95.85156 wps 16486.39 step time 0.44s\n","# Epoch 17  global step 38120 loss 0.22227 batch 1608/3281 lr 1e-05 accuracy 95.91406 wps 16507.72 step time 0.38s\n","# Epoch 17  global step 38140 loss 0.21636 batch 1628/3281 lr 1e-05 accuracy 95.99219 wps 17029.92 step time 0.39s\n","# Epoch 17  global step 38160 loss 0.23506 batch 1648/3281 lr 1e-05 accuracy 95.50781 wps 16410.38 step time 0.45s\n","# Epoch 17  global step 38180 loss 0.22933 batch 1668/3281 lr 1e-05 accuracy 95.74219 wps 17471.86 step time 0.38s\n","# Epoch 17  global step 38200 loss 0.21386 batch 1688/3281 lr 1e-05 accuracy 96.17188 wps 16614.98 step time 0.38s\n","# Epoch 17  global step 38220 loss 0.22865 batch 1708/3281 lr 1e-05 accuracy 95.87109 wps 16875.26 step time 0.44s\n","# Epoch 17  global step 38240 loss 0.22553 batch 1728/3281 lr 1e-05 accuracy 95.84766 wps 16987.44 step time 0.39s\n","# Epoch 17  global step 38260 loss 0.23377 batch 1748/3281 lr 1e-05 accuracy 95.77344 wps 17289.89 step time 0.40s\n","# Epoch 17  global step 38280 loss 0.23237 batch 1768/3281 lr 1e-05 accuracy 95.81250 wps 16416.44 step time 0.52s\n","# Epoch 17  global step 38300 loss 0.23347 batch 1788/3281 lr 1e-05 accuracy 95.68359 wps 17277.50 step time 0.48s\n","# Epoch 17  global step 38320 loss 0.23820 batch 1808/3281 lr 1e-05 accuracy 95.68359 wps 17220.70 step time 0.50s\n","# Epoch 17  global step 38340 loss 0.21260 batch 1828/3281 lr 1e-05 accuracy 96.17969 wps 16226.86 step time 0.38s\n","# Epoch 17  global step 38360 loss 0.24286 batch 1848/3281 lr 1e-05 accuracy 95.66406 wps 17093.67 step time 0.40s\n","# Epoch 17  global step 38380 loss 0.22035 batch 1868/3281 lr 1e-05 accuracy 95.94141 wps 16497.36 step time 0.42s\n","# Epoch 17  global step 38400 loss 0.23719 batch 1888/3281 lr 1e-05 accuracy 95.76172 wps 17122.54 step time 0.43s\n","# Epoch 17  global step 38420 loss 0.23276 batch 1908/3281 lr 1e-05 accuracy 95.80078 wps 17850.75 step time 0.39s\n","# Epoch 17  global step 38440 loss 0.23271 batch 1928/3281 lr 1e-05 accuracy 95.70312 wps 16732.80 step time 0.48s\n","# Epoch 17  global step 38460 loss 0.23455 batch 1948/3281 lr 1e-05 accuracy 95.67969 wps 16793.84 step time 0.40s\n","# Epoch 17  global step 38480 loss 0.24318 batch 1968/3281 lr 1e-05 accuracy 95.56250 wps 17444.02 step time 0.40s\n","# Epoch 17  global step 38500 loss 0.23244 batch 1988/3281 lr 1e-05 accuracy 95.73047 wps 17305.87 step time 0.41s\n","# Epoch 17  global step 38520 loss 0.23405 batch 2008/3281 lr 1e-05 accuracy 95.74219 wps 17034.80 step time 0.47s\n","# Epoch 17  global step 38540 loss 0.23664 batch 2028/3281 lr 1e-05 accuracy 95.61719 wps 17011.30 step time 0.43s\n","# Epoch 17  global step 38560 loss 0.22492 batch 2048/3281 lr 1e-05 accuracy 95.92188 wps 16416.22 step time 0.36s\n","# Epoch 17  global step 38580 loss 0.20904 batch 2068/3281 lr 1e-05 accuracy 96.15234 wps 16209.27 step time 0.32s\n","# Epoch 17  global step 38600 loss 0.23590 batch 2088/3281 lr 1e-05 accuracy 95.66797 wps 17075.79 step time 0.43s\n","# Epoch 17  global step 38620 loss 0.23484 batch 2108/3281 lr 1e-05 accuracy 95.56641 wps 16511.00 step time 0.50s\n","# Epoch 17  global step 38640 loss 0.22317 batch 2128/3281 lr 1e-05 accuracy 95.88672 wps 15986.03 step time 0.43s\n","# Epoch 17  global step 38660 loss 0.23457 batch 2148/3281 lr 1e-05 accuracy 95.70312 wps 16552.49 step time 0.47s\n","# Epoch 17  global step 38680 loss 0.24879 batch 2168/3281 lr 1e-05 accuracy 95.24609 wps 16386.03 step time 0.57s\n","# Epoch 17  global step 38700 loss 0.21878 batch 2188/3281 lr 1e-05 accuracy 95.99609 wps 17196.16 step time 0.39s\n","# Epoch 17  global step 38720 loss 0.22516 batch 2208/3281 lr 1e-05 accuracy 96.07812 wps 16535.70 step time 0.38s\n","# Epoch 17  global step 38740 loss 0.22448 batch 2228/3281 lr 1e-05 accuracy 95.94922 wps 16647.54 step time 0.40s\n","# Epoch 17  global step 38760 loss 0.22311 batch 2248/3281 lr 1e-05 accuracy 95.95313 wps 16950.78 step time 0.37s\n","# Epoch 17  global step 38780 loss 0.23026 batch 2268/3281 lr 1e-05 accuracy 95.72656 wps 17534.49 step time 0.43s\n","# Epoch 17  global step 38800 loss 0.23958 batch 2288/3281 lr 1e-05 accuracy 95.68750 wps 16728.29 step time 0.43s\n","# Epoch 17  global step 38820 loss 0.22066 batch 2308/3281 lr 1e-05 accuracy 95.83203 wps 16740.38 step time 0.40s\n","# Epoch 17  global step 38840 loss 0.23003 batch 2328/3281 lr 1e-05 accuracy 95.76172 wps 16886.50 step time 0.40s\n","# Epoch 17  global step 38860 loss 0.23555 batch 2348/3281 lr 1e-05 accuracy 95.67188 wps 17364.94 step time 0.42s\n","# Epoch 17  global step 38880 loss 0.22643 batch 2368/3281 lr 1e-05 accuracy 95.91406 wps 17561.22 step time 0.40s\n","# Epoch 17  global step 38900 loss 0.21486 batch 2388/3281 lr 1e-05 accuracy 95.98047 wps 16513.16 step time 0.41s\n","# Epoch 17  global step 38920 loss 0.24844 batch 2408/3281 lr 1e-05 accuracy 95.45312 wps 17604.40 step time 0.47s\n","# Epoch 17  global step 38940 loss 0.23886 batch 2428/3281 lr 1e-05 accuracy 95.68359 wps 17006.29 step time 0.47s\n","# Epoch 17  global step 38960 loss 0.24104 batch 2448/3281 lr 1e-05 accuracy 95.65625 wps 17169.81 step time 0.41s\n","# Epoch 17  global step 38980 loss 0.22541 batch 2468/3281 lr 1e-05 accuracy 95.87500 wps 16888.31 step time 0.39s\n","# Epoch 17  global step 39000 loss 0.25488 batch 2488/3281 lr 1e-05 accuracy 95.33984 wps 16869.93 step time 0.43s\n","# Epoch 17  global step 39020 loss 0.21849 batch 2508/3281 lr 1e-05 accuracy 96.10156 wps 17126.77 step time 0.37s\n","# Epoch 17  global step 39040 loss 0.24102 batch 2528/3281 lr 1e-05 accuracy 95.56250 wps 17603.44 step time 0.42s\n","# Epoch 17  global step 39060 loss 0.23539 batch 2548/3281 lr 1e-05 accuracy 95.69922 wps 16514.10 step time 0.52s\n","# Epoch 17  global step 39080 loss 0.24443 batch 2568/3281 lr 1e-05 accuracy 95.50391 wps 16274.90 step time 0.59s\n","# Epoch 17  global step 39100 loss 0.22191 batch 2588/3281 lr 1e-05 accuracy 95.94141 wps 16836.43 step time 0.39s\n","# Epoch 17  global step 39120 loss 0.24169 batch 2608/3281 lr 1e-05 accuracy 95.61719 wps 17998.86 step time 0.40s\n","# Epoch 17  global step 39140 loss 0.23271 batch 2628/3281 lr 1e-05 accuracy 95.90234 wps 17301.99 step time 0.39s\n","# Epoch 17  global step 39160 loss 0.22242 batch 2648/3281 lr 1e-05 accuracy 95.98828 wps 17219.17 step time 0.38s\n","# Epoch 17  global step 39180 loss 0.21623 batch 2668/3281 lr 1e-05 accuracy 96.07422 wps 15993.13 step time 0.40s\n","# Epoch 17  global step 39200 loss 0.22813 batch 2688/3281 lr 1e-05 accuracy 95.92578 wps 17054.93 step time 0.37s\n","# Epoch 17  global step 39220 loss 0.23397 batch 2708/3281 lr 1e-05 accuracy 95.85156 wps 16415.62 step time 0.44s\n","# Epoch 17  global step 39240 loss 0.23277 batch 2728/3281 lr 1e-05 accuracy 95.74219 wps 16746.65 step time 0.41s\n","# Epoch 17  global step 39260 loss 0.23432 batch 2748/3281 lr 1e-05 accuracy 95.77344 wps 16050.15 step time 0.59s\n","# Epoch 17  global step 39280 loss 0.22271 batch 2768/3281 lr 1e-05 accuracy 95.99219 wps 16300.65 step time 0.49s\n","# Epoch 17  global step 39300 loss 0.21810 batch 2788/3281 lr 1e-05 accuracy 96.03516 wps 16191.41 step time 0.42s\n","# Epoch 17  global step 39320 loss 0.23456 batch 2808/3281 lr 1e-05 accuracy 95.75391 wps 17130.02 step time 0.37s\n","# Epoch 17  global step 39340 loss 0.25822 batch 2828/3281 lr 1e-05 accuracy 95.34766 wps 17564.88 step time 0.52s\n","# Epoch 17  global step 39360 loss 0.21329 batch 2848/3281 lr 1e-05 accuracy 96.03906 wps 16223.69 step time 0.40s\n","# Epoch 17  global step 39380 loss 0.21751 batch 2868/3281 lr 1e-05 accuracy 96.08594 wps 16418.09 step time 0.37s\n","# Epoch 17  global step 39400 loss 0.23332 batch 2888/3281 lr 1e-05 accuracy 95.77734 wps 16692.35 step time 0.40s\n","# Epoch 17  global step 39420 loss 0.22512 batch 2908/3281 lr 1e-05 accuracy 95.80469 wps 16798.87 step time 0.45s\n","# Epoch 17  global step 39440 loss 0.21865 batch 2928/3281 lr 1e-05 accuracy 95.98047 wps 16722.47 step time 0.38s\n","# Epoch 17  global step 39460 loss 0.23449 batch 2948/3281 lr 1e-05 accuracy 95.89844 wps 16943.15 step time 0.44s\n","# Epoch 17  global step 39480 loss 0.23495 batch 2968/3281 lr 1e-05 accuracy 95.66016 wps 17320.32 step time 0.44s\n","# Epoch 17  global step 39500 loss 0.22162 batch 2988/3281 lr 1e-05 accuracy 95.96875 wps 17809.69 step time 0.39s\n","# Epoch 17  global step 39520 loss 0.21867 batch 3008/3281 lr 1e-05 accuracy 95.99609 wps 17109.72 step time 0.40s\n","# Epoch 17  global step 39540 loss 0.23902 batch 3028/3281 lr 1e-05 accuracy 95.71094 wps 16555.02 step time 0.46s\n","# Epoch 17  global step 39560 loss 0.23810 batch 3048/3281 lr 1e-05 accuracy 95.64453 wps 17131.37 step time 0.45s\n","# Epoch 17  global step 39580 loss 0.21200 batch 3068/3281 lr 1e-05 accuracy 95.86328 wps 16356.42 step time 0.38s\n","# Epoch 17  global step 39600 loss 0.21912 batch 3088/3281 lr 1e-05 accuracy 95.97266 wps 17126.71 step time 0.37s\n","# Epoch 17  global step 39620 loss 0.22892 batch 3108/3281 lr 1e-05 accuracy 95.76953 wps 17363.06 step time 0.43s\n","# Epoch 17  global step 39640 loss 0.24434 batch 3128/3281 lr 1e-05 accuracy 95.39062 wps 17055.09 step time 0.49s\n","# Epoch 17  global step 39660 loss 0.22526 batch 3148/3281 lr 1e-05 accuracy 95.93359 wps 17316.29 step time 0.40s\n","# Epoch 17  global step 39680 loss 0.22063 batch 3168/3281 lr 1e-05 accuracy 95.94531 wps 16981.67 step time 0.36s\n","# Epoch 17  global step 39700 loss 0.24108 batch 3188/3281 lr 1e-05 accuracy 95.54688 wps 17123.22 step time 0.48s\n","# Epoch 17  global step 39720 loss 0.22823 batch 3208/3281 lr 1e-05 accuracy 95.85937 wps 16834.47 step time 0.45s\n","# Epoch 17  global step 39740 loss 0.24988 batch 3228/3281 lr 1e-05 accuracy 95.40625 wps 16377.57 step time 0.59s\n","# Epoch 17  global step 39760 loss 0.23985 batch 3248/3281 lr 1e-05 accuracy 95.57813 wps 16224.84 step time 0.44s\n","# Epoch 17  global step 39780 loss 0.22932 batch 3268/3281 lr 1e-05 accuracy 95.75781 wps 17264.88 step time 0.40s\n","# Finsh epoch 17, global step 39794\n","# Epoch 18  global step 39800 loss 0.06998 batch 6/3281 lr 1e-05 accuracy 28.67969 wps 17975.50 step time 0.15s\n","# Epoch 18  global step 39820 loss 0.21648 batch 26/3281 lr 1e-05 accuracy 95.93359 wps 19056.07 step time 0.35s\n","# Epoch 18  global step 39840 loss 0.21599 batch 46/3281 lr 1e-05 accuracy 96.05078 wps 18187.94 step time 0.33s\n","# Epoch 18  global step 39860 loss 0.23582 batch 66/3281 lr 1e-05 accuracy 95.73438 wps 19087.94 step time 0.41s\n","# Epoch 18  global step 39880 loss 0.22486 batch 86/3281 lr 1e-05 accuracy 95.89453 wps 19885.63 step time 0.38s\n","# Epoch 18  global step 39900 loss 0.23194 batch 106/3281 lr 1e-05 accuracy 95.71484 wps 19956.46 step time 0.40s\n","# Epoch 18  global step 39920 loss 0.22954 batch 126/3281 lr 1e-05 accuracy 95.85938 wps 19627.33 step time 0.37s\n","# Epoch 18  global step 39940 loss 0.22798 batch 146/3281 lr 1e-05 accuracy 95.80859 wps 18622.15 step time 0.44s\n","# Epoch 18  global step 39960 loss 0.20530 batch 166/3281 lr 1e-05 accuracy 96.21094 wps 16931.52 step time 0.30s\n","# Epoch 18  global step 39980 loss 0.23547 batch 186/3281 lr 1e-05 accuracy 95.75781 wps 19730.15 step time 0.39s\n","# Epoch 18  global step 40000 loss 0.22044 batch 206/3281 lr 1e-05 accuracy 95.94141 wps 19292.89 step time 0.36s\n","# global step 40000, eval model at Fri May 22 17:09:31 2020\n","2020-05-22 17:09:33.863917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:09:33.864511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 17:09:33.864599: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 17:09:33.864656: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 17:09:33.864733: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 17:09:33.864773: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 17:09:33.864814: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 17:09:33.864852: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 17:09:33.864890: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 17:09:33.865052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:09:33.865601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:09:33.866075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 17:09:33.866182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 17:09:33.866218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 17:09:33.866232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 17:09:33.866426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:09:33.866985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:09:33.867595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6523796804190785\n","# location_distance_from_business_district - 0.5566279213432188\n","# location_easy_to_find - 0.7136348546208747\n","# service_wait_time - 0.6697684322660674\n","# service_waiters_attitude - 0.7969898424851577\n","# service_parking_convenience - 0.7427632750278573\n","# service_serving_speed - 0.764337489243057\n","# price_level - 0.7803074504869827\n","# price_cost_effective - 0.710673897410802\n","# price_discount - 0.671766884158607\n","# environment_decoration - 0.7367511561389153\n","# environment_noise - 0.7529247200281682\n","# environment_space - 0.7600047960083481\n","# environment_cleaness - 0.7478834718585731\n","# dish_portion - 0.724255382281208\n","# dish_taste - 0.7298402083728177\n","# dish_look - 0.5841858361709339\n","# dish_recommendation - 0.7425645620057812\n","# others_overall_experience - 0.5908914806088184\n","# others_willing_to_consume_again - 0.7127393257407463\n","# Eval loss 0.27088, f1 0.70706\n","# current result -0.7070645333338005, previous best result -0.7079688364247765\n","# Epoch 18  global step 40020 loss 0.20959 batch 226/3281 lr 1e-05 accuracy 96.10156 wps 18107.95 step time 0.37s\n","# Epoch 18  global step 40040 loss 0.22476 batch 246/3281 lr 1e-05 accuracy 95.89063 wps 19701.60 step time 0.38s\n","# Epoch 18  global step 40060 loss 0.23036 batch 266/3281 lr 1e-05 accuracy 95.79297 wps 18481.30 step time 0.39s\n","# Epoch 18  global step 40080 loss 0.24127 batch 286/3281 lr 1e-05 accuracy 95.58594 wps 17112.63 step time 0.50s\n","# Epoch 18  global step 40100 loss 0.22777 batch 306/3281 lr 1e-05 accuracy 95.76172 wps 17282.04 step time 0.41s\n","# Epoch 18  global step 40120 loss 0.24177 batch 326/3281 lr 1e-05 accuracy 95.82812 wps 17390.15 step time 0.51s\n","# Epoch 18  global step 40140 loss 0.24170 batch 346/3281 lr 1e-05 accuracy 95.66016 wps 16958.60 step time 0.48s\n","# Epoch 18  global step 40160 loss 0.21923 batch 366/3281 lr 1e-05 accuracy 96.00781 wps 15801.57 step time 0.49s\n","# Epoch 18  global step 40180 loss 0.21066 batch 386/3281 lr 1e-05 accuracy 96.14844 wps 16367.48 step time 0.38s\n","# Epoch 18  global step 40200 loss 0.23970 batch 406/3281 lr 1e-05 accuracy 95.62109 wps 16877.44 step time 0.47s\n","# Epoch 18  global step 40220 loss 0.21992 batch 426/3281 lr 1e-05 accuracy 95.94141 wps 16908.48 step time 0.42s\n","# Epoch 18  global step 40240 loss 0.22063 batch 446/3281 lr 1e-05 accuracy 95.88672 wps 16807.09 step time 0.39s\n","# Epoch 18  global step 40260 loss 0.22711 batch 466/3281 lr 1e-05 accuracy 95.80078 wps 17337.11 step time 0.40s\n","# Epoch 18  global step 40280 loss 0.24032 batch 486/3281 lr 1e-05 accuracy 95.57812 wps 16025.05 step time 0.53s\n","# Epoch 18  global step 40300 loss 0.23343 batch 506/3281 lr 1e-05 accuracy 95.69141 wps 16572.52 step time 0.45s\n","# Epoch 18  global step 40320 loss 0.21645 batch 526/3281 lr 1e-05 accuracy 95.99609 wps 16191.19 step time 0.46s\n","# Epoch 18  global step 40340 loss 0.24045 batch 546/3281 lr 1e-05 accuracy 95.68359 wps 16508.09 step time 0.55s\n","# Epoch 18  global step 40360 loss 0.22384 batch 566/3281 lr 1e-05 accuracy 95.92578 wps 17372.71 step time 0.37s\n","# Epoch 18  global step 40380 loss 0.23771 batch 586/3281 lr 1e-05 accuracy 95.73437 wps 16627.79 step time 0.54s\n","# Epoch 18  global step 40400 loss 0.23216 batch 606/3281 lr 1e-05 accuracy 95.83594 wps 16161.91 step time 0.44s\n","# Epoch 18  global step 40420 loss 0.23601 batch 626/3281 lr 1e-05 accuracy 95.80859 wps 17669.29 step time 0.46s\n","# Epoch 18  global step 40440 loss 0.23028 batch 646/3281 lr 1e-05 accuracy 95.84766 wps 15645.60 step time 0.50s\n","# Epoch 18  global step 40460 loss 0.21440 batch 666/3281 lr 1e-05 accuracy 96.15625 wps 17077.36 step time 0.41s\n","# Epoch 18  global step 40480 loss 0.24240 batch 686/3281 lr 1e-05 accuracy 95.62891 wps 16438.73 step time 0.42s\n","# Epoch 18  global step 40500 loss 0.23684 batch 706/3281 lr 1e-05 accuracy 95.69922 wps 17737.37 step time 0.40s\n","# Epoch 18  global step 40520 loss 0.22414 batch 726/3281 lr 1e-05 accuracy 95.99609 wps 16666.61 step time 0.44s\n","# Epoch 18  global step 40540 loss 0.24153 batch 746/3281 lr 1e-05 accuracy 95.50391 wps 17092.81 step time 0.44s\n","# Epoch 18  global step 40560 loss 0.21335 batch 766/3281 lr 1e-05 accuracy 96.03906 wps 17643.88 step time 0.38s\n","# Epoch 18  global step 40580 loss 0.23101 batch 786/3281 lr 1e-05 accuracy 95.85938 wps 17674.41 step time 0.39s\n","# Epoch 18  global step 40600 loss 0.23561 batch 806/3281 lr 1e-05 accuracy 95.60156 wps 17419.96 step time 0.43s\n","# Epoch 18  global step 40620 loss 0.22235 batch 826/3281 lr 1e-05 accuracy 95.88281 wps 16421.14 step time 0.43s\n","# Epoch 18  global step 40640 loss 0.23649 batch 846/3281 lr 1e-05 accuracy 95.69531 wps 16328.37 step time 0.47s\n","# Epoch 18  global step 40660 loss 0.22859 batch 866/3281 lr 1e-05 accuracy 95.85547 wps 17393.50 step time 0.38s\n","# Epoch 18  global step 40680 loss 0.22303 batch 886/3281 lr 1e-05 accuracy 95.88672 wps 16461.38 step time 0.49s\n","# Epoch 18  global step 40700 loss 0.23022 batch 906/3281 lr 1e-05 accuracy 95.94922 wps 16666.62 step time 0.43s\n","# Epoch 18  global step 40720 loss 0.23652 batch 926/3281 lr 1e-05 accuracy 95.72656 wps 17918.82 step time 0.42s\n","# Epoch 18  global step 40740 loss 0.22430 batch 946/3281 lr 1e-05 accuracy 95.98438 wps 16991.16 step time 0.45s\n","# Epoch 18  global step 40760 loss 0.21285 batch 966/3281 lr 1e-05 accuracy 96.12109 wps 16786.46 step time 0.36s\n","# Epoch 18  global step 40780 loss 0.22176 batch 986/3281 lr 1e-05 accuracy 95.92187 wps 16892.79 step time 0.37s\n","# Epoch 18  global step 40800 loss 0.22544 batch 1006/3281 lr 1e-05 accuracy 95.84766 wps 16154.25 step time 0.45s\n","# Epoch 18  global step 40820 loss 0.22209 batch 1026/3281 lr 1e-05 accuracy 96.06641 wps 16040.17 step time 0.44s\n","# Epoch 18  global step 40840 loss 0.23625 batch 1046/3281 lr 1e-05 accuracy 95.58594 wps 17492.39 step time 0.43s\n","# Epoch 18  global step 40860 loss 0.22542 batch 1066/3281 lr 1e-05 accuracy 95.90234 wps 15847.77 step time 0.52s\n","# Epoch 18  global step 40880 loss 0.21772 batch 1086/3281 lr 1e-05 accuracy 96.04688 wps 17422.65 step time 0.37s\n","# Epoch 18  global step 40900 loss 0.22618 batch 1106/3281 lr 1e-05 accuracy 95.83984 wps 17204.07 step time 0.39s\n","# Epoch 18  global step 40920 loss 0.23674 batch 1126/3281 lr 1e-05 accuracy 95.71484 wps 16655.70 step time 0.47s\n","# Epoch 18  global step 40940 loss 0.21783 batch 1146/3281 lr 1e-05 accuracy 96.16406 wps 17171.59 step time 0.36s\n","# Epoch 18  global step 40960 loss 0.21516 batch 1166/3281 lr 1e-05 accuracy 96.07813 wps 16701.15 step time 0.37s\n","# Epoch 18  global step 40980 loss 0.23657 batch 1186/3281 lr 1e-05 accuracy 95.60156 wps 17381.24 step time 0.51s\n","# Epoch 18  global step 41000 loss 0.24241 batch 1206/3281 lr 1e-05 accuracy 95.57422 wps 17919.08 step time 0.51s\n","# Epoch 18  global step 41020 loss 0.21029 batch 1226/3281 lr 1e-05 accuracy 96.17969 wps 17595.43 step time 0.37s\n","# Epoch 18  global step 41040 loss 0.21216 batch 1246/3281 lr 1e-05 accuracy 96.03906 wps 16766.75 step time 0.38s\n","# Epoch 18  global step 41060 loss 0.23676 batch 1266/3281 lr 1e-05 accuracy 95.74219 wps 16980.37 step time 0.51s\n","# Epoch 18  global step 41080 loss 0.20344 batch 1286/3281 lr 1e-05 accuracy 96.42188 wps 16954.96 step time 0.35s\n","# Epoch 18  global step 41100 loss 0.23270 batch 1306/3281 lr 1e-05 accuracy 95.67969 wps 16341.57 step time 0.52s\n","# Epoch 18  global step 41120 loss 0.22532 batch 1326/3281 lr 1e-05 accuracy 95.84375 wps 16800.33 step time 0.40s\n","# Epoch 18  global step 41140 loss 0.22998 batch 1346/3281 lr 1e-05 accuracy 95.77344 wps 17716.60 step time 0.40s\n","# Epoch 18  global step 41160 loss 0.21843 batch 1366/3281 lr 1e-05 accuracy 95.95313 wps 16448.80 step time 0.38s\n","# Epoch 18  global step 41180 loss 0.22491 batch 1386/3281 lr 1e-05 accuracy 95.90234 wps 17683.77 step time 0.40s\n","# Epoch 18  global step 41200 loss 0.21156 batch 1406/3281 lr 1e-05 accuracy 96.16406 wps 16356.49 step time 0.37s\n","# Epoch 18  global step 41220 loss 0.21425 batch 1426/3281 lr 1e-05 accuracy 96.09375 wps 17317.94 step time 0.41s\n","# Epoch 18  global step 41240 loss 0.22427 batch 1446/3281 lr 1e-05 accuracy 95.79297 wps 16516.79 step time 0.48s\n","# Epoch 18  global step 41260 loss 0.21111 batch 1466/3281 lr 1e-05 accuracy 96.08984 wps 16764.93 step time 0.37s\n","# Epoch 18  global step 41280 loss 0.22143 batch 1486/3281 lr 1e-05 accuracy 96.01563 wps 15426.47 step time 0.49s\n","# Epoch 18  global step 41300 loss 0.23624 batch 1506/3281 lr 1e-05 accuracy 95.67578 wps 17190.40 step time 0.41s\n","# Epoch 18  global step 41320 loss 0.22057 batch 1526/3281 lr 1e-05 accuracy 95.91406 wps 17093.23 step time 0.39s\n","# Epoch 18  global step 41340 loss 0.24011 batch 1546/3281 lr 1e-05 accuracy 95.53125 wps 17468.11 step time 0.42s\n","# Epoch 18  global step 41360 loss 0.23573 batch 1566/3281 lr 1e-05 accuracy 95.55859 wps 17447.36 step time 0.50s\n","# Epoch 18  global step 41380 loss 0.23725 batch 1586/3281 lr 1e-05 accuracy 95.77344 wps 17072.45 step time 0.40s\n","# Epoch 18  global step 41400 loss 0.22729 batch 1606/3281 lr 1e-05 accuracy 95.83203 wps 16937.06 step time 0.37s\n","# Epoch 18  global step 41420 loss 0.21779 batch 1626/3281 lr 1e-05 accuracy 96.12109 wps 16481.95 step time 0.37s\n","# Epoch 18  global step 41440 loss 0.21587 batch 1646/3281 lr 1e-05 accuracy 95.91797 wps 17272.58 step time 0.38s\n","# Epoch 18  global step 41460 loss 0.21051 batch 1666/3281 lr 1e-05 accuracy 96.12891 wps 15982.28 step time 0.41s\n","# Epoch 18  global step 41480 loss 0.23216 batch 1686/3281 lr 1e-05 accuracy 95.80078 wps 17362.80 step time 0.43s\n","# Epoch 18  global step 41500 loss 0.22238 batch 1706/3281 lr 1e-05 accuracy 96.08984 wps 16360.49 step time 0.44s\n","# Epoch 18  global step 41520 loss 0.23609 batch 1726/3281 lr 1e-05 accuracy 95.63672 wps 17583.98 step time 0.50s\n","# Epoch 18  global step 41540 loss 0.21495 batch 1746/3281 lr 1e-05 accuracy 96.05078 wps 17116.72 step time 0.38s\n","# Epoch 18  global step 41560 loss 0.23039 batch 1766/3281 lr 1e-05 accuracy 95.76953 wps 16494.95 step time 0.46s\n","# Epoch 18  global step 41580 loss 0.22259 batch 1786/3281 lr 1e-05 accuracy 95.96484 wps 17314.09 step time 0.38s\n","# Epoch 18  global step 41600 loss 0.21757 batch 1806/3281 lr 1e-05 accuracy 95.98828 wps 17284.34 step time 0.40s\n","# Epoch 18  global step 41620 loss 0.22902 batch 1826/3281 lr 1e-05 accuracy 95.81641 wps 17203.19 step time 0.38s\n","# Epoch 18  global step 41640 loss 0.24375 batch 1846/3281 lr 1e-05 accuracy 95.53125 wps 18468.97 step time 0.44s\n","# Epoch 18  global step 41660 loss 0.22939 batch 1866/3281 lr 1e-05 accuracy 95.76953 wps 16288.30 step time 0.46s\n","# Epoch 18  global step 41680 loss 0.23223 batch 1886/3281 lr 1e-05 accuracy 95.80078 wps 16328.58 step time 0.47s\n","# Epoch 18  global step 41700 loss 0.23023 batch 1906/3281 lr 1e-05 accuracy 95.78516 wps 16396.40 step time 0.45s\n","# Epoch 18  global step 41720 loss 0.23214 batch 1926/3281 lr 1e-05 accuracy 95.66406 wps 16579.82 step time 0.43s\n","# Epoch 18  global step 41740 loss 0.22246 batch 1946/3281 lr 1e-05 accuracy 95.93359 wps 16798.29 step time 0.41s\n","# Epoch 18  global step 41760 loss 0.23531 batch 1966/3281 lr 1e-05 accuracy 95.74609 wps 17191.99 step time 0.46s\n","# Epoch 18  global step 41780 loss 0.22617 batch 1986/3281 lr 1e-05 accuracy 95.92969 wps 16680.86 step time 0.39s\n","# Epoch 18  global step 41800 loss 0.23308 batch 2006/3281 lr 1e-05 accuracy 95.61719 wps 15793.40 step time 0.48s\n","# Epoch 18  global step 41820 loss 0.21382 batch 2026/3281 lr 1e-05 accuracy 96.07031 wps 16267.28 step time 0.44s\n","# Epoch 18  global step 41840 loss 0.22526 batch 2046/3281 lr 1e-05 accuracy 95.80859 wps 17509.32 step time 0.44s\n","# Epoch 18  global step 41860 loss 0.23539 batch 2066/3281 lr 1e-05 accuracy 95.52344 wps 17288.17 step time 0.53s\n","# Epoch 18  global step 41880 loss 0.21584 batch 2086/3281 lr 1e-05 accuracy 96.13281 wps 16763.05 step time 0.39s\n","# Epoch 18  global step 41900 loss 0.20441 batch 2106/3281 lr 1e-05 accuracy 96.22266 wps 16541.76 step time 0.37s\n","# Epoch 18  global step 41920 loss 0.22840 batch 2126/3281 lr 1e-05 accuracy 95.87500 wps 16888.19 step time 0.40s\n","# Epoch 18  global step 41940 loss 0.24274 batch 2146/3281 lr 1e-05 accuracy 95.62500 wps 16752.91 step time 0.43s\n","# Epoch 18  global step 41960 loss 0.22530 batch 2166/3281 lr 1e-05 accuracy 95.85547 wps 16727.70 step time 0.39s\n","# Epoch 18  global step 41980 loss 0.23504 batch 2186/3281 lr 1e-05 accuracy 95.64453 wps 16724.75 step time 0.44s\n","# Epoch 18  global step 42000 loss 0.22592 batch 2206/3281 lr 1e-05 accuracy 95.88672 wps 17273.39 step time 0.37s\n","# global step 42000, eval model at Fri May 22 17:24:55 2020\n","2020-05-22 17:24:58.493327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:24:58.494049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 17:24:58.494214: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 17:24:58.494266: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 17:24:58.494318: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 17:24:58.494356: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 17:24:58.494402: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 17:24:58.494440: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 17:24:58.494492: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 17:24:58.494654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:24:58.495298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:24:58.495852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 17:24:58.495902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 17:24:58.495921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 17:24:58.495935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 17:24:58.496062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:24:58.496653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:24:58.497212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.6454488511192349\n","# location_distance_from_business_district - 0.5528243778659809\n","# location_easy_to_find - 0.7135619870126341\n","# service_wait_time - 0.6706534081847219\n","# service_waiters_attitude - 0.7960431057379582\n","# service_parking_convenience - 0.7466160186787157\n","# service_serving_speed - 0.7656991202334583\n","# price_level - 0.7790462586433045\n","# price_cost_effective - 0.711685705071362\n","# price_discount - 0.6730371074713241\n","# environment_decoration - 0.7361346340496598\n","# environment_noise - 0.7506076406278342\n","# environment_space - 0.7604852718346278\n","# environment_cleaness - 0.7449657981368348\n","# dish_portion - 0.7219056935865025\n","# dish_taste - 0.7294686811018438\n","# dish_look - 0.5822817262136915\n","# dish_recommendation - 0.7402631753543285\n","# others_overall_experience - 0.5892092234448891\n","# others_willing_to_consume_again - 0.7141174496438059\n","# Eval loss 0.27330, f1 0.70620\n","# current result -0.7062027617006359, previous best result -0.7079688364247765\n","# Epoch 18  global step 42020 loss 0.23125 batch 2226/3281 lr 1e-05 accuracy 95.80078 wps 16985.38 step time 0.50s\n","# Epoch 18  global step 42040 loss 0.22943 batch 2246/3281 lr 1e-05 accuracy 95.66406 wps 16317.19 step time 0.45s\n","# Epoch 18  global step 42060 loss 0.22310 batch 2266/3281 lr 1e-05 accuracy 95.85937 wps 17757.96 step time 0.37s\n","# Epoch 18  global step 42080 loss 0.23093 batch 2286/3281 lr 1e-05 accuracy 95.69922 wps 17423.07 step time 0.42s\n","# Epoch 18  global step 42100 loss 0.21768 batch 2306/3281 lr 1e-05 accuracy 96.02734 wps 16827.94 step time 0.39s\n","# Epoch 18  global step 42120 loss 0.21912 batch 2326/3281 lr 1e-05 accuracy 95.96094 wps 16891.09 step time 0.35s\n","# Epoch 18  global step 42140 loss 0.23448 batch 2346/3281 lr 1e-05 accuracy 95.74609 wps 17014.79 step time 0.40s\n","# Epoch 18  global step 42160 loss 0.21586 batch 2366/3281 lr 1e-05 accuracy 96.02734 wps 17375.04 step time 0.39s\n","# Epoch 18  global step 42180 loss 0.22276 batch 2386/3281 lr 1e-05 accuracy 95.95312 wps 17320.48 step time 0.40s\n","# Epoch 18  global step 42200 loss 0.21541 batch 2406/3281 lr 1e-05 accuracy 95.95703 wps 16647.13 step time 0.43s\n","# Epoch 18  global step 42220 loss 0.22940 batch 2426/3281 lr 1e-05 accuracy 95.86328 wps 16654.69 step time 0.39s\n","# Epoch 18  global step 42240 loss 0.22211 batch 2446/3281 lr 1e-05 accuracy 95.90234 wps 16609.41 step time 0.42s\n","# Epoch 18  global step 42260 loss 0.23786 batch 2466/3281 lr 1e-05 accuracy 95.54688 wps 16367.34 step time 0.55s\n","# Epoch 18  global step 42280 loss 0.22386 batch 2486/3281 lr 1e-05 accuracy 95.87109 wps 16550.62 step time 0.37s\n","# Epoch 18  global step 42300 loss 0.21773 batch 2506/3281 lr 1e-05 accuracy 96.05859 wps 16274.73 step time 0.38s\n","# Epoch 18  global step 42320 loss 0.23500 batch 2526/3281 lr 1e-05 accuracy 95.72656 wps 17557.50 step time 0.42s\n","# Epoch 18  global step 42340 loss 0.21780 batch 2546/3281 lr 1e-05 accuracy 96.04297 wps 16702.85 step time 0.40s\n","# Epoch 18  global step 42360 loss 0.22454 batch 2566/3281 lr 1e-05 accuracy 96.00391 wps 16994.94 step time 0.38s\n","# Epoch 18  global step 42380 loss 0.23062 batch 2586/3281 lr 1e-05 accuracy 95.68359 wps 17237.58 step time 0.39s\n","# Epoch 18  global step 42400 loss 0.20473 batch 2606/3281 lr 1e-05 accuracy 96.26953 wps 16344.41 step time 0.36s\n","# Epoch 18  global step 42420 loss 0.25024 batch 2626/3281 lr 1e-05 accuracy 95.30078 wps 17816.22 step time 0.46s\n","# Epoch 18  global step 42440 loss 0.23256 batch 2646/3281 lr 1e-05 accuracy 95.66016 wps 17031.08 step time 0.43s\n","# Epoch 18  global step 42460 loss 0.21967 batch 2666/3281 lr 1e-05 accuracy 96.13672 wps 17029.21 step time 0.40s\n","# Epoch 18  global step 42480 loss 0.22793 batch 2686/3281 lr 1e-05 accuracy 95.75391 wps 17131.58 step time 0.42s\n","# Epoch 18  global step 42500 loss 0.23184 batch 2706/3281 lr 1e-05 accuracy 95.74609 wps 16883.57 step time 0.47s\n","# Epoch 18  global step 42520 loss 0.24179 batch 2726/3281 lr 1e-05 accuracy 95.54297 wps 17081.08 step time 0.47s\n","# Epoch 18  global step 42540 loss 0.23379 batch 2746/3281 lr 1e-05 accuracy 95.67187 wps 17564.80 step time 0.42s\n","# Epoch 18  global step 42560 loss 0.23104 batch 2766/3281 lr 1e-05 accuracy 95.80859 wps 16148.76 step time 0.41s\n","# Epoch 18  global step 42580 loss 0.21775 batch 2786/3281 lr 1e-05 accuracy 95.92578 wps 15541.84 step time 0.53s\n","# Epoch 18  global step 42600 loss 0.21923 batch 2806/3281 lr 1e-05 accuracy 96.03125 wps 17479.48 step time 0.41s\n","# Epoch 18  global step 42620 loss 0.22162 batch 2826/3281 lr 1e-05 accuracy 95.98437 wps 16772.28 step time 0.41s\n","# Epoch 18  global step 42640 loss 0.21765 batch 2846/3281 lr 1e-05 accuracy 96.18750 wps 16671.75 step time 0.36s\n","# Epoch 18  global step 42660 loss 0.25060 batch 2866/3281 lr 1e-05 accuracy 95.25391 wps 18032.13 step time 0.50s\n","# Epoch 18  global step 42680 loss 0.23030 batch 2886/3281 lr 1e-05 accuracy 95.74609 wps 17563.81 step time 0.44s\n","# Epoch 18  global step 42700 loss 0.23600 batch 2906/3281 lr 1e-05 accuracy 95.70312 wps 17527.29 step time 0.42s\n","# Epoch 18  global step 42720 loss 0.21234 batch 2926/3281 lr 1e-05 accuracy 96.20703 wps 17389.14 step time 0.40s\n","# Epoch 18  global step 42740 loss 0.22219 batch 2946/3281 lr 1e-05 accuracy 95.86719 wps 16833.96 step time 0.38s\n","# Epoch 18  global step 42760 loss 0.22250 batch 2966/3281 lr 1e-05 accuracy 96.00781 wps 16790.04 step time 0.38s\n","# Epoch 18  global step 42780 loss 0.20470 batch 2986/3281 lr 1e-05 accuracy 96.38672 wps 16505.68 step time 0.46s\n","# Epoch 18  global step 42800 loss 0.22738 batch 3006/3281 lr 1e-05 accuracy 95.76562 wps 16746.97 step time 0.47s\n","# Epoch 18  global step 42820 loss 0.23045 batch 3026/3281 lr 1e-05 accuracy 95.79297 wps 17074.35 step time 0.46s\n","# Epoch 18  global step 42840 loss 0.23166 batch 3046/3281 lr 1e-05 accuracy 95.73438 wps 17203.53 step time 0.44s\n","# Epoch 18  global step 42860 loss 0.24153 batch 3066/3281 lr 1e-05 accuracy 95.53516 wps 16145.71 step time 0.56s\n","# Epoch 18  global step 42880 loss 0.24865 batch 3086/3281 lr 1e-05 accuracy 95.52734 wps 17812.45 step time 0.41s\n","# Epoch 18  global step 42900 loss 0.23123 batch 3106/3281 lr 1e-05 accuracy 95.82031 wps 17044.05 step time 0.40s\n","# Epoch 18  global step 42920 loss 0.22609 batch 3126/3281 lr 1e-05 accuracy 96.05078 wps 16749.33 step time 0.38s\n","# Epoch 18  global step 42940 loss 0.22553 batch 3146/3281 lr 1e-05 accuracy 95.84375 wps 16356.76 step time 0.49s\n","# Epoch 18  global step 42960 loss 0.23620 batch 3166/3281 lr 1e-05 accuracy 95.75781 wps 17634.49 step time 0.42s\n","# Epoch 18  global step 42980 loss 0.23053 batch 3186/3281 lr 1e-05 accuracy 95.84766 wps 18318.32 step time 0.43s\n","# Epoch 18  global step 43000 loss 0.23015 batch 3206/3281 lr 1e-05 accuracy 95.77734 wps 16620.52 step time 0.50s\n","# Epoch 18  global step 43020 loss 0.25347 batch 3226/3281 lr 1e-05 accuracy 95.31641 wps 18384.89 step time 0.51s\n","# Epoch 18  global step 43040 loss 0.22278 batch 3246/3281 lr 1e-05 accuracy 96.04297 wps 17156.11 step time 0.45s\n","# Epoch 18  global step 43060 loss 0.21423 batch 3266/3281 lr 1e-05 accuracy 95.99219 wps 17241.99 step time 0.39s\n","# Finsh epoch 18, global step 43076\n","# Epoch 19  global step 43080 loss 0.04994 batch 4/3281 lr 1e-05 accuracy 19.05078 wps 18648.30 step time 0.12s\n","# Epoch 19  global step 43100 loss 0.22885 batch 24/3281 lr 1e-05 accuracy 95.60547 wps 20092.96 step time 0.46s\n","# Epoch 19  global step 43120 loss 0.23132 batch 44/3281 lr 1e-05 accuracy 95.60156 wps 18883.23 step time 0.39s\n","# Epoch 19  global step 43140 loss 0.22309 batch 64/3281 lr 1e-05 accuracy 95.91797 wps 18602.19 step time 0.39s\n","# Epoch 19  global step 43160 loss 0.20949 batch 84/3281 lr 1e-05 accuracy 96.03906 wps 18798.31 step time 0.34s\n","# Epoch 19  global step 43180 loss 0.23575 batch 104/3281 lr 1e-05 accuracy 95.79687 wps 20573.72 step time 0.42s\n","# Epoch 19  global step 43200 loss 0.23050 batch 124/3281 lr 1e-05 accuracy 95.82422 wps 18670.33 step time 0.40s\n","# Epoch 19  global step 43220 loss 0.22714 batch 144/3281 lr 1e-05 accuracy 95.90234 wps 18665.45 step time 0.45s\n","# Epoch 19  global step 43240 loss 0.22212 batch 164/3281 lr 1e-05 accuracy 95.99609 wps 18925.86 step time 0.35s\n","# Epoch 19  global step 43260 loss 0.23228 batch 184/3281 lr 1e-05 accuracy 95.61328 wps 19296.84 step time 0.37s\n","# Epoch 19  global step 43280 loss 0.22712 batch 204/3281 lr 1e-05 accuracy 95.64453 wps 18936.70 step time 0.35s\n","# Epoch 19  global step 43300 loss 0.23480 batch 224/3281 lr 1e-05 accuracy 95.79688 wps 20191.82 step time 0.41s\n","# Epoch 19  global step 43320 loss 0.21448 batch 244/3281 lr 1e-05 accuracy 96.14062 wps 18524.99 step time 0.34s\n","# Epoch 19  global step 43340 loss 0.23662 batch 264/3281 lr 1e-05 accuracy 95.65234 wps 19454.34 step time 0.37s\n","# Epoch 19  global step 43360 loss 0.22522 batch 284/3281 lr 1e-05 accuracy 95.87500 wps 19838.51 step time 0.38s\n","# Epoch 19  global step 43380 loss 0.22299 batch 304/3281 lr 1e-05 accuracy 95.90234 wps 19042.74 step time 0.37s\n","# Epoch 19  global step 43400 loss 0.22491 batch 324/3281 lr 1e-05 accuracy 95.82422 wps 19222.03 step time 0.40s\n","# Epoch 19  global step 43420 loss 0.22985 batch 344/3281 lr 1e-05 accuracy 95.79688 wps 19493.50 step time 0.37s\n","# Epoch 19  global step 43440 loss 0.21892 batch 364/3281 lr 1e-05 accuracy 96.07031 wps 19643.96 step time 0.37s\n","# Epoch 19  global step 43460 loss 0.22852 batch 384/3281 lr 1e-05 accuracy 95.91797 wps 18621.46 step time 0.44s\n","# Epoch 19  global step 43480 loss 0.21601 batch 404/3281 lr 1e-05 accuracy 96.00781 wps 18976.61 step time 0.36s\n","# Epoch 19  global step 43500 loss 0.22152 batch 424/3281 lr 1e-05 accuracy 96.14063 wps 19568.24 step time 0.42s\n","# Epoch 19  global step 43520 loss 0.23179 batch 444/3281 lr 1e-05 accuracy 95.73047 wps 19322.30 step time 0.42s\n","# Epoch 19  global step 43540 loss 0.21430 batch 464/3281 lr 1e-05 accuracy 96.07031 wps 18768.56 step time 0.39s\n","# Epoch 19  global step 43560 loss 0.23765 batch 484/3281 lr 1e-05 accuracy 95.63672 wps 19478.19 step time 0.37s\n","# Epoch 19  global step 43580 loss 0.21442 batch 504/3281 lr 1e-05 accuracy 96.10938 wps 19436.54 step time 0.37s\n","# Epoch 19  global step 43600 loss 0.21895 batch 524/3281 lr 1e-05 accuracy 96.10156 wps 18715.01 step time 0.35s\n","# Epoch 19  global step 43620 loss 0.22328 batch 544/3281 lr 1e-05 accuracy 95.96484 wps 19800.53 step time 0.37s\n","# Epoch 19  global step 43640 loss 0.22175 batch 564/3281 lr 1e-05 accuracy 96.01953 wps 18891.44 step time 0.35s\n","# Epoch 19  global step 43660 loss 0.21955 batch 584/3281 lr 1e-05 accuracy 96.03516 wps 19106.42 step time 0.36s\n","# Epoch 19  global step 43680 loss 0.21761 batch 604/3281 lr 1e-05 accuracy 95.99609 wps 19381.43 step time 0.42s\n","# Epoch 19  global step 43700 loss 0.22348 batch 624/3281 lr 1e-05 accuracy 95.84766 wps 19648.68 step time 0.37s\n","# Epoch 19  global step 43720 loss 0.21543 batch 644/3281 lr 1e-05 accuracy 96.08984 wps 19257.59 step time 0.36s\n","# Epoch 19  global step 43740 loss 0.23474 batch 664/3281 lr 1e-05 accuracy 95.71484 wps 19450.84 step time 0.37s\n","# Epoch 19  global step 43760 loss 0.22210 batch 684/3281 lr 1e-05 accuracy 95.78906 wps 20032.30 step time 0.40s\n","# Epoch 19  global step 43780 loss 0.22942 batch 704/3281 lr 1e-05 accuracy 95.83594 wps 19165.73 step time 0.36s\n","# Epoch 19  global step 43800 loss 0.22470 batch 724/3281 lr 1e-05 accuracy 95.86328 wps 18229.95 step time 0.42s\n","# Epoch 19  global step 43820 loss 0.23528 batch 744/3281 lr 1e-05 accuracy 95.78906 wps 20461.07 step time 0.41s\n","# Epoch 19  global step 43840 loss 0.23640 batch 764/3281 lr 1e-05 accuracy 95.70703 wps 20418.24 step time 0.43s\n","# Epoch 19  global step 43860 loss 0.22465 batch 784/3281 lr 1e-05 accuracy 96.01563 wps 18558.80 step time 0.34s\n","# Epoch 19  global step 43880 loss 0.22065 batch 804/3281 lr 1e-05 accuracy 95.94141 wps 19392.76 step time 0.37s\n","# Epoch 19  global step 43900 loss 0.19946 batch 824/3281 lr 1e-05 accuracy 96.29687 wps 18472.20 step time 0.34s\n","# Epoch 19  global step 43920 loss 0.21422 batch 844/3281 lr 1e-05 accuracy 96.06250 wps 18672.71 step time 0.34s\n","# Epoch 19  global step 43940 loss 0.21036 batch 864/3281 lr 1e-05 accuracy 96.19141 wps 18899.41 step time 0.35s\n","# Epoch 19  global step 43960 loss 0.21096 batch 884/3281 lr 1e-05 accuracy 96.21484 wps 18170.85 step time 0.33s\n","# Epoch 19  global step 43980 loss 0.21747 batch 904/3281 lr 1e-05 accuracy 95.91797 wps 19263.74 step time 0.41s\n","# Epoch 19  global step 44000 loss 0.22222 batch 924/3281 lr 1e-05 accuracy 95.91797 wps 20283.62 step time 0.40s\n","# global step 44000, eval model at Fri May 22 17:39:36 2020\n","2020-05-22 17:39:38.437574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:39:38.438269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-05-22 17:39:38.438409: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-22 17:39:38.438495: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-22 17:39:38.438549: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-22 17:39:38.438602: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-22 17:39:38.438656: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-22 17:39:38.438726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-22 17:39:38.438764: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 17:39:38.438918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:39:38.439532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:39:38.440168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-22 17:39:38.440268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 17:39:38.440288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-22 17:39:38.440301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-22 17:39:38.440478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:39:38.441020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 17:39:38.441524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15466 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","# location_traffic_convenience - 0.648647831286308\n","# location_distance_from_business_district - 0.5542359111542042\n","# location_easy_to_find - 0.7130418727210491\n","# service_wait_time - 0.6723240037387009\n","# service_waiters_attitude - 0.7966666909752655\n","# service_parking_convenience - 0.7448766333625038\n","# service_serving_speed - 0.7636506893812305\n","# price_level - 0.7783291187335273\n","# price_cost_effective - 0.7109663299858386\n","# price_discount - 0.6715917877814963\n","# environment_decoration - 0.7326479946067821\n","# environment_noise - 0.7483274501717712\n","# environment_space - 0.7583935424145571\n","# environment_cleaness - 0.7451282995355221\n","# dish_portion - 0.7216820833841578\n","# dish_taste - 0.7289288822312741\n","# dish_look - 0.5827702659965739\n","# dish_recommendation - 0.7353434771531304\n","# others_overall_experience - 0.5901931186496301\n","# others_willing_to_consume_again - 0.7135754772023177\n","# Eval loss 0.27592, f1 0.70557\n","# current result -0.7055660730232921, previous best result -0.7079688364247765\n","# No loss decrease, restore previous best model and set learning rate to half of previous one\n","# Early stop, exit\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zOVx_9Fn2QaH","colab_type":"code","outputId":"3993643c-bd09-4d0c-a025-b2103f853c67","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# focal loss + label smoothing\n","!bash bash/elmo_train.sh"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:142: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:33: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from scripts/data/train.json ...\n","# Got 105000 data items with 3281 batches\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from scripts/data/validation.json ...\n","# Got 15000 data items with 93 batches\n","  saving hparams to scripts/data/elmo_ema_focal_smooth/hparams\n","mode=train,data_files=['scripts/data/train.json'],eval_files=['scripts/data/validation.json'],label_file=scripts/data/labels.txt,vocab_file=scripts/data/vocab.txt,embed_file=scripts/data/embedding.txt,out_file=None,split_word=True,max_len=1200,batch_size=32,reverse=False,prob=False,num_layers=3,decay_schema=hand,encoder=elmo,decay_steps=10000,learning_rate=0.001,focal_loss=2.0,embedding_dropout=0.1,max_gradient_norm=5.0,dropout_keep_prob=0.8,weight_keep_drop=0.8,l2_loss_ratio=0.0,rnn_cell_name=lstm,embedding_size=300,num_units=300,double_decoder=False,variational_dropout=True,target_label_num=4,feature_num=20,need_early_stop=True,patient=5,debug=False,num_train_epoch=50,steps_per_stats=20,steps_per_summary=50,steps_per_eval=2000,checkpoint_dir=scripts/data/elmo_ema_focal_smooth,vocab_size=50000\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:72: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:89: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:134: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 33871 300\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:114: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:106: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","build elmo encoder\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:40: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","seq_dim is deprecated, use seq_axis instead\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","batch_dim is deprecated, use batch_axis instead\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11a6e68dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11a6e68dd8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11fa28a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11fa28a550>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11adfb8fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11adfb8fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11adfc5390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11adfc5390>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11a6e50438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11a6e50438>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11fa28a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11fa28a550>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:264: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:64: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bc436f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bc436f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:242: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bc8ef978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bc8ef978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bc8ef978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bc8ef978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1372: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py:2078: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11bd2bac50>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11a6e68748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11b904bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11bd28d400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:284: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:96: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","dim is deprecated, use axis instead\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:101: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","variable <tf.Variable 'embedding/embedding:0' shape=(50000, 300) dtype=float32_ref> with parameter number 15000000\n","variable <tf.Variable 'embedding/feature_embedding:0' shape=(20, 300) dtype=float32_ref> with parameter number 6000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/scalar:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","variable <tf.Variable 'elmo_encoder/weight:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classification/attention_semantic/memory_layer/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classification/attention_semantic/dense/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classification/attention_semantic/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classification/attention_semantic/dense_1/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classification/attention_semantic/dense_1/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classification/attention_semantic/attention_wrapper/lstm_cell/kernel:0' shape=(1200, 1200) dtype=float32_ref> with parameter number 1440000\n","variable <tf.Variable 'classification/attention_semantic/attention_wrapper/lstm_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classification/attention_semantic/attention_wrapper/luong_attention/attention_g:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classification/predict_clf/dense/kernel:0' shape=(900, 300) dtype=float32_ref> with parameter number 270000\n","variable <tf.Variable 'classification/predict_clf/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classification/predict_clf/dense_1/kernel:0' shape=(300, 4) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classification/predict_clf/dense_1/bias:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","# total parameter number 23746510\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:326: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:295: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:297: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:302: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:41: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","loading hparams from scripts/data/elmo_ema_focal_smooth/hparams\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 33871 300\n","build elmo encoder\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11babe5eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11babe5eb8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11bd17d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11bd17d828>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11ae4ced68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11ae4ced68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11a36a0198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11a36a0198>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11bd17d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11bd17d828>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11ba606080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f11ba606080>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11a36a0c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11a36a0c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ab186e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ab186e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ab186e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ab186e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f11ab186b38>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f11babd0e10>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11babd0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f11ba1ea780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","2020-05-23 06:15:08.924338: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n","2020-05-23 06:15:08.929176: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n","2020-05-23 06:15:09.105134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:15:09.105760: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23cf800 executing computations on platform CUDA. Devices:\n","2020-05-23 06:15:09.105801: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1\n","2020-05-23 06:15:09.109441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000165000 Hz\n","2020-05-23 06:15:09.109679: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23cebc0 executing computations on platform Host. Devices:\n","2020-05-23 06:15:09.109713: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n","2020-05-23 06:15:09.109957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:15:09.110352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 06:15:09.128691: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 06:15:09.339180: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 06:15:09.426509: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 06:15:09.448716: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 06:15:09.682840: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 06:15:09.827633: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 06:15:10.351785: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 06:15:10.352012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:15:10.352536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:15:10.352894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 06:15:10.353055: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 06:15:10.354248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 06:15:10.354283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 06:15:10.354297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 06:15:10.354469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:15:10.354881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:15:10.355251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","2020-05-23 06:15:10.469572: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\n","  /job:localhost/replica:0/task:0/device:CPU:0].\n","See below for details of this colocation group:\n","Colocation Debug Info:\n","Colocation group had the following types and supported devices: \n","Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\n","AssignSub: GPU CPU \n","Merge: GPU CPU XLA_CPU XLA_GPU \n","Switch: GPU CPU XLA_CPU XLA_GPU \n","L2Loss: GPU CPU XLA_CPU XLA_GPU \n","Add: GPU CPU XLA_CPU XLA_GPU \n","Assign: GPU CPU \n","Identity: GPU CPU XLA_CPU XLA_GPU \n","VariableV2: GPU CPU \n","Const: GPU CPU XLA_CPU XLA_GPU \n","Fill: GPU CPU XLA_CPU XLA_GPU \n","Sub: GPU CPU XLA_CPU XLA_GPU \n","UnsortedSegmentSum: GPU CPU XLA_CPU XLA_GPU \n","Reshape: GPU CPU XLA_CPU XLA_GPU \n","RefSwitch: GPU CPU \n","GatherV2: GPU CPU XLA_CPU XLA_GPU \n","ExpandDims: GPU CPU XLA_CPU XLA_GPU \n","RandomUniform: GPU CPU XLA_CPU XLA_GPU \n","Cast: GPU CPU XLA_CPU XLA_GPU \n","Mul: GPU CPU XLA_CPU XLA_GPU \n","Unique: GPU CPU \n","SparseApplyRMSProp: CPU \n","StridedSlice: GPU CPU XLA_CPU XLA_GPU \n","ConcatV2: GPU CPU XLA_CPU XLA_GPU \n","Shape: GPU CPU XLA_CPU XLA_GPU \n","IsVariableInitialized: GPU CPU \n","\n","Colocation members, user-requested devices, and framework assigned devices, if any:\n","  embedding/feature_embedding/Initializer/random_uniform/shape (Const) \n","  embedding/feature_embedding/Initializer/random_uniform/min (Const) \n","  embedding/feature_embedding/Initializer/random_uniform/max (Const) \n","  embedding/feature_embedding/Initializer/random_uniform/RandomUniform (RandomUniform) \n","  embedding/feature_embedding/Initializer/random_uniform/sub (Sub) \n","  embedding/feature_embedding/Initializer/random_uniform/mul (Mul) \n","  embedding/feature_embedding/Initializer/random_uniform (Add) \n","  embedding/feature_embedding (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/read (Identity) /device:GPU:0\n","  embedding_lookup_1/axis (Const) /device:GPU:0\n","  embedding_lookup_1 (GatherV2) /device:GPU:0\n","  global_norm/L2Loss_1 (L2Loss) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Shape (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Cast (Cast) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Size (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/ExpandDims/dim (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/ExpandDims (ExpandDims) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice/stack (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice/stack_1 (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice/stack_2 (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice (StridedSlice) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/concat/axis (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/concat (ConcatV2) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Reshape (Reshape) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Reshape_1 (Reshape) /device:GPU:0\n","  global_norm_1/L2Loss_1 (L2Loss) /device:GPU:0\n","  clip_by_global_norm/mul_2 (Mul) /device:GPU:0\n","  clip_by_global_norm/clip_by_global_norm/_1 (Identity) /device:GPU:0\n","  global_norm_2/L2Loss_1 (L2Loss) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Initializer/ones/shape_as_tensor (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Initializer/ones/Const (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Initializer/ones (Fill) /device:GPU:0\n","  embedding/feature_embedding/RMSProp (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/read (Identity) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Initializer/zeros/Const (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Initializer/zeros (Fill) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1 (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/read (Identity) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/Unique (Unique) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/Shape (Shape) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice/stack (Const) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice/stack_1 (Const) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice/stack_2 (Const) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice (StridedSlice) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/UnsortedSegmentSum (UnsortedSegmentSum) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/SparseApplyRMSProp (SparseApplyRMSProp) /device:GPU:0\n","  IsVariableInitialized_1 (IsVariableInitialized) /device:GPU:0\n","  cond_1/read/Switch (RefSwitch) /device:GPU:0\n","  cond_1/Switch_1 (Switch) \n","  embedding/feature_embedding/ExponentialMovingAverage (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/IsVariableInitialized (IsVariableInitialized) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/Switch (Switch) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/switch_t (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/switch_f (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/pred_id (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/read/Switch (RefSwitch) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/read (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/Switch_1 (Switch) \n","  embedding/feature_embedding/ExponentialMovingAverage/cond/Merge (Merge) /device:GPU:0\n","  cond_1/read/Switch_embedding/feature_embedding/ExponentialMovingAverage (Switch) /device:GPU:0\n","  cond_1/read_embedding/feature_embedding/ExponentialMovingAverage (Identity) /device:GPU:0\n","  cond_1/Merge_embedding/feature_embedding/ExponentialMovingAverage (Merge) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/read (Identity) /device:GPU:0\n","  ExponentialMovingAverage/AssignMovingAvg_1 (AssignSub) /device:GPU:0\n","  save/Assign_112 (Assign) /device:GPU:0\n","  save/Assign_113 (Assign) /device:GPU:0\n","  save/Assign_114 (Assign) /device:GPU:0\n","  save/Assign_115 (Assign) /device:GPU:0\n","\n","2020-05-23 06:15:11.529368: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n","unable to restore model, train from scratch\n","# Start to train with learning rate 0.001, Sat May 23 06:15:12 2020\n","# Global step 0\n","2020-05-23 06:15:15.819120: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","# Epoch 1  global step 20 loss 14.86266 batch 20/3281 lr 0.001 accuracy 78.62109 wps 9719.21 step time 0.75s\n","# Epoch 1  global step 40 loss 12.66933 batch 40/3281 lr 0.001 accuracy 81.08203 wps 14263.42 step time 0.54s\n","# Epoch 1  global step 60 loss 10.79276 batch 60/3281 lr 0.001 accuracy 81.50781 wps 14614.81 step time 0.46s\n","# Epoch 1  global step 80 loss 10.81340 batch 80/3281 lr 0.001 accuracy 81.09375 wps 15005.80 step time 0.49s\n","# Epoch 1  global step 100 loss 10.48592 batch 100/3281 lr 0.001 accuracy 82.28516 wps 14418.70 step time 0.43s\n","# Epoch 1  global step 120 loss 10.21383 batch 120/3281 lr 0.001 accuracy 83.37500 wps 14285.00 step time 0.57s\n","# Epoch 1  global step 140 loss 9.94736 batch 140/3281 lr 0.001 accuracy 83.43359 wps 15141.80 step time 0.51s\n","# Epoch 1  global step 160 loss 9.57331 batch 160/3281 lr 0.001 accuracy 84.58984 wps 13738.94 step time 0.39s\n","# Epoch 1  global step 180 loss 9.52592 batch 180/3281 lr 0.001 accuracy 84.36719 wps 14344.35 step time 0.43s\n","# Epoch 1  global step 200 loss 9.59771 batch 200/3281 lr 0.001 accuracy 83.93359 wps 13872.08 step time 0.62s\n","# Epoch 1  global step 220 loss 9.58065 batch 220/3281 lr 0.001 accuracy 83.52344 wps 14320.19 step time 0.43s\n","# Epoch 1  global step 240 loss 9.58397 batch 240/3281 lr 0.001 accuracy 84.21484 wps 14661.85 step time 0.47s\n","# Epoch 1  global step 260 loss 9.62132 batch 260/3281 lr 0.001 accuracy 83.95703 wps 14783.60 step time 0.47s\n","# Epoch 1  global step 280 loss 9.59238 batch 280/3281 lr 0.001 accuracy 83.57422 wps 14543.29 step time 0.46s\n","# Epoch 1  global step 300 loss 9.35041 batch 300/3281 lr 0.001 accuracy 84.07031 wps 14348.76 step time 0.56s\n","# Epoch 1  global step 320 loss 9.40133 batch 320/3281 lr 0.001 accuracy 84.17578 wps 14407.44 step time 0.44s\n","# Epoch 1  global step 340 loss 9.42912 batch 340/3281 lr 0.001 accuracy 84.21094 wps 14169.50 step time 0.53s\n","# Epoch 1  global step 360 loss 9.58066 batch 360/3281 lr 0.001 accuracy 83.00391 wps 14593.79 step time 0.58s\n","# Epoch 1  global step 380 loss 9.35556 batch 380/3281 lr 0.001 accuracy 83.77344 wps 14467.47 step time 0.54s\n","# Epoch 1  global step 400 loss 9.40448 batch 400/3281 lr 0.001 accuracy 83.58984 wps 15101.97 step time 0.60s\n","# Epoch 1  global step 420 loss 9.18937 batch 420/3281 lr 0.001 accuracy 84.50781 wps 14419.06 step time 0.44s\n","# Epoch 1  global step 440 loss 9.23784 batch 440/3281 lr 0.001 accuracy 84.13281 wps 14595.20 step time 0.46s\n","# Epoch 1  global step 460 loss 9.25333 batch 460/3281 lr 0.001 accuracy 84.18750 wps 14780.88 step time 0.51s\n","# Epoch 1  global step 480 loss 8.93866 batch 480/3281 lr 0.001 accuracy 85.15234 wps 14153.89 step time 0.43s\n","# Epoch 1  global step 500 loss 9.04839 batch 500/3281 lr 0.001 accuracy 84.69531 wps 14715.15 step time 0.49s\n","# Epoch 1  global step 520 loss 9.11129 batch 520/3281 lr 0.001 accuracy 84.67969 wps 14572.20 step time 0.46s\n","# Epoch 1  global step 540 loss 8.88237 batch 540/3281 lr 0.001 accuracy 84.98047 wps 14061.55 step time 0.51s\n","# Epoch 1  global step 560 loss 9.02575 batch 560/3281 lr 0.001 accuracy 84.37500 wps 15181.51 step time 0.55s\n","# Epoch 1  global step 580 loss 9.10170 batch 580/3281 lr 0.001 accuracy 84.57812 wps 14329.99 step time 0.56s\n","# Epoch 1  global step 600 loss 9.07804 batch 600/3281 lr 0.001 accuracy 84.34766 wps 14575.90 step time 0.60s\n","# Epoch 1  global step 620 loss 8.82556 batch 620/3281 lr 0.001 accuracy 85.26953 wps 14514.63 step time 0.45s\n","# Epoch 1  global step 640 loss 8.97319 batch 640/3281 lr 0.001 accuracy 85.19922 wps 15110.02 step time 0.53s\n","# Epoch 1  global step 660 loss 8.82794 batch 660/3281 lr 0.001 accuracy 85.37500 wps 13488.49 step time 0.49s\n","# Epoch 1  global step 680 loss 8.85930 batch 680/3281 lr 0.001 accuracy 85.66406 wps 14462.85 step time 0.47s\n","# Epoch 1  global step 700 loss 8.66894 batch 700/3281 lr 0.001 accuracy 85.51562 wps 14182.11 step time 0.42s\n","# Epoch 1  global step 720 loss 8.67914 batch 720/3281 lr 0.001 accuracy 85.79688 wps 14663.60 step time 0.47s\n","# Epoch 1  global step 740 loss 8.59546 batch 740/3281 lr 0.001 accuracy 85.27344 wps 15121.85 step time 0.57s\n","# Epoch 1  global step 760 loss 8.46570 batch 760/3281 lr 0.001 accuracy 86.56250 wps 14487.35 step time 0.48s\n","# Epoch 1  global step 780 loss 8.55835 batch 780/3281 lr 0.001 accuracy 86.04297 wps 14741.51 step time 0.62s\n","# Epoch 1  global step 800 loss 8.59624 batch 800/3281 lr 0.001 accuracy 85.99609 wps 14448.68 step time 0.56s\n","# Epoch 1  global step 820 loss 8.53606 batch 820/3281 lr 0.001 accuracy 86.24609 wps 14068.62 step time 0.63s\n","# Epoch 1  global step 840 loss 8.38166 batch 840/3281 lr 0.001 accuracy 86.56641 wps 14573.40 step time 0.47s\n","# Epoch 1  global step 860 loss 7.85413 batch 860/3281 lr 0.001 accuracy 88.08203 wps 14216.87 step time 0.43s\n","# Epoch 1  global step 880 loss 8.31668 batch 880/3281 lr 0.001 accuracy 86.96094 wps 14514.69 step time 0.54s\n","# Epoch 1  global step 900 loss 8.14515 batch 900/3281 lr 0.001 accuracy 87.26172 wps 14186.11 step time 0.54s\n","# Epoch 1  global step 920 loss 8.06804 batch 920/3281 lr 0.001 accuracy 87.95313 wps 14497.84 step time 0.45s\n","# Epoch 1  global step 940 loss 8.14573 batch 940/3281 lr 0.001 accuracy 87.40234 wps 14433.29 step time 0.55s\n","# Epoch 1  global step 960 loss 8.02826 batch 960/3281 lr 0.001 accuracy 87.36719 wps 14865.74 step time 0.49s\n","# Epoch 1  global step 980 loss 7.87879 batch 980/3281 lr 0.001 accuracy 88.12109 wps 14822.93 step time 0.51s\n","# Epoch 1  global step 1000 loss 7.78259 batch 1000/3281 lr 0.001 accuracy 88.36719 wps 13899.71 step time 0.40s\n","# Epoch 1  global step 1020 loss 7.68931 batch 1020/3281 lr 0.001 accuracy 88.67578 wps 14031.29 step time 0.41s\n","# Epoch 1  global step 1040 loss 8.02014 batch 1040/3281 lr 0.001 accuracy 87.74219 wps 14884.20 step time 0.52s\n","# Epoch 1  global step 1060 loss 7.64626 batch 1060/3281 lr 0.001 accuracy 88.63672 wps 14665.00 step time 0.49s\n","# Epoch 1  global step 1080 loss 7.86238 batch 1080/3281 lr 0.001 accuracy 87.73828 wps 14420.99 step time 0.58s\n","# Epoch 1  global step 1100 loss 7.46880 batch 1100/3281 lr 0.001 accuracy 89.22266 wps 14483.35 step time 0.46s\n","# Epoch 1  global step 1120 loss 7.43935 batch 1120/3281 lr 0.001 accuracy 89.33594 wps 14580.25 step time 0.46s\n","# Epoch 1  global step 1140 loss 7.42494 batch 1140/3281 lr 0.001 accuracy 89.46875 wps 14553.19 step time 0.46s\n","# Epoch 1  global step 1160 loss 7.38633 batch 1160/3281 lr 0.001 accuracy 89.59766 wps 14294.05 step time 0.44s\n","# Epoch 1  global step 1180 loss 7.48800 batch 1180/3281 lr 0.001 accuracy 89.53516 wps 14287.78 step time 0.54s\n","# Epoch 1  global step 1200 loss 7.68829 batch 1200/3281 lr 0.001 accuracy 88.68359 wps 14431.42 step time 0.57s\n","# Epoch 1  global step 1220 loss 7.30716 batch 1220/3281 lr 0.001 accuracy 89.75000 wps 14777.88 step time 0.48s\n","# Epoch 1  global step 1240 loss 7.31774 batch 1240/3281 lr 0.001 accuracy 89.67969 wps 14838.67 step time 0.48s\n","# Epoch 1  global step 1260 loss 7.47844 batch 1260/3281 lr 0.001 accuracy 89.10938 wps 13741.79 step time 0.59s\n","# Epoch 1  global step 1280 loss 7.31913 batch 1280/3281 lr 0.001 accuracy 89.53906 wps 14683.23 step time 0.46s\n","# Epoch 1  global step 1300 loss 7.09237 batch 1300/3281 lr 0.001 accuracy 90.34375 wps 14728.12 step time 0.48s\n","# Epoch 1  global step 1320 loss 7.23223 batch 1320/3281 lr 0.001 accuracy 90.02344 wps 14901.72 step time 0.50s\n","# Epoch 1  global step 1340 loss 7.00817 batch 1340/3281 lr 0.001 accuracy 90.63672 wps 14376.81 step time 0.44s\n","# Epoch 1  global step 1360 loss 7.15604 batch 1360/3281 lr 0.001 accuracy 90.14063 wps 14562.10 step time 0.46s\n","# Epoch 1  global step 1380 loss 7.15883 batch 1380/3281 lr 0.001 accuracy 90.16406 wps 14555.00 step time 0.47s\n","# Epoch 1  global step 1400 loss 7.03216 batch 1400/3281 lr 0.001 accuracy 90.58594 wps 14008.61 step time 0.51s\n","# Epoch 1  global step 1420 loss 7.10436 batch 1420/3281 lr 0.001 accuracy 90.26953 wps 13683.72 step time 0.60s\n","# Epoch 1  global step 1440 loss 7.13187 batch 1440/3281 lr 0.001 accuracy 90.21875 wps 14019.55 step time 0.52s\n","# Epoch 1  global step 1460 loss 7.05208 batch 1460/3281 lr 0.001 accuracy 90.48828 wps 14217.27 step time 0.51s\n","# Epoch 1  global step 1480 loss 6.94139 batch 1480/3281 lr 0.001 accuracy 90.66406 wps 14367.12 step time 0.57s\n","# Epoch 1  global step 1500 loss 6.82558 batch 1500/3281 lr 0.001 accuracy 91.05469 wps 14680.68 step time 0.48s\n","# Epoch 1  global step 1520 loss 7.03493 batch 1520/3281 lr 0.001 accuracy 90.41016 wps 14981.01 step time 0.50s\n","# Epoch 1  global step 1540 loss 6.81252 batch 1540/3281 lr 0.001 accuracy 91.27734 wps 14324.20 step time 0.43s\n","# Epoch 1  global step 1560 loss 7.01315 batch 1560/3281 lr 0.001 accuracy 90.46875 wps 15066.42 step time 0.51s\n","# Epoch 1  global step 1580 loss 6.74750 batch 1580/3281 lr 0.001 accuracy 91.34375 wps 14532.26 step time 0.47s\n","# Epoch 1  global step 1600 loss 6.54859 batch 1600/3281 lr 0.001 accuracy 91.79688 wps 13103.87 step time 0.46s\n","# Epoch 1  global step 1620 loss 6.91002 batch 1620/3281 lr 0.001 accuracy 90.90625 wps 14320.63 step time 0.53s\n","# Epoch 1  global step 1640 loss 6.75808 batch 1640/3281 lr 0.001 accuracy 91.19141 wps 14043.26 step time 0.51s\n","# Epoch 1  global step 1660 loss 6.66103 batch 1660/3281 lr 0.001 accuracy 91.42187 wps 14663.48 step time 0.47s\n","# Epoch 1  global step 1680 loss 6.81927 batch 1680/3281 lr 0.001 accuracy 91.10547 wps 14747.72 step time 0.59s\n","# Epoch 1  global step 1700 loss 6.62687 batch 1700/3281 lr 0.001 accuracy 91.63672 wps 14447.60 step time 0.44s\n","# Epoch 1  global step 1720 loss 6.64387 batch 1720/3281 lr 0.001 accuracy 91.47656 wps 14229.47 step time 0.56s\n","# Epoch 1  global step 1740 loss 6.85953 batch 1740/3281 lr 0.001 accuracy 90.91797 wps 14299.46 step time 0.56s\n","# Epoch 1  global step 1760 loss 6.60454 batch 1760/3281 lr 0.001 accuracy 91.73828 wps 14407.77 step time 0.44s\n","# Epoch 1  global step 1780 loss 6.71415 batch 1780/3281 lr 0.001 accuracy 91.37109 wps 14563.68 step time 0.59s\n","# Epoch 1  global step 1800 loss 6.66612 batch 1800/3281 lr 0.001 accuracy 91.41406 wps 14984.68 step time 0.51s\n","# Epoch 1  global step 1820 loss 6.72645 batch 1820/3281 lr 0.001 accuracy 91.37109 wps 14305.91 step time 0.56s\n","# Epoch 1  global step 1840 loss 6.68010 batch 1840/3281 lr 0.001 accuracy 91.46484 wps 14563.81 step time 0.46s\n","# Epoch 1  global step 1860 loss 6.85830 batch 1860/3281 lr 0.001 accuracy 90.82422 wps 14311.10 step time 0.61s\n","# Epoch 1  global step 1880 loss 6.70987 batch 1880/3281 lr 0.001 accuracy 91.35156 wps 14715.94 step time 0.47s\n","# Epoch 1  global step 1900 loss 6.65845 batch 1900/3281 lr 0.001 accuracy 91.42969 wps 14207.43 step time 0.55s\n","# Epoch 1  global step 1920 loss 6.72874 batch 1920/3281 lr 0.001 accuracy 91.41016 wps 13772.84 step time 0.62s\n","# Epoch 1  global step 1940 loss 6.68350 batch 1940/3281 lr 0.001 accuracy 91.40234 wps 14942.87 step time 0.49s\n","# Epoch 1  global step 1960 loss 6.48258 batch 1960/3281 lr 0.001 accuracy 91.92578 wps 14928.54 step time 0.51s\n","# Epoch 1  global step 1980 loss 6.56350 batch 1980/3281 lr 0.001 accuracy 91.76172 wps 14625.33 step time 0.47s\n","# Epoch 1  global step 2000 loss 6.57656 batch 2000/3281 lr 0.001 accuracy 91.53516 wps 14503.12 step time 0.47s\n","# global step 2000, eval model at Sat May 23 06:32:06 2020\n","2020-05-23 06:32:08.979665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:32:08.980008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 06:32:08.980118: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 06:32:08.980145: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 06:32:08.980165: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 06:32:08.980196: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 06:32:08.980218: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 06:32:08.980237: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 06:32:08.980258: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 06:32:08.980347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:32:08.980682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:32:08.980880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 06:32:08.980961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 06:32:08.980976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 06:32:08.980986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 06:32:08.981100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:32:08.981369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:32:08.981606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","# location_traffic_convenience - 0.21969951788317077\n","# location_distance_from_business_district - 0.22255105060668837\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14272103658536586\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.24580502489558242\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.20128261477604187\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 10.81661, f1 0.20468\n","# current result -0.20467905965685596, previous best result 1000000000\n","# Epoch 1  global step 2020 loss 6.45842 batch 2020/3281 lr 0.001 accuracy 91.93359 wps 13946.05 step time 0.43s\n","# Epoch 1  global step 2040 loss 6.60310 batch 2040/3281 lr 0.001 accuracy 91.61719 wps 14089.47 step time 0.54s\n","# Epoch 1  global step 2060 loss 6.37147 batch 2060/3281 lr 0.001 accuracy 92.16797 wps 14484.93 step time 0.46s\n","# Epoch 1  global step 2080 loss 6.48459 batch 2080/3281 lr 0.001 accuracy 91.73047 wps 15164.70 step time 0.53s\n","# Epoch 1  global step 2100 loss 6.30251 batch 2100/3281 lr 0.001 accuracy 92.45703 wps 13936.36 step time 0.40s\n","# Epoch 1  global step 2120 loss 6.40348 batch 2120/3281 lr 0.001 accuracy 92.23437 wps 14148.69 step time 0.42s\n","# Epoch 1  global step 2140 loss 6.30527 batch 2140/3281 lr 0.001 accuracy 92.44922 wps 14133.34 step time 0.42s\n","# Epoch 1  global step 2160 loss 6.44361 batch 2160/3281 lr 0.001 accuracy 91.97656 wps 14804.35 step time 0.49s\n","# Epoch 1  global step 2180 loss 6.56582 batch 2180/3281 lr 0.001 accuracy 91.77344 wps 14376.29 step time 0.54s\n","# Epoch 1  global step 2200 loss 6.29333 batch 2200/3281 lr 0.001 accuracy 92.43359 wps 13349.04 step time 0.56s\n","# Epoch 1  global step 2220 loss 6.56375 batch 2220/3281 lr 0.001 accuracy 91.58984 wps 14490.89 step time 0.58s\n","# Epoch 1  global step 2240 loss 6.41965 batch 2240/3281 lr 0.001 accuracy 92.16406 wps 14645.78 step time 0.47s\n","# Epoch 1  global step 2260 loss 6.52253 batch 2260/3281 lr 0.001 accuracy 91.81641 wps 14738.22 step time 0.48s\n","# Epoch 1  global step 2280 loss 6.49795 batch 2280/3281 lr 0.001 accuracy 91.86719 wps 14415.95 step time 0.57s\n","# Epoch 1  global step 2300 loss 6.27431 batch 2300/3281 lr 0.001 accuracy 92.50000 wps 14470.02 step time 0.45s\n","# Epoch 1  global step 2320 loss 6.58821 batch 2320/3281 lr 0.001 accuracy 91.64063 wps 14860.09 step time 0.49s\n","# Epoch 1  global step 2340 loss 6.40011 batch 2340/3281 lr 0.001 accuracy 92.14063 wps 13852.48 step time 0.54s\n","# Epoch 1  global step 2360 loss 6.64533 batch 2360/3281 lr 0.001 accuracy 91.32812 wps 14824.53 step time 0.58s\n","# Epoch 1  global step 2380 loss 6.65675 batch 2380/3281 lr 0.001 accuracy 91.38672 wps 15353.90 step time 0.55s\n","# Epoch 1  global step 2400 loss 6.29445 batch 2400/3281 lr 0.001 accuracy 92.17578 wps 14735.82 step time 0.46s\n","# Epoch 1  global step 2420 loss 6.22569 batch 2420/3281 lr 0.001 accuracy 92.50000 wps 14253.06 step time 0.43s\n","# Epoch 1  global step 2440 loss 6.41051 batch 2440/3281 lr 0.001 accuracy 92.10938 wps 13876.74 step time 0.50s\n","# Epoch 1  global step 2460 loss 6.50023 batch 2460/3281 lr 0.001 accuracy 91.89453 wps 14511.54 step time 0.54s\n","# Epoch 1  global step 2480 loss 6.25293 batch 2480/3281 lr 0.001 accuracy 92.41406 wps 14442.86 step time 0.45s\n","# Epoch 1  global step 2500 loss 6.49370 batch 2500/3281 lr 0.001 accuracy 91.75781 wps 15266.76 step time 0.54s\n","# Epoch 1  global step 2520 loss 6.26795 batch 2520/3281 lr 0.001 accuracy 92.48828 wps 14225.52 step time 0.43s\n","# Epoch 1  global step 2540 loss 6.44642 batch 2540/3281 lr 0.001 accuracy 92.01172 wps 14814.54 step time 0.51s\n","# Epoch 1  global step 2560 loss 6.35795 batch 2560/3281 lr 0.001 accuracy 92.15234 wps 14197.28 step time 0.53s\n","# Epoch 1  global step 2580 loss 6.41756 batch 2580/3281 lr 0.001 accuracy 92.24609 wps 14464.24 step time 0.45s\n","# Epoch 1  global step 2600 loss 6.34849 batch 2600/3281 lr 0.001 accuracy 92.12500 wps 14657.32 step time 0.47s\n","# Epoch 1  global step 2620 loss 6.27510 batch 2620/3281 lr 0.001 accuracy 92.22266 wps 14119.34 step time 0.50s\n","# Epoch 1  global step 2640 loss 6.38522 batch 2640/3281 lr 0.001 accuracy 91.97266 wps 14523.89 step time 0.47s\n","# Epoch 1  global step 2660 loss 6.58039 batch 2660/3281 lr 0.001 accuracy 91.44922 wps 14598.93 step time 0.59s\n","# Epoch 1  global step 2680 loss 6.33423 batch 2680/3281 lr 0.001 accuracy 92.19141 wps 14614.86 step time 0.48s\n","# Epoch 1  global step 2700 loss 6.33734 batch 2700/3281 lr 0.001 accuracy 92.27734 wps 14053.55 step time 0.54s\n","# Epoch 1  global step 2720 loss 6.39324 batch 2720/3281 lr 0.001 accuracy 92.08984 wps 14446.37 step time 0.44s\n","# Epoch 1  global step 2740 loss 6.22955 batch 2740/3281 lr 0.001 accuracy 92.60156 wps 14266.78 step time 0.53s\n","# Epoch 1  global step 2760 loss 6.23386 batch 2760/3281 lr 0.001 accuracy 92.45313 wps 14248.94 step time 0.43s\n","# Epoch 1  global step 2780 loss 6.27396 batch 2780/3281 lr 0.001 accuracy 92.60156 wps 14913.37 step time 0.49s\n","# Epoch 1  global step 2800 loss 6.27236 batch 2800/3281 lr 0.001 accuracy 92.44141 wps 14627.16 step time 0.48s\n","# Epoch 1  global step 2820 loss 6.29376 batch 2820/3281 lr 0.001 accuracy 92.26172 wps 14137.28 step time 0.52s\n","# Epoch 1  global step 2840 loss 6.72332 batch 2840/3281 lr 0.001 accuracy 91.30469 wps 15053.06 step time 0.51s\n","# Epoch 1  global step 2860 loss 6.10271 batch 2860/3281 lr 0.001 accuracy 92.86719 wps 14222.31 step time 0.42s\n","# Epoch 1  global step 2880 loss 6.29506 batch 2880/3281 lr 0.001 accuracy 92.19922 wps 13823.38 step time 0.41s\n","# Epoch 1  global step 2900 loss 6.45392 batch 2900/3281 lr 0.001 accuracy 91.96094 wps 14058.70 step time 0.54s\n","# Epoch 1  global step 2920 loss 6.62529 batch 2920/3281 lr 0.001 accuracy 91.34766 wps 14769.00 step time 0.65s\n","# Epoch 1  global step 2940 loss 6.31632 batch 2940/3281 lr 0.001 accuracy 92.16016 wps 13159.79 step time 0.56s\n","# Epoch 1  global step 2960 loss 6.32862 batch 2960/3281 lr 0.001 accuracy 92.15234 wps 12561.66 step time 0.63s\n","# Epoch 1  global step 2980 loss 6.35293 batch 2980/3281 lr 0.001 accuracy 92.23828 wps 12850.83 step time 0.57s\n","# Epoch 1  global step 3000 loss 6.16250 batch 3000/3281 lr 0.001 accuracy 92.64062 wps 13009.40 step time 0.47s\n","# Epoch 1  global step 3020 loss 6.43393 batch 3020/3281 lr 0.001 accuracy 91.94141 wps 13479.90 step time 0.61s\n","# Epoch 1  global step 3040 loss 6.40675 batch 3040/3281 lr 0.001 accuracy 91.97266 wps 13237.04 step time 0.58s\n","# Epoch 1  global step 3060 loss 6.31250 batch 3060/3281 lr 0.001 accuracy 92.26562 wps 13287.95 step time 0.47s\n","# Epoch 1  global step 3080 loss 6.37910 batch 3080/3281 lr 0.001 accuracy 92.15234 wps 12776.08 step time 0.60s\n","# Epoch 1  global step 3100 loss 6.25167 batch 3100/3281 lr 0.001 accuracy 92.26172 wps 12232.80 step time 0.66s\n","# Epoch 1  global step 3120 loss 6.40920 batch 3120/3281 lr 0.001 accuracy 91.94922 wps 12053.79 step time 0.65s\n","# Epoch 1  global step 3140 loss 6.21780 batch 3140/3281 lr 0.001 accuracy 92.55469 wps 13098.60 step time 0.55s\n","# Epoch 1  global step 3160 loss 6.21908 batch 3160/3281 lr 0.001 accuracy 92.50781 wps 12539.55 step time 0.52s\n","# Epoch 1  global step 3180 loss 6.30103 batch 3180/3281 lr 0.001 accuracy 92.44141 wps 13352.24 step time 0.53s\n","# Epoch 1  global step 3200 loss 6.15018 batch 3200/3281 lr 0.001 accuracy 92.57031 wps 11879.95 step time 0.64s\n","# Epoch 1  global step 3220 loss 6.18824 batch 3220/3281 lr 0.001 accuracy 92.62109 wps 12011.06 step time 0.57s\n","# Epoch 1  global step 3240 loss 6.31945 batch 3240/3281 lr 0.001 accuracy 92.24219 wps 12670.41 step time 0.52s\n","# Epoch 1  global step 3260 loss 6.36704 batch 3260/3281 lr 0.001 accuracy 92.07812 wps 12698.51 step time 0.65s\n","# Epoch 1  global step 3280 loss 6.17233 batch 3280/3281 lr 0.001 accuracy 92.67187 wps 13015.78 step time 0.41s\n","# Finsh epoch 1, global step 3282\n","# Epoch 2  global step 3300 loss 5.83508 batch 18/3281 lr 0.001 accuracy 82.57422 wps 14232.81 step time 0.61s\n","# Epoch 2  global step 3320 loss 6.07945 batch 38/3281 lr 0.001 accuracy 92.71484 wps 14132.58 step time 0.43s\n","# Epoch 2  global step 3340 loss 6.15567 batch 58/3281 lr 0.001 accuracy 92.27734 wps 14807.85 step time 0.47s\n","# Epoch 2  global step 3360 loss 6.12697 batch 78/3281 lr 0.001 accuracy 92.78125 wps 13843.12 step time 0.51s\n","# Epoch 2  global step 3380 loss 6.10366 batch 98/3281 lr 0.001 accuracy 92.83203 wps 14271.85 step time 0.44s\n","# Epoch 2  global step 3400 loss 6.25658 batch 118/3281 lr 0.001 accuracy 92.26562 wps 14875.04 step time 0.51s\n","# Epoch 2  global step 3420 loss 6.13367 batch 138/3281 lr 0.001 accuracy 92.65234 wps 14586.88 step time 0.47s\n","# Epoch 2  global step 3440 loss 6.34100 batch 158/3281 lr 0.001 accuracy 92.16797 wps 15167.93 step time 0.54s\n","# Epoch 2  global step 3460 loss 6.19300 batch 178/3281 lr 0.001 accuracy 92.62109 wps 14152.52 step time 0.56s\n","# Epoch 2  global step 3480 loss 6.15058 batch 198/3281 lr 0.001 accuracy 92.75781 wps 14461.13 step time 0.46s\n","# Epoch 2  global step 3500 loss 6.34132 batch 218/3281 lr 0.001 accuracy 91.86719 wps 14625.08 step time 0.60s\n","# Epoch 2  global step 3520 loss 6.13349 batch 238/3281 lr 0.001 accuracy 92.68750 wps 14713.03 step time 0.49s\n","# Epoch 2  global step 3540 loss 6.23545 batch 258/3281 lr 0.001 accuracy 92.41797 wps 14114.78 step time 0.54s\n","# Epoch 2  global step 3560 loss 6.21812 batch 278/3281 lr 0.001 accuracy 92.45703 wps 14866.03 step time 0.49s\n","# Epoch 2  global step 3580 loss 6.42356 batch 298/3281 lr 0.001 accuracy 91.71875 wps 15046.73 step time 0.50s\n","# Epoch 2  global step 3600 loss 6.22188 batch 318/3281 lr 0.001 accuracy 92.37109 wps 14152.90 step time 0.63s\n","# Epoch 2  global step 3620 loss 6.41559 batch 338/3281 lr 0.001 accuracy 91.88281 wps 13775.83 step time 0.72s\n","# Epoch 2  global step 3640 loss 6.22580 batch 358/3281 lr 0.001 accuracy 92.51172 wps 14386.29 step time 0.54s\n","# Epoch 2  global step 3660 loss 6.04385 batch 378/3281 lr 0.001 accuracy 92.83594 wps 13872.61 step time 0.50s\n","# Epoch 2  global step 3680 loss 6.30203 batch 398/3281 lr 0.001 accuracy 92.33594 wps 14408.76 step time 0.56s\n","# Epoch 2  global step 3700 loss 5.95667 batch 418/3281 lr 0.001 accuracy 93.14453 wps 14118.20 step time 0.43s\n","# Epoch 2  global step 3720 loss 6.13616 batch 438/3281 lr 0.001 accuracy 92.84766 wps 13557.96 step time 0.49s\n","# Epoch 2  global step 3740 loss 6.09584 batch 458/3281 lr 0.001 accuracy 92.71094 wps 14155.69 step time 0.52s\n","# Epoch 2  global step 3760 loss 6.29249 batch 478/3281 lr 0.001 accuracy 92.36328 wps 14967.19 step time 0.51s\n","# Epoch 2  global step 3780 loss 6.17330 batch 498/3281 lr 0.001 accuracy 92.61328 wps 14421.56 step time 0.45s\n","# Epoch 2  global step 3800 loss 6.15219 batch 518/3281 lr 0.001 accuracy 92.76953 wps 14382.32 step time 0.44s\n","# Epoch 2  global step 3820 loss 6.16542 batch 538/3281 lr 0.001 accuracy 92.51953 wps 13811.79 step time 0.51s\n","# Epoch 2  global step 3840 loss 6.16983 batch 558/3281 lr 0.001 accuracy 92.52734 wps 14524.78 step time 0.46s\n","# Epoch 2  global step 3860 loss 6.18741 batch 578/3281 lr 0.001 accuracy 92.46484 wps 14536.56 step time 0.47s\n","# Epoch 2  global step 3880 loss 6.17028 batch 598/3281 lr 0.001 accuracy 92.66406 wps 15057.48 step time 0.52s\n","# Epoch 2  global step 3900 loss 6.10878 batch 618/3281 lr 0.001 accuracy 92.81641 wps 14081.23 step time 0.44s\n","# Epoch 2  global step 3920 loss 5.98694 batch 638/3281 lr 0.001 accuracy 92.96094 wps 14292.39 step time 0.43s\n","# Epoch 2  global step 3940 loss 6.19727 batch 658/3281 lr 0.001 accuracy 92.34766 wps 14256.44 step time 0.55s\n","# Epoch 2  global step 3960 loss 6.06826 batch 678/3281 lr 0.001 accuracy 92.56641 wps 14702.11 step time 0.50s\n","# Epoch 2  global step 3980 loss 6.06499 batch 698/3281 lr 0.001 accuracy 92.78125 wps 13977.97 step time 0.42s\n","# Epoch 2  global step 4000 loss 6.37203 batch 718/3281 lr 0.001 accuracy 91.94141 wps 14577.57 step time 0.57s\n","# global step 4000, eval model at Sat May 23 06:51:31 2020\n","2020-05-23 06:51:33.757873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:51:33.758273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 06:51:33.758403: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 06:51:33.758430: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 06:51:33.758470: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 06:51:33.758493: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 06:51:33.758514: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 06:51:33.758534: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 06:51:33.758556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 06:51:33.758645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:51:33.758903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:51:33.759102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 06:51:33.759433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 06:51:33.759465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 06:51:33.759476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 06:51:33.759635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:51:33.759893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 06:51:33.760109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.222905517865359\n","# location_distance_from_business_district - 0.22255105060668837\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14272103658536586\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.13236361878232783\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.21459460544079764\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 9.57416, f1 0.19983\n","# current result -0.19983288888354042, previous best result -0.20467905965685596\n","# Epoch 2  global step 4020 loss 6.11820 batch 738/3281 lr 0.001 accuracy 92.89453 wps 13893.51 step time 0.51s\n","# Epoch 2  global step 4040 loss 6.22282 batch 758/3281 lr 0.001 accuracy 92.11719 wps 14813.51 step time 0.51s\n","# Epoch 2  global step 4060 loss 6.19049 batch 778/3281 lr 0.001 accuracy 92.66797 wps 14420.93 step time 0.58s\n","# Epoch 2  global step 4080 loss 6.16315 batch 798/3281 lr 0.001 accuracy 92.46484 wps 15125.25 step time 0.53s\n","# Epoch 2  global step 4100 loss 6.05817 batch 818/3281 lr 0.001 accuracy 92.64844 wps 14713.48 step time 0.48s\n","# Epoch 2  global step 4120 loss 6.09247 batch 838/3281 lr 0.001 accuracy 92.69922 wps 14709.88 step time 0.47s\n","# Epoch 2  global step 4140 loss 6.05624 batch 858/3281 lr 0.001 accuracy 92.69141 wps 14763.77 step time 0.50s\n","# Epoch 2  global step 4160 loss 6.00633 batch 878/3281 lr 0.001 accuracy 92.97266 wps 13468.59 step time 0.48s\n","# Epoch 2  global step 4180 loss 6.14519 batch 898/3281 lr 0.001 accuracy 92.68750 wps 14211.15 step time 0.53s\n","# Epoch 2  global step 4200 loss 6.13652 batch 918/3281 lr 0.001 accuracy 92.51172 wps 14321.20 step time 0.54s\n","# Epoch 2  global step 4220 loss 6.17461 batch 938/3281 lr 0.001 accuracy 92.47266 wps 14815.72 step time 0.54s\n","# Epoch 2  global step 4240 loss 5.91989 batch 958/3281 lr 0.001 accuracy 93.13281 wps 13856.61 step time 0.40s\n","# Epoch 2  global step 4260 loss 6.01640 batch 978/3281 lr 0.001 accuracy 92.98438 wps 14558.08 step time 0.47s\n","# Epoch 2  global step 4280 loss 6.26697 batch 998/3281 lr 0.001 accuracy 92.44531 wps 14702.09 step time 0.58s\n","# Epoch 2  global step 4300 loss 6.43358 batch 1018/3281 lr 0.001 accuracy 91.72266 wps 15248.31 step time 0.57s\n","# Epoch 2  global step 4320 loss 6.03658 batch 1038/3281 lr 0.001 accuracy 92.91016 wps 14328.73 step time 0.43s\n","# Epoch 2  global step 4340 loss 6.08093 batch 1058/3281 lr 0.001 accuracy 92.82422 wps 14553.69 step time 0.46s\n","# Epoch 2  global step 4360 loss 6.16731 batch 1078/3281 lr 0.001 accuracy 92.34375 wps 14523.00 step time 0.47s\n","# Epoch 2  global step 4380 loss 6.12161 batch 1098/3281 lr 0.001 accuracy 92.65625 wps 15333.91 step time 0.54s\n","# Epoch 2  global step 4400 loss 6.13297 batch 1118/3281 lr 0.001 accuracy 92.71094 wps 13719.97 step time 0.59s\n","# Epoch 2  global step 4420 loss 6.29957 batch 1138/3281 lr 0.001 accuracy 92.21484 wps 14150.53 step time 0.68s\n","# Epoch 2  global step 4440 loss 6.14232 batch 1158/3281 lr 0.001 accuracy 92.55859 wps 15108.28 step time 0.52s\n","# Epoch 2  global step 4460 loss 6.18921 batch 1178/3281 lr 0.001 accuracy 92.28906 wps 14363.14 step time 0.45s\n","# Epoch 2  global step 4480 loss 6.14372 batch 1198/3281 lr 0.001 accuracy 92.62109 wps 14705.10 step time 0.47s\n","# Epoch 2  global step 4500 loss 6.17734 batch 1218/3281 lr 0.001 accuracy 92.50781 wps 14768.80 step time 0.50s\n","# Epoch 2  global step 4520 loss 6.07999 batch 1238/3281 lr 0.001 accuracy 92.82422 wps 14393.84 step time 0.44s\n","# Epoch 2  global step 4540 loss 6.10605 batch 1258/3281 lr 0.001 accuracy 92.62109 wps 15020.75 step time 0.51s\n","# Epoch 2  global step 4560 loss 6.09594 batch 1278/3281 lr 0.001 accuracy 92.68359 wps 14546.75 step time 0.46s\n","# Epoch 2  global step 4580 loss 6.06354 batch 1298/3281 lr 0.001 accuracy 92.98047 wps 14232.40 step time 0.42s\n","# Epoch 2  global step 4600 loss 6.09028 batch 1318/3281 lr 0.001 accuracy 92.79687 wps 14161.77 step time 0.54s\n","# Epoch 2  global step 4620 loss 6.24470 batch 1338/3281 lr 0.001 accuracy 92.47266 wps 14541.49 step time 0.46s\n","# Epoch 2  global step 4640 loss 5.98383 batch 1358/3281 lr 0.001 accuracy 92.91406 wps 14425.50 step time 0.44s\n","# Epoch 2  global step 4660 loss 5.96683 batch 1378/3281 lr 0.001 accuracy 93.21094 wps 13951.88 step time 0.41s\n","# Epoch 2  global step 4680 loss 6.10563 batch 1398/3281 lr 0.001 accuracy 92.78906 wps 13782.83 step time 0.70s\n","# Epoch 2  global step 4700 loss 6.14500 batch 1418/3281 lr 0.001 accuracy 92.45703 wps 15015.68 step time 0.54s\n","# Epoch 2  global step 4720 loss 6.18233 batch 1438/3281 lr 0.001 accuracy 92.39063 wps 14862.93 step time 0.50s\n","# Epoch 2  global step 4740 loss 6.11017 batch 1458/3281 lr 0.001 accuracy 92.64844 wps 14565.46 step time 0.46s\n","# Epoch 2  global step 4760 loss 6.16910 batch 1478/3281 lr 0.001 accuracy 92.60937 wps 14772.47 step time 0.48s\n","# Epoch 2  global step 4780 loss 6.13074 batch 1498/3281 lr 0.001 accuracy 92.65625 wps 14618.60 step time 0.46s\n","# Epoch 2  global step 4800 loss 6.00700 batch 1518/3281 lr 0.001 accuracy 93.03125 wps 14403.46 step time 0.45s\n","# Epoch 2  global step 4820 loss 6.04142 batch 1538/3281 lr 0.001 accuracy 92.83594 wps 14674.32 step time 0.47s\n","# Epoch 2  global step 4840 loss 6.19077 batch 1558/3281 lr 0.001 accuracy 92.35938 wps 14746.05 step time 0.56s\n","# Epoch 2  global step 4860 loss 6.14547 batch 1578/3281 lr 0.001 accuracy 92.57812 wps 14610.82 step time 0.46s\n","# Epoch 2  global step 4880 loss 6.00704 batch 1598/3281 lr 0.001 accuracy 92.91797 wps 13922.44 step time 0.53s\n","# Epoch 2  global step 4900 loss 6.15861 batch 1618/3281 lr 0.001 accuracy 92.46484 wps 14569.74 step time 0.47s\n","# Epoch 2  global step 4920 loss 6.12170 batch 1638/3281 lr 0.001 accuracy 92.76953 wps 13962.07 step time 0.50s\n","# Epoch 2  global step 4940 loss 6.20078 batch 1658/3281 lr 0.001 accuracy 92.57812 wps 14484.81 step time 0.56s\n","# Epoch 2  global step 4960 loss 6.08184 batch 1678/3281 lr 0.001 accuracy 92.69531 wps 14093.34 step time 0.54s\n","# Epoch 2  global step 4980 loss 6.00671 batch 1698/3281 lr 0.001 accuracy 93.05078 wps 14239.68 step time 0.43s\n","# Epoch 2  global step 5000 loss 6.19530 batch 1718/3281 lr 0.001 accuracy 92.48828 wps 14308.20 step time 0.56s\n","# Epoch 2  global step 5020 loss 6.10971 batch 1738/3281 lr 0.001 accuracy 92.78516 wps 13873.46 step time 0.52s\n","# Epoch 2  global step 5040 loss 6.02205 batch 1758/3281 lr 0.001 accuracy 93.05859 wps 14271.69 step time 0.45s\n","# Epoch 2  global step 5060 loss 5.86739 batch 1778/3281 lr 0.001 accuracy 93.45703 wps 13949.64 step time 0.41s\n","# Epoch 2  global step 5080 loss 6.07702 batch 1798/3281 lr 0.001 accuracy 92.58594 wps 14580.11 step time 0.47s\n","# Epoch 2  global step 5100 loss 6.17908 batch 1818/3281 lr 0.001 accuracy 92.53906 wps 14404.24 step time 0.46s\n","# Epoch 2  global step 5120 loss 6.11296 batch 1838/3281 lr 0.001 accuracy 92.67969 wps 14380.90 step time 0.55s\n","# Epoch 2  global step 5140 loss 6.04165 batch 1858/3281 lr 0.001 accuracy 92.83984 wps 14981.64 step time 0.52s\n","# Epoch 2  global step 5160 loss 5.97707 batch 1878/3281 lr 0.001 accuracy 92.95313 wps 14378.49 step time 0.46s\n","# Epoch 2  global step 5180 loss 5.97921 batch 1898/3281 lr 0.001 accuracy 92.92969 wps 14619.78 step time 0.47s\n","# Epoch 2  global step 5200 loss 6.11355 batch 1918/3281 lr 0.001 accuracy 92.55469 wps 13983.15 step time 0.59s\n","# Epoch 2  global step 5220 loss 6.20139 batch 1938/3281 lr 0.001 accuracy 92.37500 wps 13602.85 step time 0.68s\n","# Epoch 2  global step 5240 loss 6.28239 batch 1958/3281 lr 0.001 accuracy 92.03906 wps 14666.80 step time 0.60s\n","# Epoch 2  global step 5260 loss 5.99198 batch 1978/3281 lr 0.001 accuracy 92.91016 wps 14399.08 step time 0.44s\n","# Epoch 2  global step 5280 loss 5.95528 batch 1998/3281 lr 0.001 accuracy 93.10547 wps 14483.79 step time 0.46s\n","# Epoch 2  global step 5300 loss 6.09439 batch 2018/3281 lr 0.001 accuracy 92.74219 wps 14728.88 step time 0.47s\n","# Epoch 2  global step 5320 loss 6.06554 batch 2038/3281 lr 0.001 accuracy 92.61328 wps 14239.23 step time 0.60s\n","# Epoch 2  global step 5340 loss 6.31595 batch 2058/3281 lr 0.001 accuracy 92.08984 wps 15427.29 step time 0.58s\n","# Epoch 2  global step 5360 loss 6.09983 batch 2078/3281 lr 0.001 accuracy 92.72266 wps 12614.83 step time 0.56s\n","# Epoch 2  global step 5380 loss 6.01399 batch 2098/3281 lr 0.001 accuracy 92.92969 wps 12765.72 step time 0.57s\n","# Epoch 2  global step 5400 loss 5.91243 batch 2118/3281 lr 0.001 accuracy 93.12891 wps 12856.19 step time 0.49s\n","# Epoch 2  global step 5420 loss 6.09160 batch 2138/3281 lr 0.001 accuracy 92.71484 wps 12846.26 step time 0.52s\n","# Epoch 2  global step 5440 loss 5.81241 batch 2158/3281 lr 0.001 accuracy 93.44141 wps 12973.64 step time 0.51s\n","# Epoch 2  global step 5460 loss 6.05169 batch 2178/3281 lr 0.001 accuracy 92.75781 wps 13064.70 step time 0.49s\n","# Epoch 2  global step 5480 loss 6.00072 batch 2198/3281 lr 0.001 accuracy 92.82031 wps 12020.23 step time 0.66s\n","# Epoch 2  global step 5500 loss 6.06372 batch 2218/3281 lr 0.001 accuracy 92.83594 wps 12618.41 step time 0.59s\n","# Epoch 2  global step 5520 loss 6.05550 batch 2238/3281 lr 0.001 accuracy 92.85938 wps 13090.32 step time 0.52s\n","# Epoch 2  global step 5540 loss 6.03332 batch 2258/3281 lr 0.001 accuracy 92.90625 wps 12857.27 step time 0.60s\n","# Epoch 2  global step 5560 loss 5.88379 batch 2278/3281 lr 0.001 accuracy 93.19922 wps 11938.83 step time 0.57s\n","# Epoch 2  global step 5580 loss 6.02606 batch 2298/3281 lr 0.001 accuracy 92.68750 wps 12485.74 step time 0.54s\n","# Epoch 2  global step 5600 loss 6.00194 batch 2318/3281 lr 0.001 accuracy 93.08594 wps 12437.29 step time 0.56s\n","# Epoch 2  global step 5620 loss 5.99676 batch 2338/3281 lr 0.001 accuracy 92.94141 wps 12473.90 step time 0.54s\n","# Epoch 2  global step 5640 loss 6.09438 batch 2358/3281 lr 0.001 accuracy 92.73047 wps 12661.25 step time 0.56s\n","# Epoch 2  global step 5660 loss 6.11365 batch 2378/3281 lr 0.001 accuracy 92.60547 wps 12007.37 step time 0.69s\n","# Epoch 2  global step 5680 loss 5.97424 batch 2398/3281 lr 0.001 accuracy 93.11719 wps 12579.66 step time 0.47s\n","# Epoch 2  global step 5700 loss 6.17772 batch 2418/3281 lr 0.001 accuracy 92.69922 wps 11761.81 step time 0.65s\n","# Epoch 2  global step 5720 loss 6.06821 batch 2438/3281 lr 0.001 accuracy 92.80469 wps 12642.40 step time 0.56s\n","# Epoch 2  global step 5740 loss 6.04674 batch 2458/3281 lr 0.001 accuracy 92.67187 wps 13245.88 step time 0.55s\n","# Epoch 2  global step 5760 loss 6.01696 batch 2478/3281 lr 0.001 accuracy 92.81250 wps 12818.58 step time 0.51s\n","# Epoch 2  global step 5780 loss 6.21571 batch 2498/3281 lr 0.001 accuracy 92.08984 wps 12919.09 step time 0.60s\n","# Epoch 2  global step 5800 loss 5.95512 batch 2518/3281 lr 0.001 accuracy 93.16797 wps 12598.85 step time 0.51s\n","# Epoch 2  global step 5820 loss 5.93772 batch 2538/3281 lr 0.001 accuracy 93.25000 wps 12757.42 step time 0.52s\n","# Epoch 2  global step 5840 loss 6.12810 batch 2558/3281 lr 0.001 accuracy 92.54297 wps 12470.43 step time 0.53s\n","# Epoch 2  global step 5860 loss 6.01244 batch 2578/3281 lr 0.001 accuracy 92.98437 wps 12041.63 step time 0.58s\n","# Epoch 2  global step 5880 loss 6.05218 batch 2598/3281 lr 0.001 accuracy 92.71875 wps 13478.67 step time 0.54s\n","# Epoch 2  global step 5900 loss 6.02812 batch 2618/3281 lr 0.001 accuracy 92.93359 wps 12743.38 step time 0.54s\n","# Epoch 2  global step 5920 loss 5.96665 batch 2638/3281 lr 0.001 accuracy 93.00781 wps 12564.53 step time 0.50s\n","# Epoch 2  global step 5940 loss 6.01156 batch 2658/3281 lr 0.001 accuracy 92.98047 wps 12085.07 step time 0.63s\n","# Epoch 2  global step 5960 loss 6.02979 batch 2678/3281 lr 0.001 accuracy 93.04687 wps 12591.14 step time 0.47s\n","# Epoch 2  global step 5980 loss 6.10509 batch 2698/3281 lr 0.001 accuracy 92.77734 wps 12124.17 step time 0.64s\n","# Epoch 2  global step 6000 loss 6.02509 batch 2718/3281 lr 0.001 accuracy 92.91016 wps 12845.70 step time 0.61s\n","# global step 6000, eval model at Sat May 23 07:11:13 2020\n","2020-05-23 07:11:15.076469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:11:15.076927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 07:11:15.077052: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 07:11:15.077084: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 07:11:15.077110: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 07:11:15.077135: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 07:11:15.077165: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 07:11:15.077189: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 07:11:15.077213: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 07:11:15.077318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:11:15.077675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:11:15.078005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 07:11:15.078355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 07:11:15.078384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 07:11:15.078398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 07:11:15.078589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:11:15.078881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:11:15.079120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.2293853858728842\n","# location_distance_from_business_district - 0.22255105060668837\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14272103658536586\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.18827635053774494\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.43581360319064094\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 8.56719, f1 0.21401\n","# current result -0.2140134687591797, previous best result -0.20467905965685596\n","# Epoch 2  global step 6020 loss 6.08691 batch 2738/3281 lr 0.001 accuracy 92.76172 wps 12334.63 step time 0.57s\n","# Epoch 2  global step 6040 loss 6.18167 batch 2758/3281 lr 0.001 accuracy 92.62891 wps 12951.17 step time 0.66s\n","# Epoch 2  global step 6060 loss 5.96428 batch 2778/3281 lr 0.001 accuracy 93.21094 wps 13407.84 step time 0.50s\n","# Epoch 2  global step 6080 loss 6.02132 batch 2798/3281 lr 0.001 accuracy 92.86328 wps 12912.88 step time 0.60s\n","# Epoch 2  global step 6100 loss 5.88030 batch 2818/3281 lr 0.001 accuracy 93.42187 wps 12890.23 step time 0.46s\n","# Epoch 2  global step 6120 loss 6.01979 batch 2838/3281 lr 0.001 accuracy 93.04687 wps 13148.47 step time 0.58s\n","# Epoch 2  global step 6140 loss 6.06103 batch 2858/3281 lr 0.001 accuracy 92.71484 wps 11625.66 step time 0.71s\n","# Epoch 2  global step 6160 loss 6.10124 batch 2878/3281 lr 0.001 accuracy 92.69531 wps 12678.98 step time 0.58s\n","# Epoch 2  global step 6180 loss 6.21728 batch 2898/3281 lr 0.001 accuracy 92.47656 wps 12342.16 step time 0.63s\n","# Epoch 2  global step 6200 loss 6.05834 batch 2918/3281 lr 0.001 accuracy 92.79297 wps 12537.68 step time 0.56s\n","# Epoch 2  global step 6220 loss 5.87432 batch 2938/3281 lr 0.001 accuracy 93.16016 wps 12787.38 step time 0.45s\n","# Epoch 2  global step 6240 loss 6.02939 batch 2958/3281 lr 0.001 accuracy 92.91797 wps 13042.32 step time 0.51s\n","# Epoch 2  global step 6260 loss 6.11218 batch 2978/3281 lr 0.001 accuracy 92.79297 wps 12700.36 step time 0.59s\n","# Epoch 2  global step 6280 loss 6.14408 batch 2998/3281 lr 0.001 accuracy 92.51953 wps 13273.77 step time 0.57s\n","# Epoch 2  global step 6300 loss 6.01567 batch 3018/3281 lr 0.001 accuracy 92.88281 wps 12861.20 step time 0.54s\n","# Epoch 2  global step 6320 loss 6.14724 batch 3038/3281 lr 0.001 accuracy 92.50391 wps 12472.42 step time 0.66s\n","# Epoch 2  global step 6340 loss 6.02548 batch 3058/3281 lr 0.001 accuracy 93.00781 wps 13204.86 step time 0.54s\n","# Epoch 2  global step 6360 loss 6.03803 batch 3078/3281 lr 0.001 accuracy 92.92969 wps 12122.59 step time 0.61s\n","# Epoch 2  global step 6380 loss 6.09420 batch 3098/3281 lr 0.001 accuracy 92.62891 wps 11909.19 step time 0.57s\n","# Epoch 2  global step 6400 loss 5.86243 batch 3118/3281 lr 0.001 accuracy 93.41016 wps 12732.24 step time 0.51s\n","# Epoch 2  global step 6420 loss 6.06998 batch 3138/3281 lr 0.001 accuracy 92.73438 wps 11998.54 step time 0.61s\n","# Epoch 2  global step 6440 loss 5.99201 batch 3158/3281 lr 0.001 accuracy 92.77344 wps 12751.48 step time 0.57s\n","# Epoch 2  global step 6460 loss 6.11959 batch 3178/3281 lr 0.001 accuracy 92.56250 wps 12819.50 step time 0.59s\n","# Epoch 2  global step 6480 loss 6.01810 batch 3198/3281 lr 0.001 accuracy 92.91406 wps 13305.03 step time 0.55s\n","# Epoch 2  global step 6500 loss 6.04687 batch 3218/3281 lr 0.001 accuracy 92.76562 wps 12320.27 step time 0.60s\n","# Epoch 2  global step 6520 loss 6.07754 batch 3238/3281 lr 0.001 accuracy 92.79687 wps 12785.57 step time 0.61s\n","# Epoch 2  global step 6540 loss 6.01007 batch 3258/3281 lr 0.001 accuracy 93.10156 wps 13253.86 step time 0.55s\n","# Epoch 2  global step 6560 loss 6.18866 batch 3278/3281 lr 0.001 accuracy 92.65625 wps 12678.97 step time 0.60s\n","# Finsh epoch 2, global step 6564\n","# Epoch 3  global step 6580 loss 4.77960 batch 16/3281 lr 0.001 accuracy 74.23828 wps 14598.92 step time 0.45s\n","# Epoch 3  global step 6600 loss 5.85041 batch 36/3281 lr 0.001 accuracy 93.20312 wps 14767.41 step time 0.49s\n","# Epoch 3  global step 6620 loss 5.84908 batch 56/3281 lr 0.001 accuracy 93.21094 wps 14219.48 step time 0.43s\n","# Epoch 3  global step 6640 loss 5.80662 batch 76/3281 lr 0.001 accuracy 93.40234 wps 14004.29 step time 0.42s\n","# Epoch 3  global step 6660 loss 6.07780 batch 96/3281 lr 0.001 accuracy 92.70703 wps 14297.17 step time 0.53s\n","# Epoch 3  global step 6680 loss 5.95939 batch 116/3281 lr 0.001 accuracy 92.97656 wps 14283.30 step time 0.56s\n","# Epoch 3  global step 6700 loss 5.83018 batch 136/3281 lr 0.001 accuracy 93.20313 wps 14472.31 step time 0.46s\n","# Epoch 3  global step 6720 loss 5.90744 batch 156/3281 lr 0.001 accuracy 93.03125 wps 14604.76 step time 0.47s\n","# Epoch 3  global step 6740 loss 5.96941 batch 176/3281 lr 0.001 accuracy 92.97656 wps 14627.62 step time 0.47s\n","# Epoch 3  global step 6760 loss 5.90603 batch 196/3281 lr 0.001 accuracy 93.17969 wps 14818.31 step time 0.49s\n","# Epoch 3  global step 6780 loss 5.72577 batch 216/3281 lr 0.001 accuracy 93.58594 wps 14137.51 step time 0.42s\n","# Epoch 3  global step 6800 loss 5.90801 batch 236/3281 lr 0.001 accuracy 93.16016 wps 14155.55 step time 0.53s\n","# Epoch 3  global step 6820 loss 5.69895 batch 256/3281 lr 0.001 accuracy 93.66406 wps 14532.47 step time 0.46s\n","# Epoch 3  global step 6840 loss 5.81005 batch 276/3281 lr 0.001 accuracy 93.40625 wps 13915.04 step time 0.51s\n","# Epoch 3  global step 6860 loss 6.16119 batch 296/3281 lr 0.001 accuracy 92.77344 wps 14679.48 step time 0.58s\n","# Epoch 3  global step 6880 loss 5.84130 batch 316/3281 lr 0.001 accuracy 93.19922 wps 14625.77 step time 0.50s\n","# Epoch 3  global step 6900 loss 5.95614 batch 336/3281 lr 0.001 accuracy 93.06641 wps 14977.00 step time 0.54s\n","# Epoch 3  global step 6920 loss 5.86284 batch 356/3281 lr 0.001 accuracy 93.40625 wps 14072.07 step time 0.41s\n","# Epoch 3  global step 6940 loss 5.88028 batch 376/3281 lr 0.001 accuracy 93.04297 wps 14561.24 step time 0.45s\n","# Epoch 3  global step 6960 loss 5.66821 batch 396/3281 lr 0.001 accuracy 93.80078 wps 13931.33 step time 0.41s\n","# Epoch 3  global step 6980 loss 5.97756 batch 416/3281 lr 0.001 accuracy 92.82031 wps 14721.08 step time 0.48s\n","# Epoch 3  global step 7000 loss 5.98400 batch 436/3281 lr 0.001 accuracy 92.94531 wps 14704.82 step time 0.47s\n","# Epoch 3  global step 7020 loss 5.99727 batch 456/3281 lr 0.001 accuracy 92.92969 wps 14657.39 step time 0.47s\n","# Epoch 3  global step 7040 loss 5.98465 batch 476/3281 lr 0.001 accuracy 92.69531 wps 13970.27 step time 0.63s\n","# Epoch 3  global step 7060 loss 5.88488 batch 496/3281 lr 0.001 accuracy 93.24219 wps 14275.95 step time 0.43s\n","# Epoch 3  global step 7080 loss 5.90154 batch 516/3281 lr 0.001 accuracy 93.03516 wps 14477.94 step time 0.45s\n","# Epoch 3  global step 7100 loss 5.98240 batch 536/3281 lr 0.001 accuracy 92.90625 wps 13667.23 step time 0.67s\n","# Epoch 3  global step 7120 loss 5.84656 batch 556/3281 lr 0.001 accuracy 93.22656 wps 13905.74 step time 0.50s\n","# Epoch 3  global step 7140 loss 6.02278 batch 576/3281 lr 0.001 accuracy 92.84766 wps 14619.18 step time 0.48s\n","# Epoch 3  global step 7160 loss 5.73967 batch 596/3281 lr 0.001 accuracy 93.60156 wps 14007.12 step time 0.42s\n","# Epoch 3  global step 7180 loss 5.93397 batch 616/3281 lr 0.001 accuracy 93.17969 wps 14454.03 step time 0.45s\n","# Epoch 3  global step 7200 loss 5.98280 batch 636/3281 lr 0.001 accuracy 93.12109 wps 13954.70 step time 0.52s\n","# Epoch 3  global step 7220 loss 5.84638 batch 656/3281 lr 0.001 accuracy 93.20312 wps 14614.23 step time 0.46s\n","# Epoch 3  global step 7240 loss 5.85976 batch 676/3281 lr 0.001 accuracy 93.19531 wps 14529.89 step time 0.47s\n","# Epoch 3  global step 7260 loss 5.98163 batch 696/3281 lr 0.001 accuracy 92.76953 wps 14031.88 step time 0.51s\n","# Epoch 3  global step 7280 loss 5.96732 batch 716/3281 lr 0.001 accuracy 93.06641 wps 13552.82 step time 0.59s\n","# Epoch 3  global step 7300 loss 5.90782 batch 736/3281 lr 0.001 accuracy 93.31641 wps 13859.03 step time 0.51s\n","# Epoch 3  global step 7320 loss 5.92971 batch 756/3281 lr 0.001 accuracy 93.23828 wps 14692.91 step time 0.48s\n","# Epoch 3  global step 7340 loss 6.05201 batch 776/3281 lr 0.001 accuracy 92.73828 wps 15171.89 step time 0.55s\n","# Epoch 3  global step 7360 loss 5.97596 batch 796/3281 lr 0.001 accuracy 92.81250 wps 15009.62 step time 0.50s\n","# Epoch 3  global step 7380 loss 5.95487 batch 816/3281 lr 0.001 accuracy 93.04297 wps 14152.51 step time 0.53s\n","# Epoch 3  global step 7400 loss 5.84431 batch 836/3281 lr 0.001 accuracy 93.09375 wps 14120.96 step time 0.42s\n","# Epoch 3  global step 7420 loss 6.04339 batch 856/3281 lr 0.001 accuracy 92.66797 wps 13652.92 step time 0.61s\n","# Epoch 3  global step 7440 loss 5.89018 batch 876/3281 lr 0.001 accuracy 93.12891 wps 14177.71 step time 0.43s\n","# Epoch 3  global step 7460 loss 6.14414 batch 896/3281 lr 0.001 accuracy 92.35547 wps 15278.73 step time 0.55s\n","# Epoch 3  global step 7480 loss 5.97732 batch 916/3281 lr 0.001 accuracy 93.07422 wps 13928.58 step time 0.51s\n","# Epoch 3  global step 7500 loss 5.97701 batch 936/3281 lr 0.001 accuracy 92.80859 wps 14609.41 step time 0.46s\n","# Epoch 3  global step 7520 loss 5.93773 batch 956/3281 lr 0.001 accuracy 93.19922 wps 14344.41 step time 0.43s\n","# Epoch 3  global step 7540 loss 5.85464 batch 976/3281 lr 0.001 accuracy 93.19141 wps 14330.03 step time 0.43s\n","# Epoch 3  global step 7560 loss 5.94029 batch 996/3281 lr 0.001 accuracy 93.02734 wps 14734.46 step time 0.47s\n","# Epoch 3  global step 7580 loss 5.84799 batch 1016/3281 lr 0.001 accuracy 93.39062 wps 14098.07 step time 0.52s\n","# Epoch 3  global step 7600 loss 5.93673 batch 1036/3281 lr 0.001 accuracy 93.05469 wps 15011.73 step time 0.51s\n","# Epoch 3  global step 7620 loss 6.16072 batch 1056/3281 lr 0.001 accuracy 92.53516 wps 13843.87 step time 0.59s\n","# Epoch 3  global step 7640 loss 6.02710 batch 1076/3281 lr 0.001 accuracy 92.88672 wps 13341.85 step time 0.50s\n","# Epoch 3  global step 7660 loss 5.98225 batch 1096/3281 lr 0.001 accuracy 93.01953 wps 12137.58 step time 0.63s\n","# Epoch 3  global step 7680 loss 6.00206 batch 1116/3281 lr 0.001 accuracy 92.82031 wps 12820.86 step time 0.63s\n","# Epoch 3  global step 7700 loss 5.87235 batch 1136/3281 lr 0.001 accuracy 93.34766 wps 12349.81 step time 0.57s\n","# Epoch 3  global step 7720 loss 5.87170 batch 1156/3281 lr 0.001 accuracy 93.41406 wps 13130.07 step time 0.57s\n","# Epoch 3  global step 7740 loss 5.77022 batch 1176/3281 lr 0.001 accuracy 93.38672 wps 12459.25 step time 0.48s\n","# Epoch 3  global step 7760 loss 5.92029 batch 1196/3281 lr 0.001 accuracy 93.14844 wps 12972.75 step time 0.53s\n","# Epoch 3  global step 7780 loss 5.99228 batch 1216/3281 lr 0.001 accuracy 92.73828 wps 13608.13 step time 0.61s\n","# Epoch 3  global step 7800 loss 5.77366 batch 1236/3281 lr 0.001 accuracy 93.54688 wps 12858.66 step time 0.47s\n","# Epoch 3  global step 7820 loss 5.81298 batch 1256/3281 lr 0.001 accuracy 93.38672 wps 12661.03 step time 0.56s\n","# Epoch 3  global step 7840 loss 6.08875 batch 1276/3281 lr 0.001 accuracy 92.73047 wps 11936.30 step time 0.60s\n","# Epoch 3  global step 7860 loss 5.66923 batch 1296/3281 lr 0.001 accuracy 93.82422 wps 12584.26 step time 0.45s\n","# Epoch 3  global step 7880 loss 5.86872 batch 1316/3281 lr 0.001 accuracy 93.26172 wps 12729.86 step time 0.62s\n","# Epoch 3  global step 7900 loss 5.83884 batch 1336/3281 lr 0.001 accuracy 93.35547 wps 12267.98 step time 0.61s\n","# Epoch 3  global step 7920 loss 5.88000 batch 1356/3281 lr 0.001 accuracy 93.17969 wps 12923.74 step time 0.61s\n","# Epoch 3  global step 7940 loss 5.88112 batch 1376/3281 lr 0.001 accuracy 93.11328 wps 13029.65 step time 0.54s\n","# Epoch 3  global step 7960 loss 6.19848 batch 1396/3281 lr 0.001 accuracy 92.36719 wps 12506.26 step time 0.83s\n","# Epoch 3  global step 7980 loss 5.99759 batch 1416/3281 lr 0.001 accuracy 92.82813 wps 13746.55 step time 0.55s\n","# Epoch 3  global step 8000 loss 6.05175 batch 1436/3281 lr 0.001 accuracy 92.79688 wps 11906.93 step time 0.72s\n","# global step 8000, eval model at Sat May 23 07:31:17 2020\n","2020-05-23 07:31:19.928050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:31:19.928692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 07:31:19.928810: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 07:31:19.928829: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 07:31:19.928844: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 07:31:19.928860: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 07:31:19.928880: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 07:31:19.928895: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 07:31:19.928911: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 07:31:19.929001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:31:19.929386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:31:19.929677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 07:31:19.930006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 07:31:19.930026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 07:31:19.930035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 07:31:19.930170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:31:19.930440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:31:19.930682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.22219629340953675\n","# location_distance_from_business_district - 0.22255105060668837\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14365095191562716\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19106622505226092\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.2752961176844626\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.5183088169955142\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 7.87117, f1 0.22219\n","# current result -0.22218583926544982, previous best result -0.2140134687591797\n","# Epoch 3  global step 8020 loss 6.04205 batch 1456/3281 lr 0.001 accuracy 92.70703 wps 12925.59 step time 0.61s\n","# Epoch 3  global step 8040 loss 6.02599 batch 1476/3281 lr 0.001 accuracy 92.85938 wps 12634.38 step time 0.58s\n","# Epoch 3  global step 8060 loss 5.98819 batch 1496/3281 lr 0.001 accuracy 92.88672 wps 13211.40 step time 0.47s\n","# Epoch 3  global step 8080 loss 5.93517 batch 1516/3281 lr 0.001 accuracy 93.00391 wps 12257.49 step time 0.67s\n","# Epoch 3  global step 8100 loss 5.98403 batch 1536/3281 lr 0.001 accuracy 92.89453 wps 13103.67 step time 0.50s\n","# Epoch 3  global step 8120 loss 6.01449 batch 1556/3281 lr 0.001 accuracy 92.76953 wps 11787.75 step time 0.66s\n","# Epoch 3  global step 8140 loss 5.84175 batch 1576/3281 lr 0.001 accuracy 93.21484 wps 12639.56 step time 0.49s\n","# Epoch 3  global step 8160 loss 5.99606 batch 1596/3281 lr 0.001 accuracy 92.98047 wps 12430.17 step time 0.63s\n","# Epoch 3  global step 8180 loss 5.87665 batch 1616/3281 lr 0.001 accuracy 93.11719 wps 12072.14 step time 0.56s\n","# Epoch 3  global step 8200 loss 5.98174 batch 1636/3281 lr 0.001 accuracy 92.91016 wps 13602.28 step time 0.54s\n","# Epoch 3  global step 8220 loss 5.91503 batch 1656/3281 lr 0.001 accuracy 92.92969 wps 12670.97 step time 0.61s\n","# Epoch 3  global step 8240 loss 5.80978 batch 1676/3281 lr 0.001 accuracy 93.41797 wps 12821.72 step time 0.54s\n","# Epoch 3  global step 8260 loss 5.90825 batch 1696/3281 lr 0.001 accuracy 93.02344 wps 13084.50 step time 0.50s\n","# Epoch 3  global step 8280 loss 6.04891 batch 1716/3281 lr 0.001 accuracy 92.71875 wps 13692.14 step time 0.67s\n","# Epoch 3  global step 8300 loss 6.03826 batch 1736/3281 lr 0.001 accuracy 92.57422 wps 12050.75 step time 0.75s\n","# Epoch 3  global step 8320 loss 5.85349 batch 1756/3281 lr 0.001 accuracy 93.17578 wps 12714.99 step time 0.58s\n","# Epoch 3  global step 8340 loss 5.87809 batch 1776/3281 lr 0.001 accuracy 93.17969 wps 12688.04 step time 0.60s\n","# Epoch 3  global step 8360 loss 5.86335 batch 1796/3281 lr 0.001 accuracy 93.17969 wps 12640.83 step time 0.57s\n","# Epoch 3  global step 8380 loss 5.93616 batch 1816/3281 lr 0.001 accuracy 93.09375 wps 12257.01 step time 0.61s\n","# Epoch 3  global step 8400 loss 5.84538 batch 1836/3281 lr 0.001 accuracy 93.41406 wps 12781.43 step time 0.52s\n","# Epoch 3  global step 8420 loss 5.84381 batch 1856/3281 lr 0.001 accuracy 93.35937 wps 12272.41 step time 0.64s\n","# Epoch 3  global step 8440 loss 5.88579 batch 1876/3281 lr 0.001 accuracy 93.22266 wps 12820.95 step time 0.48s\n","# Epoch 3  global step 8460 loss 5.97149 batch 1896/3281 lr 0.001 accuracy 92.92969 wps 13113.78 step time 0.55s\n","# Epoch 3  global step 8480 loss 5.97678 batch 1916/3281 lr 0.001 accuracy 93.08203 wps 12749.83 step time 0.54s\n","# Epoch 3  global step 8500 loss 5.91168 batch 1936/3281 lr 0.001 accuracy 93.21875 wps 11799.70 step time 0.61s\n","# Epoch 3  global step 8520 loss 6.05474 batch 1956/3281 lr 0.001 accuracy 92.76562 wps 12758.64 step time 0.54s\n","# Epoch 3  global step 8540 loss 6.00043 batch 1976/3281 lr 0.001 accuracy 92.83203 wps 12138.60 step time 0.67s\n","# Epoch 3  global step 8560 loss 5.72813 batch 1996/3281 lr 0.001 accuracy 93.63672 wps 13003.89 step time 0.45s\n","# Epoch 3  global step 8580 loss 6.09079 batch 2016/3281 lr 0.001 accuracy 92.62500 wps 13121.15 step time 0.56s\n","# Epoch 3  global step 8600 loss 6.01873 batch 2036/3281 lr 0.001 accuracy 92.82812 wps 12263.26 step time 0.68s\n","# Epoch 3  global step 8620 loss 5.92934 batch 2056/3281 lr 0.001 accuracy 92.90625 wps 12548.60 step time 0.53s\n","# Epoch 3  global step 8640 loss 5.96004 batch 2076/3281 lr 0.001 accuracy 92.93750 wps 11756.53 step time 0.77s\n","# Epoch 3  global step 8660 loss 5.91129 batch 2096/3281 lr 0.001 accuracy 93.04688 wps 13227.98 step time 0.56s\n","# Epoch 3  global step 8680 loss 5.77757 batch 2116/3281 lr 0.001 accuracy 93.38672 wps 12281.97 step time 0.52s\n","# Epoch 3  global step 8700 loss 5.92718 batch 2136/3281 lr 0.001 accuracy 93.15625 wps 12590.80 step time 0.51s\n","# Epoch 3  global step 8720 loss 5.99171 batch 2156/3281 lr 0.001 accuracy 92.85938 wps 12666.55 step time 0.60s\n","# Epoch 3  global step 8740 loss 6.00967 batch 2176/3281 lr 0.001 accuracy 92.85938 wps 12484.79 step time 0.71s\n","# Epoch 3  global step 8760 loss 6.00454 batch 2196/3281 lr 0.001 accuracy 92.92578 wps 13070.13 step time 0.58s\n","# Epoch 3  global step 8780 loss 5.95939 batch 2216/3281 lr 0.001 accuracy 93.07031 wps 12948.26 step time 0.57s\n","# Epoch 3  global step 8800 loss 5.81373 batch 2236/3281 lr 0.001 accuracy 93.46484 wps 12075.02 step time 0.58s\n","# Epoch 3  global step 8820 loss 6.19676 batch 2256/3281 lr 0.001 accuracy 92.26172 wps 13029.82 step time 0.70s\n","# Epoch 3  global step 8840 loss 5.81411 batch 2276/3281 lr 0.001 accuracy 93.42969 wps 12906.54 step time 0.49s\n","# Epoch 3  global step 8860 loss 5.90458 batch 2296/3281 lr 0.001 accuracy 93.06250 wps 12772.71 step time 0.58s\n","# Epoch 3  global step 8880 loss 5.78636 batch 2316/3281 lr 0.001 accuracy 93.32422 wps 11626.80 step time 0.61s\n","# Epoch 3  global step 8900 loss 5.88902 batch 2336/3281 lr 0.001 accuracy 93.24609 wps 12111.73 step time 0.59s\n","# Epoch 3  global step 8920 loss 5.85860 batch 2356/3281 lr 0.001 accuracy 93.26172 wps 12791.06 step time 0.51s\n","# Epoch 3  global step 8940 loss 5.80582 batch 2376/3281 lr 0.001 accuracy 93.39844 wps 11865.75 step time 0.66s\n","# Epoch 3  global step 8960 loss 5.84072 batch 2396/3281 lr 0.001 accuracy 93.21094 wps 12763.21 step time 0.51s\n","# Epoch 3  global step 8980 loss 6.02155 batch 2416/3281 lr 0.001 accuracy 92.67578 wps 11413.62 step time 0.73s\n","# Epoch 3  global step 9000 loss 5.78518 batch 2436/3281 lr 0.001 accuracy 93.42969 wps 12585.31 step time 0.48s\n","# Epoch 3  global step 9020 loss 5.89589 batch 2456/3281 lr 0.001 accuracy 93.16016 wps 12925.67 step time 0.56s\n","# Epoch 3  global step 9040 loss 5.97747 batch 2476/3281 lr 0.001 accuracy 92.98047 wps 12327.12 step time 0.57s\n","# Epoch 3  global step 9060 loss 5.97025 batch 2496/3281 lr 0.001 accuracy 92.85547 wps 12336.94 step time 0.61s\n","# Epoch 3  global step 9080 loss 6.02772 batch 2516/3281 lr 0.001 accuracy 92.76562 wps 12959.29 step time 0.73s\n","# Epoch 3  global step 9100 loss 6.00871 batch 2536/3281 lr 0.001 accuracy 92.77734 wps 12574.32 step time 0.68s\n","# Epoch 3  global step 9120 loss 5.96791 batch 2556/3281 lr 0.001 accuracy 93.00000 wps 11736.40 step time 0.69s\n","# Epoch 3  global step 9140 loss 6.18616 batch 2576/3281 lr 0.001 accuracy 92.48438 wps 13000.06 step time 0.59s\n","# Epoch 3  global step 9160 loss 5.82140 batch 2596/3281 lr 0.001 accuracy 93.39844 wps 13020.05 step time 0.56s\n","# Epoch 3  global step 9180 loss 5.87366 batch 2616/3281 lr 0.001 accuracy 93.07031 wps 12971.14 step time 0.55s\n","# Epoch 3  global step 9200 loss 5.86968 batch 2636/3281 lr 0.001 accuracy 93.33984 wps 12729.17 step time 0.49s\n","# Epoch 3  global step 9220 loss 6.02413 batch 2656/3281 lr 0.001 accuracy 92.93750 wps 12243.28 step time 0.58s\n","# Epoch 3  global step 9240 loss 5.68551 batch 2676/3281 lr 0.001 accuracy 93.72656 wps 12728.86 step time 0.43s\n","# Epoch 3  global step 9260 loss 6.01499 batch 2696/3281 lr 0.001 accuracy 93.01172 wps 13100.18 step time 0.55s\n","# Epoch 3  global step 9280 loss 5.87865 batch 2716/3281 lr 0.001 accuracy 93.15625 wps 12570.19 step time 0.57s\n","# Epoch 3  global step 9300 loss 5.88446 batch 2736/3281 lr 0.001 accuracy 93.31641 wps 12583.27 step time 0.56s\n","# Epoch 3  global step 9320 loss 6.22716 batch 2756/3281 lr 0.001 accuracy 92.39063 wps 12947.10 step time 0.74s\n","# Epoch 3  global step 9340 loss 5.90152 batch 2776/3281 lr 0.001 accuracy 93.12109 wps 11801.65 step time 0.65s\n","# Epoch 3  global step 9360 loss 5.97796 batch 2796/3281 lr 0.001 accuracy 92.88672 wps 12926.21 step time 0.67s\n","# Epoch 3  global step 9380 loss 5.88132 batch 2816/3281 lr 0.001 accuracy 93.08594 wps 12899.90 step time 0.57s\n","# Epoch 3  global step 9400 loss 5.95377 batch 2836/3281 lr 0.001 accuracy 92.94922 wps 13293.49 step time 0.57s\n","# Epoch 3  global step 9420 loss 5.94405 batch 2856/3281 lr 0.001 accuracy 92.97266 wps 12949.48 step time 0.56s\n","# Epoch 3  global step 9440 loss 5.83114 batch 2876/3281 lr 0.001 accuracy 93.36719 wps 12930.69 step time 0.50s\n","# Epoch 3  global step 9460 loss 5.99089 batch 2896/3281 lr 0.001 accuracy 92.73438 wps 12358.38 step time 0.70s\n","# Epoch 3  global step 9480 loss 5.85485 batch 2916/3281 lr 0.001 accuracy 93.40234 wps 12629.90 step time 0.49s\n","# Epoch 3  global step 9500 loss 5.81826 batch 2936/3281 lr 0.001 accuracy 93.42578 wps 13228.55 step time 0.45s\n","# Epoch 3  global step 9520 loss 5.87435 batch 2956/3281 lr 0.001 accuracy 93.25391 wps 11595.61 step time 0.65s\n","# Epoch 3  global step 9540 loss 6.04860 batch 2976/3281 lr 0.001 accuracy 92.88672 wps 12930.55 step time 0.54s\n","# Epoch 3  global step 9560 loss 5.93782 batch 2996/3281 lr 0.001 accuracy 92.87500 wps 12980.47 step time 0.50s\n","# Epoch 3  global step 9580 loss 5.74298 batch 3016/3281 lr 0.001 accuracy 93.47656 wps 12629.21 step time 0.53s\n","# Epoch 3  global step 9600 loss 5.89234 batch 3036/3281 lr 0.001 accuracy 93.39844 wps 12494.52 step time 0.47s\n","# Epoch 3  global step 9620 loss 5.79685 batch 3056/3281 lr 0.001 accuracy 93.60938 wps 12973.43 step time 0.47s\n","# Epoch 3  global step 9640 loss 5.88421 batch 3076/3281 lr 0.001 accuracy 93.38672 wps 12120.74 step time 0.63s\n","# Epoch 3  global step 9660 loss 5.93317 batch 3096/3281 lr 0.001 accuracy 93.02734 wps 12451.72 step time 0.52s\n","# Epoch 3  global step 9680 loss 5.92589 batch 3116/3281 lr 0.001 accuracy 93.10937 wps 12115.29 step time 0.69s\n","# Epoch 3  global step 9700 loss 5.84132 batch 3136/3281 lr 0.001 accuracy 93.30859 wps 12415.63 step time 0.54s\n","# Epoch 3  global step 9720 loss 6.06003 batch 3156/3281 lr 0.001 accuracy 92.77734 wps 12259.13 step time 0.68s\n","# Epoch 3  global step 9740 loss 5.83290 batch 3176/3281 lr 0.001 accuracy 93.18359 wps 12899.20 step time 0.49s\n","# Epoch 3  global step 9760 loss 5.96057 batch 3196/3281 lr 0.001 accuracy 92.87891 wps 12665.73 step time 0.57s\n","# Epoch 3  global step 9780 loss 6.08784 batch 3216/3281 lr 0.001 accuracy 92.57031 wps 12241.67 step time 0.64s\n","# Epoch 3  global step 9800 loss 5.81214 batch 3236/3281 lr 0.001 accuracy 93.35156 wps 13255.14 step time 0.50s\n","# Epoch 3  global step 9820 loss 5.99266 batch 3256/3281 lr 0.001 accuracy 92.93359 wps 12994.79 step time 0.57s\n","# Epoch 3  global step 9840 loss 5.98419 batch 3276/3281 lr 0.001 accuracy 92.93359 wps 12662.37 step time 0.53s\n","# Finsh epoch 3, global step 9846\n","# Epoch 4  global step 9860 loss 4.11214 batch 14/3281 lr 0.001 accuracy 65.12891 wps 14732.39 step time 0.44s\n","# Epoch 4  global step 9880 loss 5.96980 batch 34/3281 lr 0.001 accuracy 92.73437 wps 15132.00 step time 0.54s\n","# Epoch 4  global step 9900 loss 5.78093 batch 54/3281 lr 0.001 accuracy 93.49219 wps 13925.40 step time 0.55s\n","# Epoch 4  global step 9920 loss 5.71266 batch 74/3281 lr 0.001 accuracy 93.64062 wps 13791.56 step time 0.50s\n","# Epoch 4  global step 9940 loss 5.70458 batch 94/3281 lr 0.001 accuracy 93.48047 wps 14130.08 step time 0.44s\n","# Epoch 4  global step 9960 loss 5.75509 batch 114/3281 lr 0.001 accuracy 93.47266 wps 14153.85 step time 0.53s\n","# Epoch 4  global step 9980 loss 5.94559 batch 134/3281 lr 0.001 accuracy 92.98828 wps 13671.18 step time 0.64s\n","# Epoch 4  global step 10000 loss 5.86162 batch 154/3281 lr 0.001 accuracy 92.99609 wps 14927.77 step time 0.51s\n","# global step 10000, eval model at Sat May 23 07:52:47 2020\n","2020-05-23 07:52:50.007390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:52:50.007867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 07:52:50.008004: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 07:52:50.008040: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 07:52:50.008065: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 07:52:50.008094: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 07:52:50.008119: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 07:52:50.008142: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 07:52:50.008166: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 07:52:50.008271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:52:50.008571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:52:50.008814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 07:52:50.009160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 07:52:50.009187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 07:52:50.009200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 07:52:50.009370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:52:50.009673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 07:52:50.009904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.31055084975106484\n","# location_distance_from_business_district - 0.22385646748818105\n","# location_easy_to_find - 0.21775207424417897\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.19148426107097252\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.2088555339189581\n","# environment_decoration - 0.1720314807184603\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.38629132062640786\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.544427213915972\n","# others_willing_to_consume_again - 0.19410257671373154\n","# Eval loss 7.24187, f1 0.23699\n","# current result -0.23698688831897452, previous best result -0.22218583926544982\n","# Epoch 4  global step 10020 loss 5.86157 batch 174/3281 lr 0.001 accuracy 93.12109 wps 14594.52 step time 0.49s\n","# Epoch 4  global step 10040 loss 5.80115 batch 194/3281 lr 0.001 accuracy 93.61719 wps 14314.24 step time 0.44s\n","# Epoch 4  global step 10060 loss 5.76662 batch 214/3281 lr 0.001 accuracy 93.53906 wps 14496.85 step time 0.44s\n","# Epoch 4  global step 10080 loss 5.91567 batch 234/3281 lr 0.001 accuracy 93.03516 wps 14002.38 step time 0.62s\n","# Epoch 4  global step 10100 loss 5.82332 batch 254/3281 lr 0.001 accuracy 93.31250 wps 14822.27 step time 0.48s\n","# Epoch 4  global step 10120 loss 5.72171 batch 274/3281 lr 0.001 accuracy 93.59766 wps 14415.33 step time 0.44s\n","# Epoch 4  global step 10140 loss 5.76034 batch 294/3281 lr 0.001 accuracy 93.43359 wps 14840.74 step time 0.49s\n","# Epoch 4  global step 10160 loss 5.77176 batch 314/3281 lr 0.001 accuracy 93.41406 wps 14318.21 step time 0.43s\n","# Epoch 4  global step 10180 loss 5.64616 batch 334/3281 lr 0.001 accuracy 93.74609 wps 14338.22 step time 0.45s\n","# Epoch 4  global step 10200 loss 5.95243 batch 354/3281 lr 0.001 accuracy 93.01562 wps 14837.72 step time 0.47s\n","# Epoch 4  global step 10220 loss 5.80353 batch 374/3281 lr 0.001 accuracy 93.27344 wps 13769.49 step time 0.58s\n","# Epoch 4  global step 10240 loss 5.83701 batch 394/3281 lr 0.001 accuracy 93.03906 wps 14370.00 step time 0.55s\n","# Epoch 4  global step 10260 loss 5.86252 batch 414/3281 lr 0.001 accuracy 93.06641 wps 14668.57 step time 0.59s\n","# Epoch 4  global step 10280 loss 5.71446 batch 434/3281 lr 0.001 accuracy 93.78516 wps 14242.49 step time 0.43s\n","# Epoch 4  global step 10300 loss 5.78436 batch 454/3281 lr 0.001 accuracy 93.21484 wps 14729.74 step time 0.47s\n","# Epoch 4  global step 10320 loss 5.75878 batch 474/3281 lr 0.001 accuracy 93.61719 wps 14826.37 step time 0.47s\n","# Epoch 4  global step 10340 loss 5.89591 batch 494/3281 lr 0.001 accuracy 92.95703 wps 15183.08 step time 0.54s\n","# Epoch 4  global step 10360 loss 5.94810 batch 514/3281 lr 0.001 accuracy 93.07422 wps 14463.50 step time 0.57s\n","# Epoch 4  global step 10380 loss 5.79032 batch 534/3281 lr 0.001 accuracy 93.33594 wps 14402.88 step time 0.53s\n","# Epoch 4  global step 10400 loss 5.85871 batch 554/3281 lr 0.001 accuracy 93.10156 wps 14322.13 step time 0.55s\n","# Epoch 4  global step 10420 loss 5.81354 batch 574/3281 lr 0.001 accuracy 93.25000 wps 14885.91 step time 0.47s\n","# Epoch 4  global step 10440 loss 5.83323 batch 594/3281 lr 0.001 accuracy 93.21484 wps 14697.04 step time 0.46s\n","# Epoch 4  global step 10460 loss 5.82851 batch 614/3281 lr 0.001 accuracy 93.49609 wps 14696.34 step time 0.47s\n","# Epoch 4  global step 10480 loss 5.87924 batch 634/3281 lr 0.001 accuracy 93.13281 wps 15126.63 step time 0.54s\n","# Epoch 4  global step 10500 loss 5.78756 batch 654/3281 lr 0.001 accuracy 93.43359 wps 14822.79 step time 0.47s\n","# Epoch 4  global step 10520 loss 5.93446 batch 674/3281 lr 0.001 accuracy 92.93359 wps 14799.13 step time 0.56s\n","# Epoch 4  global step 10540 loss 5.74451 batch 694/3281 lr 0.001 accuracy 93.62891 wps 14392.19 step time 0.44s\n","# Epoch 4  global step 10560 loss 5.69374 batch 714/3281 lr 0.001 accuracy 93.74219 wps 14587.18 step time 0.54s\n","# Epoch 4  global step 10580 loss 5.68195 batch 734/3281 lr 0.001 accuracy 93.64062 wps 14226.48 step time 0.43s\n","# Epoch 4  global step 10600 loss 5.77594 batch 754/3281 lr 0.001 accuracy 93.30859 wps 14595.48 step time 0.46s\n","# Epoch 4  global step 10620 loss 5.80374 batch 774/3281 lr 0.001 accuracy 93.47656 wps 14050.38 step time 0.54s\n","# Epoch 4  global step 10640 loss 5.86179 batch 794/3281 lr 0.001 accuracy 93.08984 wps 14720.10 step time 0.48s\n","# Epoch 4  global step 10660 loss 5.98257 batch 814/3281 lr 0.001 accuracy 92.89453 wps 14617.75 step time 0.58s\n","# Epoch 4  global step 10680 loss 5.99772 batch 834/3281 lr 0.001 accuracy 92.81641 wps 14138.34 step time 0.65s\n","# Epoch 4  global step 10700 loss 5.79394 batch 854/3281 lr 0.001 accuracy 93.34375 wps 14381.69 step time 0.55s\n","# Epoch 4  global step 10720 loss 5.62886 batch 874/3281 lr 0.001 accuracy 93.83203 wps 13577.83 step time 0.47s\n","# Epoch 4  global step 10740 loss 5.69133 batch 894/3281 lr 0.001 accuracy 93.58203 wps 14473.69 step time 0.44s\n","# Epoch 4  global step 10760 loss 5.77948 batch 914/3281 lr 0.001 accuracy 93.37500 wps 14545.31 step time 0.45s\n","# Epoch 4  global step 10780 loss 5.92521 batch 934/3281 lr 0.001 accuracy 92.88672 wps 14194.75 step time 0.63s\n","# Epoch 4  global step 10800 loss 5.75189 batch 954/3281 lr 0.001 accuracy 93.62500 wps 14273.14 step time 0.55s\n","# Epoch 4  global step 10820 loss 5.83571 batch 974/3281 lr 0.001 accuracy 93.41016 wps 14539.12 step time 0.46s\n","# Epoch 4  global step 10840 loss 5.89922 batch 994/3281 lr 0.001 accuracy 93.07422 wps 14303.67 step time 0.54s\n","# Epoch 4  global step 10860 loss 5.71112 batch 1014/3281 lr 0.001 accuracy 93.68359 wps 14039.33 step time 0.42s\n","# Epoch 4  global step 10880 loss 5.88927 batch 1034/3281 lr 0.001 accuracy 93.15234 wps 14111.05 step time 0.64s\n","# Epoch 4  global step 10900 loss 5.75572 batch 1054/3281 lr 0.001 accuracy 93.60937 wps 14547.97 step time 0.44s\n","# Epoch 4  global step 10920 loss 5.71943 batch 1074/3281 lr 0.001 accuracy 93.71094 wps 14303.51 step time 0.43s\n","# Epoch 4  global step 10940 loss 6.01635 batch 1094/3281 lr 0.001 accuracy 92.62109 wps 14643.96 step time 0.60s\n","# Epoch 4  global step 10960 loss 5.82518 batch 1114/3281 lr 0.001 accuracy 93.26172 wps 14531.55 step time 0.46s\n","# Epoch 4  global step 10980 loss 5.83536 batch 1134/3281 lr 0.001 accuracy 93.38672 wps 14985.09 step time 0.51s\n","# Epoch 4  global step 11000 loss 5.92754 batch 1154/3281 lr 0.001 accuracy 93.00781 wps 14337.44 step time 0.55s\n","# Epoch 4  global step 11020 loss 5.94460 batch 1174/3281 lr 0.001 accuracy 93.01172 wps 15014.75 step time 0.51s\n","# Epoch 4  global step 11040 loss 5.82569 batch 1194/3281 lr 0.001 accuracy 93.44141 wps 14876.05 step time 0.48s\n","# Epoch 4  global step 11060 loss 5.78867 batch 1214/3281 lr 0.001 accuracy 93.28906 wps 14251.06 step time 0.55s\n","# Epoch 4  global step 11080 loss 5.88663 batch 1234/3281 lr 0.001 accuracy 93.26172 wps 14713.89 step time 0.48s\n","# Epoch 4  global step 11100 loss 5.79409 batch 1254/3281 lr 0.001 accuracy 93.50391 wps 14055.68 step time 0.54s\n","# Epoch 4  global step 11120 loss 5.95463 batch 1274/3281 lr 0.001 accuracy 93.04688 wps 14798.79 step time 0.58s\n","# Epoch 4  global step 11140 loss 5.90619 batch 1294/3281 lr 0.001 accuracy 93.05469 wps 15110.70 step time 0.52s\n","# Epoch 4  global step 11160 loss 5.68624 batch 1314/3281 lr 0.001 accuracy 93.84766 wps 14244.16 step time 0.43s\n","# Epoch 4  global step 11180 loss 5.72093 batch 1334/3281 lr 0.001 accuracy 93.57812 wps 14434.92 step time 0.45s\n","# Epoch 4  global step 11200 loss 5.81968 batch 1354/3281 lr 0.001 accuracy 93.16016 wps 14180.03 step time 0.54s\n","# Epoch 4  global step 11220 loss 5.70273 batch 1374/3281 lr 0.001 accuracy 93.52734 wps 14256.35 step time 0.44s\n","# Epoch 4  global step 11240 loss 5.68458 batch 1394/3281 lr 0.001 accuracy 93.55078 wps 14346.42 step time 0.44s\n","# Epoch 4  global step 11260 loss 5.71806 batch 1414/3281 lr 0.001 accuracy 93.64062 wps 13830.80 step time 0.52s\n","# Epoch 4  global step 11280 loss 5.68615 batch 1434/3281 lr 0.001 accuracy 93.65234 wps 13981.77 step time 0.50s\n","# Epoch 4  global step 11300 loss 5.82134 batch 1454/3281 lr 0.001 accuracy 93.24609 wps 14684.96 step time 0.47s\n","# Epoch 4  global step 11320 loss 5.99371 batch 1474/3281 lr 0.001 accuracy 92.73828 wps 14559.55 step time 0.70s\n","# Epoch 4  global step 11340 loss 5.81940 batch 1494/3281 lr 0.001 accuracy 93.22656 wps 14258.45 step time 0.56s\n","# Epoch 4  global step 11360 loss 5.80591 batch 1514/3281 lr 0.001 accuracy 93.50000 wps 14633.09 step time 0.48s\n","# Epoch 4  global step 11380 loss 5.77841 batch 1534/3281 lr 0.001 accuracy 93.48828 wps 14781.78 step time 0.50s\n","# Epoch 4  global step 11400 loss 5.98964 batch 1554/3281 lr 0.001 accuracy 92.87109 wps 14931.25 step time 0.55s\n","# Epoch 4  global step 11420 loss 5.96951 batch 1574/3281 lr 0.001 accuracy 92.80078 wps 14155.20 step time 0.55s\n","# Epoch 4  global step 11440 loss 6.01066 batch 1594/3281 lr 0.001 accuracy 92.73828 wps 15102.33 step time 0.52s\n","# Epoch 4  global step 11460 loss 5.98392 batch 1614/3281 lr 0.001 accuracy 92.94531 wps 14920.93 step time 0.50s\n","# Epoch 4  global step 11480 loss 5.85693 batch 1634/3281 lr 0.001 accuracy 93.38672 wps 14654.73 step time 0.47s\n","# Epoch 4  global step 11500 loss 5.91072 batch 1654/3281 lr 0.001 accuracy 93.16016 wps 15029.26 step time 0.52s\n","# Epoch 4  global step 11520 loss 5.89250 batch 1674/3281 lr 0.001 accuracy 93.18359 wps 14209.23 step time 0.56s\n","# Epoch 4  global step 11540 loss 5.79524 batch 1694/3281 lr 0.001 accuracy 93.26172 wps 14621.64 step time 0.47s\n","# Epoch 4  global step 11560 loss 5.77724 batch 1714/3281 lr 0.001 accuracy 93.50391 wps 14363.05 step time 0.45s\n","# Epoch 4  global step 11580 loss 5.91976 batch 1734/3281 lr 0.001 accuracy 92.88672 wps 14422.89 step time 0.57s\n","# Epoch 4  global step 11600 loss 5.68960 batch 1754/3281 lr 0.001 accuracy 93.80469 wps 13662.63 step time 0.39s\n","# Epoch 4  global step 11620 loss 5.93304 batch 1774/3281 lr 0.001 accuracy 92.92969 wps 14633.86 step time 0.47s\n","# Epoch 4  global step 11640 loss 5.82243 batch 1794/3281 lr 0.001 accuracy 93.23047 wps 14124.19 step time 0.58s\n","# Epoch 4  global step 11660 loss 5.75104 batch 1814/3281 lr 0.001 accuracy 93.59766 wps 14032.30 step time 0.56s\n","# Epoch 4  global step 11680 loss 5.61981 batch 1834/3281 lr 0.001 accuracy 93.82422 wps 14224.06 step time 0.44s\n","# Epoch 4  global step 11700 loss 5.77422 batch 1854/3281 lr 0.001 accuracy 93.41797 wps 14185.29 step time 0.46s\n","# Epoch 4  global step 11720 loss 6.05916 batch 1874/3281 lr 0.001 accuracy 92.88672 wps 14305.01 step time 0.57s\n","# Epoch 4  global step 11740 loss 5.69322 batch 1894/3281 lr 0.001 accuracy 93.56641 wps 13290.98 step time 0.47s\n","# Epoch 4  global step 11760 loss 5.82938 batch 1914/3281 lr 0.001 accuracy 93.25000 wps 14880.31 step time 0.50s\n","# Epoch 4  global step 11780 loss 5.74008 batch 1934/3281 lr 0.001 accuracy 93.44922 wps 14261.82 step time 0.44s\n","# Epoch 4  global step 11800 loss 5.86485 batch 1954/3281 lr 0.001 accuracy 93.28125 wps 14690.84 step time 0.48s\n","# Epoch 4  global step 11820 loss 5.73393 batch 1974/3281 lr 0.001 accuracy 93.73438 wps 14244.04 step time 0.43s\n","# Epoch 4  global step 11840 loss 6.01358 batch 1994/3281 lr 0.001 accuracy 92.63281 wps 14958.15 step time 0.50s\n","# Epoch 4  global step 11860 loss 5.78144 batch 2014/3281 lr 0.001 accuracy 93.55078 wps 14156.55 step time 0.45s\n","# Epoch 4  global step 11880 loss 5.81640 batch 2034/3281 lr 0.001 accuracy 93.43359 wps 14682.23 step time 0.48s\n","# Epoch 4  global step 11900 loss 5.77921 batch 2054/3281 lr 0.001 accuracy 93.35937 wps 14474.48 step time 0.47s\n","# Epoch 4  global step 11920 loss 5.80086 batch 2074/3281 lr 0.001 accuracy 93.29297 wps 14581.38 step time 0.48s\n","# Epoch 4  global step 11940 loss 5.75009 batch 2094/3281 lr 0.001 accuracy 93.64844 wps 14210.81 step time 0.44s\n","# Epoch 4  global step 11960 loss 5.77705 batch 2114/3281 lr 0.001 accuracy 93.42969 wps 14540.68 step time 0.48s\n","# Epoch 4  global step 11980 loss 5.82624 batch 2134/3281 lr 0.001 accuracy 93.18359 wps 14741.32 step time 0.48s\n","# Epoch 4  global step 12000 loss 5.78782 batch 2154/3281 lr 0.001 accuracy 93.45313 wps 14551.07 step time 0.47s\n","# global step 12000, eval model at Sat May 23 08:11:48 2020\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","2020-05-23 08:11:50.582722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:11:50.583195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 08:11:50.583329: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 08:11:50.583363: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 08:11:50.583392: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 08:11:50.583420: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 08:11:50.583465: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 08:11:50.583495: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 08:11:50.583523: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 08:11:50.583629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:11:50.583915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:11:50.584167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 08:11:50.584520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 08:11:50.584541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 08:11:50.584552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 08:11:50.584703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:11:50.584985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:11:50.585228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.4588960476990327\n","# location_distance_from_business_district - 0.2786709341946804\n","# location_easy_to_find - 0.3134846879874442\n","# service_wait_time - 0.23547230972301517\n","# service_waiters_attitude - 0.34429074510965796\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.2302725810405564\n","# price_level - 0.18878342376692003\n","# price_cost_effective - 0.2232073510776949\n","# price_discount - 0.3303230434568833\n","# environment_decoration - 0.26126319386767227\n","# environment_noise - 0.20770082897733066\n","# environment_space - 0.22573171756583396\n","# environment_cleaness - 0.2147340759364354\n","# dish_portion - 0.17877793528578245\n","# dish_taste - 0.5076313060460121\n","# dish_look - 0.20899633047987465\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.5534507275563215\n","# others_willing_to_consume_again - 0.2614550215368688\n","# Eval loss 6.41393, f1 0.28440\n","# current result -0.28440023313263235, previous best result -0.23698688831897452\n","# Epoch 4  global step 12020 loss 5.84900 batch 2174/3281 lr 0.001 accuracy 93.27344 wps 14791.60 step time 0.53s\n","# Epoch 4  global step 12040 loss 5.83930 batch 2194/3281 lr 0.001 accuracy 93.37891 wps 14009.59 step time 0.55s\n","# Epoch 4  global step 12060 loss 5.95790 batch 2214/3281 lr 0.001 accuracy 93.06641 wps 14373.45 step time 0.55s\n","# Epoch 4  global step 12080 loss 5.74053 batch 2234/3281 lr 0.001 accuracy 93.57422 wps 14587.22 step time 0.46s\n","# Epoch 4  global step 12100 loss 5.80868 batch 2254/3281 lr 0.001 accuracy 93.29297 wps 14797.45 step time 0.50s\n","# Epoch 4  global step 12120 loss 5.88315 batch 2274/3281 lr 0.001 accuracy 93.08984 wps 14893.04 step time 0.50s\n","# Epoch 4  global step 12140 loss 5.91047 batch 2294/3281 lr 0.001 accuracy 93.02344 wps 13947.44 step time 0.61s\n","# Epoch 4  global step 12160 loss 5.74932 batch 2314/3281 lr 0.001 accuracy 93.74219 wps 14568.96 step time 0.47s\n","# Epoch 4  global step 12180 loss 5.80081 batch 2334/3281 lr 0.001 accuracy 93.26953 wps 14742.86 step time 0.59s\n","# Epoch 4  global step 12200 loss 5.73576 batch 2354/3281 lr 0.001 accuracy 93.43750 wps 14137.62 step time 0.43s\n","# Epoch 4  global step 12220 loss 5.86809 batch 2374/3281 lr 0.001 accuracy 93.12891 wps 15042.97 step time 0.51s\n","# Epoch 4  global step 12240 loss 5.73833 batch 2394/3281 lr 0.001 accuracy 93.42969 wps 13469.25 step time 0.50s\n","# Epoch 4  global step 12260 loss 5.92840 batch 2414/3281 lr 0.001 accuracy 93.17969 wps 11610.31 step time 0.65s\n","# Epoch 4  global step 12280 loss 5.81944 batch 2434/3281 lr 0.001 accuracy 93.38672 wps 12512.90 step time 0.50s\n","# Epoch 4  global step 12300 loss 5.81057 batch 2454/3281 lr 0.001 accuracy 93.29297 wps 13051.36 step time 0.56s\n","# Epoch 4  global step 12320 loss 5.74055 batch 2474/3281 lr 0.001 accuracy 93.74219 wps 12813.72 step time 0.50s\n","# Epoch 4  global step 12340 loss 5.65271 batch 2494/3281 lr 0.001 accuracy 93.64062 wps 13172.34 step time 0.45s\n","# Epoch 4  global step 12360 loss 5.86936 batch 2514/3281 lr 0.001 accuracy 93.17188 wps 12462.35 step time 0.52s\n","# Epoch 4  global step 12380 loss 5.88147 batch 2534/3281 lr 0.001 accuracy 93.26562 wps 12936.68 step time 0.60s\n","# Epoch 4  global step 12400 loss 5.80637 batch 2554/3281 lr 0.001 accuracy 93.36719 wps 12818.31 step time 0.53s\n","# Epoch 4  global step 12420 loss 5.74262 batch 2574/3281 lr 0.001 accuracy 93.68359 wps 12635.81 step time 0.53s\n","# Epoch 4  global step 12440 loss 5.80378 batch 2594/3281 lr 0.001 accuracy 93.32031 wps 13115.74 step time 0.49s\n","# Epoch 4  global step 12460 loss 5.78935 batch 2614/3281 lr 0.001 accuracy 93.67578 wps 13170.80 step time 0.52s\n","# Epoch 4  global step 12480 loss 5.91137 batch 2634/3281 lr 0.001 accuracy 93.06641 wps 12810.37 step time 0.54s\n","# Epoch 4  global step 12500 loss 5.92699 batch 2654/3281 lr 0.001 accuracy 92.96484 wps 13469.19 step time 0.64s\n","# Epoch 4  global step 12520 loss 5.87516 batch 2674/3281 lr 0.001 accuracy 93.12500 wps 12491.56 step time 0.61s\n","# Epoch 4  global step 12540 loss 5.80738 batch 2694/3281 lr 0.001 accuracy 93.34375 wps 13139.02 step time 0.58s\n","# Epoch 4  global step 12560 loss 5.82133 batch 2714/3281 lr 0.001 accuracy 93.17969 wps 11643.39 step time 0.66s\n","# Epoch 4  global step 12580 loss 5.59832 batch 2734/3281 lr 0.001 accuracy 93.86719 wps 12749.78 step time 0.48s\n","# Epoch 4  global step 12600 loss 5.92174 batch 2754/3281 lr 0.001 accuracy 92.92578 wps 11712.72 step time 0.73s\n","# Epoch 4  global step 12620 loss 5.93169 batch 2774/3281 lr 0.001 accuracy 92.91406 wps 13355.46 step time 0.59s\n","# Epoch 4  global step 12640 loss 5.79372 batch 2794/3281 lr 0.001 accuracy 93.39062 wps 12703.87 step time 0.53s\n","# Epoch 4  global step 12660 loss 5.74817 batch 2814/3281 lr 0.001 accuracy 93.40234 wps 13510.09 step time 0.53s\n","# Epoch 4  global step 12680 loss 5.63925 batch 2834/3281 lr 0.001 accuracy 93.68359 wps 12416.74 step time 0.50s\n","# Epoch 4  global step 12700 loss 6.02394 batch 2854/3281 lr 0.001 accuracy 92.78125 wps 12979.00 step time 0.67s\n","# Epoch 4  global step 12720 loss 5.74601 batch 2874/3281 lr 0.001 accuracy 93.53516 wps 12937.44 step time 0.51s\n","# Epoch 4  global step 12740 loss 5.81144 batch 2894/3281 lr 0.001 accuracy 93.42578 wps 12955.65 step time 0.55s\n","# Epoch 4  global step 12760 loss 5.84165 batch 2914/3281 lr 0.001 accuracy 93.16016 wps 13078.59 step time 0.53s\n","# Epoch 4  global step 12780 loss 5.77214 batch 2934/3281 lr 0.001 accuracy 93.58594 wps 12732.35 step time 0.52s\n","# Epoch 4  global step 12800 loss 5.87715 batch 2954/3281 lr 0.001 accuracy 93.15234 wps 11767.06 step time 0.65s\n","# Epoch 4  global step 12820 loss 5.87948 batch 2974/3281 lr 0.001 accuracy 93.05859 wps 11518.49 step time 0.73s\n","# Epoch 4  global step 12840 loss 5.69102 batch 2994/3281 lr 0.001 accuracy 93.73438 wps 12396.63 step time 0.48s\n","# Epoch 4  global step 12860 loss 5.81564 batch 3014/3281 lr 0.001 accuracy 93.32422 wps 12495.62 step time 0.59s\n","# Epoch 4  global step 12880 loss 5.67399 batch 3034/3281 lr 0.001 accuracy 93.80469 wps 11913.23 step time 0.59s\n","# Epoch 4  global step 12900 loss 5.86015 batch 3054/3281 lr 0.001 accuracy 93.26172 wps 12535.12 step time 0.53s\n","# Epoch 4  global step 12920 loss 5.85686 batch 3074/3281 lr 0.001 accuracy 93.26172 wps 12825.36 step time 0.49s\n","# Epoch 4  global step 12940 loss 5.79393 batch 3094/3281 lr 0.001 accuracy 93.48047 wps 12371.53 step time 0.52s\n","# Epoch 4  global step 12960 loss 5.73843 batch 3114/3281 lr 0.001 accuracy 93.60937 wps 12398.04 step time 0.50s\n","# Epoch 4  global step 12980 loss 5.79315 batch 3134/3281 lr 0.001 accuracy 93.50391 wps 12140.68 step time 0.61s\n","# Epoch 4  global step 13000 loss 5.91991 batch 3154/3281 lr 0.001 accuracy 92.96484 wps 12347.88 step time 0.57s\n","# Epoch 4  global step 13020 loss 5.99412 batch 3174/3281 lr 0.001 accuracy 92.82813 wps 12316.37 step time 0.76s\n","# Epoch 4  global step 13040 loss 5.76489 batch 3194/3281 lr 0.001 accuracy 93.38281 wps 12930.73 step time 0.51s\n","# Epoch 4  global step 13060 loss 5.91799 batch 3214/3281 lr 0.001 accuracy 93.09375 wps 11822.30 step time 0.61s\n","# Epoch 4  global step 13080 loss 5.62090 batch 3234/3281 lr 0.001 accuracy 93.87500 wps 12462.96 step time 0.48s\n","# Epoch 4  global step 13100 loss 5.86041 batch 3254/3281 lr 0.001 accuracy 93.17187 wps 12138.54 step time 0.58s\n","# Epoch 4  global step 13120 loss 5.79256 batch 3274/3281 lr 0.001 accuracy 93.58203 wps 12519.89 step time 0.56s\n","# Finsh epoch 4, global step 13128\n","# Epoch 5  global step 13140 loss 3.42460 batch 12/3281 lr 0.001 accuracy 56.17188 wps 13686.33 step time 0.34s\n","# Epoch 5  global step 13160 loss 5.64911 batch 32/3281 lr 0.001 accuracy 93.76953 wps 14312.60 step time 0.59s\n","# Epoch 5  global step 13180 loss 5.69645 batch 52/3281 lr 0.001 accuracy 93.59766 wps 14935.95 step time 0.52s\n","# Epoch 5  global step 13200 loss 5.60664 batch 72/3281 lr 0.001 accuracy 93.88672 wps 14233.53 step time 0.43s\n","# Epoch 5  global step 13220 loss 5.83787 batch 92/3281 lr 0.001 accuracy 93.14453 wps 14317.86 step time 0.56s\n","# Epoch 5  global step 13240 loss 5.72626 batch 112/3281 lr 0.001 accuracy 93.44141 wps 13821.01 step time 0.54s\n","# Epoch 5  global step 13260 loss 5.68457 batch 132/3281 lr 0.001 accuracy 93.70312 wps 14632.69 step time 0.48s\n","# Epoch 5  global step 13280 loss 5.75256 batch 152/3281 lr 0.001 accuracy 93.48828 wps 14524.95 step time 0.47s\n","# Epoch 5  global step 13300 loss 5.65127 batch 172/3281 lr 0.001 accuracy 93.71875 wps 14659.72 step time 0.51s\n","# Epoch 5  global step 13320 loss 5.77181 batch 192/3281 lr 0.001 accuracy 93.35547 wps 14741.49 step time 0.52s\n","# Epoch 5  global step 13340 loss 5.68973 batch 212/3281 lr 0.001 accuracy 93.66406 wps 13888.77 step time 0.53s\n","# Epoch 5  global step 13360 loss 5.64166 batch 232/3281 lr 0.001 accuracy 93.71875 wps 14283.23 step time 0.45s\n","# Epoch 5  global step 13380 loss 5.70284 batch 252/3281 lr 0.001 accuracy 93.52734 wps 14238.43 step time 0.44s\n","# Epoch 5  global step 13400 loss 5.68079 batch 272/3281 lr 0.001 accuracy 93.62500 wps 13867.44 step time 0.59s\n","# Epoch 5  global step 13420 loss 5.83922 batch 292/3281 lr 0.001 accuracy 93.16016 wps 14753.12 step time 0.49s\n","# Epoch 5  global step 13440 loss 5.71155 batch 312/3281 lr 0.001 accuracy 93.39844 wps 14513.61 step time 0.48s\n","# Epoch 5  global step 13460 loss 5.75418 batch 332/3281 lr 0.001 accuracy 93.53516 wps 15037.24 step time 0.52s\n","# Epoch 5  global step 13480 loss 5.75652 batch 352/3281 lr 0.001 accuracy 93.32422 wps 14840.65 step time 0.50s\n","# Epoch 5  global step 13500 loss 5.81673 batch 372/3281 lr 0.001 accuracy 93.26172 wps 14504.52 step time 0.57s\n","# Epoch 5  global step 13520 loss 5.77221 batch 392/3281 lr 0.001 accuracy 93.40234 wps 14371.06 step time 0.44s\n","# Epoch 5  global step 13540 loss 5.65226 batch 412/3281 lr 0.001 accuracy 93.72266 wps 14315.77 step time 0.45s\n","# Epoch 5  global step 13560 loss 5.79464 batch 432/3281 lr 0.001 accuracy 93.24609 wps 14293.24 step time 0.55s\n","# Epoch 5  global step 13580 loss 5.67418 batch 452/3281 lr 0.001 accuracy 93.70703 wps 13374.47 step time 0.57s\n","# Epoch 5  global step 13600 loss 5.63559 batch 472/3281 lr 0.001 accuracy 93.71484 wps 14064.73 step time 0.43s\n","# Epoch 5  global step 13620 loss 5.61293 batch 492/3281 lr 0.001 accuracy 93.85937 wps 14013.25 step time 0.42s\n","# Epoch 5  global step 13640 loss 5.62402 batch 512/3281 lr 0.001 accuracy 93.72656 wps 14034.70 step time 0.42s\n","# Epoch 5  global step 13660 loss 5.66520 batch 532/3281 lr 0.001 accuracy 93.77344 wps 13941.53 step time 0.42s\n","# Epoch 5  global step 13680 loss 5.76099 batch 552/3281 lr 0.001 accuracy 93.37500 wps 14687.91 step time 0.49s\n","# Epoch 5  global step 13700 loss 5.79307 batch 572/3281 lr 0.001 accuracy 93.40625 wps 14544.84 step time 0.47s\n","# Epoch 5  global step 13720 loss 5.77143 batch 592/3281 lr 0.001 accuracy 93.47656 wps 14251.28 step time 0.57s\n","# Epoch 5  global step 13740 loss 5.73824 batch 612/3281 lr 0.001 accuracy 93.51172 wps 14036.47 step time 0.43s\n","# Epoch 5  global step 13760 loss 5.77343 batch 632/3281 lr 0.001 accuracy 93.50000 wps 15133.79 step time 0.54s\n","# Epoch 5  global step 13780 loss 5.78670 batch 652/3281 lr 0.001 accuracy 93.30469 wps 14507.78 step time 0.61s\n","# Epoch 5  global step 13800 loss 5.51428 batch 672/3281 lr 0.001 accuracy 93.96094 wps 14091.47 step time 0.42s\n","# Epoch 5  global step 13820 loss 5.67920 batch 692/3281 lr 0.001 accuracy 93.62109 wps 13883.34 step time 0.42s\n","# Epoch 5  global step 13840 loss 5.58538 batch 712/3281 lr 0.001 accuracy 93.97656 wps 13958.30 step time 0.42s\n","# Epoch 5  global step 13860 loss 5.73834 batch 732/3281 lr 0.001 accuracy 93.68750 wps 14563.91 step time 0.48s\n","# Epoch 5  global step 13880 loss 5.75040 batch 752/3281 lr 0.001 accuracy 93.41797 wps 14827.03 step time 0.60s\n","# Epoch 5  global step 13900 loss 5.57333 batch 772/3281 lr 0.001 accuracy 93.88672 wps 13880.15 step time 0.42s\n","# Epoch 5  global step 13920 loss 5.69701 batch 792/3281 lr 0.001 accuracy 93.43750 wps 14593.06 step time 0.49s\n","# Epoch 5  global step 13940 loss 5.74356 batch 812/3281 lr 0.001 accuracy 93.37500 wps 14336.16 step time 0.57s\n","# Epoch 5  global step 13960 loss 5.85457 batch 832/3281 lr 0.001 accuracy 93.26953 wps 14430.20 step time 0.58s\n","# Epoch 5  global step 13980 loss 5.58477 batch 852/3281 lr 0.001 accuracy 93.84766 wps 14358.70 step time 0.52s\n","# Epoch 5  global step 14000 loss 5.67424 batch 872/3281 lr 0.001 accuracy 93.74219 wps 14309.71 step time 0.45s\n","# global step 14000, eval model at Sat May 23 08:31:45 2020\n","2020-05-23 08:31:47.699234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:31:47.699727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 08:31:47.699879: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 08:31:47.699907: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 08:31:47.699933: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 08:31:47.699953: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 08:31:47.699973: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 08:31:47.699996: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 08:31:47.700020: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 08:31:47.700119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:31:47.700438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:31:47.700754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 08:31:47.701141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 08:31:47.701164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 08:31:47.701180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 08:31:47.701378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:31:47.701691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:31:47.701925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.5613564724508511\n","# location_distance_from_business_district - 0.36529850818345305\n","# location_easy_to_find - 0.5855286618458516\n","# service_wait_time - 0.30514089170050185\n","# service_waiters_attitude - 0.6171214936161027\n","# service_parking_convenience - 0.29140575605436975\n","# service_serving_speed - 0.4183360687248176\n","# price_level - 0.45979257113047256\n","# price_cost_effective - 0.47717318887122107\n","# price_discount - 0.43377327111229885\n","# environment_decoration - 0.49595251629862724\n","# environment_noise - 0.4344939574100275\n","# environment_space - 0.535714126584009\n","# environment_cleaness - 0.48351564007518716\n","# dish_portion - 0.31686863772467205\n","# dish_taste - 0.6057956454912242\n","# dish_look - 0.24217398905735046\n","# dish_recommendation - 0.2264507275781525\n","# others_overall_experience - 0.5629252387156471\n","# others_willing_to_consume_again - 0.49082747486414813\n","# Eval loss 5.62849, f1 0.44548\n","# current result -0.4454822418744492, previous best result -0.28440023313263235\n","# Epoch 5  global step 14020 loss 5.77470 batch 892/3281 lr 0.001 accuracy 93.32422 wps 14461.84 step time 0.58s\n","# Epoch 5  global step 14040 loss 5.78302 batch 912/3281 lr 0.001 accuracy 93.34375 wps 14434.76 step time 0.58s\n","# Epoch 5  global step 14060 loss 5.79337 batch 932/3281 lr 0.001 accuracy 93.33203 wps 14779.97 step time 0.50s\n","# Epoch 5  global step 14080 loss 5.63934 batch 952/3281 lr 0.001 accuracy 93.83594 wps 14499.92 step time 0.47s\n","# Epoch 5  global step 14100 loss 5.76381 batch 972/3281 lr 0.001 accuracy 93.52344 wps 14610.02 step time 0.47s\n","# Epoch 5  global step 14120 loss 5.75279 batch 992/3281 lr 0.001 accuracy 93.34375 wps 14696.70 step time 0.48s\n","# Epoch 5  global step 14140 loss 5.71499 batch 1012/3281 lr 0.001 accuracy 93.60547 wps 14474.15 step time 0.47s\n","# Epoch 5  global step 14160 loss 5.83625 batch 1032/3281 lr 0.001 accuracy 93.27734 wps 14820.54 step time 0.50s\n","# Epoch 5  global step 14180 loss 5.71116 batch 1052/3281 lr 0.001 accuracy 93.54297 wps 14618.89 step time 0.47s\n","# Epoch 5  global step 14200 loss 5.70129 batch 1072/3281 lr 0.001 accuracy 93.67188 wps 14368.97 step time 0.47s\n","# Epoch 5  global step 14220 loss 5.67184 batch 1092/3281 lr 0.001 accuracy 93.49609 wps 12825.77 step time 0.52s\n","# Epoch 5  global step 14240 loss 5.90521 batch 1112/3281 lr 0.001 accuracy 92.87500 wps 12164.42 step time 0.65s\n","# Epoch 5  global step 14260 loss 5.83083 batch 1132/3281 lr 0.001 accuracy 93.21875 wps 13614.79 step time 0.52s\n","# Epoch 5  global step 14280 loss 5.65231 batch 1152/3281 lr 0.001 accuracy 93.80859 wps 11672.67 step time 0.59s\n","# Epoch 5  global step 14300 loss 5.91397 batch 1172/3281 lr 0.001 accuracy 93.07813 wps 11724.56 step time 0.85s\n","# Epoch 5  global step 14320 loss 5.70535 batch 1192/3281 lr 0.001 accuracy 93.68359 wps 12905.43 step time 0.44s\n","# Epoch 5  global step 14340 loss 5.75709 batch 1212/3281 lr 0.001 accuracy 93.42187 wps 12150.31 step time 0.67s\n","# Epoch 5  global step 14360 loss 5.80329 batch 1232/3281 lr 0.001 accuracy 93.05469 wps 12356.27 step time 0.74s\n","# Epoch 5  global step 14380 loss 5.68861 batch 1252/3281 lr 0.001 accuracy 93.71094 wps 12275.48 step time 0.55s\n","# Epoch 5  global step 14400 loss 5.75987 batch 1272/3281 lr 0.001 accuracy 93.39063 wps 12788.19 step time 0.55s\n","# Epoch 5  global step 14420 loss 5.92707 batch 1292/3281 lr 0.001 accuracy 93.01953 wps 12789.68 step time 0.59s\n","# Epoch 5  global step 14440 loss 5.70678 batch 1312/3281 lr 0.001 accuracy 93.55078 wps 12891.47 step time 0.57s\n","# Epoch 5  global step 14460 loss 5.78738 batch 1332/3281 lr 0.001 accuracy 93.38281 wps 12858.84 step time 0.53s\n","# Epoch 5  global step 14480 loss 5.75321 batch 1352/3281 lr 0.001 accuracy 93.51953 wps 11888.41 step time 0.67s\n","# Epoch 5  global step 14500 loss 5.68392 batch 1372/3281 lr 0.001 accuracy 93.55859 wps 11949.97 step time 0.65s\n","# Epoch 5  global step 14520 loss 5.86547 batch 1392/3281 lr 0.001 accuracy 93.19922 wps 12442.85 step time 0.60s\n","# Epoch 5  global step 14540 loss 5.84006 batch 1412/3281 lr 0.001 accuracy 93.32422 wps 12244.49 step time 0.67s\n","# Epoch 5  global step 14560 loss 5.67944 batch 1432/3281 lr 0.001 accuracy 93.65234 wps 12031.03 step time 0.60s\n","# Epoch 5  global step 14580 loss 5.63650 batch 1452/3281 lr 0.001 accuracy 93.80469 wps 12130.40 step time 0.61s\n","# Epoch 5  global step 14600 loss 5.82718 batch 1472/3281 lr 0.001 accuracy 93.15625 wps 12827.58 step time 0.62s\n","# Epoch 5  global step 14620 loss 5.61322 batch 1492/3281 lr 0.001 accuracy 93.80469 wps 12998.62 step time 0.48s\n","# Epoch 5  global step 14640 loss 5.75374 batch 1512/3281 lr 0.001 accuracy 93.53125 wps 12699.07 step time 0.57s\n","# Epoch 5  global step 14660 loss 5.67882 batch 1532/3281 lr 0.001 accuracy 93.62891 wps 12123.23 step time 0.61s\n","# Epoch 5  global step 14680 loss 5.71305 batch 1552/3281 lr 0.001 accuracy 93.54687 wps 12872.34 step time 0.52s\n","# Epoch 5  global step 14700 loss 5.91317 batch 1572/3281 lr 0.001 accuracy 93.19922 wps 12299.38 step time 0.65s\n","# Epoch 5  global step 14720 loss 5.67410 batch 1592/3281 lr 0.001 accuracy 93.75000 wps 12210.17 step time 0.54s\n","# Epoch 5  global step 14740 loss 5.72119 batch 1612/3281 lr 0.001 accuracy 93.22266 wps 12069.96 step time 0.60s\n","# Epoch 5  global step 14760 loss 5.68627 batch 1632/3281 lr 0.001 accuracy 93.70312 wps 11972.35 step time 0.55s\n","# Epoch 5  global step 14780 loss 5.57339 batch 1652/3281 lr 0.001 accuracy 93.87891 wps 12726.26 step time 0.47s\n","# Epoch 5  global step 14800 loss 5.68418 batch 1672/3281 lr 0.001 accuracy 93.64062 wps 13343.64 step time 0.55s\n","# Epoch 5  global step 14820 loss 5.81759 batch 1692/3281 lr 0.001 accuracy 93.25000 wps 12233.89 step time 0.66s\n","# Epoch 5  global step 14840 loss 5.69568 batch 1712/3281 lr 0.001 accuracy 93.88281 wps 12752.07 step time 0.49s\n","# Epoch 5  global step 14860 loss 5.55685 batch 1732/3281 lr 0.001 accuracy 93.93750 wps 12757.10 step time 0.47s\n","# Epoch 5  global step 14880 loss 5.74330 batch 1752/3281 lr 0.001 accuracy 93.55859 wps 13228.34 step time 0.50s\n","# Epoch 5  global step 14900 loss 5.86669 batch 1772/3281 lr 0.001 accuracy 93.09766 wps 12726.75 step time 0.62s\n","# Epoch 5  global step 14920 loss 5.65386 batch 1792/3281 lr 0.001 accuracy 93.74219 wps 13153.38 step time 0.50s\n","# Epoch 5  global step 14940 loss 5.81231 batch 1812/3281 lr 0.001 accuracy 93.35156 wps 13042.59 step time 0.57s\n","# Epoch 5  global step 14960 loss 5.70776 batch 1832/3281 lr 0.001 accuracy 93.40625 wps 11756.39 step time 0.69s\n","# Epoch 5  global step 14980 loss 5.57428 batch 1852/3281 lr 0.001 accuracy 94.01953 wps 12632.11 step time 0.49s\n","# Epoch 5  global step 15000 loss 5.68297 batch 1872/3281 lr 0.001 accuracy 93.56641 wps 12987.18 step time 0.50s\n","# Epoch 5  global step 15020 loss 5.81742 batch 1892/3281 lr 0.001 accuracy 93.19531 wps 12760.62 step time 0.70s\n","# Epoch 5  global step 15040 loss 5.81132 batch 1912/3281 lr 0.001 accuracy 93.20703 wps 12325.07 step time 0.66s\n","# Epoch 5  global step 15060 loss 5.78949 batch 1932/3281 lr 0.001 accuracy 93.25391 wps 12446.88 step time 0.57s\n","# Epoch 5  global step 15080 loss 5.78792 batch 1952/3281 lr 0.001 accuracy 93.41406 wps 12838.75 step time 0.61s\n","# Epoch 5  global step 15100 loss 5.71101 batch 1972/3281 lr 0.001 accuracy 93.73828 wps 12902.97 step time 0.51s\n","# Epoch 5  global step 15120 loss 5.78081 batch 1992/3281 lr 0.001 accuracy 93.48047 wps 12455.41 step time 0.59s\n","# Epoch 5  global step 15140 loss 5.64734 batch 2012/3281 lr 0.001 accuracy 93.84766 wps 12082.88 step time 0.66s\n","# Epoch 5  global step 15160 loss 5.82901 batch 2032/3281 lr 0.001 accuracy 93.16016 wps 12153.63 step time 0.70s\n","# Epoch 5  global step 15180 loss 5.76812 batch 2052/3281 lr 0.001 accuracy 93.34375 wps 12357.44 step time 0.63s\n","# Epoch 5  global step 15200 loss 5.79506 batch 2072/3281 lr 0.001 accuracy 93.42969 wps 13441.46 step time 0.55s\n","# Epoch 5  global step 15220 loss 5.75374 batch 2092/3281 lr 0.001 accuracy 93.21094 wps 11852.92 step time 0.69s\n","# Epoch 5  global step 15240 loss 5.76372 batch 2112/3281 lr 0.001 accuracy 93.36328 wps 11677.30 step time 0.65s\n","# Epoch 5  global step 15260 loss 5.78137 batch 2132/3281 lr 0.001 accuracy 93.67578 wps 13046.04 step time 0.52s\n","# Epoch 5  global step 15280 loss 5.70268 batch 2152/3281 lr 0.001 accuracy 93.57422 wps 12453.89 step time 0.52s\n","# Epoch 5  global step 15300 loss 5.68982 batch 2172/3281 lr 0.001 accuracy 93.70313 wps 12653.17 step time 0.50s\n","# Epoch 5  global step 15320 loss 5.74791 batch 2192/3281 lr 0.001 accuracy 93.41797 wps 12297.88 step time 0.55s\n","# Epoch 5  global step 15340 loss 5.76648 batch 2212/3281 lr 0.001 accuracy 93.51172 wps 12590.58 step time 0.53s\n","# Epoch 5  global step 15360 loss 5.86844 batch 2232/3281 lr 0.001 accuracy 93.28906 wps 13143.01 step time 0.54s\n","# Epoch 5  global step 15380 loss 5.70995 batch 2252/3281 lr 0.001 accuracy 93.62109 wps 12610.34 step time 0.60s\n","# Epoch 5  global step 15400 loss 5.78257 batch 2272/3281 lr 0.001 accuracy 93.42578 wps 12121.27 step time 0.63s\n","# Epoch 5  global step 15420 loss 5.62057 batch 2292/3281 lr 0.001 accuracy 93.84375 wps 12659.12 step time 0.62s\n","# Epoch 5  global step 15440 loss 5.72041 batch 2312/3281 lr 0.001 accuracy 93.29297 wps 12299.77 step time 0.50s\n","# Epoch 5  global step 15460 loss 5.82644 batch 2332/3281 lr 0.001 accuracy 93.31641 wps 11542.54 step time 0.78s\n","# Epoch 5  global step 15480 loss 5.75609 batch 2352/3281 lr 0.001 accuracy 93.45703 wps 12921.41 step time 0.56s\n","# Epoch 5  global step 15500 loss 5.71801 batch 2372/3281 lr 0.001 accuracy 93.54688 wps 11860.33 step time 0.57s\n","# Epoch 5  global step 15520 loss 5.78819 batch 2392/3281 lr 0.001 accuracy 93.40625 wps 12127.57 step time 0.60s\n","# Epoch 5  global step 15540 loss 5.70885 batch 2412/3281 lr 0.001 accuracy 93.56250 wps 12925.36 step time 0.48s\n","# Epoch 5  global step 15560 loss 5.76247 batch 2432/3281 lr 0.001 accuracy 93.50391 wps 11815.82 step time 0.63s\n","# Epoch 5  global step 15580 loss 5.75636 batch 2452/3281 lr 0.001 accuracy 93.62500 wps 12219.37 step time 0.61s\n","# Epoch 5  global step 15600 loss 5.70864 batch 2472/3281 lr 0.001 accuracy 93.60547 wps 12679.65 step time 0.53s\n","# Epoch 5  global step 15620 loss 5.76521 batch 2492/3281 lr 0.001 accuracy 93.30469 wps 11498.36 step time 0.73s\n","# Epoch 5  global step 15640 loss 5.70395 batch 2512/3281 lr 0.001 accuracy 93.50000 wps 13821.93 step time 0.52s\n","# Epoch 5  global step 15660 loss 5.72838 batch 2532/3281 lr 0.001 accuracy 93.46484 wps 13144.05 step time 0.57s\n","# Epoch 5  global step 15680 loss 5.73181 batch 2552/3281 lr 0.001 accuracy 93.46875 wps 12827.12 step time 0.61s\n","# Epoch 5  global step 15700 loss 5.69806 batch 2572/3281 lr 0.001 accuracy 93.56641 wps 12607.97 step time 0.55s\n","# Epoch 5  global step 15720 loss 5.85617 batch 2592/3281 lr 0.001 accuracy 93.08984 wps 12724.40 step time 0.57s\n","# Epoch 5  global step 15740 loss 5.72633 batch 2612/3281 lr 0.001 accuracy 93.44141 wps 12865.57 step time 0.57s\n","# Epoch 5  global step 15760 loss 5.69798 batch 2632/3281 lr 0.001 accuracy 93.59766 wps 11719.21 step time 0.54s\n","# Epoch 5  global step 15780 loss 5.77102 batch 2652/3281 lr 0.001 accuracy 93.19922 wps 12886.72 step time 0.53s\n","# Epoch 5  global step 15800 loss 5.69758 batch 2672/3281 lr 0.001 accuracy 93.64453 wps 12344.31 step time 0.51s\n","# Epoch 5  global step 15820 loss 5.81984 batch 2692/3281 lr 0.001 accuracy 93.21875 wps 11528.80 step time 0.74s\n","# Epoch 5  global step 15840 loss 5.83240 batch 2712/3281 lr 0.001 accuracy 93.11328 wps 12356.79 step time 0.62s\n","# Epoch 5  global step 15860 loss 5.75679 batch 2732/3281 lr 0.001 accuracy 93.59375 wps 12453.48 step time 0.59s\n","# Epoch 5  global step 15880 loss 5.74862 batch 2752/3281 lr 0.001 accuracy 93.41406 wps 12862.18 step time 0.49s\n","# Epoch 5  global step 15900 loss 5.64950 batch 2772/3281 lr 0.001 accuracy 93.92187 wps 12605.87 step time 0.45s\n","# Epoch 5  global step 15920 loss 5.86604 batch 2792/3281 lr 0.001 accuracy 93.10156 wps 12338.65 step time 0.69s\n","# Epoch 5  global step 15940 loss 5.71980 batch 2812/3281 lr 0.001 accuracy 93.50391 wps 13369.47 step time 0.60s\n","# Epoch 5  global step 15960 loss 5.66176 batch 2832/3281 lr 0.001 accuracy 93.76953 wps 12370.17 step time 0.58s\n","# Epoch 5  global step 15980 loss 5.90237 batch 2852/3281 lr 0.001 accuracy 93.01562 wps 12010.52 step time 0.67s\n","# Epoch 5  global step 16000 loss 5.82807 batch 2872/3281 lr 0.001 accuracy 93.30469 wps 12228.68 step time 0.67s\n","# global step 16000, eval model at Sat May 23 08:53:19 2020\n","2020-05-23 08:53:22.455552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:53:22.455998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 08:53:22.456125: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 08:53:22.456162: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 08:53:22.456190: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 08:53:22.456215: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 08:53:22.456240: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 08:53:22.456263: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 08:53:22.456286: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 08:53:22.456389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:53:22.456715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:53:22.456962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 08:53:22.457333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 08:53:22.457370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 08:53:22.457383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 08:53:22.457556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:53:22.457838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 08:53:22.458073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.596456303548224\n","# location_distance_from_business_district - 0.3997144867719332\n","# location_easy_to_find - 0.6559393157755469\n","# service_wait_time - 0.42262707599538796\n","# service_waiters_attitude - 0.7261559614616917\n","# service_parking_convenience - 0.6166182232202512\n","# service_serving_speed - 0.612961525852906\n","# price_level - 0.6454676568481499\n","# price_cost_effective - 0.6224380944476411\n","# price_discount - 0.522173364304404\n","# environment_decoration - 0.6212543450065817\n","# environment_noise - 0.6662142565704148\n","# environment_space - 0.6928200245497077\n","# environment_cleaness - 0.6541367192812817\n","# dish_portion - 0.5208298280841824\n","# dish_taste - 0.6626630164501447\n","# dish_look - 0.3568250862963187\n","# dish_recommendation - 0.38212582577615295\n","# others_overall_experience - 0.5683114653085568\n","# others_willing_to_consume_again - 0.6032744836541598\n","# Eval loss 5.09964, f1 0.57745\n","# current result -0.5774503529601818, previous best result -0.4454822418744492\n","# Epoch 5  global step 16020 loss 5.82806 batch 2892/3281 lr 0.001 accuracy 93.38672 wps 12584.71 step time 0.58s\n","# Epoch 5  global step 16040 loss 5.85302 batch 2912/3281 lr 0.001 accuracy 93.22266 wps 12543.18 step time 0.72s\n","# Epoch 5  global step 16060 loss 5.62865 batch 2932/3281 lr 0.001 accuracy 93.79688 wps 12693.86 step time 0.48s\n","# Epoch 5  global step 16080 loss 5.64674 batch 2952/3281 lr 0.001 accuracy 93.58594 wps 12651.95 step time 0.55s\n","# Epoch 5  global step 16100 loss 5.69727 batch 2972/3281 lr 0.001 accuracy 93.75000 wps 12228.48 step time 0.51s\n","# Epoch 5  global step 16120 loss 5.65868 batch 2992/3281 lr 0.001 accuracy 93.57031 wps 11734.26 step time 0.57s\n","# Epoch 5  global step 16140 loss 5.78638 batch 3012/3281 lr 0.001 accuracy 93.32812 wps 12336.17 step time 0.52s\n","# Epoch 5  global step 16160 loss 5.72353 batch 3032/3281 lr 0.001 accuracy 93.48438 wps 12927.19 step time 0.60s\n","# Epoch 5  global step 16180 loss 5.89974 batch 3052/3281 lr 0.001 accuracy 92.96484 wps 11843.78 step time 0.71s\n","# Epoch 5  global step 16200 loss 5.78401 batch 3072/3281 lr 0.001 accuracy 93.46094 wps 12441.31 step time 0.50s\n","# Epoch 5  global step 16220 loss 5.77415 batch 3092/3281 lr 0.001 accuracy 93.51172 wps 13429.11 step time 0.50s\n","# Epoch 5  global step 16240 loss 5.90019 batch 3112/3281 lr 0.001 accuracy 92.99219 wps 12216.08 step time 0.67s\n","# Epoch 5  global step 16260 loss 5.64515 batch 3132/3281 lr 0.001 accuracy 93.67969 wps 12610.20 step time 0.51s\n","# Epoch 5  global step 16280 loss 5.78425 batch 3152/3281 lr 0.001 accuracy 93.36328 wps 12791.91 step time 0.53s\n","# Epoch 5  global step 16300 loss 5.84438 batch 3172/3281 lr 0.001 accuracy 93.24609 wps 12342.28 step time 0.64s\n","# Epoch 5  global step 16320 loss 6.04123 batch 3192/3281 lr 0.001 accuracy 92.72656 wps 12173.33 step time 0.73s\n","# Epoch 5  global step 16340 loss 5.71859 batch 3212/3281 lr 0.001 accuracy 93.51172 wps 13048.38 step time 0.49s\n","# Epoch 5  global step 16360 loss 5.62519 batch 3232/3281 lr 0.001 accuracy 93.76953 wps 11597.28 step time 0.63s\n","# Epoch 5  global step 16380 loss 5.77333 batch 3252/3281 lr 0.001 accuracy 93.24219 wps 12526.93 step time 0.58s\n","# Epoch 5  global step 16400 loss 5.78479 batch 3272/3281 lr 0.001 accuracy 93.27734 wps 12828.23 step time 0.52s\n","# Finsh epoch 5, global step 16410\n","# Epoch 6  global step 16420 loss 2.89565 batch 10/3281 lr 0.001 accuracy 46.75391 wps 13865.68 step time 0.32s\n","# Epoch 6  global step 16440 loss 5.54109 batch 30/3281 lr 0.001 accuracy 94.10547 wps 14283.40 step time 0.48s\n","# Epoch 6  global step 16460 loss 5.61390 batch 50/3281 lr 0.001 accuracy 93.85547 wps 11897.99 step time 0.66s\n","# Epoch 6  global step 16480 loss 5.69167 batch 70/3281 lr 0.001 accuracy 93.75000 wps 12553.34 step time 0.58s\n","# Epoch 6  global step 16500 loss 5.69523 batch 90/3281 lr 0.001 accuracy 93.32422 wps 12091.14 step time 0.78s\n","# Epoch 6  global step 16520 loss 5.72151 batch 110/3281 lr 0.001 accuracy 93.60938 wps 12953.61 step time 0.54s\n","# Epoch 6  global step 16540 loss 5.58346 batch 130/3281 lr 0.001 accuracy 93.85156 wps 12004.95 step time 0.66s\n","# Epoch 6  global step 16560 loss 5.59735 batch 150/3281 lr 0.001 accuracy 93.75781 wps 11838.58 step time 0.65s\n","# Epoch 6  global step 16580 loss 5.74286 batch 170/3281 lr 0.001 accuracy 93.44141 wps 12436.02 step time 0.58s\n","# Epoch 6  global step 16600 loss 5.51548 batch 190/3281 lr 0.001 accuracy 94.08203 wps 12490.25 step time 0.44s\n","# Epoch 6  global step 16620 loss 5.58142 batch 210/3281 lr 0.001 accuracy 93.66797 wps 11911.88 step time 0.60s\n","# Epoch 6  global step 16640 loss 5.49672 batch 230/3281 lr 0.001 accuracy 94.01563 wps 12052.33 step time 0.54s\n","# Epoch 6  global step 16660 loss 5.65481 batch 250/3281 lr 0.001 accuracy 93.57813 wps 12450.92 step time 0.54s\n","# Epoch 6  global step 16680 loss 5.63951 batch 270/3281 lr 0.001 accuracy 93.65234 wps 13209.49 step time 0.55s\n","# Epoch 6  global step 16700 loss 5.52171 batch 290/3281 lr 0.001 accuracy 94.00781 wps 12683.30 step time 0.52s\n","# Epoch 6  global step 16720 loss 5.52723 batch 310/3281 lr 0.001 accuracy 94.10156 wps 11775.27 step time 0.55s\n","# Epoch 6  global step 16740 loss 5.69762 batch 330/3281 lr 0.001 accuracy 93.58594 wps 11845.65 step time 0.66s\n","# Epoch 6  global step 16760 loss 5.69612 batch 350/3281 lr 0.001 accuracy 93.66016 wps 12425.38 step time 0.62s\n","# Epoch 6  global step 16780 loss 5.68778 batch 370/3281 lr 0.001 accuracy 93.57813 wps 13145.42 step time 0.55s\n","# Epoch 6  global step 16800 loss 5.63269 batch 390/3281 lr 0.001 accuracy 93.68359 wps 11832.28 step time 0.66s\n","# Epoch 6  global step 16820 loss 5.63630 batch 410/3281 lr 0.001 accuracy 93.65234 wps 12588.67 step time 0.52s\n","# Epoch 6  global step 16840 loss 5.53884 batch 430/3281 lr 0.001 accuracy 93.95703 wps 12428.97 step time 0.47s\n","# Epoch 6  global step 16860 loss 5.65373 batch 450/3281 lr 0.001 accuracy 93.76172 wps 13136.83 step time 0.53s\n","# Epoch 6  global step 16880 loss 5.57333 batch 470/3281 lr 0.001 accuracy 94.14062 wps 12621.94 step time 0.50s\n","# Epoch 6  global step 16900 loss 5.78648 batch 490/3281 lr 0.001 accuracy 93.29688 wps 12159.85 step time 0.69s\n","# Epoch 6  global step 16920 loss 5.59186 batch 510/3281 lr 0.001 accuracy 93.77734 wps 12367.43 step time 0.57s\n","# Epoch 6  global step 16940 loss 5.71445 batch 530/3281 lr 0.001 accuracy 93.70313 wps 12597.53 step time 0.54s\n","# Epoch 6  global step 16960 loss 5.67530 batch 550/3281 lr 0.001 accuracy 93.65625 wps 13053.94 step time 0.53s\n","# Epoch 6  global step 16980 loss 5.66747 batch 570/3281 lr 0.001 accuracy 93.59375 wps 12820.47 step time 0.53s\n","# Epoch 6  global step 17000 loss 5.59867 batch 590/3281 lr 0.001 accuracy 93.72656 wps 12136.80 step time 0.63s\n","# Epoch 6  global step 17020 loss 5.59767 batch 610/3281 lr 0.001 accuracy 93.96875 wps 13066.82 step time 0.50s\n","# Epoch 6  global step 17040 loss 5.72267 batch 630/3281 lr 0.001 accuracy 93.57812 wps 13033.04 step time 0.53s\n","# Epoch 6  global step 17060 loss 5.59769 batch 650/3281 lr 0.001 accuracy 93.79688 wps 12070.66 step time 0.68s\n","# Epoch 6  global step 17080 loss 5.60187 batch 670/3281 lr 0.001 accuracy 93.78125 wps 12840.35 step time 0.55s\n","# Epoch 6  global step 17100 loss 5.68307 batch 690/3281 lr 0.001 accuracy 93.67188 wps 12570.22 step time 0.61s\n","# Epoch 6  global step 17120 loss 5.57178 batch 710/3281 lr 0.001 accuracy 93.64063 wps 12402.17 step time 0.64s\n","# Epoch 6  global step 17140 loss 5.65362 batch 730/3281 lr 0.001 accuracy 93.64062 wps 12765.40 step time 0.51s\n","# Epoch 6  global step 17160 loss 5.83968 batch 750/3281 lr 0.001 accuracy 93.14063 wps 12408.16 step time 0.72s\n","# Epoch 6  global step 17180 loss 5.76524 batch 770/3281 lr 0.001 accuracy 93.28906 wps 12472.65 step time 0.76s\n","# Epoch 6  global step 17200 loss 5.61557 batch 790/3281 lr 0.001 accuracy 93.76953 wps 11551.27 step time 0.61s\n","# Epoch 6  global step 17220 loss 5.82597 batch 810/3281 lr 0.001 accuracy 93.27734 wps 12132.71 step time 0.72s\n","# Epoch 6  global step 17240 loss 5.65615 batch 830/3281 lr 0.001 accuracy 93.55078 wps 12243.38 step time 0.70s\n","# Epoch 6  global step 17260 loss 5.56921 batch 850/3281 lr 0.001 accuracy 93.96484 wps 12942.83 step time 0.46s\n","# Epoch 6  global step 17280 loss 5.58978 batch 870/3281 lr 0.001 accuracy 93.90234 wps 12265.97 step time 0.57s\n","# Epoch 6  global step 17300 loss 5.79696 batch 890/3281 lr 0.001 accuracy 93.27734 wps 13148.34 step time 0.66s\n","# Epoch 6  global step 17320 loss 5.70733 batch 910/3281 lr 0.001 accuracy 93.54687 wps 12547.23 step time 0.57s\n","# Epoch 6  global step 17340 loss 5.60095 batch 930/3281 lr 0.001 accuracy 93.83594 wps 13078.31 step time 0.50s\n","# Epoch 6  global step 17360 loss 5.75626 batch 950/3281 lr 0.001 accuracy 93.39844 wps 12916.25 step time 0.53s\n","# Epoch 6  global step 17380 loss 5.74449 batch 970/3281 lr 0.001 accuracy 93.33203 wps 12008.84 step time 0.73s\n","# Epoch 6  global step 17400 loss 5.79695 batch 990/3281 lr 0.001 accuracy 93.26172 wps 11470.02 step time 0.81s\n","# Epoch 6  global step 17420 loss 5.66741 batch 1010/3281 lr 0.001 accuracy 93.67578 wps 12199.03 step time 0.57s\n","# Epoch 6  global step 17440 loss 5.81108 batch 1030/3281 lr 0.001 accuracy 93.16016 wps 12443.22 step time 0.66s\n","# Epoch 6  global step 17460 loss 5.52322 batch 1050/3281 lr 0.001 accuracy 94.18359 wps 12661.26 step time 0.44s\n","# Epoch 6  global step 17480 loss 5.75471 batch 1070/3281 lr 0.001 accuracy 93.25000 wps 12263.32 step time 0.62s\n","# Epoch 6  global step 17500 loss 5.74077 batch 1090/3281 lr 0.001 accuracy 93.48047 wps 12087.86 step time 0.61s\n","# Epoch 6  global step 17520 loss 5.69766 batch 1110/3281 lr 0.001 accuracy 93.58594 wps 11927.10 step time 0.66s\n","# Epoch 6  global step 17540 loss 5.62302 batch 1130/3281 lr 0.001 accuracy 93.74609 wps 12676.69 step time 0.54s\n","# Epoch 6  global step 17560 loss 5.45370 batch 1150/3281 lr 0.001 accuracy 94.37891 wps 11920.00 step time 0.55s\n","# Epoch 6  global step 17580 loss 5.68850 batch 1170/3281 lr 0.001 accuracy 93.60938 wps 12317.91 step time 0.63s\n","# Epoch 6  global step 17600 loss 5.75745 batch 1190/3281 lr 0.001 accuracy 93.33984 wps 12785.77 step time 0.60s\n","# Epoch 6  global step 17620 loss 5.55908 batch 1210/3281 lr 0.001 accuracy 94.04297 wps 12611.22 step time 0.50s\n","# Epoch 6  global step 17640 loss 5.78995 batch 1230/3281 lr 0.001 accuracy 93.33594 wps 12891.76 step time 0.65s\n","# Epoch 6  global step 17660 loss 5.75673 batch 1250/3281 lr 0.001 accuracy 93.42188 wps 12623.44 step time 0.73s\n","# Epoch 6  global step 17680 loss 5.64986 batch 1270/3281 lr 0.001 accuracy 93.83984 wps 12476.69 step time 0.54s\n","# Epoch 6  global step 17700 loss 5.45064 batch 1290/3281 lr 0.001 accuracy 94.25000 wps 12592.30 step time 0.42s\n","# Epoch 6  global step 17720 loss 5.66966 batch 1310/3281 lr 0.001 accuracy 93.50781 wps 12299.01 step time 0.63s\n","# Epoch 6  global step 17740 loss 5.66768 batch 1330/3281 lr 0.001 accuracy 93.66406 wps 12636.51 step time 0.50s\n","# Epoch 6  global step 17760 loss 5.64080 batch 1350/3281 lr 0.001 accuracy 93.75781 wps 13384.86 step time 0.48s\n","# Epoch 6  global step 17780 loss 5.71862 batch 1370/3281 lr 0.001 accuracy 93.37500 wps 12652.32 step time 0.72s\n","# Epoch 6  global step 17800 loss 5.81730 batch 1390/3281 lr 0.001 accuracy 93.03516 wps 13794.93 step time 0.62s\n","# Epoch 6  global step 17820 loss 5.58510 batch 1410/3281 lr 0.001 accuracy 94.00000 wps 12599.29 step time 0.44s\n","# Epoch 6  global step 17840 loss 5.65515 batch 1430/3281 lr 0.001 accuracy 93.84375 wps 12979.62 step time 0.57s\n","# Epoch 6  global step 17860 loss 5.61088 batch 1450/3281 lr 0.001 accuracy 93.77734 wps 12263.67 step time 0.56s\n","# Epoch 6  global step 17880 loss 5.47145 batch 1470/3281 lr 0.001 accuracy 94.14062 wps 12809.67 step time 0.44s\n","# Epoch 6  global step 17900 loss 5.81849 batch 1490/3281 lr 0.001 accuracy 93.33203 wps 13185.56 step time 0.54s\n","# Epoch 6  global step 17920 loss 5.48589 batch 1510/3281 lr 0.001 accuracy 94.27734 wps 12694.82 step time 0.47s\n","# Epoch 6  global step 17940 loss 5.60353 batch 1530/3281 lr 0.001 accuracy 93.72656 wps 12813.19 step time 0.51s\n","# Epoch 6  global step 17960 loss 5.78726 batch 1550/3281 lr 0.001 accuracy 93.33984 wps 11670.19 step time 0.74s\n","# Epoch 6  global step 17980 loss 5.55724 batch 1570/3281 lr 0.001 accuracy 93.86719 wps 12626.43 step time 0.57s\n","# Epoch 6  global step 18000 loss 5.69970 batch 1590/3281 lr 0.001 accuracy 93.60156 wps 12900.88 step time 0.53s\n","# global step 18000, eval model at Sat May 23 09:15:01 2020\n","2020-05-23 09:15:03.590340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:15:03.590811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 09:15:03.590929: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 09:15:03.590956: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 09:15:03.590987: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 09:15:03.591009: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 09:15:03.591030: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 09:15:03.591054: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 09:15:03.591079: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 09:15:03.591181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:15:03.591479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:15:03.591757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 09:15:03.592063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 09:15:03.592082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 09:15:03.592092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 09:15:03.592245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:15:03.592543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:15:03.592767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6182422160469626\n","# location_distance_from_business_district - 0.45008865549114485\n","# location_easy_to_find - 0.6755739524945583\n","# service_wait_time - 0.5487918411573848\n","# service_waiters_attitude - 0.7638911397792034\n","# service_parking_convenience - 0.6829799185526662\n","# service_serving_speed - 0.6894779052320015\n","# price_level - 0.7230770105827978\n","# price_cost_effective - 0.6721358317482874\n","# price_discount - 0.5840798913355939\n","# environment_decoration - 0.6773602908901215\n","# environment_noise - 0.7341363280158834\n","# environment_space - 0.7393396093910639\n","# environment_cleaness - 0.7109696280399105\n","# dish_portion - 0.6199193778220029\n","# dish_taste - 0.6970406387885\n","# dish_look - 0.44790216469716027\n","# dish_recommendation - 0.6424658418976955\n","# others_overall_experience - 0.5732588936426904\n","# others_willing_to_consume_again - 0.652237052473337\n","# Eval loss 4.79140, f1 0.64515\n","# current result -0.6451484094039482, previous best result -0.5774503529601818\n","# Epoch 6  global step 18020 loss 5.64097 batch 1610/3281 lr 0.001 accuracy 93.64063 wps 11313.93 step time 0.71s\n","# Epoch 6  global step 18040 loss 5.52955 batch 1630/3281 lr 0.001 accuracy 94.07422 wps 12222.32 step time 0.50s\n","# Epoch 6  global step 18060 loss 5.72765 batch 1650/3281 lr 0.001 accuracy 93.55859 wps 11833.03 step time 0.73s\n","# Epoch 6  global step 18080 loss 5.80514 batch 1670/3281 lr 0.001 accuracy 93.42969 wps 12285.86 step time 0.64s\n","# Epoch 6  global step 18100 loss 5.59761 batch 1690/3281 lr 0.001 accuracy 94.00000 wps 11971.48 step time 0.57s\n","# Epoch 6  global step 18120 loss 5.62952 batch 1710/3281 lr 0.001 accuracy 93.75781 wps 12654.46 step time 0.54s\n","# Epoch 6  global step 18140 loss 5.67991 batch 1730/3281 lr 0.001 accuracy 93.58203 wps 12466.33 step time 0.63s\n","# Epoch 6  global step 18160 loss 5.66946 batch 1750/3281 lr 0.001 accuracy 93.53516 wps 12183.40 step time 0.61s\n","# Epoch 6  global step 18180 loss 5.67917 batch 1770/3281 lr 0.001 accuracy 93.58203 wps 11769.71 step time 0.58s\n","# Epoch 6  global step 18200 loss 5.77668 batch 1790/3281 lr 0.001 accuracy 93.45703 wps 12253.32 step time 0.67s\n","# Epoch 6  global step 18220 loss 5.59642 batch 1810/3281 lr 0.001 accuracy 93.80859 wps 12683.60 step time 0.50s\n","# Epoch 6  global step 18240 loss 5.86105 batch 1830/3281 lr 0.001 accuracy 93.25000 wps 12900.35 step time 0.62s\n","# Epoch 6  global step 18260 loss 5.57909 batch 1850/3281 lr 0.001 accuracy 93.94141 wps 12646.02 step time 0.49s\n","# Epoch 6  global step 18280 loss 5.82171 batch 1870/3281 lr 0.001 accuracy 93.57031 wps 13259.60 step time 0.53s\n","# Epoch 6  global step 18300 loss 5.78836 batch 1890/3281 lr 0.001 accuracy 93.30469 wps 12652.61 step time 0.62s\n","# Epoch 6  global step 18320 loss 5.66460 batch 1910/3281 lr 0.001 accuracy 93.60938 wps 12445.95 step time 0.58s\n","# Epoch 6  global step 18340 loss 5.77396 batch 1930/3281 lr 0.001 accuracy 93.19531 wps 11924.37 step time 0.77s\n","# Epoch 6  global step 18360 loss 5.63117 batch 1950/3281 lr 0.001 accuracy 93.96484 wps 12654.37 step time 0.53s\n","# Epoch 6  global step 18380 loss 5.70464 batch 1970/3281 lr 0.001 accuracy 93.45703 wps 12598.01 step time 0.60s\n","# Epoch 6  global step 18400 loss 5.66014 batch 1990/3281 lr 0.001 accuracy 93.69141 wps 12105.77 step time 0.65s\n","# Epoch 6  global step 18420 loss 5.59657 batch 2010/3281 lr 0.001 accuracy 93.76172 wps 12710.94 step time 0.55s\n","# Epoch 6  global step 18440 loss 5.68413 batch 2030/3281 lr 0.001 accuracy 93.54297 wps 11987.68 step time 0.58s\n","# Epoch 6  global step 18460 loss 5.72746 batch 2050/3281 lr 0.001 accuracy 93.53906 wps 12963.55 step time 0.54s\n","# Epoch 6  global step 18480 loss 5.68366 batch 2070/3281 lr 0.001 accuracy 93.49219 wps 12293.11 step time 0.58s\n","# Epoch 6  global step 18500 loss 5.77535 batch 2090/3281 lr 0.001 accuracy 93.33203 wps 12428.73 step time 0.60s\n","# Epoch 6  global step 18520 loss 5.61026 batch 2110/3281 lr 0.001 accuracy 93.92969 wps 12583.61 step time 0.61s\n","# Epoch 6  global step 18540 loss 5.78591 batch 2130/3281 lr 0.001 accuracy 93.46875 wps 12174.50 step time 0.68s\n","# Epoch 6  global step 18560 loss 5.55347 batch 2150/3281 lr 0.001 accuracy 93.87500 wps 12180.67 step time 0.56s\n","# Epoch 6  global step 18580 loss 5.60969 batch 2170/3281 lr 0.001 accuracy 93.83594 wps 12478.01 step time 0.54s\n","# Epoch 6  global step 18600 loss 5.57398 batch 2190/3281 lr 0.001 accuracy 93.94531 wps 11455.06 step time 0.63s\n","# Epoch 6  global step 18620 loss 5.67569 batch 2210/3281 lr 0.001 accuracy 93.57422 wps 11667.28 step time 0.61s\n","# Epoch 6  global step 18640 loss 5.69601 batch 2230/3281 lr 0.001 accuracy 93.37500 wps 13143.85 step time 0.61s\n","# Epoch 6  global step 18660 loss 5.70031 batch 2250/3281 lr 0.001 accuracy 93.43359 wps 13248.86 step time 0.55s\n","# Epoch 6  global step 18680 loss 5.64144 batch 2270/3281 lr 0.001 accuracy 93.76172 wps 12340.54 step time 0.53s\n","# Epoch 6  global step 18700 loss 5.67503 batch 2290/3281 lr 0.001 accuracy 93.87500 wps 12839.15 step time 0.51s\n","# Epoch 6  global step 18720 loss 5.47668 batch 2310/3281 lr 0.001 accuracy 94.34375 wps 12991.93 step time 0.46s\n","# Epoch 6  global step 18740 loss 5.56470 batch 2330/3281 lr 0.001 accuracy 93.89453 wps 12112.25 step time 0.58s\n","# Epoch 6  global step 18760 loss 5.48509 batch 2350/3281 lr 0.001 accuracy 94.16406 wps 12777.47 step time 0.46s\n","# Epoch 6  global step 18780 loss 5.73798 batch 2370/3281 lr 0.001 accuracy 93.58203 wps 11816.08 step time 0.67s\n","# Epoch 6  global step 18800 loss 5.76853 batch 2390/3281 lr 0.001 accuracy 93.50391 wps 13244.35 step time 0.54s\n","# Epoch 6  global step 18820 loss 5.73383 batch 2410/3281 lr 0.001 accuracy 93.41406 wps 13138.03 step time 0.57s\n","# Epoch 6  global step 18840 loss 5.76398 batch 2430/3281 lr 0.001 accuracy 93.50000 wps 12953.67 step time 0.56s\n","# Epoch 6  global step 18860 loss 5.67073 batch 2450/3281 lr 0.001 accuracy 93.53906 wps 12281.78 step time 0.64s\n","# Epoch 6  global step 18880 loss 5.47984 batch 2470/3281 lr 0.001 accuracy 94.11328 wps 12884.20 step time 0.50s\n","# Epoch 6  global step 18900 loss 5.69868 batch 2490/3281 lr 0.001 accuracy 93.93750 wps 13158.00 step time 0.49s\n","# Epoch 6  global step 18920 loss 5.56200 batch 2510/3281 lr 0.001 accuracy 94.05469 wps 13040.75 step time 0.46s\n","# Epoch 6  global step 18940 loss 5.64930 batch 2530/3281 lr 0.001 accuracy 93.78516 wps 12389.89 step time 0.47s\n","# Epoch 6  global step 18960 loss 5.63285 batch 2550/3281 lr 0.001 accuracy 93.79688 wps 12866.42 step time 0.52s\n","# Epoch 6  global step 18980 loss 5.63475 batch 2570/3281 lr 0.001 accuracy 93.67969 wps 13137.27 step time 0.54s\n","# Epoch 6  global step 19000 loss 5.72139 batch 2590/3281 lr 0.001 accuracy 93.75000 wps 13129.41 step time 0.54s\n","# Epoch 6  global step 19020 loss 5.55939 batch 2610/3281 lr 0.001 accuracy 94.01953 wps 13264.09 step time 0.48s\n","# Epoch 6  global step 19040 loss 5.63013 batch 2630/3281 lr 0.001 accuracy 93.71094 wps 13013.94 step time 0.51s\n","# Epoch 6  global step 19060 loss 5.82334 batch 2650/3281 lr 0.001 accuracy 93.21484 wps 13359.58 step time 0.62s\n","# Epoch 6  global step 19080 loss 5.62884 batch 2670/3281 lr 0.001 accuracy 93.97656 wps 12742.40 step time 0.52s\n","# Epoch 6  global step 19100 loss 5.57739 batch 2690/3281 lr 0.001 accuracy 94.07813 wps 13071.31 step time 0.49s\n","# Epoch 6  global step 19120 loss 5.79315 batch 2710/3281 lr 0.001 accuracy 93.25391 wps 12166.06 step time 0.72s\n","# Epoch 6  global step 19140 loss 5.73063 batch 2730/3281 lr 0.001 accuracy 93.41406 wps 11525.46 step time 0.73s\n","# Epoch 6  global step 19160 loss 5.68488 batch 2750/3281 lr 0.001 accuracy 93.62500 wps 12073.57 step time 0.62s\n","# Epoch 6  global step 19180 loss 5.63390 batch 2770/3281 lr 0.001 accuracy 93.80469 wps 12959.89 step time 0.58s\n","# Epoch 6  global step 19200 loss 5.76777 batch 2790/3281 lr 0.001 accuracy 93.42578 wps 13493.33 step time 0.59s\n","# Epoch 6  global step 19220 loss 5.77543 batch 2810/3281 lr 0.001 accuracy 93.14062 wps 12627.29 step time 0.62s\n","# Epoch 6  global step 19240 loss 5.75289 batch 2830/3281 lr 0.001 accuracy 93.46875 wps 12306.80 step time 0.60s\n","# Epoch 6  global step 19260 loss 5.78720 batch 2850/3281 lr 0.001 accuracy 93.44922 wps 13403.01 step time 0.51s\n","# Epoch 6  global step 19280 loss 5.76208 batch 2870/3281 lr 0.001 accuracy 93.26172 wps 13286.73 step time 0.65s\n","# Epoch 6  global step 19300 loss 5.65780 batch 2890/3281 lr 0.001 accuracy 93.50781 wps 12519.43 step time 0.54s\n","# Epoch 6  global step 19320 loss 5.61750 batch 2910/3281 lr 0.001 accuracy 93.67187 wps 11859.09 step time 0.59s\n","# Epoch 6  global step 19340 loss 5.78524 batch 2930/3281 lr 0.001 accuracy 93.25391 wps 12057.33 step time 0.68s\n","# Epoch 6  global step 19360 loss 5.73247 batch 2950/3281 lr 0.001 accuracy 93.37891 wps 12068.49 step time 0.60s\n","# Epoch 6  global step 19380 loss 5.53175 batch 2970/3281 lr 0.001 accuracy 94.23437 wps 12692.46 step time 0.41s\n","# Epoch 6  global step 19400 loss 5.63284 batch 2990/3281 lr 0.001 accuracy 93.53125 wps 12389.15 step time 0.53s\n","# Epoch 6  global step 19420 loss 5.79121 batch 3010/3281 lr 0.001 accuracy 93.33203 wps 12138.54 step time 0.64s\n","# Epoch 6  global step 19440 loss 5.63645 batch 3030/3281 lr 0.001 accuracy 93.69141 wps 13158.73 step time 0.51s\n","# Epoch 6  global step 19460 loss 5.82656 batch 3050/3281 lr 0.001 accuracy 93.22266 wps 13031.43 step time 0.56s\n","# Epoch 6  global step 19480 loss 5.62441 batch 3070/3281 lr 0.001 accuracy 93.59766 wps 12484.30 step time 0.49s\n","# Epoch 6  global step 19500 loss 5.70955 batch 3090/3281 lr 0.001 accuracy 93.50391 wps 12742.64 step time 0.55s\n","# Epoch 6  global step 19520 loss 5.49338 batch 3110/3281 lr 0.001 accuracy 94.30859 wps 12553.12 step time 0.44s\n","# Epoch 6  global step 19540 loss 5.81373 batch 3130/3281 lr 0.001 accuracy 93.28906 wps 12136.89 step time 0.67s\n","# Epoch 6  global step 19560 loss 5.71070 batch 3150/3281 lr 0.001 accuracy 93.34766 wps 12644.93 step time 0.61s\n","# Epoch 6  global step 19580 loss 5.56760 batch 3170/3281 lr 0.001 accuracy 94.04297 wps 12871.51 step time 0.46s\n","# Epoch 6  global step 19600 loss 5.76747 batch 3190/3281 lr 0.001 accuracy 93.43359 wps 12445.01 step time 0.67s\n","# Epoch 6  global step 19620 loss 5.70009 batch 3210/3281 lr 0.001 accuracy 93.57812 wps 12866.77 step time 0.47s\n","# Epoch 6  global step 19640 loss 5.74472 batch 3230/3281 lr 0.001 accuracy 93.54688 wps 12290.45 step time 0.64s\n","# Epoch 6  global step 19660 loss 5.64792 batch 3250/3281 lr 0.001 accuracy 93.62109 wps 12817.32 step time 0.55s\n","# Epoch 6  global step 19680 loss 5.74022 batch 3270/3281 lr 0.001 accuracy 93.31250 wps 12040.19 step time 0.70s\n","# Finsh epoch 6, global step 19692\n","# Epoch 7  global step 19700 loss 2.25322 batch 8/3281 lr 0.001 accuracy 37.60547 wps 13637.37 step time 0.26s\n","# Epoch 7  global step 19720 loss 5.53636 batch 28/3281 lr 0.001 accuracy 93.85547 wps 14207.18 step time 0.56s\n","# Epoch 7  global step 19740 loss 5.60446 batch 48/3281 lr 0.001 accuracy 93.79297 wps 14437.51 step time 0.56s\n","# Epoch 7  global step 19760 loss 5.51249 batch 68/3281 lr 0.001 accuracy 94.01563 wps 14632.85 step time 0.47s\n","# Epoch 7  global step 19780 loss 5.49163 batch 88/3281 lr 0.001 accuracy 94.03125 wps 14396.29 step time 0.45s\n","# Epoch 7  global step 19800 loss 5.56024 batch 108/3281 lr 0.001 accuracy 94.08594 wps 14541.90 step time 0.46s\n","# Epoch 7  global step 19820 loss 5.61257 batch 128/3281 lr 0.001 accuracy 93.79297 wps 13929.56 step time 0.52s\n","# Epoch 7  global step 19840 loss 5.47798 batch 148/3281 lr 0.001 accuracy 94.14062 wps 14106.26 step time 0.52s\n","# Epoch 7  global step 19860 loss 5.51960 batch 168/3281 lr 0.001 accuracy 93.98047 wps 14643.08 step time 0.47s\n","# Epoch 7  global step 19880 loss 5.68768 batch 188/3281 lr 0.001 accuracy 93.57813 wps 14753.19 step time 0.47s\n","# Epoch 7  global step 19900 loss 5.53807 batch 208/3281 lr 0.001 accuracy 93.82422 wps 14667.04 step time 0.55s\n","# Epoch 7  global step 19920 loss 5.65075 batch 228/3281 lr 0.001 accuracy 93.66406 wps 13729.02 step time 0.58s\n","# Epoch 7  global step 19940 loss 5.68549 batch 248/3281 lr 0.001 accuracy 93.55859 wps 14433.92 step time 0.57s\n","# Epoch 7  global step 19960 loss 5.71228 batch 268/3281 lr 0.001 accuracy 93.33203 wps 15158.26 step time 0.52s\n","# Epoch 7  global step 19980 loss 5.55902 batch 288/3281 lr 0.001 accuracy 93.71094 wps 14195.64 step time 0.53s\n","# Epoch 7  global step 20000 loss 5.59401 batch 308/3281 lr 0.001 accuracy 93.73828 wps 14773.10 step time 0.47s\n","# global step 20000, eval model at Sat May 23 09:36:14 2020\n","2020-05-23 09:36:16.769324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:36:16.769762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 09:36:16.769882: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 09:36:16.769909: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 09:36:16.769931: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 09:36:16.769955: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 09:36:16.769977: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 09:36:16.770002: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 09:36:16.770024: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 09:36:16.770118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:36:16.770387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:36:16.770645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 09:36:16.770958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 09:36:16.770977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 09:36:16.770994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 09:36:16.771146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:36:16.771417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:36:16.771658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6256947090704708\n","# location_distance_from_business_district - 0.4846509378401807\n","# location_easy_to_find - 0.6826677557115965\n","# service_wait_time - 0.6039809726277816\n","# service_waiters_attitude - 0.7762165654550096\n","# service_parking_convenience - 0.694351168355666\n","# service_serving_speed - 0.7280149263177821\n","# price_level - 0.7526681144639639\n","# price_cost_effective - 0.6984659298700133\n","# price_discount - 0.6210059765133258\n","# environment_decoration - 0.7013560420108564\n","# environment_noise - 0.7504878510512656\n","# environment_space - 0.7517319511134594\n","# environment_cleaness - 0.7380276168059235\n","# dish_portion - 0.6668239385534473\n","# dish_taste - 0.7134587724695276\n","# dish_look - 0.4998437621501623\n","# dish_recommendation - 0.7086328478828338\n","# others_overall_experience - 0.5754062168072211\n","# others_willing_to_consume_again - 0.6713083650924384\n","# Eval loss 4.62188, f1 0.67224\n","# current result -0.6722397210081463, previous best result -0.6451484094039482\n","# Epoch 7  global step 20020 loss 5.62950 batch 328/3281 lr 0.001 accuracy 93.66016 wps 13877.59 step time 0.53s\n","# Epoch 7  global step 20040 loss 5.61660 batch 348/3281 lr 0.001 accuracy 93.81641 wps 14517.03 step time 0.45s\n","# Epoch 7  global step 20060 loss 5.56169 batch 368/3281 lr 0.001 accuracy 93.94531 wps 14586.93 step time 0.47s\n","# Epoch 7  global step 20080 loss 5.64926 batch 388/3281 lr 0.001 accuracy 93.61328 wps 14013.51 step time 0.65s\n","# Epoch 7  global step 20100 loss 5.52292 batch 408/3281 lr 0.001 accuracy 94.06250 wps 14741.60 step time 0.47s\n","# Epoch 7  global step 20120 loss 5.48464 batch 428/3281 lr 0.001 accuracy 93.94141 wps 14571.28 step time 0.46s\n","# Epoch 7  global step 20140 loss 5.61791 batch 448/3281 lr 0.001 accuracy 93.67969 wps 14688.12 step time 0.49s\n","# Epoch 7  global step 20160 loss 5.48565 batch 468/3281 lr 0.001 accuracy 94.05859 wps 14612.94 step time 0.48s\n","# Epoch 7  global step 20180 loss 5.54897 batch 488/3281 lr 0.001 accuracy 93.83203 wps 15034.42 step time 0.52s\n","# Epoch 7  global step 20200 loss 5.70934 batch 508/3281 lr 0.001 accuracy 93.58203 wps 14277.34 step time 0.53s\n","# Epoch 7  global step 20220 loss 5.71032 batch 528/3281 lr 0.001 accuracy 93.51953 wps 15050.52 step time 0.52s\n","# Epoch 7  global step 20240 loss 5.57040 batch 548/3281 lr 0.001 accuracy 93.77734 wps 14893.58 step time 0.52s\n","# Epoch 7  global step 20260 loss 5.57967 batch 568/3281 lr 0.001 accuracy 93.80469 wps 14893.28 step time 0.49s\n","# Epoch 7  global step 20280 loss 5.43684 batch 588/3281 lr 0.001 accuracy 94.15625 wps 14218.22 step time 0.43s\n","# Epoch 7  global step 20300 loss 5.61639 batch 608/3281 lr 0.001 accuracy 93.71484 wps 13974.95 step time 0.53s\n","# Epoch 7  global step 20320 loss 5.61769 batch 628/3281 lr 0.001 accuracy 93.77344 wps 14492.43 step time 0.47s\n","# Epoch 7  global step 20340 loss 5.63821 batch 648/3281 lr 0.001 accuracy 93.59375 wps 14718.77 step time 0.49s\n","# Epoch 7  global step 20360 loss 5.62823 batch 668/3281 lr 0.001 accuracy 93.76953 wps 14591.19 step time 0.47s\n","# Epoch 7  global step 20380 loss 5.66028 batch 688/3281 lr 0.001 accuracy 93.75391 wps 15226.48 step time 0.56s\n","# Epoch 7  global step 20400 loss 5.64359 batch 708/3281 lr 0.001 accuracy 93.69922 wps 14306.15 step time 0.44s\n","# Epoch 7  global step 20420 loss 5.65626 batch 728/3281 lr 0.001 accuracy 93.91406 wps 14106.15 step time 0.51s\n","# Epoch 7  global step 20440 loss 5.45916 batch 748/3281 lr 0.001 accuracy 94.23828 wps 13866.56 step time 0.50s\n","# Epoch 7  global step 20460 loss 5.49950 batch 768/3281 lr 0.001 accuracy 93.98437 wps 13704.11 step time 0.52s\n","# Epoch 7  global step 20480 loss 5.52025 batch 788/3281 lr 0.001 accuracy 93.92188 wps 14615.59 step time 0.47s\n","# Epoch 7  global step 20500 loss 5.45067 batch 808/3281 lr 0.001 accuracy 94.32422 wps 14903.29 step time 0.51s\n","# Epoch 7  global step 20520 loss 5.61872 batch 828/3281 lr 0.001 accuracy 93.79688 wps 14821.87 step time 0.51s\n","# Epoch 7  global step 20540 loss 5.73597 batch 848/3281 lr 0.001 accuracy 93.49219 wps 14823.64 step time 0.50s\n","# Epoch 7  global step 20560 loss 5.47930 batch 868/3281 lr 0.001 accuracy 94.27344 wps 13998.55 step time 0.43s\n","# Epoch 7  global step 20580 loss 5.65619 batch 888/3281 lr 0.001 accuracy 93.63672 wps 14300.01 step time 0.45s\n","# Epoch 7  global step 20600 loss 5.45237 batch 908/3281 lr 0.001 accuracy 94.02734 wps 13986.76 step time 0.54s\n","# Epoch 7  global step 20620 loss 5.51306 batch 928/3281 lr 0.001 accuracy 93.94922 wps 14310.88 step time 0.46s\n","# Epoch 7  global step 20640 loss 5.60067 batch 948/3281 lr 0.001 accuracy 93.82422 wps 13786.31 step time 0.58s\n","# Epoch 7  global step 20660 loss 5.55169 batch 968/3281 lr 0.001 accuracy 93.96094 wps 13394.15 step time 0.54s\n","# Epoch 7  global step 20680 loss 5.71029 batch 988/3281 lr 0.001 accuracy 93.76563 wps 12284.95 step time 0.62s\n","# Epoch 7  global step 20700 loss 5.49777 batch 1008/3281 lr 0.001 accuracy 93.96484 wps 11972.50 step time 0.62s\n","# Epoch 7  global step 20720 loss 5.44803 batch 1028/3281 lr 0.001 accuracy 94.24219 wps 13147.91 step time 0.45s\n","# Epoch 7  global step 20740 loss 5.65574 batch 1048/3281 lr 0.001 accuracy 93.56641 wps 12556.21 step time 0.60s\n","# Epoch 7  global step 20760 loss 5.66877 batch 1068/3281 lr 0.001 accuracy 93.60547 wps 11706.87 step time 0.67s\n","# Epoch 7  global step 20780 loss 5.67819 batch 1088/3281 lr 0.001 accuracy 93.41797 wps 12297.11 step time 0.68s\n","# Epoch 7  global step 20800 loss 5.51454 batch 1108/3281 lr 0.001 accuracy 94.02344 wps 12174.06 step time 0.52s\n","# Epoch 7  global step 20820 loss 5.55245 batch 1128/3281 lr 0.001 accuracy 94.08984 wps 12555.81 step time 0.52s\n","# Epoch 7  global step 20840 loss 5.57765 batch 1148/3281 lr 0.001 accuracy 93.76563 wps 12247.17 step time 0.59s\n","# Epoch 7  global step 20860 loss 5.57749 batch 1168/3281 lr 0.001 accuracy 93.89453 wps 12627.88 step time 0.56s\n","# Epoch 7  global step 20880 loss 5.69726 batch 1188/3281 lr 0.001 accuracy 93.40234 wps 12725.12 step time 0.55s\n","# Epoch 7  global step 20900 loss 5.69471 batch 1208/3281 lr 0.001 accuracy 93.38672 wps 13150.72 step time 0.63s\n","# Epoch 7  global step 20920 loss 5.50847 batch 1228/3281 lr 0.001 accuracy 94.01172 wps 12501.91 step time 0.50s\n","# Epoch 7  global step 20940 loss 5.54071 batch 1248/3281 lr 0.001 accuracy 93.83984 wps 11991.58 step time 0.58s\n","# Epoch 7  global step 20960 loss 5.63596 batch 1268/3281 lr 0.001 accuracy 93.80469 wps 11956.50 step time 0.58s\n","# Epoch 7  global step 20980 loss 5.41067 batch 1288/3281 lr 0.001 accuracy 94.37891 wps 12386.70 step time 0.50s\n","# Epoch 7  global step 21000 loss 5.59789 batch 1308/3281 lr 0.001 accuracy 93.90234 wps 12791.37 step time 0.52s\n","# Epoch 7  global step 21020 loss 5.68752 batch 1328/3281 lr 0.001 accuracy 93.66406 wps 12506.68 step time 0.64s\n","# Epoch 7  global step 21040 loss 5.44561 batch 1348/3281 lr 0.001 accuracy 94.27734 wps 12767.02 step time 0.47s\n","# Epoch 7  global step 21060 loss 5.61753 batch 1368/3281 lr 0.001 accuracy 93.91016 wps 12658.04 step time 0.62s\n","# Epoch 7  global step 21080 loss 5.40656 batch 1388/3281 lr 0.001 accuracy 94.20703 wps 12823.49 step time 0.46s\n","# Epoch 7  global step 21100 loss 5.64625 batch 1408/3281 lr 0.001 accuracy 93.66406 wps 12554.71 step time 0.61s\n","# Epoch 7  global step 21120 loss 5.53544 batch 1428/3281 lr 0.001 accuracy 94.10156 wps 13075.63 step time 0.53s\n","# Epoch 7  global step 21140 loss 5.49332 batch 1448/3281 lr 0.001 accuracy 94.21484 wps 12252.27 step time 0.57s\n","# Epoch 7  global step 21160 loss 5.60988 batch 1468/3281 lr 0.001 accuracy 94.08203 wps 12964.82 step time 0.50s\n","# Epoch 7  global step 21180 loss 5.59431 batch 1488/3281 lr 0.001 accuracy 93.79297 wps 12496.57 step time 0.53s\n","# Epoch 7  global step 21200 loss 5.73650 batch 1508/3281 lr 0.001 accuracy 93.42969 wps 12234.02 step time 0.69s\n","# Epoch 7  global step 21220 loss 5.60972 batch 1528/3281 lr 0.001 accuracy 93.60156 wps 13039.27 step time 0.53s\n","# Epoch 7  global step 21240 loss 5.67552 batch 1548/3281 lr 0.001 accuracy 93.60938 wps 12802.29 step time 0.55s\n","# Epoch 7  global step 21260 loss 5.64781 batch 1568/3281 lr 0.001 accuracy 93.71484 wps 12346.22 step time 0.59s\n","# Epoch 7  global step 21280 loss 5.71686 batch 1588/3281 lr 0.001 accuracy 93.33984 wps 12107.64 step time 0.71s\n","# Epoch 7  global step 21300 loss 5.67965 batch 1608/3281 lr 0.001 accuracy 93.56641 wps 12893.15 step time 0.62s\n","# Epoch 7  global step 21320 loss 5.69843 batch 1628/3281 lr 0.001 accuracy 93.25781 wps 11575.39 step time 0.72s\n","# Epoch 7  global step 21340 loss 5.67728 batch 1648/3281 lr 0.001 accuracy 93.62500 wps 12073.80 step time 0.66s\n","# Epoch 7  global step 21360 loss 5.83776 batch 1668/3281 lr 0.001 accuracy 92.87891 wps 13232.21 step time 0.67s\n","# Epoch 7  global step 21380 loss 5.53947 batch 1688/3281 lr 0.001 accuracy 94.15625 wps 12707.53 step time 0.48s\n","# Epoch 7  global step 21400 loss 5.68832 batch 1708/3281 lr 0.001 accuracy 93.53516 wps 13362.48 step time 0.55s\n","# Epoch 7  global step 21420 loss 5.65835 batch 1728/3281 lr 0.001 accuracy 93.74609 wps 12777.65 step time 0.61s\n","# Epoch 7  global step 21440 loss 5.54109 batch 1748/3281 lr 0.001 accuracy 93.90234 wps 12617.95 step time 0.53s\n","# Epoch 7  global step 21460 loss 5.65389 batch 1768/3281 lr 0.001 accuracy 93.80859 wps 12417.60 step time 0.56s\n","# Epoch 7  global step 21480 loss 5.69981 batch 1788/3281 lr 0.001 accuracy 93.49219 wps 12948.55 step time 0.58s\n","# Epoch 7  global step 21500 loss 5.63384 batch 1808/3281 lr 0.001 accuracy 93.67188 wps 11998.84 step time 0.67s\n","# Epoch 7  global step 21520 loss 5.39392 batch 1828/3281 lr 0.001 accuracy 94.47266 wps 12708.24 step time 0.46s\n","# Epoch 7  global step 21540 loss 5.75412 batch 1848/3281 lr 0.001 accuracy 93.39844 wps 12446.28 step time 0.70s\n","# Epoch 7  global step 21560 loss 5.51616 batch 1868/3281 lr 0.001 accuracy 94.06250 wps 12345.61 step time 0.58s\n","# Epoch 7  global step 21580 loss 5.64343 batch 1888/3281 lr 0.001 accuracy 93.76172 wps 12163.05 step time 0.69s\n","# Epoch 7  global step 21600 loss 5.75309 batch 1908/3281 lr 0.001 accuracy 93.35156 wps 12819.44 step time 0.66s\n","# Epoch 7  global step 21620 loss 5.62760 batch 1928/3281 lr 0.001 accuracy 93.73437 wps 13242.67 step time 0.51s\n","# Epoch 7  global step 21640 loss 5.53033 batch 1948/3281 lr 0.001 accuracy 94.00391 wps 12708.35 step time 0.49s\n","# Epoch 7  global step 21660 loss 5.62280 batch 1968/3281 lr 0.001 accuracy 93.71484 wps 12612.60 step time 0.60s\n","# Epoch 7  global step 21680 loss 5.68854 batch 1988/3281 lr 0.001 accuracy 93.53516 wps 12202.68 step time 0.59s\n","# Epoch 7  global step 21700 loss 5.74867 batch 2008/3281 lr 0.001 accuracy 93.35547 wps 12503.41 step time 0.64s\n","# Epoch 7  global step 21720 loss 5.42166 batch 2028/3281 lr 0.001 accuracy 94.20703 wps 13119.38 step time 0.43s\n","# Epoch 7  global step 21740 loss 5.58598 batch 2048/3281 lr 0.001 accuracy 93.91016 wps 12807.63 step time 0.52s\n","# Epoch 7  global step 21760 loss 5.57197 batch 2068/3281 lr 0.001 accuracy 94.03516 wps 12865.15 step time 0.48s\n","# Epoch 7  global step 21780 loss 5.70830 batch 2088/3281 lr 0.001 accuracy 93.59375 wps 11994.37 step time 0.61s\n","# Epoch 7  global step 21800 loss 5.74411 batch 2108/3281 lr 0.001 accuracy 93.15234 wps 13017.96 step time 0.67s\n","# Epoch 7  global step 21820 loss 5.68003 batch 2128/3281 lr 0.001 accuracy 93.67578 wps 13399.03 step time 0.49s\n","# Epoch 7  global step 21840 loss 5.58548 batch 2148/3281 lr 0.001 accuracy 93.83203 wps 12328.66 step time 0.53s\n","# Epoch 7  global step 21860 loss 5.65841 batch 2168/3281 lr 0.001 accuracy 93.64063 wps 12028.01 step time 0.66s\n","# Epoch 7  global step 21880 loss 5.74477 batch 2188/3281 lr 0.001 accuracy 93.44922 wps 12368.08 step time 0.68s\n","# Epoch 7  global step 21900 loss 5.66440 batch 2208/3281 lr 0.001 accuracy 93.46094 wps 12084.13 step time 0.66s\n","# Epoch 7  global step 21920 loss 5.55644 batch 2228/3281 lr 0.001 accuracy 93.90234 wps 13254.31 step time 0.48s\n","# Epoch 7  global step 21940 loss 5.68531 batch 2248/3281 lr 0.001 accuracy 93.47266 wps 13067.28 step time 0.61s\n","# Epoch 7  global step 21960 loss 5.57973 batch 2268/3281 lr 0.001 accuracy 93.81641 wps 12505.11 step time 0.54s\n","# Epoch 7  global step 21980 loss 5.68123 batch 2288/3281 lr 0.001 accuracy 93.77734 wps 12566.33 step time 0.55s\n","# Epoch 7  global step 22000 loss 5.59474 batch 2308/3281 lr 0.001 accuracy 93.82031 wps 12645.92 step time 0.54s\n","# global step 22000, eval model at Sat May 23 09:56:55 2020\n","2020-05-23 09:56:57.759183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:56:57.759673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 09:56:57.759792: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 09:56:57.759822: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 09:56:57.759848: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 09:56:57.759874: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 09:56:57.759899: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 09:56:57.759923: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 09:56:57.759948: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 09:56:57.760063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:56:57.760346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:56:57.760600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 09:56:57.760964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 09:56:57.760997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 09:56:57.761010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 09:56:57.761173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:56:57.761473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 09:56:57.761703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6380557452266588\n","# location_distance_from_business_district - 0.5054613456990574\n","# location_easy_to_find - 0.6886705665331115\n","# service_wait_time - 0.6360451707858692\n","# service_waiters_attitude - 0.7852272336916328\n","# service_parking_convenience - 0.7147152779474658\n","# service_serving_speed - 0.7389049197955773\n","# price_level - 0.7651245979539409\n","# price_cost_effective - 0.7120227824316316\n","# price_discount - 0.6387066777885932\n","# environment_decoration - 0.7120330263015109\n","# environment_noise - 0.7538448285898842\n","# environment_space - 0.7572537302506672\n","# environment_cleaness - 0.7431587509650321\n","# dish_portion - 0.6913484590810931\n","# dish_taste - 0.7204727101362538\n","# dish_look - 0.532215216240168\n","# dish_recommendation - 0.7243129152646504\n","# others_overall_experience - 0.5787742397918686\n","# others_willing_to_consume_again - 0.684739133222469\n","# Eval loss 4.52661, f1 0.68605\n","# current result -0.6860543663848567, previous best result -0.6722397210081463\n","# Epoch 7  global step 22020 loss 5.71026 batch 2328/3281 lr 0.001 accuracy 93.65234 wps 12504.39 step time 0.58s\n","# Epoch 7  global step 22040 loss 5.61217 batch 2348/3281 lr 0.001 accuracy 93.75781 wps 12214.41 step time 0.65s\n","# Epoch 7  global step 22060 loss 5.47202 batch 2368/3281 lr 0.001 accuracy 93.98828 wps 12812.20 step time 0.48s\n","# Epoch 7  global step 22080 loss 5.63833 batch 2388/3281 lr 0.001 accuracy 93.71484 wps 11143.71 step time 0.68s\n","# Epoch 7  global step 22100 loss 5.57206 batch 2408/3281 lr 0.001 accuracy 93.83984 wps 13041.25 step time 0.57s\n","# Epoch 7  global step 22120 loss 5.61290 batch 2428/3281 lr 0.001 accuracy 93.54297 wps 12804.50 step time 0.62s\n","# Epoch 7  global step 22140 loss 5.50245 batch 2448/3281 lr 0.001 accuracy 94.06641 wps 12996.39 step time 0.50s\n","# Epoch 7  global step 22160 loss 5.65346 batch 2468/3281 lr 0.001 accuracy 93.64453 wps 11977.51 step time 0.69s\n","# Epoch 7  global step 22180 loss 5.55774 batch 2488/3281 lr 0.001 accuracy 93.88672 wps 11803.15 step time 0.67s\n","# Epoch 7  global step 22200 loss 5.55654 batch 2508/3281 lr 0.001 accuracy 94.10938 wps 12773.73 step time 0.54s\n","# Epoch 7  global step 22220 loss 5.70506 batch 2528/3281 lr 0.001 accuracy 93.66406 wps 13543.20 step time 0.51s\n","# Epoch 7  global step 22240 loss 5.51546 batch 2548/3281 lr 0.001 accuracy 94.01172 wps 13012.26 step time 0.56s\n","# Epoch 7  global step 22260 loss 5.61188 batch 2568/3281 lr 0.001 accuracy 93.82813 wps 11950.16 step time 0.63s\n","# Epoch 7  global step 22280 loss 5.74852 batch 2588/3281 lr 0.001 accuracy 93.42188 wps 13048.16 step time 0.61s\n","# Epoch 7  global step 22300 loss 5.68030 batch 2608/3281 lr 0.001 accuracy 93.46094 wps 13605.88 step time 0.57s\n","# Epoch 7  global step 22320 loss 5.65061 batch 2628/3281 lr 0.001 accuracy 93.67969 wps 12085.26 step time 0.68s\n","# Epoch 7  global step 22340 loss 5.61212 batch 2648/3281 lr 0.001 accuracy 93.85156 wps 12010.69 step time 0.52s\n","# Epoch 7  global step 22360 loss 5.47827 batch 2668/3281 lr 0.001 accuracy 94.14844 wps 12990.23 step time 0.50s\n","# Epoch 7  global step 22380 loss 5.60007 batch 2688/3281 lr 0.001 accuracy 93.85938 wps 12157.55 step time 0.59s\n","# Epoch 7  global step 22400 loss 5.67378 batch 2708/3281 lr 0.001 accuracy 93.57812 wps 12808.02 step time 0.56s\n","# Epoch 7  global step 22420 loss 5.63230 batch 2728/3281 lr 0.001 accuracy 93.83984 wps 12911.38 step time 0.50s\n","# Epoch 7  global step 22440 loss 5.55613 batch 2748/3281 lr 0.001 accuracy 93.73828 wps 13019.80 step time 0.57s\n","# Epoch 7  global step 22460 loss 5.66151 batch 2768/3281 lr 0.001 accuracy 93.57031 wps 12871.52 step time 0.52s\n","# Epoch 7  global step 22480 loss 5.61315 batch 2788/3281 lr 0.001 accuracy 93.89453 wps 12727.86 step time 0.59s\n","# Epoch 7  global step 22500 loss 5.64770 batch 2808/3281 lr 0.001 accuracy 93.67578 wps 11760.63 step time 0.66s\n","# Epoch 7  global step 22520 loss 5.59540 batch 2828/3281 lr 0.001 accuracy 93.82812 wps 13396.61 step time 0.46s\n","# Epoch 7  global step 22540 loss 5.58175 batch 2848/3281 lr 0.001 accuracy 93.83984 wps 12930.71 step time 0.56s\n","# Epoch 7  global step 22560 loss 5.55823 batch 2868/3281 lr 0.001 accuracy 93.93750 wps 12422.72 step time 0.59s\n","# Epoch 7  global step 22580 loss 5.62584 batch 2888/3281 lr 0.001 accuracy 93.74219 wps 13011.91 step time 0.49s\n","# Epoch 7  global step 22600 loss 5.75222 batch 2908/3281 lr 0.001 accuracy 93.55078 wps 13470.84 step time 0.57s\n","# Epoch 7  global step 22620 loss 5.79062 batch 2928/3281 lr 0.001 accuracy 93.43750 wps 12227.48 step time 0.69s\n","# Epoch 7  global step 22640 loss 5.71438 batch 2948/3281 lr 0.001 accuracy 93.50000 wps 11822.32 step time 0.65s\n","# Epoch 7  global step 22660 loss 5.71855 batch 2968/3281 lr 0.001 accuracy 93.51562 wps 12078.85 step time 0.67s\n","# Epoch 7  global step 22680 loss 5.72393 batch 2988/3281 lr 0.001 accuracy 93.38672 wps 12817.05 step time 0.60s\n","# Epoch 7  global step 22700 loss 5.53647 batch 3008/3281 lr 0.001 accuracy 94.08203 wps 12839.12 step time 0.54s\n","# Epoch 7  global step 22720 loss 5.66112 batch 3028/3281 lr 0.001 accuracy 93.79297 wps 13054.22 step time 0.48s\n","# Epoch 7  global step 22740 loss 5.58920 batch 3048/3281 lr 0.001 accuracy 93.88281 wps 12478.53 step time 0.52s\n","# Epoch 7  global step 22760 loss 5.69289 batch 3068/3281 lr 0.001 accuracy 93.34766 wps 12778.89 step time 0.60s\n","# Epoch 7  global step 22780 loss 5.50453 batch 3088/3281 lr 0.001 accuracy 94.00781 wps 13088.35 step time 0.53s\n","# Epoch 7  global step 22800 loss 5.64888 batch 3108/3281 lr 0.001 accuracy 93.84766 wps 12545.30 step time 0.54s\n","# Epoch 7  global step 22820 loss 5.59440 batch 3128/3281 lr 0.001 accuracy 93.83984 wps 12981.41 step time 0.51s\n","# Epoch 7  global step 22840 loss 5.59749 batch 3148/3281 lr 0.001 accuracy 93.75000 wps 11936.28 step time 0.64s\n","# Epoch 7  global step 22860 loss 5.66488 batch 3168/3281 lr 0.001 accuracy 93.76562 wps 11453.23 step time 0.58s\n","# Epoch 7  global step 22880 loss 5.51154 batch 3188/3281 lr 0.001 accuracy 94.08594 wps 12063.65 step time 0.56s\n","# Epoch 7  global step 22900 loss 5.77675 batch 3208/3281 lr 0.001 accuracy 93.25000 wps 12339.25 step time 0.62s\n","# Epoch 7  global step 22920 loss 5.61295 batch 3228/3281 lr 0.001 accuracy 93.86328 wps 12783.36 step time 0.53s\n","# Epoch 7  global step 22940 loss 5.81495 batch 3248/3281 lr 0.001 accuracy 93.33203 wps 12724.19 step time 0.65s\n","# Epoch 7  global step 22960 loss 5.69913 batch 3268/3281 lr 0.001 accuracy 93.58594 wps 13052.55 step time 0.51s\n","# Finsh epoch 7, global step 22974\n","# Epoch 8  global step 22980 loss 1.65238 batch 6/3281 lr 0.001 accuracy 28.16406 wps 13125.66 step time 0.20s\n","# Epoch 8  global step 23000 loss 5.52537 batch 26/3281 lr 0.001 accuracy 93.98047 wps 14519.74 step time 0.45s\n","# Epoch 8  global step 23020 loss 5.62101 batch 46/3281 lr 0.001 accuracy 93.66797 wps 15125.62 step time 0.51s\n","# Epoch 8  global step 23040 loss 5.50732 batch 66/3281 lr 0.001 accuracy 94.04297 wps 14438.43 step time 0.44s\n","# Epoch 8  global step 23060 loss 5.40619 batch 86/3281 lr 0.001 accuracy 94.37891 wps 14060.37 step time 0.42s\n","# Epoch 8  global step 23080 loss 5.55922 batch 106/3281 lr 0.001 accuracy 93.85156 wps 13676.94 step time 0.51s\n","# Epoch 8  global step 23100 loss 5.46566 batch 126/3281 lr 0.001 accuracy 94.08203 wps 14893.44 step time 0.52s\n","# Epoch 8  global step 23120 loss 5.59341 batch 146/3281 lr 0.001 accuracy 93.79687 wps 15154.29 step time 0.52s\n","# Epoch 8  global step 23140 loss 5.62524 batch 166/3281 lr 0.001 accuracy 93.60156 wps 14300.46 step time 0.67s\n","# Epoch 8  global step 23160 loss 5.43988 batch 186/3281 lr 0.001 accuracy 94.15234 wps 14055.23 step time 0.42s\n","# Epoch 8  global step 23180 loss 5.50629 batch 206/3281 lr 0.001 accuracy 93.96484 wps 14758.34 step time 0.48s\n","# Epoch 8  global step 23200 loss 5.55636 batch 226/3281 lr 0.001 accuracy 93.84766 wps 14787.31 step time 0.54s\n","# Epoch 8  global step 23220 loss 5.43813 batch 246/3281 lr 0.001 accuracy 94.00000 wps 14302.12 step time 0.44s\n","# Epoch 8  global step 23240 loss 5.68356 batch 266/3281 lr 0.001 accuracy 93.67187 wps 15087.28 step time 0.51s\n","# Epoch 8  global step 23260 loss 5.54637 batch 286/3281 lr 0.001 accuracy 93.84375 wps 15263.40 step time 0.52s\n","# Epoch 8  global step 23280 loss 5.36186 batch 306/3281 lr 0.001 accuracy 94.43750 wps 14227.15 step time 0.42s\n","# Epoch 8  global step 23300 loss 5.51461 batch 326/3281 lr 0.001 accuracy 93.97656 wps 14598.10 step time 0.47s\n","# Epoch 8  global step 23320 loss 5.53480 batch 346/3281 lr 0.001 accuracy 94.01953 wps 14261.76 step time 0.52s\n","# Epoch 8  global step 23340 loss 5.72769 batch 366/3281 lr 0.001 accuracy 93.42578 wps 14100.87 step time 0.63s\n","# Epoch 8  global step 23360 loss 5.50651 batch 386/3281 lr 0.001 accuracy 93.89062 wps 14717.54 step time 0.57s\n","# Epoch 8  global step 23380 loss 5.50514 batch 406/3281 lr 0.001 accuracy 94.08984 wps 14923.20 step time 0.50s\n","# Epoch 8  global step 23400 loss 5.60113 batch 426/3281 lr 0.001 accuracy 93.78516 wps 14357.91 step time 0.66s\n","# Epoch 8  global step 23420 loss 5.41433 batch 446/3281 lr 0.001 accuracy 94.21484 wps 14137.75 step time 0.55s\n","# Epoch 8  global step 23440 loss 5.52640 batch 466/3281 lr 0.001 accuracy 93.97266 wps 14701.83 step time 0.47s\n","# Epoch 8  global step 23460 loss 5.49577 batch 486/3281 lr 0.001 accuracy 94.13281 wps 14574.95 step time 0.45s\n","# Epoch 8  global step 23480 loss 5.57897 batch 506/3281 lr 0.001 accuracy 93.74219 wps 14973.59 step time 0.49s\n","# Epoch 8  global step 23500 loss 5.58733 batch 526/3281 lr 0.001 accuracy 93.87109 wps 14980.41 step time 0.49s\n","# Epoch 8  global step 23520 loss 5.51079 batch 546/3281 lr 0.001 accuracy 94.13281 wps 13965.75 step time 0.53s\n","# Epoch 8  global step 23540 loss 5.52048 batch 566/3281 lr 0.001 accuracy 93.96484 wps 14220.23 step time 0.44s\n","# Epoch 8  global step 23560 loss 5.51582 batch 586/3281 lr 0.001 accuracy 94.14062 wps 14968.22 step time 0.49s\n","# Epoch 8  global step 23580 loss 5.59385 batch 606/3281 lr 0.001 accuracy 93.80859 wps 14111.29 step time 0.55s\n","# Epoch 8  global step 23600 loss 5.40178 batch 626/3281 lr 0.001 accuracy 94.16406 wps 14154.35 step time 0.42s\n","# Epoch 8  global step 23620 loss 5.51775 batch 646/3281 lr 0.001 accuracy 93.98828 wps 14774.67 step time 0.49s\n","# Epoch 8  global step 23640 loss 5.51623 batch 666/3281 lr 0.001 accuracy 93.94531 wps 14446.79 step time 0.45s\n","# Epoch 8  global step 23660 loss 5.64603 batch 686/3281 lr 0.001 accuracy 93.58203 wps 14328.71 step time 0.55s\n","# Epoch 8  global step 23680 loss 5.42836 batch 706/3281 lr 0.001 accuracy 94.26953 wps 14749.44 step time 0.50s\n","# Epoch 8  global step 23700 loss 5.47334 batch 726/3281 lr 0.001 accuracy 94.08594 wps 14145.18 step time 0.55s\n","# Epoch 8  global step 23720 loss 5.62489 batch 746/3281 lr 0.001 accuracy 93.76562 wps 14637.11 step time 0.55s\n","# Epoch 8  global step 23740 loss 5.53162 batch 766/3281 lr 0.001 accuracy 94.16797 wps 14189.45 step time 0.45s\n","# Epoch 8  global step 23760 loss 5.64030 batch 786/3281 lr 0.001 accuracy 93.55859 wps 14996.89 step time 0.51s\n","# Epoch 8  global step 23780 loss 5.48199 batch 806/3281 lr 0.001 accuracy 93.85937 wps 14435.41 step time 0.45s\n","# Epoch 8  global step 23800 loss 5.53424 batch 826/3281 lr 0.001 accuracy 93.92578 wps 14705.27 step time 0.48s\n","# Epoch 8  global step 23820 loss 5.60027 batch 846/3281 lr 0.001 accuracy 93.96875 wps 14353.13 step time 0.58s\n","# Epoch 8  global step 23840 loss 5.48438 batch 866/3281 lr 0.001 accuracy 94.12109 wps 14037.26 step time 0.55s\n","# Epoch 8  global step 23860 loss 5.55544 batch 886/3281 lr 0.001 accuracy 93.92188 wps 14676.81 step time 0.52s\n","# Epoch 8  global step 23880 loss 5.47266 batch 906/3281 lr 0.001 accuracy 94.09375 wps 13812.61 step time 0.53s\n","# Epoch 8  global step 23900 loss 5.59507 batch 926/3281 lr 0.001 accuracy 93.96875 wps 14573.48 step time 0.47s\n","# Epoch 8  global step 23920 loss 5.58158 batch 946/3281 lr 0.001 accuracy 93.89062 wps 14876.90 step time 0.50s\n","# Epoch 8  global step 23940 loss 5.55514 batch 966/3281 lr 0.001 accuracy 93.90625 wps 14466.91 step time 0.57s\n","# Epoch 8  global step 23960 loss 5.69600 batch 986/3281 lr 0.001 accuracy 93.77344 wps 14729.31 step time 0.50s\n","# Epoch 8  global step 23980 loss 5.48827 batch 1006/3281 lr 0.001 accuracy 94.07422 wps 15026.30 step time 0.53s\n","# Epoch 8  global step 24000 loss 5.56798 batch 1026/3281 lr 0.001 accuracy 93.83594 wps 14345.69 step time 0.45s\n","# global step 24000, eval model at Sat May 23 10:17:13 2020\n","2020-05-23 10:17:15.432150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:17:15.432653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 10:17:15.435484: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 10:17:15.435586: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 10:17:15.435615: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 10:17:15.435642: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 10:17:15.435669: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 10:17:15.435693: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 10:17:15.435718: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 10:17:15.435827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:17:15.436140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:17:15.436404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 10:17:15.436814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 10:17:15.436840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 10:17:15.436852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 10:17:15.437031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:17:15.437335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:17:15.437601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6402279494744377\n","# location_distance_from_business_district - 0.5172202088335429\n","# location_easy_to_find - 0.6979722309132119\n","# service_wait_time - 0.6423389480897063\n","# service_waiters_attitude - 0.7896059164577376\n","# service_parking_convenience - 0.7197570033208549\n","# service_serving_speed - 0.7466985115751738\n","# price_level - 0.7712803202924325\n","# price_cost_effective - 0.7170578935597033\n","# price_discount - 0.651089933252137\n","# environment_decoration - 0.7195243800495668\n","# environment_noise - 0.7557517062737806\n","# environment_space - 0.7617708559877169\n","# environment_cleaness - 0.7496397155055602\n","# dish_portion - 0.7063965301795593\n","# dish_taste - 0.7265919338017247\n","# dish_look - 0.5487273859461207\n","# dish_recommendation - 0.7267563181709189\n","# others_overall_experience - 0.5818459378218148\n","# others_willing_to_consume_again - 0.6951536633274454\n","# Eval loss 4.47187, f1 0.69327\n","# current result -0.6932703671416572, previous best result -0.6860543663848567\n","# Epoch 8  global step 24020 loss 5.77019 batch 1046/3281 lr 0.001 accuracy 93.47266 wps 14617.38 step time 0.52s\n","# Epoch 8  global step 24040 loss 5.52944 batch 1066/3281 lr 0.001 accuracy 93.92188 wps 14190.85 step time 0.44s\n","# Epoch 8  global step 24060 loss 5.55857 batch 1086/3281 lr 0.001 accuracy 93.94922 wps 14808.45 step time 0.49s\n","# Epoch 8  global step 24080 loss 5.60323 batch 1106/3281 lr 0.001 accuracy 93.95703 wps 14454.15 step time 0.46s\n","# Epoch 8  global step 24100 loss 5.47380 batch 1126/3281 lr 0.001 accuracy 93.94922 wps 13854.22 step time 0.62s\n","# Epoch 8  global step 24120 loss 5.58758 batch 1146/3281 lr 0.001 accuracy 93.91016 wps 14244.88 step time 0.45s\n","# Epoch 8  global step 24140 loss 5.43504 batch 1166/3281 lr 0.001 accuracy 94.33984 wps 13966.59 step time 0.42s\n","# Epoch 8  global step 24160 loss 5.72998 batch 1186/3281 lr 0.001 accuracy 93.42969 wps 15014.95 step time 0.57s\n","# Epoch 8  global step 24180 loss 5.62676 batch 1206/3281 lr 0.001 accuracy 93.66406 wps 14958.09 step time 0.50s\n","# Epoch 8  global step 24200 loss 5.59157 batch 1226/3281 lr 0.001 accuracy 93.80859 wps 14206.03 step time 0.54s\n","# Epoch 8  global step 24220 loss 5.47116 batch 1246/3281 lr 0.001 accuracy 94.05859 wps 13710.56 step time 0.50s\n","# Epoch 8  global step 24240 loss 5.57774 batch 1266/3281 lr 0.001 accuracy 94.02734 wps 13993.81 step time 0.53s\n","# Epoch 8  global step 24260 loss 5.41743 batch 1286/3281 lr 0.001 accuracy 94.37891 wps 14290.64 step time 0.45s\n","# Epoch 8  global step 24280 loss 5.39471 batch 1306/3281 lr 0.001 accuracy 94.23828 wps 13962.58 step time 0.53s\n","# Epoch 8  global step 24300 loss 5.59182 batch 1326/3281 lr 0.001 accuracy 93.75391 wps 14695.75 step time 0.47s\n","# Epoch 8  global step 24320 loss 5.59965 batch 1346/3281 lr 0.001 accuracy 93.69531 wps 14779.99 step time 0.49s\n","# Epoch 8  global step 24340 loss 5.46278 batch 1366/3281 lr 0.001 accuracy 94.12891 wps 14382.18 step time 0.56s\n","# Epoch 8  global step 24360 loss 5.56158 batch 1386/3281 lr 0.001 accuracy 94.05859 wps 14225.18 step time 0.44s\n","# Epoch 8  global step 24380 loss 5.59372 batch 1406/3281 lr 0.001 accuracy 93.89844 wps 13980.57 step time 0.54s\n","# Epoch 8  global step 24400 loss 5.47798 batch 1426/3281 lr 0.001 accuracy 94.00000 wps 13904.58 step time 0.54s\n","# Epoch 8  global step 24420 loss 5.58904 batch 1446/3281 lr 0.001 accuracy 93.86719 wps 13782.71 step time 0.52s\n","# Epoch 8  global step 24440 loss 5.67375 batch 1466/3281 lr 0.001 accuracy 93.75000 wps 14582.68 step time 0.59s\n","# Epoch 8  global step 24460 loss 5.58638 batch 1486/3281 lr 0.001 accuracy 93.78906 wps 14181.04 step time 0.56s\n","# Epoch 8  global step 24480 loss 5.44550 batch 1506/3281 lr 0.001 accuracy 94.17188 wps 13897.27 step time 0.50s\n","# Epoch 8  global step 24500 loss 5.39108 batch 1526/3281 lr 0.001 accuracy 94.33203 wps 14129.72 step time 0.42s\n","# Epoch 8  global step 24520 loss 5.49776 batch 1546/3281 lr 0.001 accuracy 94.14063 wps 14749.76 step time 0.50s\n","# Epoch 8  global step 24540 loss 5.36291 batch 1566/3281 lr 0.001 accuracy 94.50000 wps 12530.24 step time 0.46s\n","# Epoch 8  global step 24560 loss 5.48070 batch 1586/3281 lr 0.001 accuracy 94.11719 wps 12703.22 step time 0.54s\n","# Epoch 8  global step 24580 loss 5.67019 batch 1606/3281 lr 0.001 accuracy 93.72266 wps 12976.39 step time 0.56s\n","# Epoch 8  global step 24600 loss 5.56700 batch 1626/3281 lr 0.001 accuracy 93.93359 wps 12451.60 step time 0.55s\n","# Epoch 8  global step 24620 loss 5.54258 batch 1646/3281 lr 0.001 accuracy 93.88281 wps 12446.51 step time 0.56s\n","# Epoch 8  global step 24640 loss 5.64808 batch 1666/3281 lr 0.001 accuracy 93.63281 wps 13232.25 step time 0.65s\n","# Epoch 8  global step 24660 loss 5.60271 batch 1686/3281 lr 0.001 accuracy 93.71875 wps 13357.58 step time 0.55s\n","# Epoch 8  global step 24680 loss 5.56182 batch 1706/3281 lr 0.001 accuracy 93.88672 wps 11911.43 step time 0.66s\n","# Epoch 8  global step 24700 loss 5.48688 batch 1726/3281 lr 0.001 accuracy 93.89844 wps 12965.33 step time 0.60s\n","# Epoch 8  global step 24720 loss 5.52998 batch 1746/3281 lr 0.001 accuracy 93.98047 wps 12768.18 step time 0.53s\n","# Epoch 8  global step 24740 loss 5.46383 batch 1766/3281 lr 0.001 accuracy 94.10156 wps 12902.59 step time 0.49s\n","# Epoch 8  global step 24760 loss 5.57554 batch 1786/3281 lr 0.001 accuracy 93.87109 wps 12158.41 step time 0.64s\n","# Epoch 8  global step 24780 loss 5.54824 batch 1806/3281 lr 0.001 accuracy 93.90625 wps 12739.70 step time 0.56s\n","# Epoch 8  global step 24800 loss 5.55307 batch 1826/3281 lr 0.001 accuracy 93.76953 wps 12425.25 step time 0.62s\n","# Epoch 8  global step 24820 loss 5.49432 batch 1846/3281 lr 0.001 accuracy 94.23828 wps 13020.35 step time 0.47s\n","# Epoch 8  global step 24840 loss 5.57735 batch 1866/3281 lr 0.001 accuracy 93.73828 wps 12829.03 step time 0.58s\n","# Epoch 8  global step 24860 loss 5.58269 batch 1886/3281 lr 0.001 accuracy 93.94922 wps 12949.83 step time 0.55s\n","# Epoch 8  global step 24880 loss 5.52705 batch 1906/3281 lr 0.001 accuracy 94.11328 wps 12526.08 step time 0.49s\n","# Epoch 8  global step 24900 loss 5.51064 batch 1926/3281 lr 0.001 accuracy 94.06250 wps 12728.86 step time 0.48s\n","# Epoch 8  global step 24920 loss 5.60336 batch 1946/3281 lr 0.001 accuracy 93.78125 wps 13075.19 step time 0.55s\n","# Epoch 8  global step 24940 loss 5.60281 batch 1966/3281 lr 0.001 accuracy 93.80859 wps 12332.50 step time 0.70s\n","# Epoch 8  global step 24960 loss 5.53789 batch 1986/3281 lr 0.001 accuracy 93.99219 wps 12406.57 step time 0.63s\n","# Epoch 8  global step 24980 loss 5.57042 batch 2006/3281 lr 0.001 accuracy 94.00391 wps 11735.79 step time 0.57s\n","# Epoch 8  global step 25000 loss 5.48963 batch 2026/3281 lr 0.001 accuracy 94.01562 wps 12094.31 step time 0.59s\n","# Epoch 8  global step 25020 loss 5.58631 batch 2046/3281 lr 0.001 accuracy 93.81641 wps 12085.68 step time 0.62s\n","# Epoch 8  global step 25040 loss 5.55607 batch 2066/3281 lr 0.001 accuracy 93.88281 wps 12483.40 step time 0.58s\n","# Epoch 8  global step 25060 loss 5.41383 batch 2086/3281 lr 0.001 accuracy 94.26562 wps 12617.70 step time 0.52s\n","# Epoch 8  global step 25080 loss 5.58265 batch 2106/3281 lr 0.001 accuracy 93.71094 wps 12843.44 step time 0.55s\n","# Epoch 8  global step 25100 loss 5.65024 batch 2126/3281 lr 0.001 accuracy 93.73828 wps 12014.76 step time 0.67s\n","# Epoch 8  global step 25120 loss 5.60787 batch 2146/3281 lr 0.001 accuracy 93.63672 wps 12202.24 step time 0.64s\n","# Epoch 8  global step 25140 loss 5.77263 batch 2166/3281 lr 0.001 accuracy 93.23047 wps 12167.06 step time 0.70s\n","# Epoch 8  global step 25160 loss 5.63416 batch 2186/3281 lr 0.001 accuracy 93.63281 wps 12751.20 step time 0.64s\n","# Epoch 8  global step 25180 loss 5.52636 batch 2206/3281 lr 0.001 accuracy 93.99609 wps 12911.04 step time 0.49s\n","# Epoch 8  global step 25200 loss 5.56514 batch 2226/3281 lr 0.001 accuracy 93.93750 wps 12121.11 step time 0.57s\n","# Epoch 8  global step 25220 loss 5.62369 batch 2246/3281 lr 0.001 accuracy 93.76172 wps 12022.68 step time 0.63s\n","# Epoch 8  global step 25240 loss 5.52358 batch 2266/3281 lr 0.001 accuracy 93.93359 wps 12684.25 step time 0.54s\n","# Epoch 8  global step 25260 loss 5.72798 batch 2286/3281 lr 0.001 accuracy 93.62500 wps 12609.85 step time 0.68s\n","# Epoch 8  global step 25280 loss 5.55922 batch 2306/3281 lr 0.001 accuracy 93.77734 wps 12671.05 step time 0.60s\n","# Epoch 8  global step 25300 loss 5.58353 batch 2326/3281 lr 0.001 accuracy 93.77344 wps 12530.04 step time 0.59s\n","# Epoch 8  global step 25320 loss 5.63219 batch 2346/3281 lr 0.001 accuracy 93.80469 wps 12949.96 step time 0.52s\n","# Epoch 8  global step 25340 loss 5.70040 batch 2366/3281 lr 0.001 accuracy 93.44141 wps 12546.94 step time 0.74s\n","# Epoch 8  global step 25360 loss 5.51597 batch 2386/3281 lr 0.001 accuracy 93.97656 wps 13345.01 step time 0.49s\n","# Epoch 8  global step 25380 loss 5.39338 batch 2406/3281 lr 0.001 accuracy 94.45703 wps 12955.90 step time 0.45s\n","# Epoch 8  global step 25400 loss 5.63638 batch 2426/3281 lr 0.001 accuracy 93.63672 wps 11979.23 step time 0.65s\n","# Epoch 8  global step 25420 loss 5.48361 batch 2446/3281 lr 0.001 accuracy 94.05859 wps 12114.69 step time 0.58s\n","# Epoch 8  global step 25440 loss 5.55824 batch 2466/3281 lr 0.001 accuracy 94.07031 wps 12840.98 step time 0.48s\n","# Epoch 8  global step 25460 loss 5.66092 batch 2486/3281 lr 0.001 accuracy 93.69922 wps 13039.78 step time 0.57s\n","# Epoch 8  global step 25480 loss 5.47803 batch 2506/3281 lr 0.001 accuracy 94.04297 wps 11892.85 step time 0.59s\n","# Epoch 8  global step 25500 loss 5.67432 batch 2526/3281 lr 0.001 accuracy 93.59375 wps 13529.78 step time 0.60s\n","# Epoch 8  global step 25520 loss 5.55400 batch 2546/3281 lr 0.001 accuracy 93.99609 wps 13086.51 step time 0.58s\n","# Epoch 8  global step 25540 loss 5.49232 batch 2566/3281 lr 0.001 accuracy 94.07031 wps 12620.19 step time 0.53s\n","# Epoch 8  global step 25560 loss 5.60230 batch 2586/3281 lr 0.001 accuracy 93.77734 wps 12255.96 step time 0.65s\n","# Epoch 8  global step 25580 loss 5.54434 batch 2606/3281 lr 0.001 accuracy 93.98438 wps 12623.35 step time 0.49s\n","# Epoch 8  global step 25600 loss 5.53029 batch 2626/3281 lr 0.001 accuracy 94.07031 wps 12199.75 step time 0.56s\n","# Epoch 8  global step 25620 loss 5.52035 batch 2646/3281 lr 0.001 accuracy 93.74609 wps 12030.39 step time 0.56s\n","# Epoch 8  global step 25640 loss 5.62433 batch 2666/3281 lr 0.001 accuracy 93.79297 wps 12512.27 step time 0.60s\n","# Epoch 8  global step 25660 loss 5.54031 batch 2686/3281 lr 0.001 accuracy 93.97266 wps 13444.70 step time 0.53s\n","# Epoch 8  global step 25680 loss 5.57880 batch 2706/3281 lr 0.001 accuracy 93.93750 wps 12571.43 step time 0.51s\n","# Epoch 8  global step 25700 loss 5.59872 batch 2726/3281 lr 0.001 accuracy 93.87500 wps 13184.91 step time 0.50s\n","# Epoch 8  global step 25720 loss 5.66918 batch 2746/3281 lr 0.001 accuracy 93.42188 wps 12944.34 step time 0.56s\n","# Epoch 8  global step 25740 loss 5.40169 batch 2766/3281 lr 0.001 accuracy 94.19141 wps 12762.88 step time 0.47s\n","# Epoch 8  global step 25760 loss 5.59978 batch 2786/3281 lr 0.001 accuracy 93.88281 wps 12168.83 step time 0.59s\n","# Epoch 8  global step 25780 loss 5.47272 batch 2806/3281 lr 0.001 accuracy 94.19922 wps 12705.32 step time 0.56s\n","# Epoch 8  global step 25800 loss 5.58347 batch 2826/3281 lr 0.001 accuracy 93.74609 wps 13127.55 step time 0.54s\n","# Epoch 8  global step 25820 loss 5.47449 batch 2846/3281 lr 0.001 accuracy 94.08984 wps 12015.12 step time 0.67s\n","# Epoch 8  global step 25840 loss 5.60354 batch 2866/3281 lr 0.001 accuracy 93.77734 wps 12713.39 step time 0.57s\n","# Epoch 8  global step 25860 loss 5.29596 batch 2886/3281 lr 0.001 accuracy 94.68750 wps 12776.79 step time 0.45s\n","# Epoch 8  global step 25880 loss 5.58552 batch 2906/3281 lr 0.001 accuracy 93.81641 wps 12632.47 step time 0.54s\n","# Epoch 8  global step 25900 loss 5.58705 batch 2926/3281 lr 0.001 accuracy 93.84766 wps 12915.90 step time 0.52s\n","# Epoch 8  global step 25920 loss 5.65808 batch 2946/3281 lr 0.001 accuracy 93.61328 wps 12507.92 step time 0.68s\n","# Epoch 8  global step 25940 loss 5.63558 batch 2966/3281 lr 0.001 accuracy 93.76563 wps 13110.68 step time 0.51s\n","# Epoch 8  global step 25960 loss 5.62016 batch 2986/3281 lr 0.001 accuracy 93.82812 wps 11923.81 step time 0.65s\n","# Epoch 8  global step 25980 loss 5.60499 batch 3006/3281 lr 0.001 accuracy 94.04297 wps 12866.33 step time 0.53s\n","# Epoch 8  global step 26000 loss 5.62941 batch 3026/3281 lr 0.001 accuracy 93.68750 wps 13348.71 step time 0.59s\n","# global step 26000, eval model at Sat May 23 10:37:57 2020\n","2020-05-23 10:37:59.914237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:37:59.914705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 10:37:59.914847: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 10:37:59.914874: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 10:37:59.914901: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 10:37:59.914924: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 10:37:59.914947: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 10:37:59.914969: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 10:37:59.914994: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 10:37:59.915090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:37:59.915350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:37:59.915587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 10:37:59.915974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 10:37:59.915995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 10:37:59.916005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 10:37:59.916187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:37:59.916465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:37:59.916680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6469337761487421\n","# location_distance_from_business_district - 0.5416437856736294\n","# location_easy_to_find - 0.7020791118973974\n","# service_wait_time - 0.6481094096308091\n","# service_waiters_attitude - 0.7930374419926466\n","# service_parking_convenience - 0.7252247199371402\n","# service_serving_speed - 0.7540089013207577\n","# price_level - 0.7746069183865547\n","# price_cost_effective - 0.716690326698175\n","# price_discount - 0.658814897049846\n","# environment_decoration - 0.7189511822588075\n","# environment_noise - 0.7586678732869594\n","# environment_space - 0.7606645034309917\n","# environment_cleaness - 0.7526139793265023\n","# dish_portion - 0.7130701172005862\n","# dish_taste - 0.7307666384502806\n","# dish_look - 0.5591858874526026\n","# dish_recommendation - 0.7352466607927111\n","# others_overall_experience - 0.5827849076757882\n","# others_willing_to_consume_again - 0.7008359036157219\n","# Eval loss 4.43621, f1 0.69870\n","# current result -0.6986968471113326, previous best result -0.6932703671416572\n","# Epoch 8  global step 26020 loss 5.61148 batch 3046/3281 lr 0.001 accuracy 93.68359 wps 12180.10 step time 0.60s\n","# Epoch 8  global step 26040 loss 5.62837 batch 3066/3281 lr 0.001 accuracy 93.61719 wps 11984.22 step time 0.62s\n","# Epoch 8  global step 26060 loss 5.52070 batch 3086/3281 lr 0.001 accuracy 93.98047 wps 12298.03 step time 0.52s\n","# Epoch 8  global step 26080 loss 5.53529 batch 3106/3281 lr 0.001 accuracy 94.09375 wps 12361.02 step time 0.57s\n","# Epoch 8  global step 26100 loss 5.59297 batch 3126/3281 lr 0.001 accuracy 93.71094 wps 12622.20 step time 0.62s\n","# Epoch 8  global step 26120 loss 5.46878 batch 3146/3281 lr 0.001 accuracy 94.15625 wps 12947.94 step time 0.53s\n","# Epoch 8  global step 26140 loss 5.61828 batch 3166/3281 lr 0.001 accuracy 93.89453 wps 12369.20 step time 0.56s\n","# Epoch 8  global step 26160 loss 5.58511 batch 3186/3281 lr 0.001 accuracy 93.91797 wps 12942.35 step time 0.46s\n","# Epoch 8  global step 26180 loss 5.58486 batch 3206/3281 lr 0.001 accuracy 93.73828 wps 13106.73 step time 0.54s\n","# Epoch 8  global step 26200 loss 5.72926 batch 3226/3281 lr 0.001 accuracy 93.42578 wps 12724.49 step time 0.62s\n","# Epoch 8  global step 26220 loss 5.71570 batch 3246/3281 lr 0.001 accuracy 93.46094 wps 11323.70 step time 0.82s\n","# Epoch 8  global step 26240 loss 5.50080 batch 3266/3281 lr 0.001 accuracy 94.07422 wps 13127.86 step time 0.45s\n","# Finsh epoch 8, global step 26256\n","# Epoch 9  global step 26260 loss 1.12279 batch 4/3281 lr 0.001 accuracy 18.73828 wps 13102.32 step time 0.17s\n","# Epoch 9  global step 26280 loss 5.48880 batch 24/3281 lr 0.001 accuracy 93.99219 wps 14693.54 step time 0.54s\n","# Epoch 9  global step 26300 loss 5.43191 batch 44/3281 lr 0.001 accuracy 94.18750 wps 14507.33 step time 0.46s\n","# Epoch 9  global step 26320 loss 5.50399 batch 64/3281 lr 0.001 accuracy 94.04297 wps 14381.28 step time 0.58s\n","# Epoch 9  global step 26340 loss 5.52663 batch 84/3281 lr 0.001 accuracy 93.96094 wps 14940.43 step time 0.49s\n","# Epoch 9  global step 26360 loss 5.37701 batch 104/3281 lr 0.001 accuracy 94.39453 wps 14655.79 step time 0.46s\n","# Epoch 9  global step 26380 loss 5.58289 batch 124/3281 lr 0.001 accuracy 93.64453 wps 14953.24 step time 0.62s\n","# Epoch 9  global step 26400 loss 5.55656 batch 144/3281 lr 0.001 accuracy 93.78906 wps 15087.89 step time 0.53s\n","# Epoch 9  global step 26420 loss 5.43979 batch 164/3281 lr 0.001 accuracy 94.15625 wps 14232.45 step time 0.52s\n","# Epoch 9  global step 26440 loss 5.39159 batch 184/3281 lr 0.001 accuracy 94.43750 wps 14291.47 step time 0.57s\n","# Epoch 9  global step 26460 loss 5.58613 batch 204/3281 lr 0.001 accuracy 93.84766 wps 14165.85 step time 0.61s\n","# Epoch 9  global step 26480 loss 5.53377 batch 224/3281 lr 0.001 accuracy 93.71094 wps 15044.08 step time 0.51s\n","# Epoch 9  global step 26500 loss 5.45851 batch 244/3281 lr 0.001 accuracy 94.12891 wps 14572.22 step time 0.47s\n","# Epoch 9  global step 26520 loss 5.49803 batch 264/3281 lr 0.001 accuracy 94.23828 wps 14642.14 step time 0.47s\n","# Epoch 9  global step 26540 loss 5.29315 batch 284/3281 lr 0.001 accuracy 94.50000 wps 14067.89 step time 0.51s\n","# Epoch 9  global step 26560 loss 5.50763 batch 304/3281 lr 0.001 accuracy 93.78125 wps 14487.91 step time 0.58s\n","# Epoch 9  global step 26580 loss 5.44739 batch 324/3281 lr 0.001 accuracy 94.23828 wps 14616.15 step time 0.49s\n","# Epoch 9  global step 26600 loss 5.44241 batch 344/3281 lr 0.001 accuracy 94.09766 wps 14600.67 step time 0.46s\n","# Epoch 9  global step 26620 loss 5.54640 batch 364/3281 lr 0.001 accuracy 93.85938 wps 14421.12 step time 0.57s\n","# Epoch 9  global step 26640 loss 5.50550 batch 384/3281 lr 0.001 accuracy 93.99219 wps 14761.96 step time 0.48s\n","# Epoch 9  global step 26660 loss 5.33563 batch 404/3281 lr 0.001 accuracy 94.50000 wps 13297.47 step time 0.58s\n","# Epoch 9  global step 26680 loss 5.38686 batch 424/3281 lr 0.001 accuracy 94.37500 wps 13986.31 step time 0.51s\n","# Epoch 9  global step 26700 loss 5.42522 batch 444/3281 lr 0.001 accuracy 94.23438 wps 13799.87 step time 0.60s\n","# Epoch 9  global step 26720 loss 5.54529 batch 464/3281 lr 0.001 accuracy 93.97266 wps 14356.49 step time 0.54s\n","# Epoch 9  global step 26740 loss 5.38219 batch 484/3281 lr 0.001 accuracy 94.57422 wps 13979.92 step time 0.48s\n","# Epoch 9  global step 26760 loss 5.36821 batch 504/3281 lr 0.001 accuracy 94.28516 wps 14427.43 step time 0.46s\n","# Epoch 9  global step 26780 loss 5.47256 batch 524/3281 lr 0.001 accuracy 94.27734 wps 13814.14 step time 0.49s\n","# Epoch 9  global step 26800 loss 5.31360 batch 544/3281 lr 0.001 accuracy 94.63672 wps 14332.15 step time 0.43s\n","# Epoch 9  global step 26820 loss 5.37165 batch 564/3281 lr 0.001 accuracy 94.12891 wps 14612.13 step time 0.46s\n","# Epoch 9  global step 26840 loss 5.61142 batch 584/3281 lr 0.001 accuracy 93.70703 wps 15173.46 step time 0.54s\n","# Epoch 9  global step 26860 loss 5.29893 batch 604/3281 lr 0.001 accuracy 94.45703 wps 13997.06 step time 0.43s\n","# Epoch 9  global step 26880 loss 5.56597 batch 624/3281 lr 0.001 accuracy 93.77734 wps 15047.86 step time 0.53s\n","# Epoch 9  global step 26900 loss 5.37599 batch 644/3281 lr 0.001 accuracy 94.48047 wps 14248.43 step time 0.43s\n","# Epoch 9  global step 26920 loss 5.49413 batch 664/3281 lr 0.001 accuracy 94.05859 wps 14900.93 step time 0.52s\n","# Epoch 9  global step 26940 loss 5.55945 batch 684/3281 lr 0.001 accuracy 93.92969 wps 14365.05 step time 0.54s\n","# Epoch 9  global step 26960 loss 5.45735 batch 704/3281 lr 0.001 accuracy 94.24609 wps 14064.82 step time 0.42s\n","# Epoch 9  global step 26980 loss 5.69261 batch 724/3281 lr 0.001 accuracy 93.49609 wps 15023.76 step time 0.49s\n","# Epoch 9  global step 27000 loss 5.46162 batch 744/3281 lr 0.001 accuracy 94.03906 wps 15088.47 step time 0.51s\n","# Epoch 9  global step 27020 loss 5.47586 batch 764/3281 lr 0.001 accuracy 93.98438 wps 14905.35 step time 0.49s\n","# Epoch 9  global step 27040 loss 5.47999 batch 784/3281 lr 0.001 accuracy 94.16406 wps 14423.85 step time 0.44s\n","# Epoch 9  global step 27060 loss 5.42561 batch 804/3281 lr 0.001 accuracy 94.19922 wps 13954.18 step time 0.53s\n","# Epoch 9  global step 27080 loss 5.46915 batch 824/3281 lr 0.001 accuracy 94.13672 wps 14743.38 step time 0.49s\n","# Epoch 9  global step 27100 loss 5.49695 batch 844/3281 lr 0.001 accuracy 94.18359 wps 15181.16 step time 0.57s\n","# Epoch 9  global step 27120 loss 5.45635 batch 864/3281 lr 0.001 accuracy 94.26563 wps 14264.15 step time 0.44s\n","# Epoch 9  global step 27140 loss 5.38621 batch 884/3281 lr 0.001 accuracy 94.28516 wps 13976.76 step time 0.47s\n","# Epoch 9  global step 27160 loss 5.62995 batch 904/3281 lr 0.001 accuracy 93.71484 wps 14080.25 step time 0.53s\n","# Epoch 9  global step 27180 loss 5.59490 batch 924/3281 lr 0.001 accuracy 93.80469 wps 15353.28 step time 0.57s\n","# Epoch 9  global step 27200 loss 5.47216 batch 944/3281 lr 0.001 accuracy 94.18359 wps 14518.12 step time 0.46s\n","# Epoch 9  global step 27220 loss 5.45131 batch 964/3281 lr 0.001 accuracy 94.31641 wps 13673.17 step time 0.40s\n","# Epoch 9  global step 27240 loss 5.40033 batch 984/3281 lr 0.001 accuracy 94.18750 wps 14485.95 step time 0.47s\n","# Epoch 9  global step 27260 loss 5.42259 batch 1004/3281 lr 0.001 accuracy 94.26172 wps 13666.45 step time 0.51s\n","# Epoch 9  global step 27280 loss 5.46739 batch 1024/3281 lr 0.001 accuracy 93.89453 wps 14100.11 step time 0.56s\n","# Epoch 9  global step 27300 loss 5.52738 batch 1044/3281 lr 0.001 accuracy 94.01953 wps 13627.53 step time 0.55s\n","# Epoch 9  global step 27320 loss 5.44814 batch 1064/3281 lr 0.001 accuracy 94.32812 wps 14078.59 step time 0.42s\n","# Epoch 9  global step 27340 loss 5.56401 batch 1084/3281 lr 0.001 accuracy 93.80859 wps 14475.03 step time 0.56s\n","# Epoch 9  global step 27360 loss 5.60759 batch 1104/3281 lr 0.001 accuracy 93.79687 wps 14576.85 step time 0.59s\n","# Epoch 9  global step 27380 loss 5.32739 batch 1124/3281 lr 0.001 accuracy 94.46875 wps 14174.35 step time 0.44s\n","# Epoch 9  global step 27400 loss 5.55978 batch 1144/3281 lr 0.001 accuracy 93.88672 wps 14685.65 step time 0.48s\n","# Epoch 9  global step 27420 loss 5.47658 batch 1164/3281 lr 0.001 accuracy 94.03906 wps 14374.67 step time 0.48s\n","# Epoch 9  global step 27440 loss 5.58873 batch 1184/3281 lr 0.001 accuracy 93.62500 wps 14397.99 step time 0.59s\n","# Epoch 9  global step 27460 loss 5.55493 batch 1204/3281 lr 0.001 accuracy 93.76953 wps 13952.80 step time 0.54s\n","# Epoch 9  global step 27480 loss 5.50522 batch 1224/3281 lr 0.001 accuracy 94.04687 wps 14569.06 step time 0.48s\n","# Epoch 9  global step 27500 loss 5.38163 batch 1244/3281 lr 0.001 accuracy 94.50391 wps 14402.11 step time 0.47s\n","# Epoch 9  global step 27520 loss 5.54855 batch 1264/3281 lr 0.001 accuracy 93.75391 wps 14002.76 step time 0.54s\n","# Epoch 9  global step 27540 loss 5.55371 batch 1284/3281 lr 0.001 accuracy 93.70703 wps 14260.70 step time 0.57s\n","# Epoch 9  global step 27560 loss 5.52129 batch 1304/3281 lr 0.001 accuracy 94.06641 wps 14242.36 step time 0.53s\n","# Epoch 9  global step 27580 loss 5.56648 batch 1324/3281 lr 0.001 accuracy 93.83984 wps 14297.71 step time 0.46s\n","# Epoch 9  global step 27600 loss 5.60413 batch 1344/3281 lr 0.001 accuracy 93.81250 wps 14710.69 step time 0.50s\n","# Epoch 9  global step 27620 loss 5.49215 batch 1364/3281 lr 0.001 accuracy 94.02344 wps 14906.48 step time 0.50s\n","# Epoch 9  global step 27640 loss 5.52392 batch 1384/3281 lr 0.001 accuracy 94.00781 wps 14570.72 step time 0.50s\n","# Epoch 9  global step 27660 loss 5.45548 batch 1404/3281 lr 0.001 accuracy 94.08984 wps 14188.07 step time 0.43s\n","# Epoch 9  global step 27680 loss 5.56958 batch 1424/3281 lr 0.001 accuracy 93.55859 wps 14983.58 step time 0.53s\n","# Epoch 9  global step 27700 loss 5.35720 batch 1444/3281 lr 0.001 accuracy 94.42188 wps 13634.06 step time 0.41s\n","# Epoch 9  global step 27720 loss 5.45744 batch 1464/3281 lr 0.001 accuracy 93.88281 wps 14707.08 step time 0.51s\n","# Epoch 9  global step 27740 loss 5.65250 batch 1484/3281 lr 0.001 accuracy 93.89844 wps 14505.81 step time 0.48s\n","# Epoch 9  global step 27760 loss 5.62115 batch 1504/3281 lr 0.001 accuracy 93.65234 wps 14820.41 step time 0.52s\n","# Epoch 9  global step 27780 loss 5.44529 batch 1524/3281 lr 0.001 accuracy 94.24219 wps 14198.19 step time 0.44s\n","# Epoch 9  global step 27800 loss 5.54605 batch 1544/3281 lr 0.001 accuracy 93.93750 wps 14316.64 step time 0.47s\n","# Epoch 9  global step 27820 loss 5.61638 batch 1564/3281 lr 0.001 accuracy 93.89063 wps 15080.64 step time 0.54s\n","# Epoch 9  global step 27840 loss 5.54351 batch 1584/3281 lr 0.001 accuracy 93.98828 wps 14713.63 step time 0.50s\n","# Epoch 9  global step 27860 loss 5.42839 batch 1604/3281 lr 0.001 accuracy 94.35547 wps 14345.17 step time 0.46s\n","# Epoch 9  global step 27880 loss 5.59576 batch 1624/3281 lr 0.001 accuracy 93.67969 wps 14263.84 step time 0.55s\n","# Epoch 9  global step 27900 loss 5.43573 batch 1644/3281 lr 0.001 accuracy 94.39844 wps 13953.85 step time 0.43s\n","# Epoch 9  global step 27920 loss 5.43087 batch 1664/3281 lr 0.001 accuracy 94.30469 wps 14046.19 step time 0.50s\n","# Epoch 9  global step 27940 loss 5.45447 batch 1684/3281 lr 0.001 accuracy 94.26172 wps 14029.86 step time 0.43s\n","# Epoch 9  global step 27960 loss 5.60821 batch 1704/3281 lr 0.001 accuracy 93.86719 wps 14796.46 step time 0.53s\n","# Epoch 9  global step 27980 loss 5.39553 batch 1724/3281 lr 0.001 accuracy 94.43359 wps 14478.54 step time 0.47s\n","# Epoch 9  global step 28000 loss 5.50829 batch 1744/3281 lr 0.001 accuracy 93.88672 wps 14894.99 step time 0.53s\n","# global step 28000, eval model at Sat May 23 10:57:21 2020\n","2020-05-23 10:57:24.493870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:57:24.494201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 10:57:24.494330: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 10:57:24.494356: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 10:57:24.494378: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 10:57:24.494399: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 10:57:24.494420: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 10:57:24.494443: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 10:57:24.494488: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 10:57:24.494579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:57:24.494841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:57:24.495042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 10:57:24.495088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 10:57:24.495104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 10:57:24.495113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 10:57:24.495213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:57:24.495480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 10:57:24.495693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6420895943350247\n","# location_distance_from_business_district - 0.5445912817848427\n","# location_easy_to_find - 0.7078740123851549\n","# service_wait_time - 0.6500822089271335\n","# service_waiters_attitude - 0.7942371310247438\n","# service_parking_convenience - 0.7285073820673956\n","# service_serving_speed - 0.7582992076335905\n","# price_level - 0.7770502999881155\n","# price_cost_effective - 0.7162640903788572\n","# price_discount - 0.6653774426070707\n","# environment_decoration - 0.7194661578269024\n","# environment_noise - 0.7577860990725425\n","# environment_space - 0.7624658828603933\n","# environment_cleaness - 0.7556349267138107\n","# dish_portion - 0.7201930557673806\n","# dish_taste - 0.7302902665555707\n","# dish_look - 0.5678851235140658\n","# dish_recommendation - 0.7388692510198129\n","# others_overall_experience - 0.5896211912486525\n","# others_willing_to_consume_again - 0.7053478384180762\n","# Eval loss 4.42070, f1 0.70160\n","# current result -0.7015966222064567, previous best result -0.6986968471113326\n","# Epoch 9  global step 28020 loss 5.43433 batch 1764/3281 lr 0.001 accuracy 94.23047 wps 13968.35 step time 0.46s\n","# Epoch 9  global step 28040 loss 5.42673 batch 1784/3281 lr 0.001 accuracy 94.22656 wps 14156.66 step time 0.53s\n","# Epoch 9  global step 28060 loss 5.58023 batch 1804/3281 lr 0.001 accuracy 93.87500 wps 14442.90 step time 0.59s\n","# Epoch 9  global step 28080 loss 5.40533 batch 1824/3281 lr 0.001 accuracy 94.42969 wps 14288.74 step time 0.47s\n","# Epoch 9  global step 28100 loss 5.48303 batch 1844/3281 lr 0.001 accuracy 94.10156 wps 14295.36 step time 0.47s\n","# Epoch 9  global step 28120 loss 5.49426 batch 1864/3281 lr 0.001 accuracy 94.12500 wps 14577.47 step time 0.48s\n","# Epoch 9  global step 28140 loss 5.53218 batch 1884/3281 lr 0.001 accuracy 94.00781 wps 14773.01 step time 0.54s\n","# Epoch 9  global step 28160 loss 5.48001 batch 1904/3281 lr 0.001 accuracy 94.02734 wps 14457.24 step time 0.49s\n","# Epoch 9  global step 28180 loss 5.51246 batch 1924/3281 lr 0.001 accuracy 94.17969 wps 14037.76 step time 0.56s\n","# Epoch 9  global step 28200 loss 5.46628 batch 1944/3281 lr 0.001 accuracy 94.32031 wps 14425.16 step time 0.46s\n","# Epoch 9  global step 28220 loss 5.47545 batch 1964/3281 lr 0.001 accuracy 93.93359 wps 13809.67 step time 0.54s\n","# Epoch 9  global step 28240 loss 5.40728 batch 1984/3281 lr 0.001 accuracy 94.24219 wps 14140.76 step time 0.45s\n","# Epoch 9  global step 28260 loss 5.68952 batch 2004/3281 lr 0.001 accuracy 93.49219 wps 14718.92 step time 0.53s\n","# Epoch 9  global step 28280 loss 5.56174 batch 2024/3281 lr 0.001 accuracy 93.94531 wps 14169.04 step time 0.55s\n","# Epoch 9  global step 28300 loss 5.45516 batch 2044/3281 lr 0.001 accuracy 93.94922 wps 14170.01 step time 0.44s\n","# Epoch 9  global step 28320 loss 5.51116 batch 2064/3281 lr 0.001 accuracy 94.03906 wps 14148.96 step time 0.53s\n","# Epoch 9  global step 28340 loss 5.63553 batch 2084/3281 lr 0.001 accuracy 93.71094 wps 13689.37 step time 0.64s\n","# Epoch 9  global step 28360 loss 5.70362 batch 2104/3281 lr 0.001 accuracy 93.51953 wps 14356.70 step time 0.62s\n","# Epoch 9  global step 28380 loss 5.56755 batch 2124/3281 lr 0.001 accuracy 93.87500 wps 14013.32 step time 0.55s\n","# Epoch 9  global step 28400 loss 5.56236 batch 2144/3281 lr 0.001 accuracy 93.89063 wps 14694.92 step time 0.49s\n","# Epoch 9  global step 28420 loss 5.56993 batch 2164/3281 lr 0.001 accuracy 93.80469 wps 13829.50 step time 0.52s\n","# Epoch 9  global step 28440 loss 5.43117 batch 2184/3281 lr 0.001 accuracy 94.08984 wps 14522.78 step time 0.48s\n","# Epoch 9  global step 28460 loss 5.54403 batch 2204/3281 lr 0.001 accuracy 93.88281 wps 14339.77 step time 0.56s\n","# Epoch 9  global step 28480 loss 5.60192 batch 2224/3281 lr 0.001 accuracy 93.75000 wps 14698.37 step time 0.49s\n","# Epoch 9  global step 28500 loss 5.54777 batch 2244/3281 lr 0.001 accuracy 93.82813 wps 14638.07 step time 0.51s\n","# Epoch 9  global step 28520 loss 5.42190 batch 2264/3281 lr 0.001 accuracy 94.16406 wps 14083.29 step time 0.46s\n","# Epoch 9  global step 28540 loss 5.47603 batch 2284/3281 lr 0.001 accuracy 94.11328 wps 14449.22 step time 0.49s\n","# Epoch 9  global step 28560 loss 5.55718 batch 2304/3281 lr 0.001 accuracy 93.95313 wps 14821.84 step time 0.52s\n","# Epoch 9  global step 28580 loss 5.58459 batch 2324/3281 lr 0.001 accuracy 93.78125 wps 14463.29 step time 0.47s\n","# Epoch 9  global step 28600 loss 5.60902 batch 2344/3281 lr 0.001 accuracy 93.64453 wps 14623.50 step time 0.48s\n","# Epoch 9  global step 28620 loss 5.54127 batch 2364/3281 lr 0.001 accuracy 93.90625 wps 14323.22 step time 0.46s\n","# Epoch 9  global step 28640 loss 5.48521 batch 2384/3281 lr 0.001 accuracy 94.21875 wps 14595.47 step time 0.50s\n","# Epoch 9  global step 28660 loss 5.50249 batch 2404/3281 lr 0.001 accuracy 94.08984 wps 13584.06 step time 0.51s\n","# Epoch 9  global step 28680 loss 5.49233 batch 2424/3281 lr 0.001 accuracy 94.03906 wps 14067.69 step time 0.43s\n","# Epoch 9  global step 28700 loss 5.55766 batch 2444/3281 lr 0.001 accuracy 93.84766 wps 14100.55 step time 0.56s\n","# Epoch 9  global step 28720 loss 5.39229 batch 2464/3281 lr 0.001 accuracy 94.42578 wps 14131.12 step time 0.44s\n","# Epoch 9  global step 28740 loss 5.61234 batch 2484/3281 lr 0.001 accuracy 93.67969 wps 14301.43 step time 0.59s\n","# Epoch 9  global step 28760 loss 5.54248 batch 2504/3281 lr 0.001 accuracy 94.07422 wps 14390.97 step time 0.46s\n","# Epoch 9  global step 28780 loss 5.61010 batch 2524/3281 lr 0.001 accuracy 93.95703 wps 14116.23 step time 0.54s\n","# Epoch 9  global step 28800 loss 5.53154 batch 2544/3281 lr 0.001 accuracy 94.07813 wps 14071.11 step time 0.54s\n","# Epoch 9  global step 28820 loss 5.59679 batch 2564/3281 lr 0.001 accuracy 94.00000 wps 14175.00 step time 0.45s\n","# Epoch 9  global step 28840 loss 5.56274 batch 2584/3281 lr 0.001 accuracy 94.08203 wps 14128.13 step time 0.46s\n","# Epoch 9  global step 28860 loss 5.44941 batch 2604/3281 lr 0.001 accuracy 94.15234 wps 14614.28 step time 0.49s\n","# Epoch 9  global step 28880 loss 5.40247 batch 2624/3281 lr 0.001 accuracy 94.39063 wps 14228.74 step time 0.46s\n","# Epoch 9  global step 28900 loss 5.56838 batch 2644/3281 lr 0.001 accuracy 93.89453 wps 14110.51 step time 0.56s\n","# Epoch 9  global step 28920 loss 5.55808 batch 2664/3281 lr 0.001 accuracy 93.86719 wps 13994.24 step time 0.55s\n","# Epoch 9  global step 28940 loss 5.49989 batch 2684/3281 lr 0.001 accuracy 94.21094 wps 14096.27 step time 0.55s\n","# Epoch 9  global step 28960 loss 5.55073 batch 2704/3281 lr 0.001 accuracy 93.86719 wps 14391.16 step time 0.59s\n","# Epoch 9  global step 28980 loss 5.49482 batch 2724/3281 lr 0.001 accuracy 94.01953 wps 13998.61 step time 0.47s\n","# Epoch 9  global step 29000 loss 5.60760 batch 2744/3281 lr 0.001 accuracy 93.70312 wps 12701.61 step time 0.54s\n","# Epoch 9  global step 29020 loss 5.44233 batch 2764/3281 lr 0.001 accuracy 94.33984 wps 11882.41 step time 0.53s\n","# Epoch 9  global step 29040 loss 5.56344 batch 2784/3281 lr 0.001 accuracy 93.75000 wps 12114.11 step time 0.70s\n","# Epoch 9  global step 29060 loss 5.51594 batch 2804/3281 lr 0.001 accuracy 93.87891 wps 12777.24 step time 0.50s\n","# Epoch 9  global step 29080 loss 5.49844 batch 2824/3281 lr 0.001 accuracy 94.00781 wps 12714.30 step time 0.49s\n","# Epoch 9  global step 29100 loss 5.45852 batch 2844/3281 lr 0.001 accuracy 94.23828 wps 11420.28 step time 0.67s\n","# Epoch 9  global step 29120 loss 5.48360 batch 2864/3281 lr 0.001 accuracy 93.91406 wps 12514.18 step time 0.58s\n","# Epoch 9  global step 29140 loss 5.43209 batch 2884/3281 lr 0.001 accuracy 94.31641 wps 12058.08 step time 0.56s\n","# Epoch 9  global step 29160 loss 5.53432 batch 2904/3281 lr 0.001 accuracy 93.88281 wps 12680.42 step time 0.52s\n","# Epoch 9  global step 29180 loss 5.53099 batch 2924/3281 lr 0.001 accuracy 93.94922 wps 11801.25 step time 0.60s\n","# Epoch 9  global step 29200 loss 5.50870 batch 2944/3281 lr 0.001 accuracy 93.91016 wps 12799.63 step time 0.61s\n","# Epoch 9  global step 29220 loss 5.53201 batch 2964/3281 lr 0.001 accuracy 93.73047 wps 12749.10 step time 0.61s\n","# Epoch 9  global step 29240 loss 5.54399 batch 2984/3281 lr 0.001 accuracy 93.67969 wps 12855.44 step time 0.54s\n","# Epoch 9  global step 29260 loss 5.61506 batch 3004/3281 lr 0.001 accuracy 93.89063 wps 11849.37 step time 0.67s\n","# Epoch 9  global step 29280 loss 5.32374 batch 3024/3281 lr 0.001 accuracy 94.45313 wps 12622.42 step time 0.47s\n","# Epoch 9  global step 29300 loss 5.57410 batch 3044/3281 lr 0.001 accuracy 94.00781 wps 11810.09 step time 0.65s\n","# Epoch 9  global step 29320 loss 5.45880 batch 3064/3281 lr 0.001 accuracy 94.25391 wps 12904.29 step time 0.49s\n","# Epoch 9  global step 29340 loss 5.51212 batch 3084/3281 lr 0.001 accuracy 94.05078 wps 12723.34 step time 0.53s\n","# Epoch 9  global step 29360 loss 5.51399 batch 3104/3281 lr 0.001 accuracy 93.74219 wps 11951.12 step time 0.64s\n","# Epoch 9  global step 29380 loss 5.60547 batch 3124/3281 lr 0.001 accuracy 93.82812 wps 12125.52 step time 0.60s\n","# Epoch 9  global step 29400 loss 5.59734 batch 3144/3281 lr 0.001 accuracy 93.80859 wps 11842.68 step time 0.73s\n","# Epoch 9  global step 29420 loss 5.58656 batch 3164/3281 lr 0.001 accuracy 93.67969 wps 12776.17 step time 0.58s\n","# Epoch 9  global step 29440 loss 5.54712 batch 3184/3281 lr 0.001 accuracy 94.03906 wps 12197.64 step time 0.57s\n","# Epoch 9  global step 29460 loss 5.49465 batch 3204/3281 lr 0.001 accuracy 94.13672 wps 12200.37 step time 0.61s\n","# Epoch 9  global step 29480 loss 5.61240 batch 3224/3281 lr 0.001 accuracy 93.72266 wps 12641.85 step time 0.66s\n","# Epoch 9  global step 29500 loss 5.52888 batch 3244/3281 lr 0.001 accuracy 94.04688 wps 12411.48 step time 0.54s\n","# Epoch 9  global step 29520 loss 5.48203 batch 3264/3281 lr 0.001 accuracy 94.10938 wps 12990.51 step time 0.49s\n","# Finsh epoch 9, global step 29538\n","# Epoch 10  global step 29540 loss 0.57040 batch 2/3281 lr 0.001 accuracy 9.30469 wps 13311.36 step time 0.12s\n","# Epoch 10  global step 29560 loss 5.42877 batch 22/3281 lr 0.001 accuracy 94.33203 wps 14672.98 step time 0.49s\n","# Epoch 10  global step 29580 loss 5.44857 batch 42/3281 lr 0.001 accuracy 94.00781 wps 14441.00 step time 0.55s\n","# Epoch 10  global step 29600 loss 5.43430 batch 62/3281 lr 0.001 accuracy 94.01172 wps 14866.65 step time 0.51s\n","# Epoch 10  global step 29620 loss 5.46370 batch 82/3281 lr 0.001 accuracy 94.12891 wps 14120.66 step time 0.55s\n","# Epoch 10  global step 29640 loss 5.54511 batch 102/3281 lr 0.001 accuracy 93.78516 wps 14363.29 step time 0.56s\n","# Epoch 10  global step 29660 loss 5.35173 batch 122/3281 lr 0.001 accuracy 94.39453 wps 14266.18 step time 0.45s\n","# Epoch 10  global step 29680 loss 5.31595 batch 142/3281 lr 0.001 accuracy 94.67969 wps 14372.46 step time 0.46s\n","# Epoch 10  global step 29700 loss 5.39926 batch 162/3281 lr 0.001 accuracy 94.25391 wps 14444.72 step time 0.48s\n","# Epoch 10  global step 29720 loss 5.45730 batch 182/3281 lr 0.001 accuracy 94.20703 wps 13696.61 step time 0.62s\n","# Epoch 10  global step 29740 loss 5.36872 batch 202/3281 lr 0.001 accuracy 94.33594 wps 14509.56 step time 0.46s\n","# Epoch 10  global step 29760 loss 5.48815 batch 222/3281 lr 0.001 accuracy 94.13672 wps 14741.95 step time 0.48s\n","# Epoch 10  global step 29780 loss 5.42699 batch 242/3281 lr 0.001 accuracy 94.10938 wps 14452.80 step time 0.46s\n","# Epoch 10  global step 29800 loss 5.43279 batch 262/3281 lr 0.001 accuracy 94.32813 wps 14275.54 step time 0.49s\n","# Epoch 10  global step 29820 loss 5.63430 batch 282/3281 lr 0.001 accuracy 93.78906 wps 14887.33 step time 0.52s\n","# Epoch 10  global step 29840 loss 5.28957 batch 302/3281 lr 0.001 accuracy 94.51562 wps 14149.66 step time 0.44s\n","# Epoch 10  global step 29860 loss 5.47328 batch 322/3281 lr 0.001 accuracy 93.96484 wps 15002.07 step time 0.53s\n","# Epoch 10  global step 29880 loss 5.29201 batch 342/3281 lr 0.001 accuracy 94.60156 wps 14335.40 step time 0.45s\n","# Epoch 10  global step 29900 loss 5.50403 batch 362/3281 lr 0.001 accuracy 94.06641 wps 14582.66 step time 0.59s\n","# Epoch 10  global step 29920 loss 5.45758 batch 382/3281 lr 0.001 accuracy 94.09375 wps 14570.06 step time 0.48s\n","# Epoch 10  global step 29940 loss 5.32501 batch 402/3281 lr 0.001 accuracy 94.37109 wps 14527.90 step time 0.46s\n","# Epoch 10  global step 29960 loss 5.38156 batch 422/3281 lr 0.001 accuracy 94.27734 wps 14399.30 step time 0.46s\n","# Epoch 10  global step 29980 loss 5.42320 batch 442/3281 lr 0.001 accuracy 94.08594 wps 14180.93 step time 0.56s\n","# Epoch 10  global step 30000 loss 5.37994 batch 462/3281 lr 0.001 accuracy 94.53516 wps 14537.57 step time 0.47s\n","# global step 30000, eval model at Sat May 23 11:17:17 2020\n","2020-05-23 11:17:19.743201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:17:19.743578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 11:17:19.743694: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 11:17:19.743719: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 11:17:19.743741: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 11:17:19.743761: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 11:17:19.743782: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 11:17:19.743801: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 11:17:19.743822: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 11:17:19.743917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:17:19.744176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:17:19.744397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 11:17:19.744751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 11:17:19.744773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 11:17:19.744783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 11:17:19.744950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:17:19.745214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:17:19.745432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6422768219255407\n","# location_distance_from_business_district - 0.5382981862223788\n","# location_easy_to_find - 0.7088884548567645\n","# service_wait_time - 0.6553611140759954\n","# service_waiters_attitude - 0.7959831869730578\n","# service_parking_convenience - 0.736178368439782\n","# service_serving_speed - 0.759537108079399\n","# price_level - 0.7780702608240498\n","# price_cost_effective - 0.7150904804911744\n","# price_discount - 0.6728105622680319\n","# environment_decoration - 0.7230304997227348\n","# environment_noise - 0.7602377053741967\n","# environment_space - 0.7652515118272938\n","# environment_cleaness - 0.7559395581769661\n","# dish_portion - 0.7212327067793152\n","# dish_taste - 0.7301042560425393\n","# dish_look - 0.5723514294552587\n","# dish_recommendation - 0.7386102907430094\n","# others_overall_experience - 0.5899435522300106\n","# others_willing_to_consume_again - 0.7082309525404644\n","# Eval loss 4.41147, f1 0.70337\n","# current result -0.7033713503523982, previous best result -0.7015966222064567\n","# Epoch 10  global step 30020 loss 5.43583 batch 482/3281 lr 0.001 accuracy 94.12109 wps 14361.89 step time 0.49s\n","# Epoch 10  global step 30040 loss 5.47636 batch 502/3281 lr 0.001 accuracy 94.04297 wps 14310.68 step time 0.44s\n","# Epoch 10  global step 30060 loss 5.49037 batch 522/3281 lr 0.001 accuracy 93.93359 wps 14460.54 step time 0.58s\n","# Epoch 10  global step 30080 loss 5.43174 batch 542/3281 lr 0.001 accuracy 94.20312 wps 14411.43 step time 0.55s\n","# Epoch 10  global step 30100 loss 5.35891 batch 562/3281 lr 0.001 accuracy 94.59766 wps 14207.03 step time 0.45s\n","# Epoch 10  global step 30120 loss 5.49201 batch 582/3281 lr 0.001 accuracy 94.17969 wps 14706.54 step time 0.65s\n","# Epoch 10  global step 30140 loss 5.40992 batch 602/3281 lr 0.001 accuracy 94.19922 wps 14556.59 step time 0.49s\n","# Epoch 10  global step 30160 loss 5.40688 batch 622/3281 lr 0.001 accuracy 94.33984 wps 14520.33 step time 0.47s\n","# Epoch 10  global step 30180 loss 5.50341 batch 642/3281 lr 0.001 accuracy 94.06250 wps 14653.87 step time 0.48s\n","# Epoch 10  global step 30200 loss 5.47552 batch 662/3281 lr 0.001 accuracy 94.00781 wps 13883.33 step time 0.53s\n","# Epoch 10  global step 30220 loss 5.32122 batch 682/3281 lr 0.001 accuracy 94.45703 wps 14318.10 step time 0.46s\n","# Epoch 10  global step 30240 loss 5.42666 batch 702/3281 lr 0.001 accuracy 94.25000 wps 14125.54 step time 0.56s\n","# Epoch 10  global step 30260 loss 5.27499 batch 722/3281 lr 0.001 accuracy 94.70703 wps 13939.17 step time 0.43s\n","# Epoch 10  global step 30280 loss 5.41343 batch 742/3281 lr 0.001 accuracy 94.26563 wps 14683.36 step time 0.49s\n","# Epoch 10  global step 30300 loss 5.33012 batch 762/3281 lr 0.001 accuracy 94.40234 wps 14515.94 step time 0.46s\n","# Epoch 10  global step 30320 loss 5.49807 batch 782/3281 lr 0.001 accuracy 93.97656 wps 15213.08 step time 0.55s\n","# Epoch 10  global step 30340 loss 5.30361 batch 802/3281 lr 0.001 accuracy 94.73828 wps 13794.90 step time 0.40s\n","# Epoch 10  global step 30360 loss 5.40019 batch 822/3281 lr 0.001 accuracy 94.32031 wps 14334.65 step time 0.45s\n","# Epoch 10  global step 30380 loss 5.37330 batch 842/3281 lr 0.001 accuracy 94.44141 wps 14389.61 step time 0.45s\n","# Epoch 10  global step 30400 loss 5.38479 batch 862/3281 lr 0.001 accuracy 94.44141 wps 14676.93 step time 0.49s\n","# Epoch 10  global step 30420 loss 5.51180 batch 882/3281 lr 0.001 accuracy 93.95312 wps 14110.65 step time 0.54s\n","# Epoch 10  global step 30440 loss 5.46543 batch 902/3281 lr 0.001 accuracy 94.05078 wps 14961.09 step time 0.52s\n","# Epoch 10  global step 30460 loss 5.48414 batch 922/3281 lr 0.001 accuracy 94.06641 wps 14150.32 step time 0.46s\n","# Epoch 10  global step 30480 loss 5.48812 batch 942/3281 lr 0.001 accuracy 94.01953 wps 13956.84 step time 0.56s\n","# Epoch 10  global step 30500 loss 5.38684 batch 962/3281 lr 0.001 accuracy 94.21875 wps 13702.14 step time 0.49s\n","# Epoch 10  global step 30520 loss 5.48623 batch 982/3281 lr 0.001 accuracy 93.97266 wps 13769.50 step time 0.51s\n","# Epoch 10  global step 30540 loss 5.35693 batch 1002/3281 lr 0.001 accuracy 94.31641 wps 14442.15 step time 0.46s\n","# Epoch 10  global step 30560 loss 5.45469 batch 1022/3281 lr 0.001 accuracy 94.15234 wps 14498.01 step time 0.47s\n","# Epoch 10  global step 30580 loss 5.36319 batch 1042/3281 lr 0.001 accuracy 94.44531 wps 13912.66 step time 0.42s\n","# Epoch 10  global step 30600 loss 5.38541 batch 1062/3281 lr 0.001 accuracy 94.26172 wps 14022.22 step time 0.52s\n","# Epoch 10  global step 30620 loss 5.44080 batch 1082/3281 lr 0.001 accuracy 94.16016 wps 14481.86 step time 0.46s\n","# Epoch 10  global step 30640 loss 5.32608 batch 1102/3281 lr 0.001 accuracy 94.66016 wps 13948.34 step time 0.53s\n","# Epoch 10  global step 30660 loss 5.53229 batch 1122/3281 lr 0.001 accuracy 93.94141 wps 14898.25 step time 0.50s\n","# Epoch 10  global step 30680 loss 5.33202 batch 1142/3281 lr 0.001 accuracy 94.48437 wps 14248.23 step time 0.44s\n","# Epoch 10  global step 30700 loss 5.41479 batch 1162/3281 lr 0.001 accuracy 94.26172 wps 14521.09 step time 0.47s\n","# Epoch 10  global step 30720 loss 5.39846 batch 1182/3281 lr 0.001 accuracy 94.39063 wps 13928.46 step time 0.52s\n","# Epoch 10  global step 30740 loss 5.47226 batch 1202/3281 lr 0.001 accuracy 94.03906 wps 13957.94 step time 0.53s\n","# Epoch 10  global step 30760 loss 5.37426 batch 1222/3281 lr 0.001 accuracy 94.28516 wps 14334.15 step time 0.44s\n","# Epoch 10  global step 30780 loss 5.41232 batch 1242/3281 lr 0.001 accuracy 94.26562 wps 14439.50 step time 0.45s\n","# Epoch 10  global step 30800 loss 5.51233 batch 1262/3281 lr 0.001 accuracy 93.98438 wps 14198.00 step time 0.56s\n","# Epoch 10  global step 30820 loss 5.50214 batch 1282/3281 lr 0.001 accuracy 93.84766 wps 14306.50 step time 0.54s\n","# Epoch 10  global step 30840 loss 5.54541 batch 1302/3281 lr 0.001 accuracy 93.97656 wps 14544.25 step time 0.48s\n","# Epoch 10  global step 30860 loss 5.51322 batch 1322/3281 lr 0.001 accuracy 93.85547 wps 14755.41 step time 0.51s\n","# Epoch 10  global step 30880 loss 5.39768 batch 1342/3281 lr 0.001 accuracy 94.22656 wps 14574.75 step time 0.47s\n","# Epoch 10  global step 30900 loss 5.37233 batch 1362/3281 lr 0.001 accuracy 94.39844 wps 14378.34 step time 0.46s\n","# Epoch 10  global step 30920 loss 5.48888 batch 1382/3281 lr 0.001 accuracy 93.97266 wps 14683.06 step time 0.50s\n","# Epoch 10  global step 30940 loss 5.57826 batch 1402/3281 lr 0.001 accuracy 93.81641 wps 14388.77 step time 0.58s\n","# Epoch 10  global step 30960 loss 5.57557 batch 1422/3281 lr 0.001 accuracy 94.06641 wps 14968.03 step time 0.55s\n","# Epoch 10  global step 30980 loss 5.59735 batch 1442/3281 lr 0.001 accuracy 93.67187 wps 15028.72 step time 0.53s\n","# Epoch 10  global step 31000 loss 5.32712 batch 1462/3281 lr 0.001 accuracy 94.67188 wps 14108.84 step time 0.43s\n","# Epoch 10  global step 31020 loss 5.43575 batch 1482/3281 lr 0.001 accuracy 93.85937 wps 14590.65 step time 0.46s\n","# Epoch 10  global step 31040 loss 5.41773 batch 1502/3281 lr 0.001 accuracy 94.15234 wps 14526.77 step time 0.46s\n","# Epoch 10  global step 31060 loss 5.54176 batch 1522/3281 lr 0.001 accuracy 94.12109 wps 14500.55 step time 0.57s\n","# Epoch 10  global step 31080 loss 5.43385 batch 1542/3281 lr 0.001 accuracy 94.09766 wps 13674.04 step time 0.50s\n","# Epoch 10  global step 31100 loss 5.40754 batch 1562/3281 lr 0.001 accuracy 94.33594 wps 14320.72 step time 0.44s\n","# Epoch 10  global step 31120 loss 5.33894 batch 1582/3281 lr 0.001 accuracy 94.58984 wps 14123.78 step time 0.42s\n","# Epoch 10  global step 31140 loss 5.41171 batch 1602/3281 lr 0.001 accuracy 94.32422 wps 14020.14 step time 0.53s\n","# Epoch 10  global step 31160 loss 5.44576 batch 1622/3281 lr 0.001 accuracy 94.10937 wps 14168.67 step time 0.53s\n","# Epoch 10  global step 31180 loss 5.42380 batch 1642/3281 lr 0.001 accuracy 94.31250 wps 14568.44 step time 0.46s\n","# Epoch 10  global step 31200 loss 5.47928 batch 1662/3281 lr 0.001 accuracy 93.88672 wps 14933.19 step time 0.52s\n","# Epoch 10  global step 31220 loss 5.38268 batch 1682/3281 lr 0.001 accuracy 94.47656 wps 13988.77 step time 0.52s\n","# Epoch 10  global step 31240 loss 5.40429 batch 1702/3281 lr 0.001 accuracy 94.09766 wps 13381.04 step time 0.57s\n","# Epoch 10  global step 31260 loss 5.43930 batch 1722/3281 lr 0.001 accuracy 94.20703 wps 12964.33 step time 0.55s\n","# Epoch 10  global step 31280 loss 5.50157 batch 1742/3281 lr 0.001 accuracy 93.91797 wps 11987.25 step time 0.65s\n","# Epoch 10  global step 31300 loss 5.52204 batch 1762/3281 lr 0.001 accuracy 94.14063 wps 13125.95 step time 0.55s\n","# Epoch 10  global step 31320 loss 5.19009 batch 1782/3281 lr 0.001 accuracy 95.05078 wps 12451.23 step time 0.44s\n","# Epoch 10  global step 31340 loss 5.41590 batch 1802/3281 lr 0.001 accuracy 94.10937 wps 12321.22 step time 0.65s\n","# Epoch 10  global step 31360 loss 5.36686 batch 1822/3281 lr 0.001 accuracy 94.26563 wps 12504.42 step time 0.52s\n","# Epoch 10  global step 31380 loss 5.38742 batch 1842/3281 lr 0.001 accuracy 94.46875 wps 13318.21 step time 0.55s\n","# Epoch 10  global step 31400 loss 5.63021 batch 1862/3281 lr 0.001 accuracy 93.59766 wps 12562.10 step time 0.77s\n","# Epoch 10  global step 31420 loss 5.46896 batch 1882/3281 lr 0.001 accuracy 94.02734 wps 12851.03 step time 0.54s\n","# Epoch 10  global step 31440 loss 5.43330 batch 1902/3281 lr 0.001 accuracy 94.36719 wps 12441.88 step time 0.59s\n","# Epoch 10  global step 31460 loss 5.45185 batch 1922/3281 lr 0.001 accuracy 94.07031 wps 11281.94 step time 0.68s\n","# Epoch 10  global step 31480 loss 5.50412 batch 1942/3281 lr 0.001 accuracy 94.11328 wps 12479.58 step time 0.56s\n","# Epoch 10  global step 31500 loss 5.44004 batch 1962/3281 lr 0.001 accuracy 94.02344 wps 13283.38 step time 0.52s\n","# Epoch 10  global step 31520 loss 5.52162 batch 1982/3281 lr 0.001 accuracy 94.01563 wps 11626.64 step time 0.65s\n","# Epoch 10  global step 31540 loss 5.58731 batch 2002/3281 lr 0.001 accuracy 93.71094 wps 12679.36 step time 0.63s\n","# Epoch 10  global step 31560 loss 5.44292 batch 2022/3281 lr 0.001 accuracy 94.40234 wps 11501.65 step time 0.62s\n","# Epoch 10  global step 31580 loss 5.47972 batch 2042/3281 lr 0.001 accuracy 94.10938 wps 12810.83 step time 0.57s\n","# Epoch 10  global step 31600 loss 5.51442 batch 2062/3281 lr 0.001 accuracy 94.01562 wps 13071.95 step time 0.54s\n","# Epoch 10  global step 31620 loss 5.42268 batch 2082/3281 lr 0.001 accuracy 94.30078 wps 12391.42 step time 0.51s\n","# Epoch 10  global step 31640 loss 5.42566 batch 2102/3281 lr 0.001 accuracy 94.17187 wps 11842.34 step time 0.63s\n","# Epoch 10  global step 31660 loss 5.52693 batch 2122/3281 lr 0.001 accuracy 93.91797 wps 12636.99 step time 0.56s\n","# Epoch 10  global step 31680 loss 5.40016 batch 2142/3281 lr 0.001 accuracy 94.30859 wps 12500.64 step time 0.54s\n","# Epoch 10  global step 31700 loss 5.58419 batch 2162/3281 lr 0.001 accuracy 93.59375 wps 12416.69 step time 0.77s\n","# Epoch 10  global step 31720 loss 5.50027 batch 2182/3281 lr 0.001 accuracy 93.86719 wps 11535.21 step time 0.73s\n","# Epoch 10  global step 31740 loss 5.51458 batch 2202/3281 lr 0.001 accuracy 93.98828 wps 12468.24 step time 0.60s\n","# Epoch 10  global step 31760 loss 5.65917 batch 2222/3281 lr 0.001 accuracy 93.51562 wps 12623.37 step time 0.68s\n","# Epoch 10  global step 31780 loss 5.47617 batch 2242/3281 lr 0.001 accuracy 94.29297 wps 12894.00 step time 0.57s\n","# Epoch 10  global step 31800 loss 5.47118 batch 2262/3281 lr 0.001 accuracy 94.11719 wps 12701.03 step time 0.53s\n","# Epoch 10  global step 31820 loss 5.44059 batch 2282/3281 lr 0.001 accuracy 94.32813 wps 12634.86 step time 0.54s\n","# Epoch 10  global step 31840 loss 5.54717 batch 2302/3281 lr 0.001 accuracy 93.90625 wps 13543.57 step time 0.51s\n","# Epoch 10  global step 31860 loss 5.52145 batch 2322/3281 lr 0.001 accuracy 93.96875 wps 12670.41 step time 0.53s\n","# Epoch 10  global step 31880 loss 5.35508 batch 2342/3281 lr 0.001 accuracy 94.41016 wps 13142.56 step time 0.46s\n","# Epoch 10  global step 31900 loss 5.48398 batch 2362/3281 lr 0.001 accuracy 93.87891 wps 12981.53 step time 0.52s\n","# Epoch 10  global step 31920 loss 5.48718 batch 2382/3281 lr 0.001 accuracy 94.06641 wps 12834.48 step time 0.51s\n","# Epoch 10  global step 31940 loss 5.48230 batch 2402/3281 lr 0.001 accuracy 93.97266 wps 12468.64 step time 0.55s\n","# Epoch 10  global step 31960 loss 5.36703 batch 2422/3281 lr 0.001 accuracy 94.43359 wps 13064.47 step time 0.49s\n","# Epoch 10  global step 31980 loss 5.33902 batch 2442/3281 lr 0.001 accuracy 94.55078 wps 13441.75 step time 0.50s\n","# Epoch 10  global step 32000 loss 5.49308 batch 2462/3281 lr 0.001 accuracy 94.23828 wps 12994.94 step time 0.52s\n","# global step 32000, eval model at Sat May 23 11:37:07 2020\n","2020-05-23 11:37:09.818456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:37:09.818821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 11:37:09.818923: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 11:37:09.818953: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 11:37:09.818976: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 11:37:09.819002: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 11:37:09.819024: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 11:37:09.819043: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 11:37:09.819064: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 11:37:09.819149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:37:09.819413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:37:09.819662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 11:37:09.819747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 11:37:09.819763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 11:37:09.819773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 11:37:09.819877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:37:09.820139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:37:09.820349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6486696971162034\n","# location_distance_from_business_district - 0.5273707307202515\n","# location_easy_to_find - 0.706284202916222\n","# service_wait_time - 0.6618293095019413\n","# service_waiters_attitude - 0.7944948241373413\n","# service_parking_convenience - 0.733943327822753\n","# service_serving_speed - 0.7590207256457222\n","# price_level - 0.7773673466631652\n","# price_cost_effective - 0.714017068155151\n","# price_discount - 0.6750862207326211\n","# environment_decoration - 0.7215145154559391\n","# environment_noise - 0.7597904711044403\n","# environment_space - 0.7653198958382473\n","# environment_cleaness - 0.7562182580727553\n","# dish_portion - 0.7205067672371679\n","# dish_taste - 0.7318242434032628\n","# dish_look - 0.5756030586704685\n","# dish_recommendation - 0.7348742082019086\n","# others_overall_experience - 0.591165138707664\n","# others_willing_to_consume_again - 0.7116103916275\n","# Eval loss 4.40994, f1 0.70333\n","# current result -0.7033255200865364, previous best result -0.7033713503523982\n","# Epoch 10  global step 32020 loss 5.45619 batch 2482/3281 lr 0.001 accuracy 94.11719 wps 11896.44 step time 0.63s\n","# Epoch 10  global step 32040 loss 5.49305 batch 2502/3281 lr 0.001 accuracy 93.94922 wps 12433.79 step time 0.53s\n","# Epoch 10  global step 32060 loss 5.45846 batch 2522/3281 lr 0.001 accuracy 94.17578 wps 13051.03 step time 0.52s\n","# Epoch 10  global step 32080 loss 5.54896 batch 2542/3281 lr 0.001 accuracy 94.04687 wps 11720.77 step time 0.73s\n","# Epoch 10  global step 32100 loss 5.55839 batch 2562/3281 lr 0.001 accuracy 93.92188 wps 12719.51 step time 0.70s\n","# Epoch 10  global step 32120 loss 5.46987 batch 2582/3281 lr 0.001 accuracy 94.09766 wps 13089.72 step time 0.61s\n","# Epoch 10  global step 32140 loss 5.52455 batch 2602/3281 lr 0.001 accuracy 93.85547 wps 11996.71 step time 0.61s\n","# Epoch 10  global step 32160 loss 5.47501 batch 2622/3281 lr 0.001 accuracy 94.15625 wps 12976.85 step time 0.57s\n","# Epoch 10  global step 32180 loss 5.43879 batch 2642/3281 lr 0.001 accuracy 94.26172 wps 11995.35 step time 0.56s\n","# Epoch 10  global step 32200 loss 5.48715 batch 2662/3281 lr 0.001 accuracy 94.11328 wps 12145.54 step time 0.63s\n","# Epoch 10  global step 32220 loss 5.61919 batch 2682/3281 lr 0.001 accuracy 93.62891 wps 13136.80 step time 0.60s\n","# Epoch 10  global step 32240 loss 5.53048 batch 2702/3281 lr 0.001 accuracy 93.92187 wps 12738.69 step time 0.55s\n","# Epoch 10  global step 32260 loss 5.50636 batch 2722/3281 lr 0.001 accuracy 94.01172 wps 12838.86 step time 0.58s\n","# Epoch 10  global step 32280 loss 5.53000 batch 2742/3281 lr 0.001 accuracy 94.03125 wps 11720.89 step time 0.79s\n","# Epoch 10  global step 32300 loss 5.37654 batch 2762/3281 lr 0.001 accuracy 94.53516 wps 12697.48 step time 0.44s\n","# Epoch 10  global step 32320 loss 5.56354 batch 2782/3281 lr 0.001 accuracy 93.89844 wps 12200.88 step time 0.66s\n","# Epoch 10  global step 32340 loss 5.47994 batch 2802/3281 lr 0.001 accuracy 94.12109 wps 12292.31 step time 0.63s\n","# Epoch 10  global step 32360 loss 5.45535 batch 2822/3281 lr 0.001 accuracy 94.00391 wps 12276.93 step time 0.56s\n","# Epoch 10  global step 32380 loss 5.44776 batch 2842/3281 lr 0.001 accuracy 94.37500 wps 12416.12 step time 0.51s\n","# Epoch 10  global step 32400 loss 5.62958 batch 2862/3281 lr 0.001 accuracy 93.43750 wps 12015.07 step time 0.75s\n","# Epoch 10  global step 32420 loss 5.51290 batch 2882/3281 lr 0.001 accuracy 93.86328 wps 11897.11 step time 0.66s\n","# Epoch 10  global step 32440 loss 5.38869 batch 2902/3281 lr 0.001 accuracy 94.41797 wps 12838.81 step time 0.50s\n","# Epoch 10  global step 32460 loss 5.41243 batch 2922/3281 lr 0.001 accuracy 94.42188 wps 13294.02 step time 0.46s\n","# Epoch 10  global step 32480 loss 5.41521 batch 2942/3281 lr 0.001 accuracy 94.12500 wps 12330.42 step time 0.63s\n","# Epoch 10  global step 32500 loss 5.35473 batch 2962/3281 lr 0.001 accuracy 94.51562 wps 12563.52 step time 0.49s\n","# Epoch 10  global step 32520 loss 5.27282 batch 2982/3281 lr 0.001 accuracy 94.65625 wps 12835.86 step time 0.44s\n","# Epoch 10  global step 32540 loss 5.59481 batch 3002/3281 lr 0.001 accuracy 93.43750 wps 12062.09 step time 0.64s\n","# Epoch 10  global step 32560 loss 5.56089 batch 3022/3281 lr 0.001 accuracy 93.93750 wps 12371.77 step time 0.64s\n","# Epoch 10  global step 32580 loss 5.44664 batch 3042/3281 lr 0.001 accuracy 94.02344 wps 12548.42 step time 0.53s\n","# Epoch 10  global step 32600 loss 5.60399 batch 3062/3281 lr 0.001 accuracy 93.75781 wps 13015.55 step time 0.53s\n","# Epoch 10  global step 32620 loss 5.56000 batch 3082/3281 lr 0.001 accuracy 93.82422 wps 12786.27 step time 0.60s\n","# Epoch 10  global step 32640 loss 5.35813 batch 3102/3281 lr 0.001 accuracy 94.26563 wps 12743.05 step time 0.53s\n","# Epoch 10  global step 32660 loss 5.58381 batch 3122/3281 lr 0.001 accuracy 93.80469 wps 13641.88 step time 0.55s\n","# Epoch 10  global step 32680 loss 5.52823 batch 3142/3281 lr 0.001 accuracy 93.79688 wps 12384.21 step time 0.68s\n","# Epoch 10  global step 32700 loss 5.52739 batch 3162/3281 lr 0.001 accuracy 94.15234 wps 12683.55 step time 0.53s\n","# Epoch 10  global step 32720 loss 5.50577 batch 3182/3281 lr 0.001 accuracy 94.08984 wps 12314.23 step time 0.72s\n","# Epoch 10  global step 32740 loss 5.49107 batch 3202/3281 lr 0.001 accuracy 94.03906 wps 12532.38 step time 0.60s\n","# Epoch 10  global step 32760 loss 5.50057 batch 3222/3281 lr 0.001 accuracy 94.19141 wps 12440.28 step time 0.68s\n","# Epoch 10  global step 32780 loss 5.59579 batch 3242/3281 lr 0.001 accuracy 93.69922 wps 12976.95 step time 0.56s\n","# Epoch 10  global step 32800 loss 5.48550 batch 3262/3281 lr 0.001 accuracy 93.92969 wps 12395.78 step time 0.62s\n","# Epoch 10  global step 32820 loss 5.58630 batch 3282/3281 lr 0.001 accuracy 93.69531 wps 11929.67 step time 0.72s\n","# Finsh epoch 10, global step 32820\n","# Epoch 11  global step 32840 loss 5.26845 batch 20/3281 lr 0.001 accuracy 94.46094 wps 13325.56 step time 0.58s\n","# Epoch 11  global step 32860 loss 5.41963 batch 40/3281 lr 0.001 accuracy 94.20312 wps 13865.15 step time 0.58s\n","# Epoch 11  global step 32880 loss 5.25608 batch 60/3281 lr 0.001 accuracy 94.69531 wps 14675.45 step time 0.47s\n","# Epoch 11  global step 32900 loss 5.32891 batch 80/3281 lr 0.001 accuracy 94.44922 wps 14578.91 step time 0.48s\n","# Epoch 11  global step 32920 loss 5.46159 batch 100/3281 lr 0.001 accuracy 94.16797 wps 14615.17 step time 0.47s\n","# Epoch 11  global step 32940 loss 5.38085 batch 120/3281 lr 0.001 accuracy 94.32031 wps 13873.59 step time 0.53s\n","# Epoch 11  global step 32960 loss 5.31154 batch 140/3281 lr 0.001 accuracy 94.48047 wps 12983.96 step time 0.50s\n","# Epoch 11  global step 32980 loss 5.34599 batch 160/3281 lr 0.001 accuracy 94.39062 wps 11933.11 step time 0.64s\n","# Epoch 11  global step 33000 loss 5.38656 batch 180/3281 lr 0.001 accuracy 94.42187 wps 12064.21 step time 0.58s\n","# Epoch 11  global step 33020 loss 5.29428 batch 200/3281 lr 0.001 accuracy 94.43750 wps 11605.79 step time 0.69s\n","# Epoch 11  global step 33040 loss 5.35312 batch 220/3281 lr 0.001 accuracy 94.35547 wps 11841.35 step time 0.64s\n","# Epoch 11  global step 33060 loss 5.31493 batch 240/3281 lr 0.001 accuracy 94.50781 wps 12409.03 step time 0.50s\n","# Epoch 11  global step 33080 loss 5.41042 batch 260/3281 lr 0.001 accuracy 94.22656 wps 12874.20 step time 0.63s\n","# Epoch 11  global step 33100 loss 5.46563 batch 280/3281 lr 0.001 accuracy 94.11328 wps 13091.41 step time 0.56s\n","# Epoch 11  global step 33120 loss 5.47895 batch 300/3281 lr 0.001 accuracy 94.04297 wps 12083.52 step time 0.70s\n","# Epoch 11  global step 33140 loss 5.36403 batch 320/3281 lr 0.001 accuracy 94.32031 wps 12693.99 step time 0.56s\n","# Epoch 11  global step 33160 loss 5.39640 batch 340/3281 lr 0.001 accuracy 94.16797 wps 12874.53 step time 0.47s\n","# Epoch 11  global step 33180 loss 5.54624 batch 360/3281 lr 0.001 accuracy 93.83203 wps 12661.92 step time 0.66s\n","# Epoch 11  global step 33200 loss 5.28457 batch 380/3281 lr 0.001 accuracy 94.58984 wps 12605.39 step time 0.51s\n","# Epoch 11  global step 33220 loss 5.29432 batch 400/3281 lr 0.001 accuracy 94.51172 wps 12159.54 step time 0.54s\n","# Epoch 11  global step 33240 loss 5.33503 batch 420/3281 lr 0.001 accuracy 94.45703 wps 12411.31 step time 0.54s\n","# Epoch 11  global step 33260 loss 5.43772 batch 440/3281 lr 0.001 accuracy 94.28125 wps 13143.57 step time 0.56s\n","# Epoch 11  global step 33280 loss 5.46109 batch 460/3281 lr 0.001 accuracy 94.01172 wps 13202.11 step time 0.60s\n","# Epoch 11  global step 33300 loss 5.33135 batch 480/3281 lr 0.001 accuracy 94.51953 wps 12749.02 step time 0.52s\n","# Epoch 11  global step 33320 loss 5.32775 batch 500/3281 lr 0.001 accuracy 94.46484 wps 12843.44 step time 0.54s\n","# Epoch 11  global step 33340 loss 5.27758 batch 520/3281 lr 0.001 accuracy 94.50000 wps 13054.60 step time 0.55s\n","# Epoch 11  global step 33360 loss 5.45858 batch 540/3281 lr 0.001 accuracy 94.18359 wps 12282.04 step time 0.64s\n","# Epoch 11  global step 33380 loss 5.61944 batch 560/3281 lr 0.001 accuracy 93.65234 wps 12298.74 step time 0.80s\n","# Epoch 11  global step 33400 loss 5.49475 batch 580/3281 lr 0.001 accuracy 93.91406 wps 12614.09 step time 0.67s\n","# Epoch 11  global step 33420 loss 5.44129 batch 600/3281 lr 0.001 accuracy 94.37891 wps 11681.34 step time 0.62s\n","# Epoch 11  global step 33440 loss 5.25935 batch 620/3281 lr 0.001 accuracy 94.54297 wps 13209.57 step time 0.51s\n","# Epoch 11  global step 33460 loss 5.57864 batch 640/3281 lr 0.001 accuracy 93.74609 wps 12735.73 step time 0.67s\n","# Epoch 11  global step 33480 loss 5.41020 batch 660/3281 lr 0.001 accuracy 94.32422 wps 13242.67 step time 0.55s\n","# Epoch 11  global step 33500 loss 5.38042 batch 680/3281 lr 0.001 accuracy 94.37109 wps 12630.57 step time 0.68s\n","# Epoch 11  global step 33520 loss 5.25383 batch 700/3281 lr 0.001 accuracy 94.64062 wps 11841.53 step time 0.53s\n","# Epoch 11  global step 33540 loss 5.36509 batch 720/3281 lr 0.001 accuracy 94.25000 wps 12957.59 step time 0.54s\n","# Epoch 11  global step 33560 loss 5.37296 batch 740/3281 lr 0.001 accuracy 94.42969 wps 12922.47 step time 0.51s\n","# Epoch 11  global step 33580 loss 5.40306 batch 760/3281 lr 0.001 accuracy 94.24219 wps 13066.08 step time 0.50s\n","# Epoch 11  global step 33600 loss 5.35664 batch 780/3281 lr 0.001 accuracy 94.22266 wps 12159.72 step time 0.61s\n","# Epoch 11  global step 33620 loss 5.29210 batch 800/3281 lr 0.001 accuracy 94.47266 wps 12292.33 step time 0.61s\n","# Epoch 11  global step 33640 loss 5.27682 batch 820/3281 lr 0.001 accuracy 94.56641 wps 13208.84 step time 0.53s\n","# Epoch 11  global step 33660 loss 5.36983 batch 840/3281 lr 0.001 accuracy 94.37500 wps 12689.59 step time 0.64s\n","# Epoch 11  global step 33680 loss 5.25657 batch 860/3281 lr 0.001 accuracy 94.64844 wps 12417.89 step time 0.48s\n","# Epoch 11  global step 33700 loss 5.30097 batch 880/3281 lr 0.001 accuracy 94.57031 wps 13247.26 step time 0.49s\n","# Epoch 11  global step 33720 loss 5.37413 batch 900/3281 lr 0.001 accuracy 94.36328 wps 12646.04 step time 0.56s\n","# Epoch 11  global step 33740 loss 5.34604 batch 920/3281 lr 0.001 accuracy 94.36328 wps 12042.19 step time 0.55s\n","# Epoch 11  global step 33760 loss 5.37657 batch 940/3281 lr 0.001 accuracy 94.30859 wps 12529.67 step time 0.58s\n","# Epoch 11  global step 33780 loss 5.50625 batch 960/3281 lr 0.001 accuracy 93.92578 wps 13524.73 step time 0.60s\n","# Epoch 11  global step 33800 loss 5.39438 batch 980/3281 lr 0.001 accuracy 94.25781 wps 13050.12 step time 0.54s\n","# Epoch 11  global step 33820 loss 5.46420 batch 1000/3281 lr 0.001 accuracy 94.25000 wps 13014.36 step time 0.54s\n","# Epoch 11  global step 33840 loss 5.29748 batch 1020/3281 lr 0.001 accuracy 94.57812 wps 12641.89 step time 0.47s\n","# Epoch 11  global step 33860 loss 5.41030 batch 1040/3281 lr 0.001 accuracy 94.19922 wps 12531.05 step time 0.55s\n","# Epoch 11  global step 33880 loss 5.41083 batch 1060/3281 lr 0.001 accuracy 94.27344 wps 12200.14 step time 0.61s\n","# Epoch 11  global step 33900 loss 5.50585 batch 1080/3281 lr 0.001 accuracy 94.02344 wps 11771.95 step time 0.77s\n","# Epoch 11  global step 33920 loss 5.22935 batch 1100/3281 lr 0.001 accuracy 94.91797 wps 11736.36 step time 0.55s\n","# Epoch 11  global step 33940 loss 5.56092 batch 1120/3281 lr 0.001 accuracy 93.74219 wps 12378.48 step time 0.71s\n","# Epoch 11  global step 33960 loss 5.41815 batch 1140/3281 lr 0.001 accuracy 94.18750 wps 13490.73 step time 0.58s\n","# Epoch 11  global step 33980 loss 5.46435 batch 1160/3281 lr 0.001 accuracy 94.12891 wps 11951.40 step time 0.64s\n","# Epoch 11  global step 34000 loss 5.40377 batch 1180/3281 lr 0.001 accuracy 94.22266 wps 13255.25 step time 0.59s\n","# global step 34000, eval model at Sat May 23 11:58:54 2020\n","2020-05-23 11:58:56.978824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:58:56.979272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 11:58:56.979386: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 11:58:56.979415: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 11:58:56.979466: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 11:58:56.979490: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 11:58:56.979511: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 11:58:56.979531: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 11:58:56.979551: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 11:58:56.979642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:58:56.979914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:58:56.980141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 11:58:56.980525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 11:58:56.980547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 11:58:56.980557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 11:58:56.980726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:58:56.980997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 11:58:56.981208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6502888779035567\n","# location_distance_from_business_district - 0.5259353497594478\n","# location_easy_to_find - 0.7044902893555973\n","# service_wait_time - 0.6623598764886255\n","# service_waiters_attitude - 0.7939275601189523\n","# service_parking_convenience - 0.7403393832235956\n","# service_serving_speed - 0.7595882514797179\n","# price_level - 0.777508726975123\n","# price_cost_effective - 0.7177636785734957\n","# price_discount - 0.6788551618661516\n","# environment_decoration - 0.7185759859184382\n","# environment_noise - 0.7600181081768357\n","# environment_space - 0.7653311247685307\n","# environment_cleaness - 0.7554307559364793\n","# dish_portion - 0.7197507098342977\n","# dish_taste - 0.7318940172081567\n","# dish_look - 0.5767609825083563\n","# dish_recommendation - 0.7362153814320048\n","# others_overall_experience - 0.590879110868418\n","# others_willing_to_consume_again - 0.7114710188978024\n","# Eval loss 4.41528, f1 0.70387\n","# current result -0.7038692175646791, previous best result -0.7033713503523982\n","# Epoch 11  global step 34020 loss 5.48635 batch 1200/3281 lr 0.001 accuracy 94.03516 wps 12566.58 step time 0.55s\n","# Epoch 11  global step 34040 loss 5.35493 batch 1220/3281 lr 0.001 accuracy 94.27344 wps 13132.56 step time 0.50s\n","# Epoch 11  global step 34060 loss 5.39849 batch 1240/3281 lr 0.001 accuracy 94.37891 wps 12971.79 step time 0.48s\n","# Epoch 11  global step 34080 loss 5.42052 batch 1260/3281 lr 0.001 accuracy 94.39062 wps 12837.08 step time 0.54s\n","# Epoch 11  global step 34100 loss 5.56324 batch 1280/3281 lr 0.001 accuracy 93.84375 wps 11949.22 step time 0.85s\n","# Epoch 11  global step 34120 loss 5.28173 batch 1300/3281 lr 0.001 accuracy 94.69531 wps 12738.23 step time 0.52s\n","# Epoch 11  global step 34140 loss 5.37925 batch 1320/3281 lr 0.001 accuracy 94.28516 wps 12469.70 step time 0.52s\n","# Epoch 11  global step 34160 loss 5.47527 batch 1340/3281 lr 0.001 accuracy 94.07813 wps 12355.25 step time 0.56s\n","# Epoch 11  global step 34180 loss 5.47292 batch 1360/3281 lr 0.001 accuracy 94.01562 wps 12471.64 step time 0.65s\n","# Epoch 11  global step 34200 loss 5.28436 batch 1380/3281 lr 0.001 accuracy 94.53906 wps 12398.21 step time 0.53s\n","# Epoch 11  global step 34220 loss 5.41187 batch 1400/3281 lr 0.001 accuracy 94.44531 wps 12372.87 step time 0.54s\n","# Epoch 11  global step 34240 loss 5.36274 batch 1420/3281 lr 0.001 accuracy 94.07813 wps 12355.07 step time 0.59s\n","# Epoch 11  global step 34260 loss 5.31833 batch 1440/3281 lr 0.001 accuracy 94.51953 wps 12908.20 step time 0.50s\n","# Epoch 11  global step 34280 loss 5.42030 batch 1460/3281 lr 0.001 accuracy 94.25000 wps 12399.61 step time 0.52s\n","# Epoch 11  global step 34300 loss 5.33300 batch 1480/3281 lr 0.001 accuracy 94.59375 wps 12877.80 step time 0.49s\n","# Epoch 11  global step 34320 loss 5.39271 batch 1500/3281 lr 0.001 accuracy 94.48437 wps 12343.20 step time 0.60s\n","# Epoch 11  global step 34340 loss 5.47037 batch 1520/3281 lr 0.001 accuracy 94.05078 wps 12018.92 step time 0.76s\n","# Epoch 11  global step 34360 loss 5.40675 batch 1540/3281 lr 0.001 accuracy 94.31250 wps 13092.16 step time 0.52s\n","# Epoch 11  global step 34380 loss 5.54126 batch 1560/3281 lr 0.001 accuracy 93.90234 wps 11812.49 step time 0.72s\n","# Epoch 11  global step 34400 loss 5.31794 batch 1580/3281 lr 0.001 accuracy 94.48828 wps 12977.02 step time 0.57s\n","# Epoch 11  global step 34420 loss 5.43343 batch 1600/3281 lr 0.001 accuracy 94.17187 wps 12766.77 step time 0.47s\n","# Epoch 11  global step 34440 loss 5.50469 batch 1620/3281 lr 0.001 accuracy 94.07812 wps 12656.77 step time 0.53s\n","# Epoch 11  global step 34460 loss 5.36983 batch 1640/3281 lr 0.001 accuracy 94.26562 wps 12276.76 step time 0.55s\n","# Epoch 11  global step 34480 loss 5.34637 batch 1660/3281 lr 0.001 accuracy 94.43359 wps 12451.09 step time 0.50s\n","# Epoch 11  global step 34500 loss 5.43451 batch 1680/3281 lr 0.001 accuracy 94.48047 wps 12870.04 step time 0.53s\n","# Epoch 11  global step 34520 loss 5.31313 batch 1700/3281 lr 0.001 accuracy 94.56250 wps 12928.66 step time 0.52s\n","# Epoch 11  global step 34540 loss 5.50690 batch 1720/3281 lr 0.001 accuracy 94.04297 wps 13416.19 step time 0.60s\n","# Epoch 11  global step 34560 loss 5.56482 batch 1740/3281 lr 0.001 accuracy 93.74609 wps 13877.97 step time 0.70s\n","# Epoch 11  global step 34580 loss 5.41333 batch 1760/3281 lr 0.001 accuracy 94.19141 wps 12870.95 step time 0.55s\n","# Epoch 11  global step 34600 loss 5.55044 batch 1780/3281 lr 0.001 accuracy 93.83984 wps 12517.69 step time 0.67s\n","# Epoch 11  global step 34620 loss 5.45472 batch 1800/3281 lr 0.001 accuracy 94.19531 wps 12293.66 step time 0.53s\n","# Epoch 11  global step 34640 loss 5.36381 batch 1820/3281 lr 0.001 accuracy 94.39063 wps 12496.03 step time 0.57s\n","# Epoch 11  global step 34660 loss 5.42240 batch 1840/3281 lr 0.001 accuracy 94.41406 wps 12756.68 step time 0.53s\n","# Epoch 11  global step 34680 loss 5.35811 batch 1860/3281 lr 0.001 accuracy 94.31250 wps 12655.94 step time 0.56s\n","# Epoch 11  global step 34700 loss 5.44958 batch 1880/3281 lr 0.001 accuracy 94.03906 wps 12879.83 step time 0.53s\n","# Epoch 11  global step 34720 loss 5.36970 batch 1900/3281 lr 0.001 accuracy 94.53125 wps 12754.94 step time 0.44s\n","# Epoch 11  global step 34740 loss 5.42484 batch 1920/3281 lr 0.001 accuracy 94.23828 wps 12903.43 step time 0.48s\n","# Epoch 11  global step 34760 loss 5.29557 batch 1940/3281 lr 0.001 accuracy 94.53906 wps 12948.09 step time 0.48s\n","# Epoch 11  global step 34780 loss 5.39685 batch 1960/3281 lr 0.001 accuracy 94.15234 wps 12248.89 step time 0.54s\n","# Epoch 11  global step 34800 loss 5.35788 batch 1980/3281 lr 0.001 accuracy 94.37109 wps 12527.77 step time 0.48s\n","# Epoch 11  global step 34820 loss 5.28720 batch 2000/3281 lr 0.001 accuracy 94.62891 wps 12753.50 step time 0.48s\n","# Epoch 11  global step 34840 loss 5.42494 batch 2020/3281 lr 0.001 accuracy 94.35156 wps 12398.16 step time 0.59s\n","# Epoch 11  global step 34860 loss 5.32689 batch 2040/3281 lr 0.001 accuracy 94.48047 wps 12548.42 step time 0.47s\n","# Epoch 11  global step 34880 loss 5.40573 batch 2060/3281 lr 0.001 accuracy 94.14844 wps 12366.85 step time 0.56s\n","# Epoch 11  global step 34900 loss 5.45336 batch 2080/3281 lr 0.001 accuracy 94.16797 wps 13052.87 step time 0.55s\n","# Epoch 11  global step 34920 loss 5.53565 batch 2100/3281 lr 0.001 accuracy 93.72266 wps 12332.03 step time 0.67s\n","# Epoch 11  global step 34940 loss 5.46309 batch 2120/3281 lr 0.001 accuracy 94.04297 wps 12954.33 step time 0.60s\n","# Epoch 11  global step 34960 loss 5.46519 batch 2140/3281 lr 0.001 accuracy 94.12891 wps 11827.10 step time 0.63s\n","# Epoch 11  global step 34980 loss 5.34937 batch 2160/3281 lr 0.001 accuracy 94.35547 wps 12682.43 step time 0.46s\n","# Epoch 11  global step 35000 loss 5.38371 batch 2180/3281 lr 0.001 accuracy 94.23437 wps 11381.03 step time 0.70s\n","# Epoch 11  global step 35020 loss 5.60357 batch 2200/3281 lr 0.001 accuracy 93.85156 wps 12917.61 step time 0.60s\n","# Epoch 11  global step 35040 loss 5.34346 batch 2220/3281 lr 0.001 accuracy 94.40625 wps 12379.39 step time 0.49s\n","# Epoch 11  global step 35060 loss 5.44864 batch 2240/3281 lr 0.001 accuracy 94.21484 wps 12196.40 step time 0.64s\n","# Epoch 11  global step 35080 loss 5.59856 batch 2260/3281 lr 0.001 accuracy 93.82813 wps 12780.17 step time 0.63s\n","# Epoch 11  global step 35100 loss 5.31694 batch 2280/3281 lr 0.001 accuracy 94.44531 wps 12171.10 step time 0.58s\n","# Epoch 11  global step 35120 loss 5.42242 batch 2300/3281 lr 0.001 accuracy 94.29297 wps 11818.56 step time 0.57s\n","# Epoch 11  global step 35140 loss 5.37389 batch 2320/3281 lr 0.001 accuracy 94.40625 wps 12354.58 step time 0.49s\n","# Epoch 11  global step 35160 loss 5.48938 batch 2340/3281 lr 0.001 accuracy 94.05469 wps 12178.21 step time 0.63s\n","# Epoch 11  global step 35180 loss 5.45237 batch 2360/3281 lr 0.001 accuracy 94.17188 wps 11988.18 step time 0.60s\n","# Epoch 11  global step 35200 loss 5.40333 batch 2380/3281 lr 0.001 accuracy 94.25391 wps 11693.69 step time 0.69s\n","# Epoch 11  global step 35220 loss 5.45359 batch 2400/3281 lr 0.001 accuracy 94.24219 wps 12726.88 step time 0.52s\n","# Epoch 11  global step 35240 loss 5.54658 batch 2420/3281 lr 0.001 accuracy 93.55469 wps 11812.19 step time 0.73s\n","# Epoch 11  global step 35260 loss 5.51611 batch 2440/3281 lr 0.001 accuracy 93.82422 wps 13222.32 step time 0.54s\n","# Epoch 11  global step 35280 loss 5.40183 batch 2460/3281 lr 0.001 accuracy 94.22656 wps 11611.56 step time 0.60s\n","# Epoch 11  global step 35300 loss 5.55843 batch 2480/3281 lr 0.001 accuracy 93.80078 wps 11661.12 step time 0.84s\n","# Epoch 11  global step 35320 loss 5.54213 batch 2500/3281 lr 0.001 accuracy 93.91016 wps 12328.96 step time 0.64s\n","# Epoch 11  global step 35340 loss 5.32084 batch 2520/3281 lr 0.001 accuracy 94.37109 wps 12816.48 step time 0.50s\n","# Epoch 11  global step 35360 loss 5.23202 batch 2540/3281 lr 0.001 accuracy 94.75391 wps 12584.92 step time 0.44s\n","# Epoch 11  global step 35380 loss 5.54352 batch 2560/3281 lr 0.001 accuracy 93.93750 wps 13266.44 step time 0.60s\n","# Epoch 11  global step 35400 loss 5.41314 batch 2580/3281 lr 0.001 accuracy 94.26953 wps 12636.44 step time 0.50s\n","# Epoch 11  global step 35420 loss 5.36292 batch 2600/3281 lr 0.001 accuracy 94.31250 wps 12543.79 step time 0.45s\n","# Epoch 11  global step 35440 loss 5.37830 batch 2620/3281 lr 0.001 accuracy 94.16797 wps 11948.30 step time 0.68s\n","# Epoch 11  global step 35460 loss 5.63088 batch 2640/3281 lr 0.001 accuracy 93.49609 wps 12693.22 step time 0.74s\n","# Epoch 11  global step 35480 loss 5.26381 batch 2660/3281 lr 0.001 accuracy 94.71094 wps 12906.24 step time 0.50s\n","# Epoch 11  global step 35500 loss 5.47460 batch 2680/3281 lr 0.001 accuracy 94.18750 wps 12146.49 step time 0.75s\n","# Epoch 11  global step 35520 loss 5.39797 batch 2700/3281 lr 0.001 accuracy 94.21484 wps 12675.11 step time 0.57s\n","# Epoch 11  global step 35540 loss 5.42420 batch 2720/3281 lr 0.001 accuracy 94.12500 wps 12604.81 step time 0.56s\n","# Epoch 11  global step 35560 loss 5.51119 batch 2740/3281 lr 0.001 accuracy 94.06250 wps 11340.03 step time 0.75s\n","# Epoch 11  global step 35580 loss 5.47819 batch 2760/3281 lr 0.001 accuracy 94.26953 wps 12842.65 step time 0.54s\n","# Epoch 11  global step 35600 loss 5.38858 batch 2780/3281 lr 0.001 accuracy 94.34766 wps 12096.77 step time 0.47s\n","# Epoch 11  global step 35620 loss 5.43888 batch 2800/3281 lr 0.001 accuracy 94.19141 wps 12042.30 step time 0.60s\n","# Epoch 11  global step 35640 loss 5.48105 batch 2820/3281 lr 0.001 accuracy 94.00000 wps 11980.61 step time 0.68s\n","# Epoch 11  global step 35660 loss 5.38637 batch 2840/3281 lr 0.001 accuracy 94.30078 wps 12567.90 step time 0.59s\n","# Epoch 11  global step 35680 loss 5.49283 batch 2860/3281 lr 0.001 accuracy 93.96875 wps 12318.40 step time 0.55s\n","# Epoch 11  global step 35700 loss 5.28198 batch 2880/3281 lr 0.001 accuracy 94.65234 wps 12262.60 step time 0.50s\n","# Epoch 11  global step 35720 loss 5.44294 batch 2900/3281 lr 0.001 accuracy 94.27344 wps 11981.45 step time 0.68s\n","# Epoch 11  global step 35740 loss 5.50100 batch 2920/3281 lr 0.001 accuracy 93.89062 wps 11628.78 step time 0.79s\n","# Epoch 11  global step 35760 loss 5.50966 batch 2940/3281 lr 0.001 accuracy 94.05859 wps 12545.01 step time 0.55s\n","# Epoch 11  global step 35780 loss 5.42260 batch 2960/3281 lr 0.001 accuracy 94.26563 wps 12754.75 step time 0.52s\n","# Epoch 11  global step 35800 loss 5.42790 batch 2980/3281 lr 0.001 accuracy 94.42578 wps 12855.50 step time 0.54s\n","# Epoch 11  global step 35820 loss 5.31315 batch 3000/3281 lr 0.001 accuracy 94.48438 wps 12599.40 step time 0.53s\n","# Epoch 11  global step 35840 loss 5.48391 batch 3020/3281 lr 0.001 accuracy 93.99609 wps 13568.73 step time 0.58s\n","# Epoch 11  global step 35860 loss 5.43707 batch 3040/3281 lr 0.001 accuracy 94.12891 wps 13315.02 step time 0.57s\n","# Epoch 11  global step 35880 loss 5.48711 batch 3060/3281 lr 0.001 accuracy 94.07031 wps 11967.61 step time 0.63s\n","# Epoch 11  global step 35900 loss 5.67038 batch 3080/3281 lr 0.001 accuracy 93.36719 wps 12060.83 step time 0.80s\n","# Epoch 11  global step 35920 loss 5.35870 batch 3100/3281 lr 0.001 accuracy 94.48438 wps 12809.94 step time 0.48s\n","# Epoch 11  global step 35940 loss 5.57367 batch 3120/3281 lr 0.001 accuracy 93.89453 wps 12596.94 step time 0.70s\n","# Epoch 11  global step 35960 loss 5.40256 batch 3140/3281 lr 0.001 accuracy 94.27734 wps 12411.01 step time 0.49s\n","# Epoch 11  global step 35980 loss 5.47355 batch 3160/3281 lr 0.001 accuracy 94.25000 wps 12685.34 step time 0.52s\n","# Epoch 11  global step 36000 loss 5.42971 batch 3180/3281 lr 0.001 accuracy 94.15234 wps 13165.74 step time 0.55s\n","# global step 36000, eval model at Sat May 23 12:20:26 2020\n","2020-05-23 12:20:29.383700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 12:20:29.384204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 12:20:29.384325: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 12:20:29.384354: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 12:20:29.384379: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 12:20:29.384404: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 12:20:29.384435: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 12:20:29.384478: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 12:20:29.384507: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 12:20:29.384620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 12:20:29.384916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 12:20:29.385165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 12:20:29.385552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 12:20:29.385574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 12:20:29.385586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 12:20:29.385768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 12:20:29.386051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 12:20:29.386282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6517474885856072\n","# location_distance_from_business_district - 0.525991292959425\n","# location_easy_to_find - 0.7019735235022311\n","# service_wait_time - 0.6649907758910099\n","# service_waiters_attitude - 0.7949142064085739\n","# service_parking_convenience - 0.7446560963625279\n","# service_serving_speed - 0.7605025029902995\n","# price_level - 0.7774789439117841\n","# price_cost_effective - 0.7166407688930811\n","# price_discount - 0.6758786561913215\n","# environment_decoration - 0.7202490843592056\n","# environment_noise - 0.7586553976015725\n","# environment_space - 0.7634298831773965\n","# environment_cleaness - 0.7537651650528436\n","# dish_portion - 0.7197630730355944\n","# dish_taste - 0.7335166435349083\n","# dish_look - 0.5766605005453854\n","# dish_recommendation - 0.7343486658602494\n","# others_overall_experience - 0.5910928862252282\n","# others_willing_to_consume_again - 0.712648392588223\n","# Eval loss 4.42413, f1 0.70395\n","# current result -0.7039451973838233, previous best result -0.7038692175646791\n","# Epoch 11  global step 36020 loss 5.38390 batch 3200/3281 lr 0.001 accuracy 94.20313 wps 12252.77 step time 0.53s\n","# Epoch 11  global step 36040 loss 5.41001 batch 3220/3281 lr 0.001 accuracy 94.11719 wps 12603.92 step time 0.54s\n","# Epoch 11  global step 36060 loss 5.49559 batch 3240/3281 lr 0.001 accuracy 94.00391 wps 12171.14 step time 0.63s\n","# Epoch 11  global step 36080 loss 5.47025 batch 3260/3281 lr 0.001 accuracy 94.23438 wps 11697.41 step time 0.62s\n","# Epoch 11  global step 36100 loss 5.49688 batch 3280/3281 lr 0.001 accuracy 94.10547 wps 12032.29 step time 0.68s\n","# Finsh epoch 11, global step 36102\n","# Epoch 12  global step 36120 loss 4.71366 batch 18/3281 lr 0.001 accuracy 85.30469 wps 13652.17 step time 0.47s\n","# Epoch 12  global step 36140 loss 5.24568 batch 38/3281 lr 0.001 accuracy 94.69922 wps 14160.45 step time 0.45s\n","# Epoch 12  global step 36160 loss 5.46046 batch 58/3281 lr 0.001 accuracy 93.85547 wps 13849.52 step time 0.63s\n","# Epoch 12  global step 36180 loss 5.34391 batch 78/3281 lr 0.001 accuracy 94.30469 wps 14373.52 step time 0.47s\n","# Epoch 12  global step 36200 loss 5.22892 batch 98/3281 lr 0.001 accuracy 94.72656 wps 13929.34 step time 0.43s\n","# Epoch 12  global step 36220 loss 5.33408 batch 118/3281 lr 0.001 accuracy 94.38672 wps 14343.76 step time 0.47s\n","# Epoch 12  global step 36240 loss 5.40676 batch 138/3281 lr 0.001 accuracy 94.48437 wps 13675.77 step time 0.59s\n","# Epoch 12  global step 36260 loss 5.27243 batch 158/3281 lr 0.001 accuracy 94.71875 wps 14510.24 step time 0.49s\n","# Epoch 12  global step 36280 loss 5.31681 batch 178/3281 lr 0.001 accuracy 94.63281 wps 14217.00 step time 0.45s\n","# Epoch 12  global step 36300 loss 5.28438 batch 198/3281 lr 0.001 accuracy 94.57422 wps 14605.37 step time 0.50s\n","# Epoch 12  global step 36320 loss 5.27427 batch 218/3281 lr 0.001 accuracy 94.42188 wps 14365.58 step time 0.47s\n","# Epoch 12  global step 36340 loss 5.24605 batch 238/3281 lr 0.001 accuracy 94.75781 wps 13944.80 step time 0.49s\n","# Epoch 12  global step 36360 loss 5.34865 batch 258/3281 lr 0.001 accuracy 94.23437 wps 14636.77 step time 0.50s\n","# Epoch 12  global step 36380 loss 5.23366 batch 278/3281 lr 0.001 accuracy 94.75391 wps 14044.99 step time 0.44s\n","# Epoch 12  global step 36400 loss 5.39818 batch 298/3281 lr 0.001 accuracy 94.11719 wps 14350.39 step time 0.57s\n","# Epoch 12  global step 36420 loss 5.28502 batch 318/3281 lr 0.001 accuracy 94.53906 wps 13684.04 step time 0.60s\n","# Epoch 12  global step 36440 loss 5.29966 batch 338/3281 lr 0.001 accuracy 94.35937 wps 14261.09 step time 0.58s\n","# Epoch 12  global step 36460 loss 5.37229 batch 358/3281 lr 0.001 accuracy 94.35938 wps 14594.77 step time 0.50s\n","# Epoch 12  global step 36480 loss 5.22579 batch 378/3281 lr 0.001 accuracy 94.66797 wps 14250.73 step time 0.46s\n","# Epoch 12  global step 36500 loss 5.37230 batch 398/3281 lr 0.001 accuracy 94.21484 wps 14770.71 step time 0.53s\n","# Epoch 12  global step 36520 loss 5.47724 batch 418/3281 lr 0.001 accuracy 93.93750 wps 14667.29 step time 0.62s\n","# Epoch 12  global step 36540 loss 5.32067 batch 438/3281 lr 0.001 accuracy 94.52344 wps 13673.51 step time 0.53s\n","# Epoch 12  global step 36560 loss 5.25339 batch 458/3281 lr 0.001 accuracy 94.48047 wps 14504.77 step time 0.50s\n","# Epoch 12  global step 36580 loss 5.24931 batch 478/3281 lr 0.001 accuracy 94.62891 wps 14331.46 step time 0.47s\n","# Epoch 12  global step 36600 loss 5.23127 batch 498/3281 lr 0.001 accuracy 94.75391 wps 14402.62 step time 0.46s\n","# Epoch 12  global step 36620 loss 5.41451 batch 518/3281 lr 0.001 accuracy 94.18359 wps 14289.36 step time 0.64s\n","# Epoch 12  global step 36640 loss 5.31389 batch 538/3281 lr 0.001 accuracy 94.49219 wps 13975.70 step time 0.44s\n","# Epoch 12  global step 36660 loss 5.46556 batch 558/3281 lr 0.001 accuracy 93.96484 wps 14287.22 step time 0.67s\n","# Epoch 12  global step 36680 loss 5.38884 batch 578/3281 lr 0.001 accuracy 94.34766 wps 14796.35 step time 0.51s\n","# Epoch 12  global step 36700 loss 5.37066 batch 598/3281 lr 0.001 accuracy 94.33984 wps 13707.68 step time 0.60s\n","# Epoch 12  global step 36720 loss 5.26519 batch 618/3281 lr 0.001 accuracy 94.75391 wps 14534.68 step time 0.50s\n","# Epoch 12  global step 36740 loss 5.50200 batch 638/3281 lr 0.001 accuracy 93.95703 wps 14445.30 step time 0.58s\n","# Epoch 12  global step 36760 loss 5.43237 batch 658/3281 lr 0.001 accuracy 94.19141 wps 13879.82 step time 0.71s\n","# Epoch 12  global step 36780 loss 5.35470 batch 678/3281 lr 0.001 accuracy 94.32812 wps 14544.45 step time 0.49s\n","# Epoch 12  global step 36800 loss 5.45279 batch 698/3281 lr 0.001 accuracy 94.15625 wps 14610.97 step time 0.51s\n","# Epoch 12  global step 36820 loss 5.39142 batch 718/3281 lr 0.001 accuracy 94.22656 wps 13912.35 step time 0.63s\n","# Epoch 12  global step 36840 loss 5.49606 batch 738/3281 lr 0.001 accuracy 93.94531 wps 14449.52 step time 0.48s\n","# Epoch 12  global step 36860 loss 5.43236 batch 758/3281 lr 0.001 accuracy 94.12500 wps 13904.95 step time 0.63s\n","# Epoch 12  global step 36880 loss 5.30839 batch 778/3281 lr 0.001 accuracy 94.47656 wps 14359.13 step time 0.46s\n","# Epoch 12  global step 36900 loss 5.66501 batch 798/3281 lr 0.001 accuracy 93.74219 wps 14804.59 step time 0.68s\n","# Epoch 12  global step 36920 loss 5.32801 batch 818/3281 lr 0.001 accuracy 94.43359 wps 14410.68 step time 0.50s\n","# Epoch 12  global step 36940 loss 5.29725 batch 838/3281 lr 0.001 accuracy 94.50781 wps 14208.28 step time 0.47s\n","# Epoch 12  global step 36960 loss 5.45400 batch 858/3281 lr 0.001 accuracy 94.23828 wps 14306.62 step time 0.46s\n","# Epoch 12  global step 36980 loss 5.29568 batch 878/3281 lr 0.001 accuracy 94.45312 wps 14577.27 step time 0.50s\n","# Epoch 12  global step 37000 loss 5.34390 batch 898/3281 lr 0.001 accuracy 94.51172 wps 14089.63 step time 0.45s\n","# Epoch 12  global step 37020 loss 5.45175 batch 918/3281 lr 0.001 accuracy 94.14453 wps 14088.17 step time 0.57s\n","# Epoch 12  global step 37040 loss 5.26108 batch 938/3281 lr 0.001 accuracy 94.66016 wps 14192.36 step time 0.46s\n","# Epoch 12  global step 37060 loss 5.26256 batch 958/3281 lr 0.001 accuracy 94.62891 wps 14402.28 step time 0.48s\n","# Epoch 12  global step 37080 loss 5.44487 batch 978/3281 lr 0.001 accuracy 94.30078 wps 14575.30 step time 0.52s\n","# Epoch 12  global step 37100 loss 5.36615 batch 998/3281 lr 0.001 accuracy 94.21875 wps 14408.98 step time 0.47s\n","# Epoch 12  global step 37120 loss 5.37328 batch 1018/3281 lr 0.001 accuracy 94.23438 wps 14696.44 step time 0.51s\n","# Epoch 12  global step 37140 loss 5.17862 batch 1038/3281 lr 0.001 accuracy 95.08594 wps 14091.00 step time 0.44s\n","# Epoch 12  global step 37160 loss 5.42383 batch 1058/3281 lr 0.001 accuracy 94.35547 wps 14368.06 step time 0.49s\n","# Epoch 12  global step 37180 loss 5.38368 batch 1078/3281 lr 0.001 accuracy 94.30078 wps 14303.48 step time 0.57s\n","# Epoch 12  global step 37200 loss 5.38562 batch 1098/3281 lr 0.001 accuracy 94.19141 wps 14166.39 step time 0.58s\n","# Epoch 12  global step 37220 loss 5.33075 batch 1118/3281 lr 0.001 accuracy 94.56641 wps 14164.23 step time 0.45s\n","# Epoch 12  global step 37240 loss 5.29163 batch 1138/3281 lr 0.001 accuracy 94.53125 wps 14209.91 step time 0.46s\n","# Epoch 12  global step 37260 loss 5.41632 batch 1158/3281 lr 0.001 accuracy 94.16797 wps 14249.77 step time 0.56s\n","# Epoch 12  global step 37280 loss 5.33670 batch 1178/3281 lr 0.001 accuracy 94.46094 wps 14425.02 step time 0.48s\n","# Epoch 12  global step 37300 loss 5.33746 batch 1198/3281 lr 0.001 accuracy 94.37109 wps 14780.28 step time 0.52s\n","# Epoch 12  global step 37320 loss 5.39727 batch 1218/3281 lr 0.001 accuracy 94.32031 wps 13964.20 step time 0.55s\n","# Epoch 12  global step 37340 loss 5.32326 batch 1238/3281 lr 0.001 accuracy 94.46875 wps 14500.96 step time 0.47s\n","# Epoch 12  global step 37360 loss 5.38195 batch 1258/3281 lr 0.001 accuracy 94.42578 wps 14426.97 step time 0.47s\n","# Epoch 12  global step 37380 loss 5.41360 batch 1278/3281 lr 0.001 accuracy 94.16797 wps 14331.75 step time 0.46s\n","# Epoch 12  global step 37400 loss 5.27948 batch 1298/3281 lr 0.001 accuracy 94.74219 wps 14052.65 step time 0.46s\n","# Epoch 12  global step 37420 loss 5.47344 batch 1318/3281 lr 0.001 accuracy 94.11328 wps 14688.47 step time 0.50s\n","# Epoch 12  global step 37440 loss 5.32851 batch 1338/3281 lr 0.001 accuracy 94.56250 wps 14531.51 step time 0.49s\n","# Epoch 12  global step 37460 loss 5.32576 batch 1358/3281 lr 0.001 accuracy 94.47656 wps 14686.09 step time 0.53s\n","# Epoch 12  global step 37480 loss 5.36305 batch 1378/3281 lr 0.001 accuracy 94.47266 wps 14336.99 step time 0.48s\n","# Epoch 12  global step 37500 loss 5.49425 batch 1398/3281 lr 0.001 accuracy 94.25391 wps 14646.79 step time 0.51s\n","# Epoch 12  global step 37520 loss 5.50924 batch 1418/3281 lr 0.001 accuracy 93.99609 wps 14423.42 step time 0.59s\n","# Epoch 12  global step 37540 loss 5.26207 batch 1438/3281 lr 0.001 accuracy 94.65234 wps 14071.48 step time 0.43s\n","# Epoch 12  global step 37560 loss 5.35812 batch 1458/3281 lr 0.001 accuracy 94.39453 wps 14318.60 step time 0.46s\n","# Epoch 12  global step 37580 loss 5.29191 batch 1478/3281 lr 0.001 accuracy 94.61719 wps 14264.69 step time 0.45s\n","# Epoch 12  global step 37600 loss 5.41280 batch 1498/3281 lr 0.001 accuracy 94.17969 wps 14522.88 step time 0.48s\n","# Epoch 12  global step 37620 loss 5.45253 batch 1518/3281 lr 0.001 accuracy 94.13672 wps 13982.53 step time 0.54s\n","# Epoch 12  global step 37640 loss 5.39160 batch 1538/3281 lr 0.001 accuracy 94.42187 wps 14574.48 step time 0.52s\n","# Epoch 12  global step 37660 loss 5.39159 batch 1558/3281 lr 0.001 accuracy 94.24219 wps 13803.65 step time 0.56s\n","# Epoch 12  global step 37680 loss 5.18110 batch 1578/3281 lr 0.001 accuracy 94.86328 wps 13776.78 step time 0.41s\n","# Epoch 12  global step 37700 loss 5.48550 batch 1598/3281 lr 0.001 accuracy 94.04688 wps 13835.10 step time 0.55s\n","# Epoch 12  global step 37720 loss 5.33182 batch 1618/3281 lr 0.001 accuracy 94.47266 wps 14250.78 step time 0.45s\n","# Epoch 12  global step 37740 loss 5.46334 batch 1638/3281 lr 0.001 accuracy 93.97266 wps 14856.32 step time 0.54s\n","# Epoch 12  global step 37760 loss 5.28650 batch 1658/3281 lr 0.001 accuracy 94.55859 wps 13847.13 step time 0.51s\n","# Epoch 12  global step 37780 loss 5.43618 batch 1678/3281 lr 0.001 accuracy 94.26562 wps 14559.64 step time 0.50s\n","# Epoch 12  global step 37800 loss 5.29813 batch 1698/3281 lr 0.001 accuracy 94.72266 wps 13749.48 step time 0.51s\n","# Epoch 12  global step 37820 loss 5.34360 batch 1718/3281 lr 0.001 accuracy 94.66797 wps 14409.25 step time 0.46s\n","# Epoch 12  global step 37840 loss 5.37780 batch 1738/3281 lr 0.001 accuracy 94.36719 wps 14713.60 step time 0.51s\n","# Epoch 12  global step 37860 loss 5.29689 batch 1758/3281 lr 0.001 accuracy 94.44141 wps 14785.83 step time 0.53s\n","# Epoch 12  global step 37880 loss 5.32938 batch 1778/3281 lr 0.001 accuracy 94.45703 wps 13581.89 step time 0.60s\n","# Epoch 12  global step 37900 loss 5.26459 batch 1798/3281 lr 0.001 accuracy 94.74219 wps 14182.73 step time 0.44s\n","# Epoch 12  global step 37920 loss 5.34759 batch 1818/3281 lr 0.001 accuracy 94.48828 wps 14256.31 step time 0.45s\n","# Epoch 12  global step 37940 loss 5.34982 batch 1838/3281 lr 0.001 accuracy 94.54297 wps 14110.13 step time 0.45s\n","# Epoch 12  global step 37960 loss 5.44729 batch 1858/3281 lr 0.001 accuracy 94.16406 wps 14442.89 step time 0.47s\n","# Epoch 12  global step 37980 loss 5.50937 batch 1878/3281 lr 0.001 accuracy 93.85547 wps 14096.68 step time 0.58s\n","# Epoch 12  global step 38000 loss 5.36193 batch 1898/3281 lr 0.001 accuracy 94.31250 wps 13882.75 step time 0.53s\n","# global step 38000, eval model at Sat May 23 12:39:57 2020\n","2020-05-23 12:40:00.069952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 12:40:00.070293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 12:40:00.070403: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 12:40:00.070435: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 12:40:00.070494: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 12:40:00.070523: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 12:40:00.070546: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 12:40:00.070566: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 12:40:00.070587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 12:40:00.070680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 12:40:00.070974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 12:40:00.071166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 12:40:00.071213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 12:40:00.071227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 12:40:00.071237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 12:40:00.071337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 12:40:00.071619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 12:40:00.071831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.654655456213659\n","# location_distance_from_business_district - 0.5231856529130794\n","# location_easy_to_find - 0.7041286382585914\n","# service_wait_time - 0.6675315593759081\n","# service_waiters_attitude - 0.7939978014842195\n","# service_parking_convenience - 0.7440326729532716\n","# service_serving_speed - 0.7615604386322403\n","# price_level - 0.7781765900002969\n","# price_cost_effective - 0.7161571701868565\n","# price_discount - 0.6758643404602118\n","# environment_decoration - 0.721667548786037\n","# environment_noise - 0.7584954453327506\n","# environment_space - 0.7614297525518424\n","# environment_cleaness - 0.7543142502719624\n","# dish_portion - 0.7204034531798611\n","# dish_taste - 0.7338674203267851\n","# dish_look - 0.5793718628762041\n","# dish_recommendation - 0.7352871526971122\n","# others_overall_experience - 0.5909545301037684\n","# others_willing_to_consume_again - 0.7088650313700022\n","# Eval loss 4.43581, f1 0.70420\n","# current result -0.704197338398733, previous best result -0.7039451973838233\n","# Epoch 12  global step 38020 loss 5.26274 batch 1918/3281 lr 0.001 accuracy 94.78125 wps 13724.54 step time 0.44s\n","# Epoch 12  global step 38040 loss 5.41727 batch 1938/3281 lr 0.001 accuracy 94.22266 wps 13810.05 step time 0.67s\n","# Epoch 12  global step 38060 loss 5.33573 batch 1958/3281 lr 0.001 accuracy 94.50781 wps 13057.85 step time 0.49s\n","# Epoch 12  global step 38080 loss 5.39865 batch 1978/3281 lr 0.001 accuracy 94.41797 wps 12603.71 step time 0.50s\n","# Epoch 12  global step 38100 loss 5.41011 batch 1998/3281 lr 0.001 accuracy 94.55859 wps 12483.74 step time 0.61s\n","# Epoch 12  global step 38120 loss 5.33610 batch 2018/3281 lr 0.001 accuracy 94.57813 wps 12547.16 step time 0.55s\n","# Epoch 12  global step 38140 loss 5.31548 batch 2038/3281 lr 0.001 accuracy 94.50781 wps 12891.11 step time 0.49s\n","# Epoch 12  global step 38160 loss 5.62546 batch 2058/3281 lr 0.001 accuracy 93.45703 wps 12163.57 step time 0.81s\n","# Epoch 12  global step 38180 loss 5.40271 batch 2078/3281 lr 0.001 accuracy 94.29297 wps 12558.45 step time 0.58s\n","# Epoch 12  global step 38200 loss 5.51861 batch 2098/3281 lr 0.001 accuracy 94.04297 wps 12212.47 step time 0.64s\n","# Epoch 12  global step 38220 loss 5.40048 batch 2118/3281 lr 0.001 accuracy 94.41797 wps 11432.47 step time 0.71s\n","# Epoch 12  global step 38240 loss 5.41848 batch 2138/3281 lr 0.001 accuracy 94.24609 wps 12623.32 step time 0.62s\n","# Epoch 12  global step 38260 loss 5.38065 batch 2158/3281 lr 0.001 accuracy 94.13672 wps 12916.72 step time 0.55s\n","# Epoch 12  global step 38280 loss 5.39103 batch 2178/3281 lr 0.001 accuracy 94.39844 wps 12859.46 step time 0.50s\n","# Epoch 12  global step 38300 loss 5.38536 batch 2198/3281 lr 0.001 accuracy 94.42969 wps 12599.88 step time 0.51s\n","# Epoch 12  global step 38320 loss 5.41863 batch 2218/3281 lr 0.001 accuracy 94.25781 wps 11884.67 step time 0.70s\n","# Epoch 12  global step 38340 loss 5.41841 batch 2238/3281 lr 0.001 accuracy 94.09766 wps 12906.92 step time 0.50s\n","# Epoch 12  global step 38360 loss 5.45628 batch 2258/3281 lr 0.001 accuracy 94.21094 wps 12885.08 step time 0.60s\n","# Epoch 12  global step 38380 loss 5.31895 batch 2278/3281 lr 0.001 accuracy 94.61328 wps 12780.14 step time 0.50s\n","# Epoch 12  global step 38400 loss 5.32199 batch 2298/3281 lr 0.001 accuracy 94.52344 wps 12564.67 step time 0.52s\n","# Epoch 12  global step 38420 loss 5.44605 batch 2318/3281 lr 0.001 accuracy 94.04687 wps 12760.42 step time 0.57s\n","# Epoch 12  global step 38440 loss 5.49473 batch 2338/3281 lr 0.001 accuracy 93.93750 wps 11848.48 step time 0.74s\n","# Epoch 12  global step 38460 loss 5.44463 batch 2358/3281 lr 0.001 accuracy 94.32422 wps 12921.69 step time 0.51s\n","# Epoch 12  global step 38480 loss 5.49063 batch 2378/3281 lr 0.001 accuracy 94.15234 wps 13121.31 step time 0.63s\n","# Epoch 12  global step 38500 loss 5.54158 batch 2398/3281 lr 0.001 accuracy 93.76953 wps 13075.86 step time 0.58s\n","# Epoch 12  global step 38520 loss 5.35211 batch 2418/3281 lr 0.001 accuracy 94.26172 wps 12256.96 step time 0.61s\n","# Epoch 12  global step 38540 loss 5.35076 batch 2438/3281 lr 0.001 accuracy 94.36328 wps 11617.45 step time 0.65s\n","# Epoch 12  global step 38560 loss 5.29996 batch 2458/3281 lr 0.001 accuracy 94.57031 wps 12752.01 step time 0.50s\n","# Epoch 12  global step 38580 loss 5.33728 batch 2478/3281 lr 0.001 accuracy 94.49219 wps 12451.19 step time 0.52s\n","# Epoch 12  global step 38600 loss 5.47633 batch 2498/3281 lr 0.001 accuracy 94.00781 wps 12973.90 step time 0.62s\n","# Epoch 12  global step 38620 loss 5.35950 batch 2518/3281 lr 0.001 accuracy 94.40234 wps 11590.48 step time 0.62s\n","# Epoch 12  global step 38640 loss 5.33746 batch 2538/3281 lr 0.001 accuracy 94.44922 wps 12802.43 step time 0.48s\n","# Epoch 12  global step 38660 loss 5.40584 batch 2558/3281 lr 0.001 accuracy 94.39063 wps 11810.83 step time 0.71s\n","# Epoch 12  global step 38680 loss 5.45104 batch 2578/3281 lr 0.001 accuracy 94.26953 wps 12772.01 step time 0.57s\n","# Epoch 12  global step 38700 loss 5.38289 batch 2598/3281 lr 0.001 accuracy 94.36719 wps 12934.64 step time 0.58s\n","# Epoch 12  global step 38720 loss 5.27516 batch 2618/3281 lr 0.001 accuracy 94.71094 wps 12910.94 step time 0.52s\n","# Epoch 12  global step 38740 loss 5.35968 batch 2638/3281 lr 0.001 accuracy 94.44531 wps 12423.09 step time 0.54s\n","# Epoch 12  global step 38760 loss 5.41929 batch 2658/3281 lr 0.001 accuracy 94.26953 wps 11556.26 step time 0.76s\n","# Epoch 12  global step 38780 loss 5.50418 batch 2678/3281 lr 0.001 accuracy 94.01953 wps 11847.54 step time 0.63s\n","# Epoch 12  global step 38800 loss 5.44372 batch 2698/3281 lr 0.001 accuracy 94.15234 wps 12744.05 step time 0.61s\n","# Epoch 12  global step 38820 loss 5.31525 batch 2718/3281 lr 0.001 accuracy 94.45312 wps 12272.94 step time 0.56s\n","# Epoch 12  global step 38840 loss 5.28790 batch 2738/3281 lr 0.001 accuracy 94.81250 wps 12361.85 step time 0.48s\n","# Epoch 12  global step 38860 loss 5.49830 batch 2758/3281 lr 0.001 accuracy 94.05469 wps 12721.76 step time 0.54s\n","# Epoch 12  global step 38880 loss 5.35677 batch 2778/3281 lr 0.001 accuracy 94.30469 wps 12304.90 step time 0.56s\n","# Epoch 12  global step 38900 loss 5.24293 batch 2798/3281 lr 0.001 accuracy 94.60938 wps 12237.61 step time 0.50s\n","# Epoch 12  global step 38920 loss 5.28551 batch 2818/3281 lr 0.001 accuracy 94.58594 wps 12367.47 step time 0.58s\n","# Epoch 12  global step 38940 loss 5.39160 batch 2838/3281 lr 0.001 accuracy 94.28906 wps 11755.22 step time 0.71s\n","# Epoch 12  global step 38960 loss 5.40706 batch 2858/3281 lr 0.001 accuracy 94.22266 wps 12458.39 step time 0.55s\n","# Epoch 12  global step 38980 loss 5.41865 batch 2878/3281 lr 0.001 accuracy 94.09375 wps 12002.92 step time 0.60s\n","# Epoch 12  global step 39000 loss 5.30781 batch 2898/3281 lr 0.001 accuracy 94.58594 wps 12137.27 step time 0.48s\n","# Epoch 12  global step 39020 loss 5.55281 batch 2918/3281 lr 0.001 accuracy 93.95703 wps 12362.20 step time 0.60s\n","# Epoch 12  global step 39040 loss 5.47367 batch 2938/3281 lr 0.001 accuracy 94.07813 wps 12528.06 step time 0.57s\n","# Epoch 12  global step 39060 loss 5.32324 batch 2958/3281 lr 0.001 accuracy 94.46875 wps 12930.57 step time 0.53s\n","# Epoch 12  global step 39080 loss 5.48440 batch 2978/3281 lr 0.001 accuracy 94.18750 wps 12716.34 step time 0.58s\n","# Epoch 12  global step 39100 loss 5.39993 batch 2998/3281 lr 0.001 accuracy 94.02344 wps 12910.05 step time 0.62s\n","# Epoch 12  global step 39120 loss 5.43442 batch 3018/3281 lr 0.001 accuracy 94.07031 wps 12508.41 step time 0.48s\n","# Epoch 12  global step 39140 loss 5.47183 batch 3038/3281 lr 0.001 accuracy 94.22656 wps 12225.28 step time 0.57s\n","# Epoch 12  global step 39160 loss 5.33026 batch 3058/3281 lr 0.001 accuracy 94.38281 wps 12815.10 step time 0.49s\n","# Epoch 12  global step 39180 loss 5.47191 batch 3078/3281 lr 0.001 accuracy 94.03125 wps 12884.50 step time 0.70s\n","# Epoch 12  global step 39200 loss 5.42883 batch 3098/3281 lr 0.001 accuracy 94.32031 wps 12424.12 step time 0.54s\n","# Epoch 12  global step 39220 loss 5.39097 batch 3118/3281 lr 0.001 accuracy 94.27734 wps 12926.86 step time 0.49s\n","# Epoch 12  global step 39240 loss 5.45389 batch 3138/3281 lr 0.001 accuracy 94.28125 wps 12027.90 step time 0.61s\n","# Epoch 12  global step 39260 loss 5.34354 batch 3158/3281 lr 0.001 accuracy 94.37109 wps 12424.37 step time 0.52s\n","# Epoch 12  global step 39280 loss 5.40313 batch 3178/3281 lr 0.001 accuracy 94.15234 wps 12097.90 step time 0.64s\n","# Epoch 12  global step 39300 loss 5.38187 batch 3198/3281 lr 0.001 accuracy 94.49219 wps 12795.33 step time 0.52s\n","# Epoch 12  global step 39320 loss 5.27549 batch 3218/3281 lr 0.001 accuracy 94.67187 wps 12768.88 step time 0.43s\n","# Epoch 12  global step 39340 loss 5.50801 batch 3238/3281 lr 0.001 accuracy 93.85547 wps 12441.73 step time 0.63s\n","# Epoch 12  global step 39360 loss 5.37136 batch 3258/3281 lr 0.001 accuracy 94.39844 wps 11966.65 step time 0.56s\n","# Epoch 12  global step 39380 loss 5.36294 batch 3278/3281 lr 0.001 accuracy 94.46484 wps 12618.91 step time 0.53s\n","# Finsh epoch 12, global step 39384\n","# Epoch 13  global step 39400 loss 4.21142 batch 16/3281 lr 0.001 accuracy 75.71875 wps 13484.57 step time 0.60s\n","# Epoch 13  global step 39420 loss 5.25541 batch 36/3281 lr 0.001 accuracy 94.60156 wps 14481.31 step time 0.49s\n","# Epoch 13  global step 39440 loss 5.35263 batch 56/3281 lr 0.001 accuracy 94.42188 wps 15031.25 step time 0.57s\n","# Epoch 13  global step 39460 loss 5.25932 batch 76/3281 lr 0.001 accuracy 94.51172 wps 14506.58 step time 0.48s\n","# Epoch 13  global step 39480 loss 5.20016 batch 96/3281 lr 0.001 accuracy 94.91406 wps 14200.25 step time 0.44s\n","# Epoch 13  global step 39500 loss 5.21371 batch 116/3281 lr 0.001 accuracy 94.86719 wps 14130.54 step time 0.46s\n","# Epoch 13  global step 39520 loss 5.21279 batch 136/3281 lr 0.001 accuracy 94.74609 wps 14265.45 step time 0.46s\n","# Epoch 13  global step 39540 loss 5.26106 batch 156/3281 lr 0.001 accuracy 94.63672 wps 14207.45 step time 0.46s\n","# Epoch 13  global step 39560 loss 5.33621 batch 176/3281 lr 0.001 accuracy 94.42578 wps 13440.69 step time 0.71s\n","# Epoch 13  global step 39580 loss 5.21001 batch 196/3281 lr 0.001 accuracy 94.79297 wps 13980.13 step time 0.43s\n","# Epoch 13  global step 39600 loss 5.33616 batch 216/3281 lr 0.001 accuracy 94.33594 wps 14627.81 step time 0.60s\n","# Epoch 13  global step 39620 loss 5.49121 batch 236/3281 lr 0.001 accuracy 93.94531 wps 14687.09 step time 0.61s\n","# Epoch 13  global step 39640 loss 5.26942 batch 256/3281 lr 0.001 accuracy 94.67187 wps 14492.61 step time 0.48s\n","# Epoch 13  global step 39660 loss 5.33983 batch 276/3281 lr 0.001 accuracy 94.42188 wps 14107.76 step time 0.53s\n","# Epoch 13  global step 39680 loss 5.29325 batch 296/3281 lr 0.001 accuracy 94.58984 wps 14300.84 step time 0.56s\n","# Epoch 13  global step 39700 loss 5.27118 batch 316/3281 lr 0.001 accuracy 94.66797 wps 14461.08 step time 0.49s\n","# Epoch 13  global step 39720 loss 5.29961 batch 336/3281 lr 0.001 accuracy 94.49609 wps 14692.39 step time 0.51s\n","# Epoch 13  global step 39740 loss 5.31721 batch 356/3281 lr 0.001 accuracy 94.50781 wps 13707.24 step time 0.53s\n","# Epoch 13  global step 39760 loss 5.19515 batch 376/3281 lr 0.001 accuracy 94.87109 wps 14207.61 step time 0.45s\n","# Epoch 13  global step 39780 loss 5.29482 batch 396/3281 lr 0.001 accuracy 94.57031 wps 13729.12 step time 0.42s\n","# Epoch 13  global step 39800 loss 5.40889 batch 416/3281 lr 0.001 accuracy 94.19531 wps 13793.38 step time 0.61s\n","# Epoch 13  global step 39820 loss 5.24665 batch 436/3281 lr 0.001 accuracy 94.73047 wps 14464.78 step time 0.48s\n","# Epoch 13  global step 39840 loss 5.26151 batch 456/3281 lr 0.001 accuracy 94.78516 wps 14206.12 step time 0.45s\n","# Epoch 13  global step 39860 loss 5.22680 batch 476/3281 lr 0.001 accuracy 94.51172 wps 14226.55 step time 0.45s\n","# Epoch 13  global step 39880 loss 5.15827 batch 496/3281 lr 0.001 accuracy 94.92578 wps 13734.41 step time 0.51s\n","# Epoch 13  global step 39900 loss 5.21233 batch 516/3281 lr 0.001 accuracy 94.85937 wps 14344.17 step time 0.49s\n","# Epoch 13  global step 39920 loss 5.42856 batch 536/3281 lr 0.001 accuracy 94.32812 wps 14028.38 step time 0.57s\n","# Epoch 13  global step 39940 loss 5.33158 batch 556/3281 lr 0.001 accuracy 94.45312 wps 14422.50 step time 0.47s\n","# Epoch 13  global step 39960 loss 5.39163 batch 576/3281 lr 0.001 accuracy 94.32031 wps 13965.97 step time 0.54s\n","# Epoch 13  global step 39980 loss 5.28005 batch 596/3281 lr 0.001 accuracy 94.69922 wps 14148.69 step time 0.44s\n","# Epoch 13  global step 40000 loss 5.37889 batch 616/3281 lr 0.001 accuracy 94.23438 wps 14522.05 step time 0.62s\n","# global step 40000, eval model at Sat May 23 13:00:48 2020\n","2020-05-23 13:00:50.689457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:00:50.689885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 13:00:50.690025: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 13:00:50.690057: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 13:00:50.690081: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 13:00:50.690108: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 13:00:50.690135: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 13:00:50.690156: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 13:00:50.690178: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 13:00:50.690269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:00:50.690615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:00:50.690831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 13:00:50.691204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 13:00:50.691227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 13:00:50.691238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 13:00:50.691474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:00:50.691875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:00:50.692213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6516706379522282\n","# location_distance_from_business_district - 0.5382262507306723\n","# location_easy_to_find - 0.7063484960092228\n","# service_wait_time - 0.6695459115575317\n","# service_waiters_attitude - 0.7940396522440999\n","# service_parking_convenience - 0.7465920376833314\n","# service_serving_speed - 0.7605741656321487\n","# price_level - 0.7781642478372891\n","# price_cost_effective - 0.715528895195304\n","# price_discount - 0.6765554829324867\n","# environment_decoration - 0.72353718300751\n","# environment_noise - 0.7591834221467504\n","# environment_space - 0.7614727045606058\n","# environment_cleaness - 0.757290104929782\n","# dish_portion - 0.7200132856304771\n","# dish_taste - 0.7330622047860991\n","# dish_look - 0.5749518363211217\n","# dish_recommendation - 0.7332357485634309\n","# others_overall_experience - 0.5918170124006326\n","# others_willing_to_consume_again - 0.7134217873801195\n","# Eval loss 4.44845, f1 0.70526\n","# current result -0.705261553375042, previous best result -0.704197338398733\n","# Epoch 13  global step 40020 loss 5.23456 batch 636/3281 lr 0.001 accuracy 94.80469 wps 14395.03 step time 0.49s\n","# Epoch 13  global step 40040 loss 5.30846 batch 656/3281 lr 0.001 accuracy 94.55469 wps 14103.25 step time 0.56s\n","# Epoch 13  global step 40060 loss 5.38381 batch 676/3281 lr 0.001 accuracy 94.23438 wps 14962.53 step time 0.52s\n","# Epoch 13  global step 40080 loss 5.17944 batch 696/3281 lr 0.001 accuracy 94.59375 wps 14060.32 step time 0.43s\n","# Epoch 13  global step 40100 loss 5.21968 batch 716/3281 lr 0.001 accuracy 94.79297 wps 14583.02 step time 0.49s\n","# Epoch 13  global step 40120 loss 5.38671 batch 736/3281 lr 0.001 accuracy 94.26172 wps 14462.04 step time 0.47s\n","# Epoch 13  global step 40140 loss 5.24366 batch 756/3281 lr 0.001 accuracy 94.50391 wps 14480.20 step time 0.48s\n","# Epoch 13  global step 40160 loss 5.33369 batch 776/3281 lr 0.001 accuracy 94.38672 wps 14427.46 step time 0.46s\n","# Epoch 13  global step 40180 loss 5.44790 batch 796/3281 lr 0.001 accuracy 94.08984 wps 13825.30 step time 0.62s\n","# Epoch 13  global step 40200 loss 5.37216 batch 816/3281 lr 0.001 accuracy 94.43359 wps 14300.54 step time 0.56s\n","# Epoch 13  global step 40220 loss 5.46302 batch 836/3281 lr 0.001 accuracy 94.15625 wps 14000.81 step time 0.55s\n","# Epoch 13  global step 40240 loss 5.36746 batch 856/3281 lr 0.001 accuracy 94.28125 wps 14847.92 step time 0.52s\n","# Epoch 13  global step 40260 loss 5.33211 batch 876/3281 lr 0.001 accuracy 94.51953 wps 14311.36 step time 0.46s\n","# Epoch 13  global step 40280 loss 5.20393 batch 896/3281 lr 0.001 accuracy 94.79297 wps 13301.82 step time 0.48s\n","# Epoch 13  global step 40300 loss 5.23639 batch 916/3281 lr 0.001 accuracy 94.60937 wps 14388.72 step time 0.45s\n","# Epoch 13  global step 40320 loss 5.25231 batch 936/3281 lr 0.001 accuracy 94.60547 wps 13663.18 step time 0.51s\n","# Epoch 13  global step 40340 loss 5.40032 batch 956/3281 lr 0.001 accuracy 94.29297 wps 14375.01 step time 0.46s\n","# Epoch 13  global step 40360 loss 5.34766 batch 976/3281 lr 0.001 accuracy 94.45703 wps 14643.99 step time 0.49s\n","# Epoch 13  global step 40380 loss 5.34455 batch 996/3281 lr 0.001 accuracy 94.44141 wps 14532.09 step time 0.49s\n","# Epoch 13  global step 40400 loss 5.26627 batch 1016/3281 lr 0.001 accuracy 94.56250 wps 13621.10 step time 0.50s\n","# Epoch 13  global step 40420 loss 5.23291 batch 1036/3281 lr 0.001 accuracy 94.73438 wps 14206.52 step time 0.44s\n","# Epoch 13  global step 40440 loss 5.20251 batch 1056/3281 lr 0.001 accuracy 94.94531 wps 14136.65 step time 0.46s\n","# Epoch 13  global step 40460 loss 5.38871 batch 1076/3281 lr 0.001 accuracy 94.26953 wps 13865.51 step time 0.52s\n","# Epoch 13  global step 40480 loss 5.36574 batch 1096/3281 lr 0.001 accuracy 94.28906 wps 14567.05 step time 0.48s\n","# Epoch 13  global step 40500 loss 5.11760 batch 1116/3281 lr 0.001 accuracy 94.97266 wps 13915.71 step time 0.43s\n","# Epoch 13  global step 40520 loss 5.37981 batch 1136/3281 lr 0.001 accuracy 94.30859 wps 14478.85 step time 0.48s\n","# Epoch 13  global step 40540 loss 5.20198 batch 1156/3281 lr 0.001 accuracy 94.91016 wps 14440.92 step time 0.47s\n","# Epoch 13  global step 40560 loss 5.34825 batch 1176/3281 lr 0.001 accuracy 94.50391 wps 14483.91 step time 0.48s\n","# Epoch 13  global step 40580 loss 5.25458 batch 1196/3281 lr 0.001 accuracy 94.65625 wps 14416.38 step time 0.48s\n","# Epoch 13  global step 40600 loss 5.29184 batch 1216/3281 lr 0.001 accuracy 94.61719 wps 14138.19 step time 0.55s\n","# Epoch 13  global step 40620 loss 5.37229 batch 1236/3281 lr 0.001 accuracy 94.38281 wps 14623.36 step time 0.58s\n","# Epoch 13  global step 40640 loss 5.28780 batch 1256/3281 lr 0.001 accuracy 94.44141 wps 13796.96 step time 0.63s\n","# Epoch 13  global step 40660 loss 5.28162 batch 1276/3281 lr 0.001 accuracy 94.52344 wps 13612.67 step time 0.52s\n","# Epoch 13  global step 40680 loss 5.23143 batch 1296/3281 lr 0.001 accuracy 94.67578 wps 14076.52 step time 0.44s\n","# Epoch 13  global step 40700 loss 5.41547 batch 1316/3281 lr 0.001 accuracy 94.16016 wps 14065.48 step time 0.55s\n","# Epoch 13  global step 40720 loss 5.39003 batch 1336/3281 lr 0.001 accuracy 94.21484 wps 14378.85 step time 0.60s\n","# Epoch 13  global step 40740 loss 5.34114 batch 1356/3281 lr 0.001 accuracy 94.39844 wps 14319.83 step time 0.45s\n","# Epoch 13  global step 40760 loss 5.47780 batch 1376/3281 lr 0.001 accuracy 94.06641 wps 14706.77 step time 0.51s\n","# Epoch 13  global step 40780 loss 5.27050 batch 1396/3281 lr 0.001 accuracy 94.68359 wps 14013.10 step time 0.43s\n","# Epoch 13  global step 40800 loss 5.28996 batch 1416/3281 lr 0.001 accuracy 94.64453 wps 14493.87 step time 0.48s\n","# Epoch 13  global step 40820 loss 5.32152 batch 1436/3281 lr 0.001 accuracy 94.32422 wps 14932.33 step time 0.53s\n","# Epoch 13  global step 40840 loss 5.29293 batch 1456/3281 lr 0.001 accuracy 94.61719 wps 13911.32 step time 0.43s\n","# Epoch 13  global step 40860 loss 5.15282 batch 1476/3281 lr 0.001 accuracy 94.90234 wps 14345.98 step time 0.46s\n","# Epoch 13  global step 40880 loss 5.28169 batch 1496/3281 lr 0.001 accuracy 94.78516 wps 14062.26 step time 0.44s\n","# Epoch 13  global step 40900 loss 5.33953 batch 1516/3281 lr 0.001 accuracy 94.49609 wps 14290.02 step time 0.56s\n","# Epoch 13  global step 40920 loss 5.34957 batch 1536/3281 lr 0.001 accuracy 94.21875 wps 13593.97 step time 0.57s\n","# Epoch 13  global step 40940 loss 5.33140 batch 1556/3281 lr 0.001 accuracy 94.48828 wps 14626.37 step time 0.48s\n","# Epoch 13  global step 40960 loss 5.28855 batch 1576/3281 lr 0.001 accuracy 94.52734 wps 15036.83 step time 0.54s\n","# Epoch 13  global step 40980 loss 5.41301 batch 1596/3281 lr 0.001 accuracy 94.31641 wps 14539.32 step time 0.47s\n","# Epoch 13  global step 41000 loss 5.29506 batch 1616/3281 lr 0.001 accuracy 94.66406 wps 14282.31 step time 0.45s\n","# Epoch 13  global step 41020 loss 5.26528 batch 1636/3281 lr 0.001 accuracy 94.81250 wps 14276.23 step time 0.45s\n","# Epoch 13  global step 41040 loss 5.29958 batch 1656/3281 lr 0.001 accuracy 94.43750 wps 14932.14 step time 0.53s\n","# Epoch 13  global step 41060 loss 5.30253 batch 1676/3281 lr 0.001 accuracy 94.55859 wps 14422.02 step time 0.48s\n","# Epoch 13  global step 41080 loss 5.41248 batch 1696/3281 lr 0.001 accuracy 94.16016 wps 14921.27 step time 0.52s\n","# Epoch 13  global step 41100 loss 5.28181 batch 1716/3281 lr 0.001 accuracy 94.66016 wps 14416.51 step time 0.46s\n","# Epoch 13  global step 41120 loss 5.43613 batch 1736/3281 lr 0.001 accuracy 94.12500 wps 14680.49 step time 0.48s\n","# Epoch 13  global step 41140 loss 5.39458 batch 1756/3281 lr 0.001 accuracy 94.24219 wps 13477.73 step time 0.69s\n","# Epoch 13  global step 41160 loss 5.38144 batch 1776/3281 lr 0.001 accuracy 94.41797 wps 13893.99 step time 0.52s\n","# Epoch 13  global step 41180 loss 5.31260 batch 1796/3281 lr 0.001 accuracy 94.48828 wps 14607.27 step time 0.48s\n","# Epoch 13  global step 41200 loss 5.40910 batch 1816/3281 lr 0.001 accuracy 94.20312 wps 14153.66 step time 0.57s\n","# Epoch 13  global step 41220 loss 5.51106 batch 1836/3281 lr 0.001 accuracy 93.82422 wps 14715.87 step time 0.62s\n","# Epoch 13  global step 41240 loss 5.22674 batch 1856/3281 lr 0.001 accuracy 94.81250 wps 13776.37 step time 0.52s\n","# Epoch 13  global step 41260 loss 5.34547 batch 1876/3281 lr 0.001 accuracy 94.60547 wps 14707.57 step time 0.49s\n","# Epoch 13  global step 41280 loss 5.39391 batch 1896/3281 lr 0.001 accuracy 94.26953 wps 14967.76 step time 0.53s\n","# Epoch 13  global step 41300 loss 5.25319 batch 1916/3281 lr 0.001 accuracy 94.67969 wps 14697.44 step time 0.49s\n","# Epoch 13  global step 41320 loss 5.50838 batch 1936/3281 lr 0.001 accuracy 93.99609 wps 14914.44 step time 0.52s\n","# Epoch 13  global step 41340 loss 5.36407 batch 1956/3281 lr 0.001 accuracy 94.14063 wps 14378.51 step time 0.46s\n","# Epoch 13  global step 41360 loss 5.31469 batch 1976/3281 lr 0.001 accuracy 94.60156 wps 14253.51 step time 0.45s\n","# Epoch 13  global step 41380 loss 5.42925 batch 1996/3281 lr 0.001 accuracy 94.22656 wps 14435.12 step time 0.55s\n","# Epoch 13  global step 41400 loss 5.37756 batch 2016/3281 lr 0.001 accuracy 94.32031 wps 14252.23 step time 0.46s\n","# Epoch 13  global step 41420 loss 5.37419 batch 2036/3281 lr 0.001 accuracy 94.29297 wps 14676.49 step time 0.50s\n","# Epoch 13  global step 41440 loss 5.29293 batch 2056/3281 lr 0.001 accuracy 94.57031 wps 14044.49 step time 0.54s\n","# Epoch 13  global step 41460 loss 5.30389 batch 2076/3281 lr 0.001 accuracy 94.53906 wps 14162.79 step time 0.44s\n","# Epoch 13  global step 41480 loss 5.35875 batch 2096/3281 lr 0.001 accuracy 94.39453 wps 14590.42 step time 0.49s\n","# Epoch 13  global step 41500 loss 5.44336 batch 2116/3281 lr 0.001 accuracy 94.10938 wps 14386.94 step time 0.47s\n","# Epoch 13  global step 41520 loss 5.43712 batch 2136/3281 lr 0.001 accuracy 94.09375 wps 14894.00 step time 0.52s\n","# Epoch 13  global step 41540 loss 5.25765 batch 2156/3281 lr 0.001 accuracy 94.62109 wps 14359.39 step time 0.44s\n","# Epoch 13  global step 41560 loss 5.37002 batch 2176/3281 lr 0.001 accuracy 94.42188 wps 14446.35 step time 0.46s\n","# Epoch 13  global step 41580 loss 5.31862 batch 2196/3281 lr 0.001 accuracy 94.48437 wps 14546.26 step time 0.48s\n","# Epoch 13  global step 41600 loss 5.27595 batch 2216/3281 lr 0.001 accuracy 94.58594 wps 14292.40 step time 0.46s\n","# Epoch 13  global step 41620 loss 5.23665 batch 2236/3281 lr 0.001 accuracy 94.58203 wps 13853.85 step time 0.42s\n","# Epoch 13  global step 41640 loss 5.36665 batch 2256/3281 lr 0.001 accuracy 94.37109 wps 14035.53 step time 0.54s\n","# Epoch 13  global step 41660 loss 5.34759 batch 2276/3281 lr 0.001 accuracy 94.31641 wps 14757.67 step time 0.51s\n","# Epoch 13  global step 41680 loss 5.33473 batch 2296/3281 lr 0.001 accuracy 94.37500 wps 14706.92 step time 0.51s\n","# Epoch 13  global step 41700 loss 5.37835 batch 2316/3281 lr 0.001 accuracy 94.22656 wps 14427.62 step time 0.57s\n","# Epoch 13  global step 41720 loss 5.33124 batch 2336/3281 lr 0.001 accuracy 94.56250 wps 13413.61 step time 0.60s\n","# Epoch 13  global step 41740 loss 5.26027 batch 2356/3281 lr 0.001 accuracy 94.60547 wps 14371.52 step time 0.48s\n","# Epoch 13  global step 41760 loss 5.33691 batch 2376/3281 lr 0.001 accuracy 94.58984 wps 14439.09 step time 0.48s\n","# Epoch 13  global step 41780 loss 5.29990 batch 2396/3281 lr 0.001 accuracy 94.50000 wps 14236.59 step time 0.45s\n","# Epoch 13  global step 41800 loss 5.42745 batch 2416/3281 lr 0.001 accuracy 94.33594 wps 14402.56 step time 0.47s\n","# Epoch 13  global step 41820 loss 5.48197 batch 2436/3281 lr 0.001 accuracy 94.11719 wps 14252.95 step time 0.61s\n","# Epoch 13  global step 41840 loss 5.31745 batch 2456/3281 lr 0.001 accuracy 94.51172 wps 14200.36 step time 0.46s\n","# Epoch 13  global step 41860 loss 5.37273 batch 2476/3281 lr 0.001 accuracy 94.33594 wps 14622.18 step time 0.51s\n","# Epoch 13  global step 41880 loss 5.41139 batch 2496/3281 lr 0.001 accuracy 94.22656 wps 14125.62 step time 0.56s\n","# Epoch 13  global step 41900 loss 5.26875 batch 2516/3281 lr 0.001 accuracy 94.51562 wps 13450.41 step time 0.51s\n","# Epoch 13  global step 41920 loss 5.55509 batch 2536/3281 lr 0.001 accuracy 93.92187 wps 14223.74 step time 0.57s\n","# Epoch 13  global step 41940 loss 5.34734 batch 2556/3281 lr 0.001 accuracy 94.39063 wps 14443.00 step time 0.49s\n","# Epoch 13  global step 41960 loss 5.38326 batch 2576/3281 lr 0.001 accuracy 94.31641 wps 14535.82 step time 0.51s\n","# Epoch 13  global step 41980 loss 5.34393 batch 2596/3281 lr 0.001 accuracy 94.53516 wps 14330.80 step time 0.49s\n","# Epoch 13  global step 42000 loss 5.27924 batch 2616/3281 lr 0.001 accuracy 94.79688 wps 14166.65 step time 0.45s\n","# global step 42000, eval model at Sat May 23 13:19:50 2020\n","2020-05-23 13:19:52.814629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:19:52.814960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 13:19:52.815074: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 13:19:52.815103: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 13:19:52.815129: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 13:19:52.815155: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 13:19:52.815181: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 13:19:52.815203: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 13:19:52.815227: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 13:19:52.815314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:19:52.815690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:19:52.815907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 13:19:52.815957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 13:19:52.815973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 13:19:52.815983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 13:19:52.816082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:19:52.816366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:19:52.816593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6586827151609268\n","# location_distance_from_business_district - 0.5508263916440084\n","# location_easy_to_find - 0.7060579302046472\n","# service_wait_time - 0.6678006481801512\n","# service_waiters_attitude - 0.7972795614536929\n","# service_parking_convenience - 0.7488179789627336\n","# service_serving_speed - 0.7582703463327651\n","# price_level - 0.7778289688696652\n","# price_cost_effective - 0.7120426617342213\n","# price_discount - 0.6735632947797102\n","# environment_decoration - 0.7219660646588238\n","# environment_noise - 0.7577226984374219\n","# environment_space - 0.7593766175566158\n","# environment_cleaness - 0.7559265944952425\n","# dish_portion - 0.7180131850113245\n","# dish_taste - 0.7320887049281665\n","# dish_look - 0.5731401995286916\n","# dish_recommendation - 0.7332405631156513\n","# others_overall_experience - 0.5912442813035661\n","# others_willing_to_consume_again - 0.7121867593234326\n","# Eval loss 4.46270, f1 0.70530\n","# current result -0.705303808284073, previous best result -0.705261553375042\n","# Epoch 13  global step 42020 loss 5.37486 batch 2636/3281 lr 0.001 accuracy 94.29297 wps 14196.38 step time 0.49s\n","# Epoch 13  global step 42040 loss 5.35956 batch 2656/3281 lr 0.001 accuracy 94.42578 wps 13992.69 step time 0.42s\n","# Epoch 13  global step 42060 loss 5.39400 batch 2676/3281 lr 0.001 accuracy 94.19922 wps 14337.45 step time 0.46s\n","# Epoch 13  global step 42080 loss 5.38007 batch 2696/3281 lr 0.001 accuracy 94.35938 wps 14758.48 step time 0.51s\n","# Epoch 13  global step 42100 loss 5.42607 batch 2716/3281 lr 0.001 accuracy 94.07812 wps 14826.99 step time 0.53s\n","# Epoch 13  global step 42120 loss 5.32588 batch 2736/3281 lr 0.001 accuracy 94.25781 wps 14638.27 step time 0.49s\n","# Epoch 13  global step 42140 loss 5.45452 batch 2756/3281 lr 0.001 accuracy 94.08594 wps 14132.46 step time 0.65s\n","# Epoch 13  global step 42160 loss 5.57418 batch 2776/3281 lr 0.001 accuracy 93.82031 wps 14588.46 step time 0.55s\n","# Epoch 13  global step 42180 loss 5.36890 batch 2796/3281 lr 0.001 accuracy 94.30469 wps 11697.32 step time 0.62s\n","# Epoch 13  global step 42200 loss 5.34776 batch 2816/3281 lr 0.001 accuracy 94.44531 wps 12321.45 step time 0.58s\n","# Epoch 13  global step 42220 loss 5.41211 batch 2836/3281 lr 0.001 accuracy 94.21484 wps 12832.39 step time 0.48s\n","# Epoch 13  global step 42240 loss 5.36298 batch 2856/3281 lr 0.001 accuracy 94.23437 wps 12577.41 step time 0.53s\n","# Epoch 13  global step 42260 loss 5.47850 batch 2876/3281 lr 0.001 accuracy 94.08984 wps 12024.86 step time 0.77s\n","# Epoch 13  global step 42280 loss 5.47327 batch 2896/3281 lr 0.001 accuracy 94.01172 wps 11703.37 step time 0.64s\n","# Epoch 13  global step 42300 loss 5.34425 batch 2916/3281 lr 0.001 accuracy 94.34766 wps 12328.24 step time 0.57s\n","# Epoch 13  global step 42320 loss 5.27903 batch 2936/3281 lr 0.001 accuracy 94.66797 wps 12673.52 step time 0.53s\n","# Epoch 13  global step 42340 loss 5.39345 batch 2956/3281 lr 0.001 accuracy 94.27734 wps 12892.23 step time 0.50s\n","# Epoch 13  global step 42360 loss 5.39429 batch 2976/3281 lr 0.001 accuracy 94.39453 wps 12562.43 step time 0.53s\n","# Epoch 13  global step 42380 loss 5.42799 batch 2996/3281 lr 0.001 accuracy 94.12500 wps 13097.58 step time 0.57s\n","# Epoch 13  global step 42400 loss 5.33902 batch 3016/3281 lr 0.001 accuracy 94.15234 wps 11832.39 step time 0.75s\n","# Epoch 13  global step 42420 loss 5.42453 batch 3036/3281 lr 0.001 accuracy 94.15625 wps 12762.65 step time 0.58s\n","# Epoch 13  global step 42440 loss 5.39774 batch 3056/3281 lr 0.001 accuracy 94.43359 wps 13365.81 step time 0.56s\n","# Epoch 13  global step 42460 loss 5.44233 batch 3076/3281 lr 0.001 accuracy 94.28516 wps 12865.48 step time 0.57s\n","# Epoch 13  global step 42480 loss 5.46285 batch 3096/3281 lr 0.001 accuracy 94.12500 wps 13306.29 step time 0.56s\n","# Epoch 13  global step 42500 loss 5.34946 batch 3116/3281 lr 0.001 accuracy 94.51562 wps 12922.19 step time 0.50s\n","# Epoch 13  global step 42520 loss 5.49339 batch 3136/3281 lr 0.001 accuracy 93.85547 wps 12149.84 step time 0.72s\n","# Epoch 13  global step 42540 loss 5.37674 batch 3156/3281 lr 0.001 accuracy 94.12500 wps 12032.69 step time 0.64s\n","# Epoch 13  global step 42560 loss 5.42049 batch 3176/3281 lr 0.001 accuracy 94.25391 wps 12981.36 step time 0.59s\n","# Epoch 13  global step 42580 loss 5.40762 batch 3196/3281 lr 0.001 accuracy 94.43750 wps 12953.58 step time 0.54s\n","# Epoch 13  global step 42600 loss 5.27983 batch 3216/3281 lr 0.001 accuracy 94.65625 wps 13233.40 step time 0.54s\n","# Epoch 13  global step 42620 loss 5.40149 batch 3236/3281 lr 0.001 accuracy 94.33203 wps 12906.40 step time 0.57s\n","# Epoch 13  global step 42640 loss 5.49918 batch 3256/3281 lr 0.001 accuracy 93.97656 wps 11675.73 step time 0.71s\n","# Epoch 13  global step 42660 loss 5.39520 batch 3276/3281 lr 0.001 accuracy 94.08203 wps 12919.81 step time 0.50s\n","# Finsh epoch 13, global step 42666\n","# Epoch 14  global step 42680 loss 3.62800 batch 14/3281 lr 0.001 accuracy 66.31641 wps 13851.12 step time 0.39s\n","# Epoch 14  global step 42700 loss 5.31292 batch 34/3281 lr 0.001 accuracy 94.46875 wps 14493.26 step time 0.47s\n","# Epoch 14  global step 42720 loss 5.17754 batch 54/3281 lr 0.001 accuracy 94.76172 wps 14861.05 step time 0.54s\n","# Epoch 14  global step 42740 loss 5.14438 batch 74/3281 lr 0.001 accuracy 95.13281 wps 14377.30 step time 0.46s\n","# Epoch 14  global step 42760 loss 5.15070 batch 94/3281 lr 0.001 accuracy 94.93750 wps 14016.81 step time 0.43s\n","# Epoch 14  global step 42780 loss 5.36620 batch 114/3281 lr 0.001 accuracy 94.56250 wps 14318.71 step time 0.55s\n","# Epoch 14  global step 42800 loss 5.36026 batch 134/3281 lr 0.001 accuracy 94.26563 wps 13995.28 step time 0.60s\n","# Epoch 14  global step 42820 loss 5.32016 batch 154/3281 lr 0.001 accuracy 94.48437 wps 14410.14 step time 0.59s\n","# Epoch 14  global step 42840 loss 5.24179 batch 174/3281 lr 0.001 accuracy 94.73047 wps 14440.55 step time 0.46s\n","# Epoch 14  global step 42860 loss 5.26017 batch 194/3281 lr 0.001 accuracy 94.59375 wps 14146.75 step time 0.52s\n","# Epoch 14  global step 42880 loss 5.24462 batch 214/3281 lr 0.001 accuracy 94.59766 wps 14524.03 step time 0.47s\n","# Epoch 14  global step 42900 loss 5.22565 batch 234/3281 lr 0.001 accuracy 94.71094 wps 14465.20 step time 0.46s\n","# Epoch 14  global step 42920 loss 5.28614 batch 254/3281 lr 0.001 accuracy 94.55078 wps 14434.31 step time 0.48s\n","# Epoch 14  global step 42940 loss 5.09058 batch 274/3281 lr 0.001 accuracy 95.18359 wps 14087.98 step time 0.43s\n","# Epoch 14  global step 42960 loss 5.17665 batch 294/3281 lr 0.001 accuracy 94.99219 wps 14462.00 step time 0.47s\n","# Epoch 14  global step 42980 loss 5.26598 batch 314/3281 lr 0.001 accuracy 94.66016 wps 14066.68 step time 0.43s\n","# Epoch 14  global step 43000 loss 5.31450 batch 334/3281 lr 0.001 accuracy 94.47266 wps 14578.95 step time 0.47s\n","# Epoch 14  global step 43020 loss 5.17262 batch 354/3281 lr 0.001 accuracy 94.94141 wps 14071.75 step time 0.43s\n","# Epoch 14  global step 43040 loss 5.32594 batch 374/3281 lr 0.001 accuracy 94.59766 wps 14869.10 step time 0.50s\n","# Epoch 14  global step 43060 loss 5.19499 batch 394/3281 lr 0.001 accuracy 94.68750 wps 14527.26 step time 0.48s\n","# Epoch 14  global step 43080 loss 5.38262 batch 414/3281 lr 0.001 accuracy 94.33984 wps 14730.71 step time 0.48s\n","# Epoch 14  global step 43100 loss 5.35335 batch 434/3281 lr 0.001 accuracy 94.35156 wps 14477.48 step time 0.59s\n","# Epoch 14  global step 43120 loss 5.22813 batch 454/3281 lr 0.001 accuracy 94.75781 wps 13944.08 step time 0.51s\n","# Epoch 14  global step 43140 loss 5.31837 batch 474/3281 lr 0.001 accuracy 94.58203 wps 14469.95 step time 0.47s\n","# Epoch 14  global step 43160 loss 5.25331 batch 494/3281 lr 0.001 accuracy 94.66406 wps 14662.95 step time 0.48s\n","# Epoch 14  global step 43180 loss 5.32566 batch 514/3281 lr 0.001 accuracy 94.46094 wps 14481.58 step time 0.57s\n","# Epoch 14  global step 43200 loss 5.24708 batch 534/3281 lr 0.001 accuracy 94.77344 wps 13570.82 step time 0.62s\n","# Epoch 14  global step 43220 loss 5.26991 batch 554/3281 lr 0.001 accuracy 94.46094 wps 14670.39 step time 0.49s\n","# Epoch 14  global step 43240 loss 5.34807 batch 574/3281 lr 0.001 accuracy 94.41797 wps 13820.55 step time 0.63s\n","# Epoch 14  global step 43260 loss 5.24701 batch 594/3281 lr 0.001 accuracy 94.58594 wps 14417.51 step time 0.46s\n","# Epoch 14  global step 43280 loss 5.24113 batch 614/3281 lr 0.001 accuracy 94.63281 wps 14656.70 step time 0.48s\n","# Epoch 14  global step 43300 loss 5.34124 batch 634/3281 lr 0.001 accuracy 94.40234 wps 14847.03 step time 0.53s\n","# Epoch 14  global step 43320 loss 5.34247 batch 654/3281 lr 0.001 accuracy 94.34375 wps 14156.98 step time 0.57s\n","# Epoch 14  global step 43340 loss 5.13414 batch 674/3281 lr 0.001 accuracy 95.04687 wps 14203.89 step time 0.43s\n","# Epoch 14  global step 43360 loss 5.37975 batch 694/3281 lr 0.001 accuracy 94.17578 wps 15105.39 step time 0.53s\n","# Epoch 14  global step 43380 loss 5.38031 batch 714/3281 lr 0.001 accuracy 94.42188 wps 14593.86 step time 0.57s\n","# Epoch 14  global step 43400 loss 5.19185 batch 734/3281 lr 0.001 accuracy 94.85156 wps 14598.73 step time 0.47s\n","# Epoch 14  global step 43420 loss 5.25323 batch 754/3281 lr 0.001 accuracy 94.85156 wps 14008.50 step time 0.42s\n","# Epoch 14  global step 43440 loss 5.25787 batch 774/3281 lr 0.001 accuracy 94.63672 wps 13893.11 step time 0.41s\n","# Epoch 14  global step 43460 loss 5.18556 batch 794/3281 lr 0.001 accuracy 94.71875 wps 14280.18 step time 0.54s\n","# Epoch 14  global step 43480 loss 5.22815 batch 814/3281 lr 0.001 accuracy 94.64453 wps 14091.73 step time 0.54s\n","# Epoch 14  global step 43500 loss 5.25621 batch 834/3281 lr 0.001 accuracy 94.49609 wps 14161.08 step time 0.44s\n","# Epoch 14  global step 43520 loss 5.36734 batch 854/3281 lr 0.001 accuracy 94.31250 wps 14611.70 step time 0.58s\n","# Epoch 14  global step 43540 loss 5.31252 batch 874/3281 lr 0.001 accuracy 94.57812 wps 14738.92 step time 0.49s\n","# Epoch 14  global step 43560 loss 5.38599 batch 894/3281 lr 0.001 accuracy 94.30859 wps 14736.54 step time 0.51s\n","# Epoch 14  global step 43580 loss 5.46434 batch 914/3281 lr 0.001 accuracy 94.08203 wps 14302.25 step time 0.57s\n","# Epoch 14  global step 43600 loss 5.27179 batch 934/3281 lr 0.001 accuracy 94.83203 wps 14048.52 step time 0.53s\n","# Epoch 14  global step 43620 loss 5.31423 batch 954/3281 lr 0.001 accuracy 94.41797 wps 14795.45 step time 0.50s\n","# Epoch 14  global step 43640 loss 5.34740 batch 974/3281 lr 0.001 accuracy 94.34375 wps 14879.70 step time 0.51s\n","# Epoch 14  global step 43660 loss 5.39884 batch 994/3281 lr 0.001 accuracy 94.46484 wps 14741.82 step time 0.50s\n","# Epoch 14  global step 43680 loss 5.34668 batch 1014/3281 lr 0.001 accuracy 94.42969 wps 13608.46 step time 0.59s\n","# Epoch 14  global step 43700 loss 5.43291 batch 1034/3281 lr 0.001 accuracy 94.10938 wps 11395.39 step time 0.70s\n","# Epoch 14  global step 43720 loss 5.42038 batch 1054/3281 lr 0.001 accuracy 94.23047 wps 11789.47 step time 0.65s\n","# Epoch 14  global step 43740 loss 5.27002 batch 1074/3281 lr 0.001 accuracy 94.59375 wps 13133.71 step time 0.51s\n","# Epoch 14  global step 43760 loss 5.29773 batch 1094/3281 lr 0.001 accuracy 94.58984 wps 11836.78 step time 0.63s\n","# Epoch 14  global step 43780 loss 5.26677 batch 1114/3281 lr 0.001 accuracy 94.59766 wps 12248.71 step time 0.60s\n","# Epoch 14  global step 43800 loss 5.41470 batch 1134/3281 lr 0.001 accuracy 94.27344 wps 12331.84 step time 0.70s\n","# Epoch 14  global step 43820 loss 5.28087 batch 1154/3281 lr 0.001 accuracy 94.57031 wps 12580.24 step time 0.50s\n","# Epoch 14  global step 43840 loss 5.26908 batch 1174/3281 lr 0.001 accuracy 94.63281 wps 12663.99 step time 0.63s\n","# Epoch 14  global step 43860 loss 5.34298 batch 1194/3281 lr 0.001 accuracy 94.32422 wps 12807.05 step time 0.55s\n","# Epoch 14  global step 43880 loss 5.42707 batch 1214/3281 lr 0.001 accuracy 94.23438 wps 12697.90 step time 0.60s\n","# Epoch 14  global step 43900 loss 5.25356 batch 1234/3281 lr 0.001 accuracy 94.64453 wps 12736.85 step time 0.52s\n","# Epoch 14  global step 43920 loss 5.30862 batch 1254/3281 lr 0.001 accuracy 94.43750 wps 13191.21 step time 0.53s\n","# Epoch 14  global step 43940 loss 5.34233 batch 1274/3281 lr 0.001 accuracy 94.39063 wps 13591.15 step time 0.62s\n","# Epoch 14  global step 43960 loss 5.25938 batch 1294/3281 lr 0.001 accuracy 94.74609 wps 11960.12 step time 0.57s\n","# Epoch 14  global step 43980 loss 5.28985 batch 1314/3281 lr 0.001 accuracy 94.55469 wps 12331.04 step time 0.53s\n","# Epoch 14  global step 44000 loss 5.21605 batch 1334/3281 lr 0.001 accuracy 94.84375 wps 12929.32 step time 0.44s\n","# global step 44000, eval model at Sat May 23 13:40:06 2020\n","2020-05-23 13:40:08.252307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:40:08.252826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 13:40:08.252957: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 13:40:08.252989: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 13:40:08.253012: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 13:40:08.253038: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 13:40:08.253064: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 13:40:08.253087: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 13:40:08.253111: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 13:40:08.253214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:40:08.253514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:40:08.253770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 13:40:08.254108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 13:40:08.254135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 13:40:08.254148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 13:40:08.254354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:40:08.254668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 13:40:08.254900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6552905234178392\n","# location_distance_from_business_district - 0.5616004738879392\n","# location_easy_to_find - 0.7066529109858983\n","# service_wait_time - 0.6656936601135974\n","# service_waiters_attitude - 0.7962013347923308\n","# service_parking_convenience - 0.7437747213652177\n","# service_serving_speed - 0.7550105710082606\n","# price_level - 0.7786637577090973\n","# price_cost_effective - 0.7126554531129423\n","# price_discount - 0.6708897029588943\n","# environment_decoration - 0.7149266948861674\n","# environment_noise - 0.7580046771578368\n","# environment_space - 0.7592703421893057\n","# environment_cleaness - 0.7540551170669275\n","# dish_portion - 0.7154162965104115\n","# dish_taste - 0.7293288055842857\n","# dish_look - 0.571931996290684\n","# dish_recommendation - 0.733915977842741\n","# others_overall_experience - 0.5910577749345703\n","# others_willing_to_consume_again - 0.7102655880332649\n","# Eval loss 4.47623, f1 0.70423\n","# current result -0.7042303189924107, previous best result -0.705303808284073\n","# Epoch 14  global step 44020 loss 5.25597 batch 1354/3281 lr 0.001 accuracy 94.55859 wps 12363.79 step time 0.50s\n","# Epoch 14  global step 44040 loss 5.37636 batch 1374/3281 lr 0.001 accuracy 94.33594 wps 12917.23 step time 0.62s\n","# Epoch 14  global step 44060 loss 5.35679 batch 1394/3281 lr 0.001 accuracy 94.44531 wps 12400.60 step time 0.64s\n","# Epoch 14  global step 44080 loss 5.40554 batch 1414/3281 lr 0.001 accuracy 94.23828 wps 12950.02 step time 0.53s\n","# Epoch 14  global step 44100 loss 5.26755 batch 1434/3281 lr 0.001 accuracy 94.62500 wps 12478.69 step time 0.54s\n","# Epoch 14  global step 44120 loss 5.22633 batch 1454/3281 lr 0.001 accuracy 94.87500 wps 11746.21 step time 0.61s\n","# Epoch 14  global step 44140 loss 5.36466 batch 1474/3281 lr 0.001 accuracy 94.27344 wps 13272.88 step time 0.49s\n","# Epoch 14  global step 44160 loss 5.36535 batch 1494/3281 lr 0.001 accuracy 94.43359 wps 11105.22 step time 0.79s\n","# Epoch 14  global step 44180 loss 5.40309 batch 1514/3281 lr 0.001 accuracy 94.37500 wps 13012.14 step time 0.57s\n","# Epoch 14  global step 44200 loss 5.30738 batch 1534/3281 lr 0.001 accuracy 94.64063 wps 12554.70 step time 0.56s\n","# Epoch 14  global step 44220 loss 5.26308 batch 1554/3281 lr 0.001 accuracy 94.73047 wps 12829.40 step time 0.47s\n","# Epoch 14  global step 44240 loss 5.21847 batch 1574/3281 lr 0.001 accuracy 94.74609 wps 12542.55 step time 0.52s\n","# Epoch 14  global step 44260 loss 5.34711 batch 1594/3281 lr 0.001 accuracy 94.50000 wps 11252.53 step time 0.73s\n","# Epoch 14  global step 44280 loss 5.31742 batch 1614/3281 lr 0.001 accuracy 94.53516 wps 13111.97 step time 0.44s\n","# Epoch 14  global step 44300 loss 5.37564 batch 1634/3281 lr 0.001 accuracy 94.29688 wps 11147.61 step time 0.71s\n","# Epoch 14  global step 44320 loss 5.37074 batch 1654/3281 lr 0.001 accuracy 94.30469 wps 12749.57 step time 0.55s\n","# Epoch 14  global step 44340 loss 5.33860 batch 1674/3281 lr 0.001 accuracy 94.33594 wps 12231.15 step time 0.65s\n","# Epoch 14  global step 44360 loss 5.35683 batch 1694/3281 lr 0.001 accuracy 94.29687 wps 13071.38 step time 0.58s\n","# Epoch 14  global step 44380 loss 5.24299 batch 1714/3281 lr 0.001 accuracy 94.76172 wps 13073.85 step time 0.51s\n","# Epoch 14  global step 44400 loss 5.41064 batch 1734/3281 lr 0.001 accuracy 94.14844 wps 13526.96 step time 0.63s\n","# Epoch 14  global step 44420 loss 5.27909 batch 1754/3281 lr 0.001 accuracy 94.74609 wps 12748.87 step time 0.47s\n","# Epoch 14  global step 44440 loss 5.51682 batch 1774/3281 lr 0.001 accuracy 93.90234 wps 12584.08 step time 0.65s\n","# Epoch 14  global step 44460 loss 5.22701 batch 1794/3281 lr 0.001 accuracy 94.66406 wps 12927.98 step time 0.55s\n","# Epoch 14  global step 44480 loss 5.37552 batch 1814/3281 lr 0.001 accuracy 94.31641 wps 11921.47 step time 0.64s\n","# Epoch 14  global step 44500 loss 5.23753 batch 1834/3281 lr 0.001 accuracy 94.75781 wps 12706.54 step time 0.53s\n","# Epoch 14  global step 44520 loss 5.36677 batch 1854/3281 lr 0.001 accuracy 94.29297 wps 12525.76 step time 0.67s\n","# Epoch 14  global step 44540 loss 5.29605 batch 1874/3281 lr 0.001 accuracy 94.70703 wps 12709.80 step time 0.44s\n","# Epoch 14  global step 44560 loss 5.18231 batch 1894/3281 lr 0.001 accuracy 95.02734 wps 12589.89 step time 0.47s\n","# Epoch 14  global step 44580 loss 5.29535 batch 1914/3281 lr 0.001 accuracy 94.37891 wps 12103.06 step time 0.68s\n","# Epoch 14  global step 44600 loss 5.30938 batch 1934/3281 lr 0.001 accuracy 94.49609 wps 12227.48 step time 0.57s\n","# Epoch 14  global step 44620 loss 5.26343 batch 1954/3281 lr 0.001 accuracy 94.73438 wps 12598.04 step time 0.54s\n","# Epoch 14  global step 44640 loss 5.35147 batch 1974/3281 lr 0.001 accuracy 94.49219 wps 12238.11 step time 0.53s\n","# Epoch 14  global step 44660 loss 5.40411 batch 1994/3281 lr 0.001 accuracy 94.24219 wps 12108.29 step time 0.70s\n","# Epoch 14  global step 44680 loss 5.32962 batch 2014/3281 lr 0.001 accuracy 94.33203 wps 12574.63 step time 0.56s\n","# Epoch 14  global step 44700 loss 5.27863 batch 2034/3281 lr 0.001 accuracy 94.55859 wps 12428.70 step time 0.60s\n","# Epoch 14  global step 44720 loss 5.33904 batch 2054/3281 lr 0.001 accuracy 94.39844 wps 11946.09 step time 0.63s\n","# Epoch 14  global step 44740 loss 5.27790 batch 2074/3281 lr 0.001 accuracy 94.60547 wps 12860.55 step time 0.56s\n","# Epoch 14  global step 44760 loss 5.34195 batch 2094/3281 lr 0.001 accuracy 94.30469 wps 13039.35 step time 0.52s\n","# Epoch 14  global step 44780 loss 5.15776 batch 2114/3281 lr 0.001 accuracy 94.94531 wps 12552.75 step time 0.48s\n","# Epoch 14  global step 44800 loss 5.30144 batch 2134/3281 lr 0.001 accuracy 94.59766 wps 12613.28 step time 0.53s\n","# Epoch 14  global step 44820 loss 5.43439 batch 2154/3281 lr 0.001 accuracy 94.03516 wps 12878.16 step time 0.61s\n","# Epoch 14  global step 44840 loss 5.33926 batch 2174/3281 lr 0.001 accuracy 94.48828 wps 12591.83 step time 0.51s\n","# Epoch 14  global step 44860 loss 5.43311 batch 2194/3281 lr 0.001 accuracy 93.88672 wps 11894.28 step time 0.76s\n","# Epoch 14  global step 44880 loss 5.31166 batch 2214/3281 lr 0.001 accuracy 94.61328 wps 12315.96 step time 0.60s\n","# Epoch 14  global step 44900 loss 5.26295 batch 2234/3281 lr 0.001 accuracy 94.48047 wps 10604.01 step time 0.79s\n","# Epoch 14  global step 44920 loss 5.31846 batch 2254/3281 lr 0.001 accuracy 94.46484 wps 12851.62 step time 0.51s\n","# Epoch 14  global step 44940 loss 5.41368 batch 2274/3281 lr 0.001 accuracy 94.46484 wps 12974.35 step time 0.54s\n","# Epoch 14  global step 44960 loss 5.29922 batch 2294/3281 lr 0.001 accuracy 94.58984 wps 11946.10 step time 0.60s\n","# Epoch 14  global step 44980 loss 5.26562 batch 2314/3281 lr 0.001 accuracy 94.66016 wps 12298.98 step time 0.56s\n","# Epoch 14  global step 45000 loss 5.27234 batch 2334/3281 lr 0.001 accuracy 94.49609 wps 12538.22 step time 0.53s\n","# Epoch 14  global step 45020 loss 5.33097 batch 2354/3281 lr 0.001 accuracy 94.48828 wps 12682.38 step time 0.55s\n","# Epoch 14  global step 45040 loss 5.30187 batch 2374/3281 lr 0.001 accuracy 94.68750 wps 11543.11 step time 0.58s\n","# Epoch 14  global step 45060 loss 5.34427 batch 2394/3281 lr 0.001 accuracy 94.42578 wps 12409.53 step time 0.68s\n","# Epoch 14  global step 45080 loss 5.30761 batch 2414/3281 lr 0.001 accuracy 94.43750 wps 12920.85 step time 0.54s\n","# Epoch 14  global step 45100 loss 5.49529 batch 2434/3281 lr 0.001 accuracy 93.96094 wps 12364.17 step time 0.68s\n","# Epoch 14  global step 45120 loss 5.41919 batch 2454/3281 lr 0.001 accuracy 94.27344 wps 12942.35 step time 0.64s\n","# Epoch 14  global step 45140 loss 5.42930 batch 2474/3281 lr 0.001 accuracy 94.28516 wps 13272.79 step time 0.56s\n","# Epoch 14  global step 45160 loss 5.21643 batch 2494/3281 lr 0.001 accuracy 94.53516 wps 13097.78 step time 0.52s\n","# Epoch 14  global step 45180 loss 5.38308 batch 2514/3281 lr 0.001 accuracy 94.50000 wps 11900.83 step time 0.62s\n","# Epoch 14  global step 45200 loss 5.36008 batch 2534/3281 lr 0.001 accuracy 94.51953 wps 12140.24 step time 0.65s\n","# Epoch 14  global step 45220 loss 5.40336 batch 2554/3281 lr 0.001 accuracy 94.44922 wps 12566.98 step time 0.55s\n","# Epoch 14  global step 45240 loss 5.26351 batch 2574/3281 lr 0.001 accuracy 94.67578 wps 12870.49 step time 0.50s\n","# Epoch 14  global step 45260 loss 5.25942 batch 2594/3281 lr 0.001 accuracy 94.69531 wps 13000.82 step time 0.50s\n","# Epoch 14  global step 45280 loss 5.27033 batch 2614/3281 lr 0.001 accuracy 94.71875 wps 12571.15 step time 0.53s\n","# Epoch 14  global step 45300 loss 5.45045 batch 2634/3281 lr 0.001 accuracy 93.97656 wps 13329.34 step time 0.65s\n","# Epoch 14  global step 45320 loss 5.31053 batch 2654/3281 lr 0.001 accuracy 94.53125 wps 13205.98 step time 0.49s\n","# Epoch 14  global step 45340 loss 5.29013 batch 2674/3281 lr 0.001 accuracy 94.55469 wps 12531.02 step time 0.53s\n","# Epoch 14  global step 45360 loss 5.24149 batch 2694/3281 lr 0.001 accuracy 94.78516 wps 12420.40 step time 0.53s\n","# Epoch 14  global step 45380 loss 5.40882 batch 2714/3281 lr 0.001 accuracy 94.01953 wps 12138.79 step time 0.61s\n","# Epoch 14  global step 45400 loss 5.40096 batch 2734/3281 lr 0.001 accuracy 94.34766 wps 12497.81 step time 0.68s\n","# Epoch 14  global step 45420 loss 5.31510 batch 2754/3281 lr 0.001 accuracy 94.51563 wps 12344.78 step time 0.63s\n","# Epoch 14  global step 45440 loss 5.26131 batch 2774/3281 lr 0.001 accuracy 94.50000 wps 11960.25 step time 0.56s\n","# Epoch 14  global step 45460 loss 5.51188 batch 2794/3281 lr 0.001 accuracy 94.05469 wps 12476.58 step time 0.66s\n","# Epoch 14  global step 45480 loss 5.50025 batch 2814/3281 lr 0.001 accuracy 94.09766 wps 12324.14 step time 0.70s\n","# Epoch 14  global step 45500 loss 5.45888 batch 2834/3281 lr 0.001 accuracy 94.15625 wps 13057.74 step time 0.55s\n","# Epoch 14  global step 45520 loss 5.36455 batch 2854/3281 lr 0.001 accuracy 94.32031 wps 13007.63 step time 0.53s\n","# Epoch 14  global step 45540 loss 5.34552 batch 2874/3281 lr 0.001 accuracy 94.56641 wps 12676.52 step time 0.55s\n","# Epoch 14  global step 45560 loss 5.36274 batch 2894/3281 lr 0.001 accuracy 94.44531 wps 12446.72 step time 0.55s\n","# Epoch 14  global step 45580 loss 5.32818 batch 2914/3281 lr 0.001 accuracy 94.48047 wps 12233.33 step time 0.50s\n","# Epoch 14  global step 45600 loss 5.20620 batch 2934/3281 lr 0.001 accuracy 94.73047 wps 12480.77 step time 0.49s\n","# Epoch 14  global step 45620 loss 5.21120 batch 2954/3281 lr 0.001 accuracy 94.80469 wps 13602.89 step time 0.48s\n","# Epoch 14  global step 45640 loss 5.29115 batch 2974/3281 lr 0.001 accuracy 94.61719 wps 11890.37 step time 0.67s\n","# Epoch 14  global step 45660 loss 5.38693 batch 2994/3281 lr 0.001 accuracy 94.38672 wps 12682.27 step time 0.57s\n","# Epoch 14  global step 45680 loss 5.38204 batch 3014/3281 lr 0.001 accuracy 94.37109 wps 11882.30 step time 0.60s\n","# Epoch 14  global step 45700 loss 5.30771 batch 3034/3281 lr 0.001 accuracy 94.51562 wps 12748.77 step time 0.51s\n","# Epoch 14  global step 45720 loss 5.41661 batch 3054/3281 lr 0.001 accuracy 94.35547 wps 12722.94 step time 0.59s\n","# Epoch 14  global step 45740 loss 5.38931 batch 3074/3281 lr 0.001 accuracy 94.23828 wps 12136.69 step time 0.67s\n","# Epoch 14  global step 45760 loss 5.36812 batch 3094/3281 lr 0.001 accuracy 94.34375 wps 11783.84 step time 0.66s\n","# Epoch 14  global step 45780 loss 5.36565 batch 3114/3281 lr 0.001 accuracy 94.40234 wps 12620.97 step time 0.58s\n","# Epoch 14  global step 45800 loss 5.42736 batch 3134/3281 lr 0.001 accuracy 94.03125 wps 12847.24 step time 0.66s\n","# Epoch 14  global step 45820 loss 5.38736 batch 3154/3281 lr 0.001 accuracy 94.27344 wps 11856.34 step time 0.67s\n","# Epoch 14  global step 45840 loss 5.19863 batch 3174/3281 lr 0.001 accuracy 94.88672 wps 12855.66 step time 0.51s\n","# Epoch 14  global step 45860 loss 5.38573 batch 3194/3281 lr 0.001 accuracy 94.25781 wps 12984.85 step time 0.59s\n","# Epoch 14  global step 45880 loss 5.34253 batch 3214/3281 lr 0.001 accuracy 94.32031 wps 11662.69 step time 0.70s\n","# Epoch 14  global step 45900 loss 5.34198 batch 3234/3281 lr 0.001 accuracy 94.57031 wps 12745.34 step time 0.50s\n","# Epoch 14  global step 45920 loss 5.35560 batch 3254/3281 lr 0.001 accuracy 94.55078 wps 12654.64 step time 0.53s\n","# Epoch 14  global step 45940 loss 5.36568 batch 3274/3281 lr 0.001 accuracy 94.39063 wps 12644.60 step time 0.58s\n","# Finsh epoch 14, global step 45948\n","# Epoch 15  global step 45960 loss 3.24340 batch 12/3281 lr 0.001 accuracy 56.41797 wps 14515.25 step time 0.37s\n","# Epoch 15  global step 45980 loss 5.12364 batch 32/3281 lr 0.001 accuracy 94.97656 wps 14156.86 step time 0.43s\n","# Epoch 15  global step 46000 loss 5.21952 batch 52/3281 lr 0.001 accuracy 94.87500 wps 14490.85 step time 0.46s\n","# global step 46000, eval model at Sat May 23 14:01:39 2020\n","2020-05-23 14:01:42.097259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:01:42.097741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 14:01:42.097862: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 14:01:42.097888: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 14:01:42.097914: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 14:01:42.097942: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 14:01:42.097967: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 14:01:42.097990: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 14:01:42.098014: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 14:01:42.098109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:01:42.098390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:01:42.098655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 14:01:42.099061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 14:01:42.099090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 14:01:42.099099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 14:01:42.099276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:01:42.099553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:01:42.099764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6532997814468663\n","# location_distance_from_business_district - 0.5614589816487872\n","# location_easy_to_find - 0.7079363125639277\n","# service_wait_time - 0.6660128565989417\n","# service_waiters_attitude - 0.7962454880993013\n","# service_parking_convenience - 0.7402365407385647\n","# service_serving_speed - 0.7538592178871625\n","# price_level - 0.7783469131836338\n","# price_cost_effective - 0.7129899392056127\n","# price_discount - 0.6718526682542193\n","# environment_decoration - 0.7156249749127255\n","# environment_noise - 0.757741641941226\n","# environment_space - 0.7571869181654376\n","# environment_cleaness - 0.7556681451116628\n","# dish_portion - 0.717195529221196\n","# dish_taste - 0.729173780492658\n","# dish_look - 0.5710162192986853\n","# dish_recommendation - 0.7329754302587947\n","# others_overall_experience - 0.5907443504587013\n","# others_willing_to_consume_again - 0.712264337842475\n","# Eval loss 4.48818, f1 0.70409\n","# current result -0.7040915013665291, previous best result -0.705303808284073\n","# Epoch 15  global step 46020 loss 5.19817 batch 72/3281 lr 0.001 accuracy 94.89453 wps 13739.34 step time 0.49s\n","# Epoch 15  global step 46040 loss 5.17498 batch 92/3281 lr 0.001 accuracy 94.90625 wps 14119.71 step time 0.44s\n","# Epoch 15  global step 46060 loss 5.23458 batch 112/3281 lr 0.001 accuracy 94.81250 wps 13846.99 step time 0.58s\n","# Epoch 15  global step 46080 loss 5.21358 batch 132/3281 lr 0.001 accuracy 94.79297 wps 14043.69 step time 0.52s\n","# Epoch 15  global step 46100 loss 5.14187 batch 152/3281 lr 0.001 accuracy 94.93750 wps 14526.17 step time 0.46s\n","# Epoch 15  global step 46120 loss 5.10978 batch 172/3281 lr 0.001 accuracy 95.20312 wps 14196.81 step time 0.55s\n","# Epoch 15  global step 46140 loss 5.26951 batch 192/3281 lr 0.001 accuracy 94.75000 wps 14753.63 step time 0.48s\n","# Epoch 15  global step 46160 loss 5.15372 batch 212/3281 lr 0.001 accuracy 94.98437 wps 14545.68 step time 0.47s\n","# Epoch 15  global step 46180 loss 5.20919 batch 232/3281 lr 0.001 accuracy 94.81250 wps 13775.65 step time 0.60s\n","# Epoch 15  global step 46200 loss 5.12765 batch 252/3281 lr 0.001 accuracy 95.05859 wps 14272.30 step time 0.45s\n","# Epoch 15  global step 46220 loss 5.23916 batch 272/3281 lr 0.001 accuracy 94.64453 wps 14788.49 step time 0.48s\n","# Epoch 15  global step 46240 loss 5.17159 batch 292/3281 lr 0.001 accuracy 94.95703 wps 14057.89 step time 0.43s\n","# Epoch 15  global step 46260 loss 5.15721 batch 312/3281 lr 0.001 accuracy 94.85156 wps 14460.60 step time 0.46s\n","# Epoch 15  global step 46280 loss 5.11292 batch 332/3281 lr 0.001 accuracy 94.89062 wps 14637.68 step time 0.47s\n","# Epoch 15  global step 46300 loss 5.24781 batch 352/3281 lr 0.001 accuracy 94.69922 wps 13882.13 step time 0.52s\n","# Epoch 15  global step 46320 loss 5.30606 batch 372/3281 lr 0.001 accuracy 94.54687 wps 14195.65 step time 0.52s\n","# Epoch 15  global step 46340 loss 5.31817 batch 392/3281 lr 0.001 accuracy 94.53906 wps 14499.41 step time 0.47s\n","# Epoch 15  global step 46360 loss 5.36125 batch 412/3281 lr 0.001 accuracy 94.33203 wps 14737.68 step time 0.49s\n","# Epoch 15  global step 46380 loss 5.17868 batch 432/3281 lr 0.001 accuracy 94.83203 wps 14590.24 step time 0.47s\n","# Epoch 15  global step 46400 loss 5.14791 batch 452/3281 lr 0.001 accuracy 95.05078 wps 14131.92 step time 0.43s\n","# Epoch 15  global step 46420 loss 5.20056 batch 472/3281 lr 0.001 accuracy 94.69531 wps 14758.90 step time 0.51s\n","# Epoch 15  global step 46440 loss 5.28333 batch 492/3281 lr 0.001 accuracy 94.66016 wps 14847.22 step time 0.52s\n","# Epoch 15  global step 46460 loss 5.24484 batch 512/3281 lr 0.001 accuracy 94.73828 wps 14942.77 step time 0.53s\n","# Epoch 15  global step 46480 loss 5.16734 batch 532/3281 lr 0.001 accuracy 95.04688 wps 14248.77 step time 0.45s\n","# Epoch 15  global step 46500 loss 5.21166 batch 552/3281 lr 0.001 accuracy 94.84766 wps 14541.50 step time 0.47s\n","# Epoch 15  global step 46520 loss 5.17850 batch 572/3281 lr 0.001 accuracy 95.08984 wps 14079.39 step time 0.43s\n","# Epoch 15  global step 46540 loss 5.24343 batch 592/3281 lr 0.001 accuracy 94.63672 wps 14272.81 step time 0.45s\n","# Epoch 15  global step 46560 loss 5.13419 batch 612/3281 lr 0.001 accuracy 94.95703 wps 14151.55 step time 0.44s\n","# Epoch 15  global step 46580 loss 5.20748 batch 632/3281 lr 0.001 accuracy 94.79297 wps 14461.64 step time 0.50s\n","# Epoch 15  global step 46600 loss 5.22063 batch 652/3281 lr 0.001 accuracy 94.57031 wps 14563.67 step time 0.48s\n","# Epoch 15  global step 46620 loss 5.14735 batch 672/3281 lr 0.001 accuracy 94.76172 wps 14574.60 step time 0.47s\n","# Epoch 15  global step 46640 loss 5.27181 batch 692/3281 lr 0.001 accuracy 94.60156 wps 14866.91 step time 0.54s\n","# Epoch 15  global step 46660 loss 5.19917 batch 712/3281 lr 0.001 accuracy 94.60938 wps 13971.43 step time 0.43s\n","# Epoch 15  global step 46680 loss 5.27251 batch 732/3281 lr 0.001 accuracy 94.63672 wps 14379.79 step time 0.46s\n","# Epoch 15  global step 46700 loss 5.23098 batch 752/3281 lr 0.001 accuracy 94.70313 wps 13735.03 step time 0.51s\n","# Epoch 15  global step 46720 loss 5.31938 batch 772/3281 lr 0.001 accuracy 94.36719 wps 14723.49 step time 0.50s\n","# Epoch 15  global step 46740 loss 5.23993 batch 792/3281 lr 0.001 accuracy 94.67578 wps 14416.00 step time 0.48s\n","# Epoch 15  global step 46760 loss 5.36262 batch 812/3281 lr 0.001 accuracy 94.55469 wps 14570.60 step time 0.57s\n","# Epoch 15  global step 46780 loss 5.42445 batch 832/3281 lr 0.001 accuracy 94.37500 wps 13885.09 step time 0.65s\n","# Epoch 15  global step 46800 loss 5.36681 batch 852/3281 lr 0.001 accuracy 94.21094 wps 15049.69 step time 0.57s\n","# Epoch 15  global step 46820 loss 5.38999 batch 872/3281 lr 0.001 accuracy 94.30859 wps 14234.73 step time 0.56s\n","# Epoch 15  global step 46840 loss 5.35013 batch 892/3281 lr 0.001 accuracy 94.45703 wps 14222.66 step time 0.57s\n","# Epoch 15  global step 46860 loss 5.28127 batch 912/3281 lr 0.001 accuracy 94.58203 wps 14570.87 step time 0.47s\n","# Epoch 15  global step 46880 loss 5.29389 batch 932/3281 lr 0.001 accuracy 94.64453 wps 14826.66 step time 0.51s\n","# Epoch 15  global step 46900 loss 5.21932 batch 952/3281 lr 0.001 accuracy 94.66406 wps 13686.70 step time 0.70s\n","# Epoch 15  global step 46920 loss 5.32731 batch 972/3281 lr 0.001 accuracy 94.48828 wps 13849.13 step time 0.54s\n","# Epoch 15  global step 46940 loss 5.35586 batch 992/3281 lr 0.001 accuracy 94.38672 wps 14146.26 step time 0.55s\n","# Epoch 15  global step 46960 loss 5.27109 batch 1012/3281 lr 0.001 accuracy 94.71484 wps 14287.06 step time 0.46s\n","# Epoch 15  global step 46980 loss 5.31158 batch 1032/3281 lr 0.001 accuracy 94.55859 wps 14188.58 step time 0.56s\n","# Epoch 15  global step 47000 loss 5.40299 batch 1052/3281 lr 0.001 accuracy 94.02734 wps 14610.52 step time 0.60s\n","# Epoch 15  global step 47020 loss 5.31988 batch 1072/3281 lr 0.001 accuracy 94.68750 wps 14334.69 step time 0.45s\n","# Epoch 15  global step 47040 loss 5.31198 batch 1092/3281 lr 0.001 accuracy 94.30078 wps 13323.47 step time 0.67s\n","# Epoch 15  global step 47060 loss 5.26982 batch 1112/3281 lr 0.001 accuracy 94.79297 wps 14248.22 step time 0.44s\n","# Epoch 15  global step 47080 loss 5.22032 batch 1132/3281 lr 0.001 accuracy 94.81641 wps 14325.16 step time 0.45s\n","# Epoch 15  global step 47100 loss 5.31784 batch 1152/3281 lr 0.001 accuracy 94.54687 wps 14206.12 step time 0.57s\n","# Epoch 15  global step 47120 loss 5.44620 batch 1172/3281 lr 0.001 accuracy 94.17188 wps 14055.45 step time 0.65s\n","# Epoch 15  global step 47140 loss 5.08785 batch 1192/3281 lr 0.001 accuracy 95.25781 wps 13902.05 step time 0.42s\n","# Epoch 15  global step 47160 loss 5.17369 batch 1212/3281 lr 0.001 accuracy 94.88672 wps 14197.09 step time 0.44s\n","# Epoch 15  global step 47180 loss 5.26259 batch 1232/3281 lr 0.001 accuracy 94.44531 wps 14625.09 step time 0.51s\n","# Epoch 15  global step 47200 loss 5.35290 batch 1252/3281 lr 0.001 accuracy 94.51563 wps 14688.06 step time 0.49s\n","# Epoch 15  global step 47220 loss 5.30222 batch 1272/3281 lr 0.001 accuracy 94.52734 wps 14359.59 step time 0.57s\n","# Epoch 15  global step 47240 loss 5.29792 batch 1292/3281 lr 0.001 accuracy 94.42578 wps 14247.44 step time 0.56s\n","# Epoch 15  global step 47260 loss 5.23634 batch 1312/3281 lr 0.001 accuracy 94.75000 wps 14700.84 step time 0.49s\n","# Epoch 15  global step 47280 loss 5.28991 batch 1332/3281 lr 0.001 accuracy 94.67969 wps 14797.23 step time 0.49s\n","# Epoch 15  global step 47300 loss 5.21512 batch 1352/3281 lr 0.001 accuracy 94.78906 wps 14663.06 step time 0.48s\n","# Epoch 15  global step 47320 loss 5.38746 batch 1372/3281 lr 0.001 accuracy 94.28516 wps 13988.02 step time 0.56s\n","# Epoch 15  global step 47340 loss 5.30556 batch 1392/3281 lr 0.001 accuracy 94.56641 wps 14981.40 step time 0.51s\n","# Epoch 15  global step 47360 loss 5.28599 batch 1412/3281 lr 0.001 accuracy 94.64063 wps 14075.10 step time 0.55s\n","# Epoch 15  global step 47380 loss 5.16520 batch 1432/3281 lr 0.001 accuracy 94.92188 wps 13969.23 step time 0.42s\n","# Epoch 15  global step 47400 loss 5.31530 batch 1452/3281 lr 0.001 accuracy 94.52734 wps 14898.46 step time 0.50s\n","# Epoch 15  global step 47420 loss 5.20408 batch 1472/3281 lr 0.001 accuracy 94.66797 wps 14632.81 step time 0.49s\n","# Epoch 15  global step 47440 loss 5.29291 batch 1492/3281 lr 0.001 accuracy 94.49609 wps 14418.87 step time 0.58s\n","# Epoch 15  global step 47460 loss 5.26432 batch 1512/3281 lr 0.001 accuracy 94.62500 wps 13767.31 step time 0.53s\n","# Epoch 15  global step 47480 loss 5.18184 batch 1532/3281 lr 0.001 accuracy 94.97656 wps 14295.81 step time 0.45s\n","# Epoch 15  global step 47500 loss 5.29289 batch 1552/3281 lr 0.001 accuracy 94.57813 wps 14460.82 step time 0.47s\n","# Epoch 15  global step 47520 loss 5.20913 batch 1572/3281 lr 0.001 accuracy 94.92188 wps 13898.70 step time 0.51s\n","# Epoch 15  global step 47540 loss 5.25433 batch 1592/3281 lr 0.001 accuracy 94.72266 wps 14266.86 step time 0.45s\n","# Epoch 15  global step 47560 loss 5.24461 batch 1612/3281 lr 0.001 accuracy 94.59375 wps 14921.73 step time 0.51s\n","# Epoch 15  global step 47580 loss 5.33549 batch 1632/3281 lr 0.001 accuracy 94.37891 wps 14488.12 step time 0.48s\n","# Epoch 15  global step 47600 loss 5.15491 batch 1652/3281 lr 0.001 accuracy 94.95703 wps 14263.25 step time 0.45s\n","# Epoch 15  global step 47620 loss 5.27712 batch 1672/3281 lr 0.001 accuracy 94.54688 wps 14487.41 step time 0.48s\n","# Epoch 15  global step 47640 loss 5.29087 batch 1692/3281 lr 0.001 accuracy 94.56641 wps 14387.71 step time 0.48s\n","# Epoch 15  global step 47660 loss 5.35184 batch 1712/3281 lr 0.001 accuracy 94.55859 wps 13690.56 step time 0.61s\n","# Epoch 15  global step 47680 loss 5.37818 batch 1732/3281 lr 0.001 accuracy 94.33594 wps 14994.53 step time 0.53s\n","# Epoch 15  global step 47700 loss 5.32191 batch 1752/3281 lr 0.001 accuracy 94.35156 wps 13979.94 step time 0.57s\n","# Epoch 15  global step 47720 loss 5.20254 batch 1772/3281 lr 0.001 accuracy 94.69531 wps 14312.47 step time 0.46s\n","# Epoch 15  global step 47740 loss 5.47646 batch 1792/3281 lr 0.001 accuracy 94.23047 wps 14470.02 step time 0.48s\n","# Epoch 15  global step 47760 loss 5.35663 batch 1812/3281 lr 0.001 accuracy 94.32812 wps 14718.59 step time 0.53s\n","# Epoch 15  global step 47780 loss 5.33061 batch 1832/3281 lr 0.001 accuracy 94.42188 wps 14094.40 step time 0.56s\n","# Epoch 15  global step 47800 loss 5.12953 batch 1852/3281 lr 0.001 accuracy 94.92578 wps 14198.73 step time 0.69s\n","# Epoch 15  global step 47820 loss 5.26811 batch 1872/3281 lr 0.001 accuracy 94.56250 wps 12787.92 step time 0.57s\n","# Epoch 15  global step 47840 loss 5.25193 batch 1892/3281 lr 0.001 accuracy 94.58984 wps 12740.90 step time 0.58s\n","# Epoch 15  global step 47860 loss 5.30972 batch 1912/3281 lr 0.001 accuracy 94.38672 wps 11753.16 step time 0.62s\n","# Epoch 15  global step 47880 loss 5.25417 batch 1932/3281 lr 0.001 accuracy 94.43359 wps 11961.57 step time 0.58s\n","# Epoch 15  global step 47900 loss 5.26027 batch 1952/3281 lr 0.001 accuracy 94.57813 wps 12125.61 step time 0.58s\n","# Epoch 15  global step 47920 loss 5.36792 batch 1972/3281 lr 0.001 accuracy 94.42969 wps 13136.71 step time 0.58s\n","# Epoch 15  global step 47940 loss 5.31628 batch 1992/3281 lr 0.001 accuracy 94.40625 wps 12364.14 step time 0.67s\n","# Epoch 15  global step 47960 loss 5.20933 batch 2012/3281 lr 0.001 accuracy 94.88281 wps 13305.13 step time 0.52s\n","# Epoch 15  global step 47980 loss 5.26824 batch 2032/3281 lr 0.001 accuracy 94.74219 wps 12572.51 step time 0.53s\n","# Epoch 15  global step 48000 loss 5.23131 batch 2052/3281 lr 0.001 accuracy 94.66797 wps 12463.48 step time 0.60s\n","# global step 48000, eval model at Sat May 23 14:21:09 2020\n","2020-05-23 14:21:12.094862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:21:12.095181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 14:21:12.095299: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 14:21:12.095327: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 14:21:12.095353: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 14:21:12.095383: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 14:21:12.095407: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 14:21:12.095430: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 14:21:12.095476: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 14:21:12.095572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:21:12.095837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:21:12.096037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 14:21:12.096084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 14:21:12.096099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 14:21:12.096109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 14:21:12.096208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:21:12.096484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:21:12.096696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6592830385244315\n","# location_distance_from_business_district - 0.5584623258657744\n","# location_easy_to_find - 0.7034290776814396\n","# service_wait_time - 0.6666562063076665\n","# service_waiters_attitude - 0.7952917061722714\n","# service_parking_convenience - 0.7398561182818237\n","# service_serving_speed - 0.7538259647891501\n","# price_level - 0.7767690328872459\n","# price_cost_effective - 0.7091383008386699\n","# price_discount - 0.6743435907712534\n","# environment_decoration - 0.7148484000517104\n","# environment_noise - 0.758689128453665\n","# environment_space - 0.7589457763121156\n","# environment_cleaness - 0.7547118945597118\n","# dish_portion - 0.7166019255426297\n","# dish_taste - 0.7283568153750204\n","# dish_look - 0.5723783702013936\n","# dish_recommendation - 0.735067226147466\n","# others_overall_experience - 0.5897670673585921\n","# others_willing_to_consume_again - 0.7107310481769271\n","# Eval loss 4.50475, f1 0.70386\n","# current result -0.7038576507149479, previous best result -0.705303808284073\n","# Epoch 15  global step 48020 loss 5.40778 batch 2072/3281 lr 0.001 accuracy 94.19922 wps 12679.37 step time 0.59s\n","# Epoch 15  global step 48040 loss 5.23276 batch 2092/3281 lr 0.001 accuracy 94.67187 wps 13011.18 step time 0.47s\n","# Epoch 15  global step 48060 loss 5.17898 batch 2112/3281 lr 0.001 accuracy 94.97266 wps 12578.57 step time 0.56s\n","# Epoch 15  global step 48080 loss 5.36961 batch 2132/3281 lr 0.001 accuracy 94.33203 wps 12834.65 step time 0.56s\n","# Epoch 15  global step 48100 loss 5.05706 batch 2152/3281 lr 0.001 accuracy 95.03125 wps 12452.30 step time 0.48s\n","# Epoch 15  global step 48120 loss 5.33181 batch 2172/3281 lr 0.001 accuracy 94.43750 wps 11922.66 step time 0.70s\n","# Epoch 15  global step 48140 loss 5.34659 batch 2192/3281 lr 0.001 accuracy 94.55859 wps 12976.87 step time 0.58s\n","# Epoch 15  global step 48160 loss 5.21396 batch 2212/3281 lr 0.001 accuracy 94.78516 wps 12789.45 step time 0.53s\n","# Epoch 15  global step 48180 loss 5.39210 batch 2232/3281 lr 0.001 accuracy 94.21484 wps 12316.06 step time 0.72s\n","# Epoch 15  global step 48200 loss 5.28199 batch 2252/3281 lr 0.001 accuracy 94.52344 wps 13221.56 step time 0.52s\n","# Epoch 15  global step 48220 loss 5.33540 batch 2272/3281 lr 0.001 accuracy 94.61328 wps 12630.11 step time 0.55s\n","# Epoch 15  global step 48240 loss 5.30315 batch 2292/3281 lr 0.001 accuracy 94.76172 wps 12861.10 step time 0.51s\n","# Epoch 15  global step 48260 loss 5.41774 batch 2312/3281 lr 0.001 accuracy 94.14844 wps 12270.31 step time 0.62s\n","# Epoch 15  global step 48280 loss 5.33488 batch 2332/3281 lr 0.001 accuracy 94.43359 wps 12286.09 step time 0.55s\n","# Epoch 15  global step 48300 loss 5.53784 batch 2352/3281 lr 0.001 accuracy 93.89063 wps 12550.86 step time 0.69s\n","# Epoch 15  global step 48320 loss 5.30484 batch 2372/3281 lr 0.001 accuracy 94.71484 wps 12562.29 step time 0.57s\n","# Epoch 15  global step 48340 loss 5.33214 batch 2392/3281 lr 0.001 accuracy 94.43750 wps 12689.50 step time 0.56s\n","# Epoch 15  global step 48360 loss 5.32868 batch 2412/3281 lr 0.001 accuracy 94.34375 wps 12178.04 step time 0.62s\n","# Epoch 15  global step 48380 loss 5.35054 batch 2432/3281 lr 0.001 accuracy 94.39844 wps 12845.55 step time 0.64s\n","# Epoch 15  global step 48400 loss 5.29349 batch 2452/3281 lr 0.001 accuracy 94.60156 wps 13104.55 step time 0.57s\n","# Epoch 15  global step 48420 loss 5.23730 batch 2472/3281 lr 0.001 accuracy 94.60547 wps 13023.61 step time 0.55s\n","# Epoch 15  global step 48440 loss 5.21500 batch 2492/3281 lr 0.001 accuracy 94.62500 wps 12937.17 step time 0.49s\n","# Epoch 15  global step 48460 loss 5.28987 batch 2512/3281 lr 0.001 accuracy 94.61328 wps 12525.69 step time 0.53s\n","# Epoch 15  global step 48480 loss 5.32075 batch 2532/3281 lr 0.001 accuracy 94.35547 wps 12338.62 step time 0.70s\n","# Epoch 15  global step 48500 loss 5.21029 batch 2552/3281 lr 0.001 accuracy 94.85156 wps 12743.29 step time 0.47s\n","# Epoch 15  global step 48520 loss 5.18861 batch 2572/3281 lr 0.001 accuracy 94.83203 wps 12473.39 step time 0.50s\n","# Epoch 15  global step 48540 loss 5.28171 batch 2592/3281 lr 0.001 accuracy 94.46875 wps 11819.84 step time 0.69s\n","# Epoch 15  global step 48560 loss 5.36441 batch 2612/3281 lr 0.001 accuracy 94.36328 wps 12573.28 step time 0.59s\n","# Epoch 15  global step 48580 loss 5.30425 batch 2632/3281 lr 0.001 accuracy 94.54297 wps 12491.08 step time 0.58s\n","# Epoch 15  global step 48600 loss 5.36038 batch 2652/3281 lr 0.001 accuracy 94.35938 wps 13064.98 step time 0.57s\n","# Epoch 15  global step 48620 loss 5.21854 batch 2672/3281 lr 0.001 accuracy 94.61719 wps 13000.87 step time 0.54s\n","# Epoch 15  global step 48640 loss 5.23025 batch 2692/3281 lr 0.001 accuracy 94.78516 wps 12297.88 step time 0.48s\n","# Epoch 15  global step 48660 loss 5.27740 batch 2712/3281 lr 0.001 accuracy 94.62500 wps 12275.50 step time 0.55s\n","# Epoch 15  global step 48680 loss 5.39709 batch 2732/3281 lr 0.001 accuracy 94.14453 wps 12334.02 step time 0.69s\n","# Epoch 15  global step 48700 loss 5.36648 batch 2752/3281 lr 0.001 accuracy 94.28516 wps 11545.87 step time 0.76s\n","# Epoch 15  global step 48720 loss 5.28872 batch 2772/3281 lr 0.001 accuracy 94.60937 wps 12652.31 step time 0.52s\n","# Epoch 15  global step 48740 loss 5.40964 batch 2792/3281 lr 0.001 accuracy 94.32031 wps 12919.62 step time 0.51s\n","# Epoch 15  global step 48760 loss 5.30753 batch 2812/3281 lr 0.001 accuracy 94.42969 wps 12445.83 step time 0.63s\n","# Epoch 15  global step 48780 loss 5.32357 batch 2832/3281 lr 0.001 accuracy 94.51563 wps 12469.51 step time 0.61s\n","# Epoch 15  global step 48800 loss 5.37671 batch 2852/3281 lr 0.001 accuracy 94.32031 wps 12985.26 step time 0.51s\n","# Epoch 15  global step 48820 loss 5.50276 batch 2872/3281 lr 0.001 accuracy 93.95313 wps 12212.82 step time 0.75s\n","# Epoch 15  global step 48840 loss 5.30209 batch 2892/3281 lr 0.001 accuracy 94.43359 wps 12518.18 step time 0.49s\n","# Epoch 15  global step 48860 loss 5.14669 batch 2912/3281 lr 0.001 accuracy 94.80859 wps 12461.29 step time 0.49s\n","# Epoch 15  global step 48880 loss 5.35756 batch 2932/3281 lr 0.001 accuracy 94.45312 wps 11953.09 step time 0.64s\n","# Epoch 15  global step 48900 loss 5.36515 batch 2952/3281 lr 0.001 accuracy 94.35937 wps 11790.17 step time 0.64s\n","# Epoch 15  global step 48920 loss 5.33550 batch 2972/3281 lr 0.001 accuracy 94.32812 wps 12759.00 step time 0.52s\n","# Epoch 15  global step 48940 loss 5.35850 batch 2992/3281 lr 0.001 accuracy 94.44922 wps 12146.59 step time 0.61s\n","# Epoch 15  global step 48960 loss 5.38034 batch 3012/3281 lr 0.001 accuracy 94.31641 wps 12750.62 step time 0.56s\n","# Epoch 15  global step 48980 loss 5.31040 batch 3032/3281 lr 0.001 accuracy 94.53906 wps 11917.19 step time 0.63s\n","# Epoch 15  global step 49000 loss 5.31753 batch 3052/3281 lr 0.001 accuracy 94.44141 wps 12653.98 step time 0.59s\n","# Epoch 15  global step 49020 loss 5.37461 batch 3072/3281 lr 0.001 accuracy 94.44531 wps 11478.10 step time 0.74s\n","# Epoch 15  global step 49040 loss 5.27034 batch 3092/3281 lr 0.001 accuracy 94.53125 wps 12565.77 step time 0.54s\n","# Epoch 15  global step 49060 loss 5.28459 batch 3112/3281 lr 0.001 accuracy 94.73437 wps 12516.31 step time 0.48s\n","# Epoch 15  global step 49080 loss 5.30201 batch 3132/3281 lr 0.001 accuracy 94.54688 wps 12568.15 step time 0.45s\n","# Epoch 15  global step 49100 loss 5.45246 batch 3152/3281 lr 0.001 accuracy 94.06641 wps 12458.88 step time 0.61s\n","# Epoch 15  global step 49120 loss 5.29934 batch 3172/3281 lr 0.001 accuracy 94.56250 wps 12816.23 step time 0.51s\n","# Epoch 15  global step 49140 loss 5.33939 batch 3192/3281 lr 0.001 accuracy 94.54687 wps 12488.28 step time 0.51s\n","# Epoch 15  global step 49160 loss 5.20332 batch 3212/3281 lr 0.001 accuracy 94.72266 wps 13323.02 step time 0.52s\n","# Epoch 15  global step 49180 loss 5.25486 batch 3232/3281 lr 0.001 accuracy 94.62891 wps 13156.36 step time 0.57s\n","# Epoch 15  global step 49200 loss 5.43911 batch 3252/3281 lr 0.001 accuracy 94.31641 wps 12556.90 step time 0.61s\n","# Epoch 15  global step 49220 loss 5.20938 batch 3272/3281 lr 0.001 accuracy 94.76172 wps 12069.92 step time 0.53s\n","# Finsh epoch 15, global step 49230\n","# Epoch 16  global step 49240 loss 2.63784 batch 10/3281 lr 0.001 accuracy 47.34766 wps 13931.13 step time 0.30s\n","# Epoch 16  global step 49260 loss 5.09180 batch 30/3281 lr 0.001 accuracy 95.04687 wps 14000.32 step time 0.43s\n","# Epoch 16  global step 49280 loss 5.16136 batch 50/3281 lr 0.001 accuracy 94.96875 wps 14663.00 step time 0.47s\n","# Epoch 16  global step 49300 loss 5.14860 batch 70/3281 lr 0.001 accuracy 94.83984 wps 14356.89 step time 0.45s\n","# Epoch 16  global step 49320 loss 5.11825 batch 90/3281 lr 0.001 accuracy 95.05469 wps 13820.10 step time 0.52s\n","# Epoch 16  global step 49340 loss 5.35654 batch 110/3281 lr 0.001 accuracy 94.40234 wps 14005.75 step time 0.73s\n","# Epoch 16  global step 49360 loss 5.29191 batch 130/3281 lr 0.001 accuracy 94.65234 wps 15068.97 step time 0.53s\n","# Epoch 16  global step 49380 loss 5.17537 batch 150/3281 lr 0.001 accuracy 95.00391 wps 14144.28 step time 0.45s\n","# Epoch 16  global step 49400 loss 5.19667 batch 170/3281 lr 0.001 accuracy 94.68750 wps 14011.04 step time 0.55s\n","# Epoch 16  global step 49420 loss 5.18378 batch 190/3281 lr 0.001 accuracy 95.00391 wps 14788.48 step time 0.51s\n","# Epoch 16  global step 49440 loss 5.20576 batch 210/3281 lr 0.001 accuracy 94.84375 wps 14625.76 step time 0.48s\n","# Epoch 16  global step 49460 loss 5.16229 batch 230/3281 lr 0.001 accuracy 95.03906 wps 14139.77 step time 0.44s\n","# Epoch 16  global step 49480 loss 5.37439 batch 250/3281 lr 0.001 accuracy 94.37109 wps 14969.65 step time 0.54s\n","# Epoch 16  global step 49500 loss 5.13798 batch 270/3281 lr 0.001 accuracy 95.03516 wps 14297.82 step time 0.47s\n","# Epoch 16  global step 49520 loss 5.19896 batch 290/3281 lr 0.001 accuracy 94.72266 wps 14757.92 step time 0.51s\n","# Epoch 16  global step 49540 loss 5.14527 batch 310/3281 lr 0.001 accuracy 94.91406 wps 13476.52 step time 0.49s\n","# Epoch 16  global step 49560 loss 5.22522 batch 330/3281 lr 0.001 accuracy 94.60938 wps 14058.69 step time 0.54s\n","# Epoch 16  global step 49580 loss 5.22755 batch 350/3281 lr 0.001 accuracy 94.77734 wps 14422.23 step time 0.57s\n","# Epoch 16  global step 49600 loss 5.22516 batch 370/3281 lr 0.001 accuracy 94.78125 wps 14938.03 step time 0.52s\n","# Epoch 16  global step 49620 loss 5.18387 batch 390/3281 lr 0.001 accuracy 94.89844 wps 14255.20 step time 0.45s\n","# Epoch 16  global step 49640 loss 5.14511 batch 410/3281 lr 0.001 accuracy 95.15625 wps 14402.07 step time 0.45s\n","# Epoch 16  global step 49660 loss 5.24036 batch 430/3281 lr 0.001 accuracy 94.61328 wps 14637.05 step time 0.50s\n","# Epoch 16  global step 49680 loss 5.37836 batch 450/3281 lr 0.001 accuracy 94.46094 wps 14933.71 step time 0.53s\n","# Epoch 16  global step 49700 loss 5.23075 batch 470/3281 lr 0.001 accuracy 94.72266 wps 14915.18 step time 0.54s\n","# Epoch 16  global step 49720 loss 5.11127 batch 490/3281 lr 0.001 accuracy 94.78906 wps 14335.25 step time 0.45s\n","# Epoch 16  global step 49740 loss 5.33214 batch 510/3281 lr 0.001 accuracy 94.46875 wps 14641.31 step time 0.61s\n","# Epoch 16  global step 49760 loss 5.18391 batch 530/3281 lr 0.001 accuracy 94.91016 wps 14124.85 step time 0.52s\n","# Epoch 16  global step 49780 loss 5.27751 batch 550/3281 lr 0.001 accuracy 94.54297 wps 14620.37 step time 0.59s\n","# Epoch 16  global step 49800 loss 5.27365 batch 570/3281 lr 0.001 accuracy 94.75000 wps 14147.11 step time 0.54s\n","# Epoch 16  global step 49820 loss 5.12437 batch 590/3281 lr 0.001 accuracy 94.97656 wps 14222.47 step time 0.46s\n","# Epoch 16  global step 49840 loss 5.13573 batch 610/3281 lr 0.001 accuracy 95.01953 wps 13828.59 step time 0.42s\n","# Epoch 16  global step 49860 loss 5.14474 batch 630/3281 lr 0.001 accuracy 94.98438 wps 14468.67 step time 0.46s\n","# Epoch 16  global step 49880 loss 5.04585 batch 650/3281 lr 0.001 accuracy 95.10156 wps 13448.98 step time 0.49s\n","# Epoch 16  global step 49900 loss 5.20992 batch 670/3281 lr 0.001 accuracy 94.74609 wps 14861.97 step time 0.50s\n","# Epoch 16  global step 49920 loss 5.19855 batch 690/3281 lr 0.001 accuracy 94.91406 wps 14428.45 step time 0.46s\n","# Epoch 16  global step 49940 loss 5.14759 batch 710/3281 lr 0.001 accuracy 95.08984 wps 14151.99 step time 0.44s\n","# Epoch 16  global step 49960 loss 5.34912 batch 730/3281 lr 0.001 accuracy 94.29297 wps 14536.90 step time 0.58s\n","# Epoch 16  global step 49980 loss 5.22613 batch 750/3281 lr 0.001 accuracy 94.56250 wps 14025.06 step time 0.53s\n","# Epoch 16  global step 50000 loss 5.31701 batch 770/3281 lr 0.001 accuracy 94.42187 wps 14716.22 step time 0.53s\n","# global step 50000, eval model at Sat May 23 14:41:44 2020\n","2020-05-23 14:41:47.126076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:41:47.126724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 14:41:47.126896: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 14:41:47.126945: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 14:41:47.126974: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 14:41:47.127010: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 14:41:47.127034: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 14:41:47.127056: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 14:41:47.127079: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 14:41:47.127216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:41:47.127586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:41:47.127844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 14:41:47.128350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 14:41:47.128384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 14:41:47.128400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 14:41:47.128663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:41:47.128949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 14:41:47.129206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6542603373237221\n","# location_distance_from_business_district - 0.5634670340025959\n","# location_easy_to_find - 0.7011895842003952\n","# service_wait_time - 0.6659920925975709\n","# service_waiters_attitude - 0.7950012405830297\n","# service_parking_convenience - 0.7440712458316515\n","# service_serving_speed - 0.7561526030204699\n","# price_level - 0.7779316718493386\n","# price_cost_effective - 0.7092521369894857\n","# price_discount - 0.6769661658670779\n","# environment_decoration - 0.7137962725781191\n","# environment_noise - 0.7583213894614178\n","# environment_space - 0.758683596709316\n","# environment_cleaness - 0.7532849109438825\n","# dish_portion - 0.7134023703754987\n","# dish_taste - 0.7276676862432196\n","# dish_look - 0.5712358504789191\n","# dish_recommendation - 0.7373952853953606\n","# others_overall_experience - 0.5884575209697247\n","# others_willing_to_consume_again - 0.7096695634507378\n","# Eval loss 4.51970, f1 0.70381\n","# current result -0.7038099279435767, previous best result -0.705303808284073\n","# No loss decrease, restore previous best model and set learning rate to half of previous one\n","# Epoch 16  global step 42020 loss 5.20095 batch 790/3281 lr 0.0001 accuracy 94.59375 wps 14453.33 step time 0.47s\n","# Epoch 16  global step 42040 loss 5.32613 batch 810/3281 lr 0.0001 accuracy 94.49219 wps 14432.82 step time 0.46s\n","# Epoch 16  global step 42060 loss 5.18835 batch 830/3281 lr 0.0001 accuracy 94.73047 wps 14640.13 step time 0.48s\n","# Epoch 16  global step 42080 loss 5.19463 batch 850/3281 lr 0.0001 accuracy 94.78516 wps 14231.75 step time 0.56s\n","# Epoch 16  global step 42100 loss 5.11727 batch 870/3281 lr 0.0001 accuracy 94.90625 wps 14312.49 step time 0.45s\n","# Epoch 16  global step 42120 loss 5.19723 batch 890/3281 lr 0.0001 accuracy 94.75781 wps 14609.85 step time 0.47s\n","# Epoch 16  global step 42140 loss 5.14323 batch 910/3281 lr 0.0001 accuracy 94.83203 wps 14011.72 step time 0.43s\n","# Epoch 16  global step 42160 loss 5.24150 batch 930/3281 lr 0.0001 accuracy 94.77734 wps 14679.17 step time 0.49s\n","# Epoch 16  global step 42180 loss 5.15563 batch 950/3281 lr 0.0001 accuracy 94.93750 wps 14811.35 step time 0.50s\n","# Epoch 16  global step 42200 loss 5.23169 batch 970/3281 lr 0.0001 accuracy 94.67969 wps 14754.75 step time 0.52s\n","# Epoch 16  global step 42220 loss 5.13186 batch 990/3281 lr 0.0001 accuracy 95.13672 wps 14458.69 step time 0.47s\n","# Epoch 16  global step 42240 loss 5.18047 batch 1010/3281 lr 0.0001 accuracy 94.83203 wps 14234.36 step time 0.45s\n","# Epoch 16  global step 42260 loss 5.18941 batch 1030/3281 lr 0.0001 accuracy 94.69141 wps 14094.79 step time 0.44s\n","# Epoch 16  global step 42280 loss 5.07500 batch 1050/3281 lr 0.0001 accuracy 95.07031 wps 14196.29 step time 0.44s\n","# Epoch 16  global step 42300 loss 5.23143 batch 1070/3281 lr 0.0001 accuracy 94.73047 wps 14581.49 step time 0.48s\n","# Epoch 16  global step 42320 loss 5.19616 batch 1090/3281 lr 0.0001 accuracy 94.78906 wps 14229.12 step time 0.45s\n","# Epoch 16  global step 42340 loss 5.09450 batch 1110/3281 lr 0.0001 accuracy 94.98047 wps 14059.43 step time 0.44s\n","# Epoch 16  global step 42360 loss 5.16105 batch 1130/3281 lr 0.0001 accuracy 94.89453 wps 13950.78 step time 0.60s\n","# Epoch 16  global step 42380 loss 5.26893 batch 1150/3281 lr 0.0001 accuracy 94.61328 wps 14224.44 step time 0.55s\n","# Epoch 16  global step 42400 loss 5.17773 batch 1170/3281 lr 0.0001 accuracy 94.91797 wps 14135.61 step time 0.43s\n","# Epoch 16  global step 42420 loss 5.39388 batch 1190/3281 lr 0.0001 accuracy 94.24219 wps 13887.87 step time 0.64s\n","# Epoch 16  global step 42440 loss 5.11263 batch 1210/3281 lr 0.0001 accuracy 95.05469 wps 13748.78 step time 0.50s\n","# Epoch 16  global step 42460 loss 5.31727 batch 1230/3281 lr 0.0001 accuracy 94.39453 wps 15222.73 step time 0.54s\n","# Epoch 16  global step 42480 loss 5.09488 batch 1250/3281 lr 0.0001 accuracy 95.19922 wps 14229.71 step time 0.44s\n","# Epoch 16  global step 42500 loss 5.18546 batch 1270/3281 lr 0.0001 accuracy 94.81641 wps 14444.48 step time 0.46s\n","# Epoch 16  global step 42520 loss 5.17112 batch 1290/3281 lr 0.0001 accuracy 95.05078 wps 14389.48 step time 0.46s\n","# Epoch 16  global step 42540 loss 5.10627 batch 1310/3281 lr 0.0001 accuracy 95.02734 wps 14247.91 step time 0.45s\n","# Epoch 16  global step 42560 loss 5.12668 batch 1330/3281 lr 0.0001 accuracy 94.92969 wps 14614.99 step time 0.49s\n","# Epoch 16  global step 42580 loss 5.17318 batch 1350/3281 lr 0.0001 accuracy 95.01563 wps 14352.44 step time 0.46s\n","# Epoch 16  global step 42600 loss 5.14534 batch 1370/3281 lr 0.0001 accuracy 95.01563 wps 14016.83 step time 0.44s\n","# Epoch 16  global step 42620 loss 5.26569 batch 1390/3281 lr 0.0001 accuracy 94.66016 wps 14598.54 step time 0.48s\n","# Epoch 16  global step 42640 loss 5.18149 batch 1410/3281 lr 0.0001 accuracy 94.73828 wps 14271.26 step time 0.44s\n","# Epoch 16  global step 42660 loss 5.24101 batch 1430/3281 lr 0.0001 accuracy 94.65234 wps 14355.54 step time 0.46s\n","# Epoch 16  global step 42680 loss 5.08343 batch 1450/3281 lr 0.0001 accuracy 95.04687 wps 14013.07 step time 0.43s\n","# Epoch 16  global step 42700 loss 5.07894 batch 1470/3281 lr 0.0001 accuracy 94.96484 wps 14411.92 step time 0.47s\n","# Epoch 16  global step 42720 loss 5.21970 batch 1490/3281 lr 0.0001 accuracy 94.67578 wps 13978.94 step time 0.45s\n","# Epoch 16  global step 42740 loss 5.38467 batch 1510/3281 lr 0.0001 accuracy 94.40625 wps 14923.57 step time 0.53s\n","# Epoch 16  global step 42760 loss 5.13128 batch 1530/3281 lr 0.0001 accuracy 95.01172 wps 14490.30 step time 0.46s\n","# Epoch 16  global step 42780 loss 5.26222 batch 1550/3281 lr 0.0001 accuracy 94.63672 wps 14496.15 step time 0.47s\n","# Epoch 16  global step 42800 loss 5.09568 batch 1570/3281 lr 0.0001 accuracy 95.08203 wps 14321.99 step time 0.45s\n","# Epoch 16  global step 42820 loss 5.18079 batch 1590/3281 lr 0.0001 accuracy 94.87109 wps 13553.48 step time 0.49s\n","# Epoch 16  global step 42840 loss 5.19153 batch 1610/3281 lr 0.0001 accuracy 94.70703 wps 13950.67 step time 0.53s\n","# Epoch 16  global step 42860 loss 5.08369 batch 1630/3281 lr 0.0001 accuracy 95.19922 wps 14347.61 step time 0.47s\n","# Epoch 16  global step 42880 loss 5.24467 batch 1650/3281 lr 0.0001 accuracy 94.49219 wps 13748.74 step time 0.65s\n","# Epoch 16  global step 42900 loss 5.14026 batch 1670/3281 lr 0.0001 accuracy 94.88672 wps 14642.13 step time 0.49s\n","# Epoch 16  global step 42920 loss 5.14667 batch 1690/3281 lr 0.0001 accuracy 94.73437 wps 14398.01 step time 0.45s\n","# Epoch 16  global step 42940 loss 5.13465 batch 1710/3281 lr 0.0001 accuracy 94.96875 wps 14160.75 step time 0.45s\n","# Epoch 16  global step 42960 loss 5.21884 batch 1730/3281 lr 0.0001 accuracy 94.70313 wps 14284.47 step time 0.58s\n","# Epoch 16  global step 42980 loss 5.18532 batch 1750/3281 lr 0.0001 accuracy 94.64453 wps 14025.84 step time 0.54s\n","# Epoch 16  global step 43000 loss 5.39960 batch 1770/3281 lr 0.0001 accuracy 94.21484 wps 14941.21 step time 0.66s\n","# Epoch 16  global step 43020 loss 5.11992 batch 1790/3281 lr 0.0001 accuracy 94.83594 wps 14555.22 step time 0.48s\n","# Epoch 16  global step 43040 loss 5.15311 batch 1810/3281 lr 0.0001 accuracy 94.70703 wps 14454.48 step time 0.47s\n","# Epoch 16  global step 43060 loss 5.35503 batch 1830/3281 lr 0.0001 accuracy 94.40625 wps 14675.99 step time 0.50s\n","# Epoch 16  global step 43080 loss 5.18060 batch 1850/3281 lr 0.0001 accuracy 94.83984 wps 14529.39 step time 0.48s\n","# Epoch 16  global step 43100 loss 5.26200 batch 1870/3281 lr 0.0001 accuracy 94.62109 wps 14848.10 step time 0.54s\n","# Epoch 16  global step 43120 loss 5.09148 batch 1890/3281 lr 0.0001 accuracy 95.03516 wps 14094.98 step time 0.45s\n","# Epoch 16  global step 43140 loss 5.06595 batch 1910/3281 lr 0.0001 accuracy 95.25391 wps 13606.93 step time 0.51s\n","# Epoch 16  global step 43160 loss 5.24221 batch 1930/3281 lr 0.0001 accuracy 94.54687 wps 14854.25 step time 0.52s\n","# Epoch 16  global step 43180 loss 5.26448 batch 1950/3281 lr 0.0001 accuracy 94.59375 wps 14947.29 step time 0.55s\n","# Epoch 16  global step 43200 loss 5.20268 batch 1970/3281 lr 0.0001 accuracy 94.60937 wps 15113.35 step time 0.54s\n","# Epoch 16  global step 43220 loss 5.13829 batch 1990/3281 lr 0.0001 accuracy 94.87109 wps 14451.05 step time 0.46s\n","# Epoch 16  global step 43240 loss 5.15079 batch 2010/3281 lr 0.0001 accuracy 94.92187 wps 13938.49 step time 0.55s\n","# Epoch 16  global step 43260 loss 5.16499 batch 2030/3281 lr 0.0001 accuracy 95.00391 wps 13493.82 step time 0.60s\n","# Epoch 16  global step 43280 loss 5.21629 batch 2050/3281 lr 0.0001 accuracy 94.71875 wps 13618.33 step time 0.59s\n","# Epoch 16  global step 43300 loss 5.06037 batch 2070/3281 lr 0.0001 accuracy 95.16016 wps 13897.14 step time 0.42s\n","# Epoch 16  global step 43320 loss 5.32458 batch 2090/3281 lr 0.0001 accuracy 94.33984 wps 14989.51 step time 0.50s\n","# Epoch 16  global step 43340 loss 5.11509 batch 2110/3281 lr 0.0001 accuracy 95.08984 wps 13651.87 step time 0.49s\n","# Epoch 16  global step 43360 loss 5.09859 batch 2130/3281 lr 0.0001 accuracy 94.94531 wps 13408.56 step time 0.49s\n","# Epoch 16  global step 43380 loss 5.10524 batch 2150/3281 lr 0.0001 accuracy 95.16016 wps 13839.13 step time 0.42s\n","# Epoch 16  global step 43400 loss 5.20050 batch 2170/3281 lr 0.0001 accuracy 94.73047 wps 14461.49 step time 0.47s\n","# Epoch 16  global step 43420 loss 5.31262 batch 2190/3281 lr 0.0001 accuracy 94.46484 wps 14736.61 step time 0.50s\n","# Epoch 16  global step 43440 loss 5.13392 batch 2210/3281 lr 0.0001 accuracy 95.07812 wps 14212.16 step time 0.45s\n","# Epoch 16  global step 43460 loss 5.25379 batch 2230/3281 lr 0.0001 accuracy 94.38672 wps 14478.44 step time 0.67s\n","# Epoch 16  global step 43480 loss 5.23956 batch 2250/3281 lr 0.0001 accuracy 94.53906 wps 14419.83 step time 0.57s\n","# Epoch 16  global step 43500 loss 5.33188 batch 2270/3281 lr 0.0001 accuracy 94.42969 wps 14216.12 step time 0.65s\n","# Epoch 16  global step 43520 loss 4.95325 batch 2290/3281 lr 0.0001 accuracy 95.39844 wps 13002.38 step time 0.46s\n","# Epoch 16  global step 43540 loss 5.23708 batch 2310/3281 lr 0.0001 accuracy 94.53906 wps 12726.34 step time 0.64s\n","# Epoch 16  global step 43560 loss 5.20785 batch 2330/3281 lr 0.0001 accuracy 94.78125 wps 13087.48 step time 0.49s\n","# Epoch 16  global step 43580 loss 5.22840 batch 2350/3281 lr 0.0001 accuracy 94.80078 wps 12171.45 step time 0.59s\n","# Epoch 16  global step 43600 loss 5.24960 batch 2370/3281 lr 0.0001 accuracy 94.78906 wps 12114.60 step time 0.71s\n","# Epoch 16  global step 43620 loss 5.10754 batch 2390/3281 lr 0.0001 accuracy 95.08594 wps 11926.85 step time 0.62s\n","# Epoch 16  global step 43640 loss 5.21759 batch 2410/3281 lr 0.0001 accuracy 94.66016 wps 11802.83 step time 0.69s\n","# Epoch 16  global step 43660 loss 5.27945 batch 2430/3281 lr 0.0001 accuracy 94.64063 wps 13283.60 step time 0.54s\n","# Epoch 16  global step 43680 loss 5.13389 batch 2450/3281 lr 0.0001 accuracy 94.90234 wps 12317.04 step time 0.60s\n","# Epoch 16  global step 43700 loss 5.11305 batch 2470/3281 lr 0.0001 accuracy 94.98047 wps 12079.10 step time 0.65s\n","# Epoch 16  global step 43720 loss 5.18524 batch 2490/3281 lr 0.0001 accuracy 94.70703 wps 12278.53 step time 0.64s\n","# Epoch 16  global step 43740 loss 5.18091 batch 2510/3281 lr 0.0001 accuracy 94.69141 wps 12712.32 step time 0.64s\n","# Epoch 16  global step 43760 loss 5.32341 batch 2530/3281 lr 0.0001 accuracy 94.45703 wps 12325.72 step time 0.75s\n","# Epoch 16  global step 43780 loss 5.17838 batch 2550/3281 lr 0.0001 accuracy 94.66797 wps 12565.82 step time 0.61s\n","# Epoch 16  global step 43800 loss 5.12706 batch 2570/3281 lr 0.0001 accuracy 95.01953 wps 13455.88 step time 0.53s\n","# Epoch 16  global step 43820 loss 5.37981 batch 2590/3281 lr 0.0001 accuracy 94.31641 wps 11809.25 step time 0.78s\n","# Epoch 16  global step 43840 loss 5.23664 batch 2610/3281 lr 0.0001 accuracy 94.87891 wps 11581.81 step time 0.71s\n","# Epoch 16  global step 43860 loss 5.23003 batch 2630/3281 lr 0.0001 accuracy 94.75391 wps 12686.39 step time 0.51s\n","# Epoch 16  global step 43880 loss 5.26791 batch 2650/3281 lr 0.0001 accuracy 94.50000 wps 11832.96 step time 0.72s\n","# Epoch 16  global step 43900 loss 5.25301 batch 2670/3281 lr 0.0001 accuracy 94.65234 wps 12297.57 step time 0.63s\n","# Epoch 16  global step 43920 loss 5.23633 batch 2690/3281 lr 0.0001 accuracy 94.58594 wps 12972.61 step time 0.60s\n","# Epoch 16  global step 43940 loss 5.28011 batch 2710/3281 lr 0.0001 accuracy 94.58203 wps 12430.34 step time 0.67s\n","# Epoch 16  global step 43960 loss 5.14185 batch 2730/3281 lr 0.0001 accuracy 94.84375 wps 12258.36 step time 0.63s\n","# Epoch 16  global step 43980 loss 5.17985 batch 2750/3281 lr 0.0001 accuracy 94.80859 wps 12397.97 step time 0.55s\n","# Epoch 16  global step 44000 loss 5.28480 batch 2770/3281 lr 0.0001 accuracy 94.52344 wps 12734.35 step time 0.62s\n","# global step 44000, eval model at Sat May 23 15:01:40 2020\n","2020-05-23 15:01:42.526581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:01:42.526897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 15:01:42.527005: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 15:01:42.527031: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 15:01:42.527053: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 15:01:42.527076: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 15:01:42.527098: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 15:01:42.527118: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 15:01:42.527139: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 15:01:42.527225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:01:42.527513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:01:42.527755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 15:01:42.527818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 15:01:42.527833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 15:01:42.527843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 15:01:42.527948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:01:42.528218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:01:42.528426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6599671917570604\n","# location_distance_from_business_district - 0.562754775338728\n","# location_easy_to_find - 0.7053696665185718\n","# service_wait_time - 0.6655306637207483\n","# service_waiters_attitude - 0.7966950662469178\n","# service_parking_convenience - 0.7457988895301492\n","# service_serving_speed - 0.758292205443701\n","# price_level - 0.7764059025669756\n","# price_cost_effective - 0.7127496442396721\n","# price_discount - 0.6689760713903643\n","# environment_decoration - 0.7155042967246178\n","# environment_noise - 0.7585359832755656\n","# environment_space - 0.7587508430733202\n","# environment_cleaness - 0.7547829553352371\n","# dish_portion - 0.715876479131937\n","# dish_taste - 0.7295789091415169\n","# dish_look - 0.5758372765730728\n","# dish_recommendation - 0.7330208007480776\n","# others_overall_experience - 0.5906286692193434\n","# others_willing_to_consume_again - 0.709117376980684\n","# Eval loss 4.47702, f1 0.70471\n","# current result -0.704708683347813, previous best result -0.705303808284073\n","# Epoch 16  global step 44020 loss 5.14673 batch 2790/3281 lr 0.0001 accuracy 94.82813 wps 12174.96 step time 0.61s\n","# Epoch 16  global step 44040 loss 5.17785 batch 2810/3281 lr 0.0001 accuracy 94.82422 wps 12613.55 step time 0.49s\n","# Epoch 16  global step 44060 loss 5.09951 batch 2830/3281 lr 0.0001 accuracy 95.12500 wps 11599.19 step time 0.61s\n","# Epoch 16  global step 44080 loss 5.08471 batch 2850/3281 lr 0.0001 accuracy 95.12500 wps 12786.11 step time 0.54s\n","# Epoch 16  global step 44100 loss 5.16634 batch 2870/3281 lr 0.0001 accuracy 94.84766 wps 11968.19 step time 0.59s\n","# Epoch 16  global step 44120 loss 5.23956 batch 2890/3281 lr 0.0001 accuracy 94.61328 wps 12067.53 step time 0.67s\n","# Epoch 16  global step 44140 loss 5.20293 batch 2910/3281 lr 0.0001 accuracy 94.68750 wps 12686.51 step time 0.54s\n","# Epoch 16  global step 44160 loss 5.29264 batch 2930/3281 lr 0.0001 accuracy 94.58984 wps 11912.44 step time 0.63s\n","# Epoch 16  global step 44180 loss 5.12396 batch 2950/3281 lr 0.0001 accuracy 94.93359 wps 12917.28 step time 0.54s\n","# Epoch 16  global step 44200 loss 5.21134 batch 2970/3281 lr 0.0001 accuracy 94.83594 wps 12467.09 step time 0.55s\n","# Epoch 16  global step 44220 loss 5.23373 batch 2990/3281 lr 0.0001 accuracy 94.73828 wps 12938.26 step time 0.61s\n","# Epoch 16  global step 44240 loss 5.22264 batch 3010/3281 lr 0.0001 accuracy 94.70703 wps 12898.66 step time 0.65s\n","# Epoch 16  global step 44260 loss 5.15637 batch 3030/3281 lr 0.0001 accuracy 94.86328 wps 13017.13 step time 0.55s\n","# Epoch 16  global step 44280 loss 5.02115 batch 3050/3281 lr 0.0001 accuracy 95.19922 wps 12798.97 step time 0.49s\n","# Epoch 16  global step 44300 loss 5.15536 batch 3070/3281 lr 0.0001 accuracy 95.03906 wps 12778.00 step time 0.50s\n","# Epoch 16  global step 44320 loss 5.31646 batch 3090/3281 lr 0.0001 accuracy 94.53125 wps 13014.59 step time 0.61s\n","# Epoch 16  global step 44340 loss 5.15217 batch 3110/3281 lr 0.0001 accuracy 94.98437 wps 12941.67 step time 0.55s\n","# Epoch 16  global step 44360 loss 5.33518 batch 3130/3281 lr 0.0001 accuracy 94.31641 wps 13104.08 step time 0.56s\n","# Epoch 16  global step 44380 loss 5.17953 batch 3150/3281 lr 0.0001 accuracy 94.78125 wps 11971.61 step time 0.64s\n","# Epoch 16  global step 44400 loss 5.11992 batch 3170/3281 lr 0.0001 accuracy 94.93359 wps 11858.48 step time 0.58s\n","# Epoch 16  global step 44420 loss 5.16024 batch 3190/3281 lr 0.0001 accuracy 94.87891 wps 12857.17 step time 0.58s\n","# Epoch 16  global step 44440 loss 5.08733 batch 3210/3281 lr 0.0001 accuracy 95.09766 wps 12542.94 step time 0.52s\n","# Epoch 16  global step 44460 loss 5.18708 batch 3230/3281 lr 0.0001 accuracy 94.72266 wps 12676.59 step time 0.52s\n","# Epoch 16  global step 44480 loss 5.15174 batch 3250/3281 lr 0.0001 accuracy 95.03906 wps 12879.39 step time 0.48s\n","# Epoch 16  global step 44500 loss 5.18689 batch 3270/3281 lr 0.0001 accuracy 94.71094 wps 12690.00 step time 0.55s\n","# Finsh epoch 16, global step 44512\n","# Epoch 17  global step 44520 loss 2.12307 batch 8/3281 lr 0.0001 accuracy 37.71094 wps 14101.90 step time 0.28s\n","# Epoch 17  global step 44540 loss 5.15630 batch 28/3281 lr 0.0001 accuracy 94.76953 wps 14054.67 step time 0.56s\n","# Epoch 17  global step 44560 loss 5.23437 batch 48/3281 lr 0.0001 accuracy 94.57812 wps 14500.33 step time 0.56s\n","# Epoch 17  global step 44580 loss 5.22841 batch 68/3281 lr 0.0001 accuracy 94.68359 wps 14313.93 step time 0.69s\n","# Epoch 17  global step 44600 loss 5.16851 batch 88/3281 lr 0.0001 accuracy 94.82813 wps 13863.22 step time 0.52s\n","# Epoch 17  global step 44620 loss 5.26882 batch 108/3281 lr 0.0001 accuracy 94.52734 wps 15021.22 step time 0.54s\n","# Epoch 17  global step 44640 loss 5.15240 batch 128/3281 lr 0.0001 accuracy 94.86328 wps 13812.87 step time 0.52s\n","# Epoch 17  global step 44660 loss 5.11335 batch 148/3281 lr 0.0001 accuracy 95.03516 wps 14312.86 step time 0.44s\n","# Epoch 17  global step 44680 loss 5.21302 batch 168/3281 lr 0.0001 accuracy 94.80859 wps 13969.11 step time 0.53s\n","# Epoch 17  global step 44700 loss 5.05560 batch 188/3281 lr 0.0001 accuracy 95.33203 wps 14080.75 step time 0.43s\n","# Epoch 17  global step 44720 loss 5.11439 batch 208/3281 lr 0.0001 accuracy 94.93359 wps 14412.33 step time 0.45s\n","# Epoch 17  global step 44740 loss 5.12166 batch 228/3281 lr 0.0001 accuracy 95.01953 wps 14117.20 step time 0.43s\n","# Epoch 17  global step 44760 loss 5.31775 batch 248/3281 lr 0.0001 accuracy 94.35547 wps 14111.68 step time 0.74s\n","# Epoch 17  global step 44780 loss 5.26841 batch 268/3281 lr 0.0001 accuracy 94.30078 wps 13930.19 step time 0.65s\n","# Epoch 17  global step 44800 loss 5.12600 batch 288/3281 lr 0.0001 accuracy 94.88281 wps 14471.98 step time 0.46s\n","# Epoch 17  global step 44820 loss 5.22551 batch 308/3281 lr 0.0001 accuracy 94.67969 wps 13977.18 step time 0.64s\n","# Epoch 17  global step 44840 loss 5.11307 batch 328/3281 lr 0.0001 accuracy 95.00391 wps 14301.53 step time 0.46s\n","# Epoch 17  global step 44860 loss 5.05795 batch 348/3281 lr 0.0001 accuracy 94.96094 wps 14539.60 step time 0.47s\n","# Epoch 17  global step 44880 loss 5.13489 batch 368/3281 lr 0.0001 accuracy 95.02344 wps 13988.47 step time 0.53s\n","# Epoch 17  global step 44900 loss 5.02042 batch 388/3281 lr 0.0001 accuracy 95.38281 wps 14111.03 step time 0.42s\n","# Epoch 17  global step 44920 loss 5.12675 batch 408/3281 lr 0.0001 accuracy 94.92188 wps 14714.77 step time 0.49s\n","# Epoch 17  global step 44940 loss 5.04266 batch 428/3281 lr 0.0001 accuracy 95.24219 wps 13563.02 step time 0.49s\n","# Epoch 17  global step 44960 loss 5.12864 batch 448/3281 lr 0.0001 accuracy 94.96875 wps 14636.81 step time 0.48s\n","# Epoch 17  global step 44980 loss 5.35999 batch 468/3281 lr 0.0001 accuracy 94.27344 wps 14263.04 step time 0.64s\n","# Epoch 17  global step 45000 loss 5.11966 batch 488/3281 lr 0.0001 accuracy 95.00781 wps 14385.98 step time 0.46s\n","# Epoch 17  global step 45020 loss 5.27723 batch 508/3281 lr 0.0001 accuracy 94.40234 wps 14798.52 step time 0.63s\n","# Epoch 17  global step 45040 loss 5.13789 batch 528/3281 lr 0.0001 accuracy 94.96875 wps 13851.21 step time 0.51s\n","# Epoch 17  global step 45060 loss 5.19906 batch 548/3281 lr 0.0001 accuracy 94.71484 wps 14282.50 step time 0.55s\n","# Epoch 17  global step 45080 loss 5.10068 batch 568/3281 lr 0.0001 accuracy 94.85156 wps 13272.81 step time 0.60s\n","# Epoch 17  global step 45100 loss 5.08985 batch 588/3281 lr 0.0001 accuracy 95.17578 wps 14160.52 step time 0.45s\n","# Epoch 17  global step 45120 loss 5.18802 batch 608/3281 lr 0.0001 accuracy 94.80859 wps 14803.88 step time 0.50s\n","# Epoch 17  global step 45140 loss 5.13426 batch 628/3281 lr 0.0001 accuracy 95.08984 wps 14570.05 step time 0.49s\n","# Epoch 17  global step 45160 loss 5.19679 batch 648/3281 lr 0.0001 accuracy 94.78906 wps 14536.87 step time 0.57s\n","# Epoch 17  global step 45180 loss 5.12076 batch 668/3281 lr 0.0001 accuracy 95.02734 wps 14197.85 step time 0.45s\n","# Epoch 17  global step 45200 loss 5.19268 batch 688/3281 lr 0.0001 accuracy 94.66406 wps 14952.59 step time 0.53s\n","# Epoch 17  global step 45220 loss 5.13615 batch 708/3281 lr 0.0001 accuracy 94.85937 wps 15166.38 step time 0.54s\n","# Epoch 17  global step 45240 loss 5.08074 batch 728/3281 lr 0.0001 accuracy 95.19141 wps 14318.69 step time 0.44s\n","# Epoch 17  global step 45260 loss 5.20781 batch 748/3281 lr 0.0001 accuracy 94.73828 wps 14731.54 step time 0.48s\n","# Epoch 17  global step 45280 loss 5.11036 batch 768/3281 lr 0.0001 accuracy 95.07812 wps 13693.39 step time 0.50s\n","# Epoch 17  global step 45300 loss 5.13098 batch 788/3281 lr 0.0001 accuracy 94.92969 wps 14293.45 step time 0.45s\n","# Epoch 17  global step 45320 loss 5.21931 batch 808/3281 lr 0.0001 accuracy 94.71484 wps 14509.61 step time 0.57s\n","# Epoch 17  global step 45340 loss 5.11956 batch 828/3281 lr 0.0001 accuracy 94.91406 wps 14463.63 step time 0.48s\n","# Epoch 17  global step 45360 loss 5.05899 batch 848/3281 lr 0.0001 accuracy 95.21875 wps 14273.18 step time 0.44s\n","# Epoch 17  global step 45380 loss 5.19564 batch 868/3281 lr 0.0001 accuracy 94.96875 wps 15021.64 step time 0.51s\n","# Epoch 17  global step 45400 loss 5.18096 batch 888/3281 lr 0.0001 accuracy 94.82813 wps 14771.64 step time 0.51s\n","# Epoch 17  global step 45420 loss 5.20479 batch 908/3281 lr 0.0001 accuracy 94.71484 wps 14735.61 step time 0.49s\n","# Epoch 17  global step 45440 loss 5.16325 batch 928/3281 lr 0.0001 accuracy 94.91406 wps 14061.41 step time 0.57s\n","# Epoch 17  global step 45460 loss 5.25182 batch 948/3281 lr 0.0001 accuracy 94.65234 wps 14474.34 step time 0.62s\n","# Epoch 17  global step 45480 loss 5.19016 batch 968/3281 lr 0.0001 accuracy 94.62500 wps 14161.69 step time 0.59s\n","# Epoch 17  global step 45500 loss 5.07550 batch 988/3281 lr 0.0001 accuracy 95.16797 wps 14138.99 step time 0.44s\n","# Epoch 17  global step 45520 loss 5.06538 batch 1008/3281 lr 0.0001 accuracy 95.07813 wps 13271.90 step time 0.49s\n","# Epoch 17  global step 45540 loss 5.16086 batch 1028/3281 lr 0.0001 accuracy 94.91797 wps 13971.24 step time 0.53s\n","# Epoch 17  global step 45560 loss 5.24184 batch 1048/3281 lr 0.0001 accuracy 94.63281 wps 14151.23 step time 0.58s\n","# Epoch 17  global step 45580 loss 5.06333 batch 1068/3281 lr 0.0001 accuracy 95.16406 wps 13963.53 step time 0.43s\n","# Epoch 17  global step 45600 loss 5.13083 batch 1088/3281 lr 0.0001 accuracy 95.06250 wps 14208.71 step time 0.45s\n","# Epoch 17  global step 45620 loss 5.23471 batch 1108/3281 lr 0.0001 accuracy 94.83203 wps 14386.40 step time 0.47s\n","# Epoch 17  global step 45640 loss 5.10031 batch 1128/3281 lr 0.0001 accuracy 94.80469 wps 14111.52 step time 0.44s\n","# Epoch 17  global step 45660 loss 5.01001 batch 1148/3281 lr 0.0001 accuracy 95.35547 wps 14174.84 step time 0.44s\n","# Epoch 17  global step 45680 loss 5.20194 batch 1168/3281 lr 0.0001 accuracy 94.73828 wps 13751.65 step time 0.68s\n","# Epoch 17  global step 45700 loss 5.00843 batch 1188/3281 lr 0.0001 accuracy 95.26562 wps 14235.32 step time 0.47s\n","# Epoch 17  global step 45720 loss 5.07410 batch 1208/3281 lr 0.0001 accuracy 95.16797 wps 14353.88 step time 0.46s\n","# Epoch 17  global step 45740 loss 5.16961 batch 1228/3281 lr 0.0001 accuracy 94.84375 wps 14517.34 step time 0.51s\n","# Epoch 17  global step 45760 loss 5.17521 batch 1248/3281 lr 0.0001 accuracy 94.76953 wps 14776.99 step time 0.52s\n","# Epoch 17  global step 45780 loss 5.14746 batch 1268/3281 lr 0.0001 accuracy 94.76953 wps 14342.47 step time 0.47s\n","# Epoch 17  global step 45800 loss 5.14336 batch 1288/3281 lr 0.0001 accuracy 94.82422 wps 14530.82 step time 0.49s\n","# Epoch 17  global step 45820 loss 5.26897 batch 1308/3281 lr 0.0001 accuracy 94.56641 wps 14278.53 step time 0.60s\n","# Epoch 17  global step 45840 loss 5.27775 batch 1328/3281 lr 0.0001 accuracy 94.50781 wps 14836.72 step time 0.52s\n","# Epoch 17  global step 45860 loss 5.09107 batch 1348/3281 lr 0.0001 accuracy 95.13281 wps 14666.69 step time 0.51s\n","# Epoch 17  global step 45880 loss 5.12011 batch 1368/3281 lr 0.0001 accuracy 94.89062 wps 14552.74 step time 0.48s\n","# Epoch 17  global step 45900 loss 5.24810 batch 1388/3281 lr 0.0001 accuracy 94.69531 wps 14862.36 step time 0.56s\n","# Epoch 17  global step 45920 loss 5.24385 batch 1408/3281 lr 0.0001 accuracy 94.60156 wps 14758.41 step time 0.52s\n","# Epoch 17  global step 45940 loss 5.18048 batch 1428/3281 lr 0.0001 accuracy 94.97266 wps 14448.36 step time 0.48s\n","# Epoch 17  global step 45960 loss 5.17568 batch 1448/3281 lr 0.0001 accuracy 94.81250 wps 14371.45 step time 0.48s\n","# Epoch 17  global step 45980 loss 5.06352 batch 1468/3281 lr 0.0001 accuracy 95.10938 wps 13735.78 step time 0.42s\n","# Epoch 17  global step 46000 loss 5.16805 batch 1488/3281 lr 0.0001 accuracy 94.96094 wps 14461.27 step time 0.60s\n","# global step 46000, eval model at Sat May 23 15:21:34 2020\n","2020-05-23 15:21:36.648178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:21:36.648742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 15:21:36.648886: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 15:21:36.648930: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 15:21:36.648964: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 15:21:36.648995: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 15:21:36.649023: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 15:21:36.649071: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 15:21:36.649103: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 15:21:36.649259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:21:36.649640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:21:36.649914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 15:21:36.650342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 15:21:36.650383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 15:21:36.650414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 15:21:36.650639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:21:36.650944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:21:36.651183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6629245467069647\n","# location_distance_from_business_district - 0.5675316953119142\n","# location_easy_to_find - 0.703289669059652\n","# service_wait_time - 0.6668655873632421\n","# service_waiters_attitude - 0.7962216115115435\n","# service_parking_convenience - 0.7424069710829831\n","# service_serving_speed - 0.7599868900420849\n","# price_level - 0.7771714735443584\n","# price_cost_effective - 0.7091368421011367\n","# price_discount - 0.6713306875134372\n","# environment_decoration - 0.7139353649020299\n","# environment_noise - 0.7569848574344953\n","# environment_space - 0.7566825099461976\n","# environment_cleaness - 0.7558915895403179\n","# dish_portion - 0.7151119163337891\n","# dish_taste - 0.726594620728356\n","# dish_look - 0.5732159712604289\n","# dish_recommendation - 0.73286236566598\n","# others_overall_experience - 0.5908669715505477\n","# others_willing_to_consume_again - 0.7080827302015262\n","# Eval loss 4.49070, f1 0.70435\n","# current result -0.7043547435900493, previous best result -0.705303808284073\n","# Epoch 17  global step 46020 loss 5.13077 batch 1508/3281 lr 0.0001 accuracy 95.11328 wps 14246.98 step time 0.45s\n","# Epoch 17  global step 46040 loss 5.23647 batch 1528/3281 lr 0.0001 accuracy 94.63672 wps 14208.73 step time 0.59s\n","# Epoch 17  global step 46060 loss 5.11351 batch 1548/3281 lr 0.0001 accuracy 95.09766 wps 14327.83 step time 0.47s\n","# Epoch 17  global step 46080 loss 5.23405 batch 1568/3281 lr 0.0001 accuracy 94.68750 wps 14769.47 step time 0.60s\n","# Epoch 17  global step 46100 loss 5.20416 batch 1588/3281 lr 0.0001 accuracy 94.99219 wps 13706.47 step time 0.60s\n","# Epoch 17  global step 46120 loss 5.11504 batch 1608/3281 lr 0.0001 accuracy 95.07422 wps 14718.08 step time 0.51s\n","# Epoch 17  global step 46140 loss 5.10044 batch 1628/3281 lr 0.0001 accuracy 95.00000 wps 14303.83 step time 0.47s\n","# Epoch 17  global step 46160 loss 5.21769 batch 1648/3281 lr 0.0001 accuracy 94.83984 wps 14572.04 step time 0.48s\n","# Epoch 17  global step 46180 loss 5.21604 batch 1668/3281 lr 0.0001 accuracy 94.65234 wps 14742.72 step time 0.59s\n","# Epoch 17  global step 46200 loss 5.14402 batch 1688/3281 lr 0.0001 accuracy 95.20313 wps 14284.00 step time 0.45s\n","# Epoch 17  global step 46220 loss 5.13952 batch 1708/3281 lr 0.0001 accuracy 94.87109 wps 14330.11 step time 0.59s\n","# Epoch 17  global step 46240 loss 5.11976 batch 1728/3281 lr 0.0001 accuracy 95.06641 wps 14313.01 step time 0.46s\n","# Epoch 17  global step 46260 loss 5.26302 batch 1748/3281 lr 0.0001 accuracy 94.66016 wps 14674.36 step time 0.49s\n","# Epoch 17  global step 46280 loss 5.05083 batch 1768/3281 lr 0.0001 accuracy 95.07813 wps 13797.16 step time 0.41s\n","# Epoch 17  global step 46300 loss 5.11680 batch 1788/3281 lr 0.0001 accuracy 95.02344 wps 13631.41 step time 0.50s\n","# Epoch 17  global step 46320 loss 5.11272 batch 1808/3281 lr 0.0001 accuracy 94.94922 wps 14305.80 step time 0.46s\n","# Epoch 17  global step 46340 loss 5.01419 batch 1828/3281 lr 0.0001 accuracy 95.37109 wps 14159.44 step time 0.44s\n","# Epoch 17  global step 46360 loss 5.05400 batch 1848/3281 lr 0.0001 accuracy 95.07812 wps 14250.24 step time 0.44s\n","# Epoch 17  global step 46380 loss 5.09326 batch 1868/3281 lr 0.0001 accuracy 94.92578 wps 14554.19 step time 0.48s\n","# Epoch 17  global step 46400 loss 5.09050 batch 1888/3281 lr 0.0001 accuracy 95.25000 wps 13939.90 step time 0.43s\n","# Epoch 17  global step 46420 loss 5.16245 batch 1908/3281 lr 0.0001 accuracy 94.85937 wps 14113.69 step time 0.45s\n","# Epoch 17  global step 46440 loss 5.05947 batch 1928/3281 lr 0.0001 accuracy 95.10156 wps 14135.44 step time 0.45s\n","# Epoch 17  global step 46460 loss 5.10529 batch 1948/3281 lr 0.0001 accuracy 94.89844 wps 14456.03 step time 0.47s\n","# Epoch 17  global step 46480 loss 5.06196 batch 1968/3281 lr 0.0001 accuracy 95.09375 wps 13561.09 step time 0.41s\n","# Epoch 17  global step 46500 loss 5.20322 batch 1988/3281 lr 0.0001 accuracy 94.75000 wps 15024.40 step time 0.55s\n","# Epoch 17  global step 46520 loss 5.31679 batch 2008/3281 lr 0.0001 accuracy 94.39063 wps 14488.81 step time 0.60s\n","# Epoch 17  global step 46540 loss 5.21041 batch 2028/3281 lr 0.0001 accuracy 94.66016 wps 14729.99 step time 0.48s\n","# Epoch 17  global step 46560 loss 5.30906 batch 2048/3281 lr 0.0001 accuracy 94.50000 wps 14209.10 step time 0.56s\n","# Epoch 17  global step 46580 loss 5.28760 batch 2068/3281 lr 0.0001 accuracy 94.50781 wps 14784.82 step time 0.50s\n","# Epoch 17  global step 46600 loss 5.09620 batch 2088/3281 lr 0.0001 accuracy 94.93750 wps 14380.61 step time 0.46s\n","# Epoch 17  global step 46620 loss 5.20452 batch 2108/3281 lr 0.0001 accuracy 94.72266 wps 14365.87 step time 0.56s\n","# Epoch 17  global step 46640 loss 5.20371 batch 2128/3281 lr 0.0001 accuracy 94.70312 wps 14191.00 step time 0.57s\n","# Epoch 17  global step 46660 loss 5.04598 batch 2148/3281 lr 0.0001 accuracy 95.16406 wps 14660.63 step time 0.47s\n","# Epoch 17  global step 46680 loss 5.14982 batch 2168/3281 lr 0.0001 accuracy 94.82031 wps 14075.76 step time 0.56s\n","# Epoch 17  global step 46700 loss 5.23712 batch 2188/3281 lr 0.0001 accuracy 94.75000 wps 14872.63 step time 0.51s\n","# Epoch 17  global step 46720 loss 5.10373 batch 2208/3281 lr 0.0001 accuracy 95.10938 wps 14429.77 step time 0.46s\n","# Epoch 17  global step 46740 loss 5.22896 batch 2228/3281 lr 0.0001 accuracy 94.85156 wps 14706.56 step time 0.50s\n","# Epoch 17  global step 46760 loss 5.09471 batch 2248/3281 lr 0.0001 accuracy 94.90625 wps 14605.43 step time 0.48s\n","# Epoch 17  global step 46780 loss 5.10565 batch 2268/3281 lr 0.0001 accuracy 95.00781 wps 14752.35 step time 0.50s\n","# Epoch 17  global step 46800 loss 5.10066 batch 2288/3281 lr 0.0001 accuracy 95.00391 wps 14786.99 step time 0.49s\n","# Epoch 17  global step 46820 loss 5.12194 batch 2308/3281 lr 0.0001 accuracy 94.92188 wps 14620.49 step time 0.49s\n","# Epoch 17  global step 46840 loss 5.06748 batch 2328/3281 lr 0.0001 accuracy 95.12109 wps 14105.16 step time 0.43s\n","# Epoch 17  global step 46860 loss 5.13846 batch 2348/3281 lr 0.0001 accuracy 95.12500 wps 12430.27 step time 0.52s\n","# Epoch 17  global step 46880 loss 5.13403 batch 2368/3281 lr 0.0001 accuracy 94.94141 wps 12097.06 step time 0.60s\n","# Epoch 17  global step 46900 loss 5.14936 batch 2388/3281 lr 0.0001 accuracy 94.97656 wps 12916.90 step time 0.58s\n","# Epoch 17  global step 46920 loss 5.13511 batch 2408/3281 lr 0.0001 accuracy 95.02734 wps 13210.23 step time 0.58s\n","# Epoch 17  global step 46940 loss 5.19929 batch 2428/3281 lr 0.0001 accuracy 94.74609 wps 12438.90 step time 0.70s\n","# Epoch 17  global step 46960 loss 5.13472 batch 2448/3281 lr 0.0001 accuracy 94.99609 wps 12506.43 step time 0.53s\n","# Epoch 17  global step 46980 loss 5.20551 batch 2468/3281 lr 0.0001 accuracy 94.85547 wps 13621.29 step time 0.48s\n","# Epoch 17  global step 47000 loss 5.10852 batch 2488/3281 lr 0.0001 accuracy 94.96094 wps 12363.28 step time 0.50s\n","# Epoch 17  global step 47020 loss 5.11875 batch 2508/3281 lr 0.0001 accuracy 94.93750 wps 12782.19 step time 0.54s\n","# Epoch 17  global step 47040 loss 5.15775 batch 2528/3281 lr 0.0001 accuracy 94.94141 wps 12253.63 step time 0.71s\n","# Epoch 17  global step 47060 loss 5.19491 batch 2548/3281 lr 0.0001 accuracy 94.70703 wps 12857.11 step time 0.63s\n","# Epoch 17  global step 47080 loss 5.15485 batch 2568/3281 lr 0.0001 accuracy 94.88672 wps 12812.18 step time 0.52s\n","# Epoch 17  global step 47100 loss 5.00220 batch 2588/3281 lr 0.0001 accuracy 95.26953 wps 12044.48 step time 0.55s\n","# Epoch 17  global step 47120 loss 5.02159 batch 2608/3281 lr 0.0001 accuracy 95.25391 wps 12970.60 step time 0.47s\n","# Epoch 17  global step 47140 loss 5.24708 batch 2628/3281 lr 0.0001 accuracy 94.75781 wps 12662.39 step time 0.58s\n","# Epoch 17  global step 47160 loss 5.08511 batch 2648/3281 lr 0.0001 accuracy 95.06250 wps 12201.36 step time 0.59s\n","# Epoch 17  global step 47180 loss 5.03926 batch 2668/3281 lr 0.0001 accuracy 95.16016 wps 13067.31 step time 0.48s\n","# Epoch 17  global step 47200 loss 5.08283 batch 2688/3281 lr 0.0001 accuracy 95.16406 wps 12776.59 step time 0.47s\n","# Epoch 17  global step 47220 loss 5.17832 batch 2708/3281 lr 0.0001 accuracy 94.73047 wps 12726.41 step time 0.58s\n","# Epoch 17  global step 47240 loss 5.13755 batch 2728/3281 lr 0.0001 accuracy 95.01563 wps 12367.79 step time 0.51s\n","# Epoch 17  global step 47260 loss 5.12578 batch 2748/3281 lr 0.0001 accuracy 94.98438 wps 12478.68 step time 0.53s\n","# Epoch 17  global step 47280 loss 5.07835 batch 2768/3281 lr 0.0001 accuracy 95.09375 wps 12387.55 step time 0.52s\n","# Epoch 17  global step 47300 loss 5.11808 batch 2788/3281 lr 0.0001 accuracy 94.99609 wps 12495.70 step time 0.50s\n","# Epoch 17  global step 47320 loss 5.13682 batch 2808/3281 lr 0.0001 accuracy 94.88672 wps 11685.31 step time 0.62s\n","# Epoch 17  global step 47340 loss 5.13270 batch 2828/3281 lr 0.0001 accuracy 94.98828 wps 12728.30 step time 0.51s\n","# Epoch 17  global step 47360 loss 5.13350 batch 2848/3281 lr 0.0001 accuracy 94.88281 wps 12961.95 step time 0.56s\n","# Epoch 17  global step 47380 loss 5.26339 batch 2868/3281 lr 0.0001 accuracy 94.66797 wps 12391.75 step time 0.66s\n","# Epoch 17  global step 47400 loss 5.07813 batch 2888/3281 lr 0.0001 accuracy 95.10547 wps 12036.79 step time 0.53s\n","# Epoch 17  global step 47420 loss 5.18160 batch 2908/3281 lr 0.0001 accuracy 94.89453 wps 12138.10 step time 0.59s\n","# Epoch 17  global step 47440 loss 5.15448 batch 2928/3281 lr 0.0001 accuracy 94.99609 wps 13015.14 step time 0.49s\n","# Epoch 17  global step 47460 loss 5.19965 batch 2948/3281 lr 0.0001 accuracy 94.73828 wps 11658.00 step time 0.86s\n","# Epoch 17  global step 47480 loss 5.12034 batch 2968/3281 lr 0.0001 accuracy 94.87891 wps 12654.49 step time 0.54s\n","# Epoch 17  global step 47500 loss 5.13754 batch 2988/3281 lr 0.0001 accuracy 95.04297 wps 12492.22 step time 0.62s\n","# Epoch 17  global step 47520 loss 5.16504 batch 3008/3281 lr 0.0001 accuracy 94.76563 wps 12189.05 step time 0.61s\n","# Epoch 17  global step 47540 loss 5.11861 batch 3028/3281 lr 0.0001 accuracy 95.06250 wps 12879.11 step time 0.51s\n","# Epoch 17  global step 47560 loss 5.14481 batch 3048/3281 lr 0.0001 accuracy 95.01172 wps 12222.71 step time 0.52s\n","# Epoch 17  global step 47580 loss 5.07456 batch 3068/3281 lr 0.0001 accuracy 95.22266 wps 12790.84 step time 0.50s\n","# Epoch 17  global step 47600 loss 5.14167 batch 3088/3281 lr 0.0001 accuracy 94.85937 wps 11806.64 step time 0.67s\n","# Epoch 17  global step 47620 loss 5.23836 batch 3108/3281 lr 0.0001 accuracy 94.64453 wps 11235.80 step time 0.74s\n","# Epoch 17  global step 47640 loss 5.06113 batch 3128/3281 lr 0.0001 accuracy 95.03125 wps 12477.37 step time 0.51s\n","# Epoch 17  global step 47660 loss 5.28539 batch 3148/3281 lr 0.0001 accuracy 94.44922 wps 12923.81 step time 0.67s\n","# Epoch 17  global step 47680 loss 5.15589 batch 3168/3281 lr 0.0001 accuracy 94.79687 wps 12614.41 step time 0.52s\n","# Epoch 17  global step 47700 loss 5.08068 batch 3188/3281 lr 0.0001 accuracy 95.06250 wps 12743.47 step time 0.52s\n","# Epoch 17  global step 47720 loss 5.17862 batch 3208/3281 lr 0.0001 accuracy 94.84766 wps 12055.38 step time 0.62s\n","# Epoch 17  global step 47740 loss 5.14780 batch 3228/3281 lr 0.0001 accuracy 94.94922 wps 12286.82 step time 0.59s\n","# Epoch 17  global step 47760 loss 5.23258 batch 3248/3281 lr 0.0001 accuracy 94.67969 wps 13121.42 step time 0.53s\n","# Epoch 17  global step 47780 loss 5.18497 batch 3268/3281 lr 0.0001 accuracy 94.66406 wps 12209.75 step time 0.67s\n","# Finsh epoch 17, global step 47794\n","# Epoch 18  global step 47800 loss 1.60697 batch 6/3281 lr 0.0001 accuracy 28.36719 wps 13727.88 step time 0.21s\n","# Epoch 18  global step 47820 loss 5.19292 batch 26/3281 lr 0.0001 accuracy 94.77344 wps 14823.41 step time 0.51s\n","# Epoch 18  global step 47840 loss 5.10941 batch 46/3281 lr 0.0001 accuracy 95.09375 wps 14746.76 step time 0.49s\n","# Epoch 18  global step 47860 loss 5.13798 batch 66/3281 lr 0.0001 accuracy 95.08984 wps 14478.34 step time 0.47s\n","# Epoch 18  global step 47880 loss 5.31730 batch 86/3281 lr 0.0001 accuracy 94.53125 wps 14461.42 step time 0.57s\n","# Epoch 18  global step 47900 loss 5.01158 batch 106/3281 lr 0.0001 accuracy 95.29687 wps 13968.57 step time 0.54s\n","# Epoch 18  global step 47920 loss 5.09823 batch 126/3281 lr 0.0001 accuracy 95.04687 wps 14216.24 step time 0.45s\n","# Epoch 18  global step 47940 loss 4.99616 batch 146/3281 lr 0.0001 accuracy 95.35547 wps 14450.79 step time 0.47s\n","# Epoch 18  global step 47960 loss 5.07098 batch 166/3281 lr 0.0001 accuracy 95.11719 wps 14166.95 step time 0.43s\n","# Epoch 18  global step 47980 loss 5.09816 batch 186/3281 lr 0.0001 accuracy 95.05859 wps 14792.99 step time 0.50s\n","# Epoch 18  global step 48000 loss 5.11179 batch 206/3281 lr 0.0001 accuracy 95.00000 wps 14043.33 step time 0.42s\n","# global step 48000, eval model at Sat May 23 15:41:34 2020\n","2020-05-23 15:41:37.103684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:41:37.104024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 15:41:37.104131: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 15:41:37.104161: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 15:41:37.104186: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 15:41:37.104212: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 15:41:37.104238: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 15:41:37.104261: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 15:41:37.104286: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 15:41:37.104382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:41:37.104712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:41:37.104946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 15:41:37.104998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 15:41:37.105015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 15:41:37.105036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 15:41:37.105146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:41:37.105438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 15:41:37.105693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6613942518650482\n","# location_distance_from_business_district - 0.5660295825014044\n","# location_easy_to_find - 0.7043129680222825\n","# service_wait_time - 0.6647915774588986\n","# service_waiters_attitude - 0.7955741335693768\n","# service_parking_convenience - 0.7467348917126866\n","# service_serving_speed - 0.7576399324318845\n","# price_level - 0.7765782396957626\n","# price_cost_effective - 0.7110722446195021\n","# price_discount - 0.6703022260398157\n","# environment_decoration - 0.7118987267467644\n","# environment_noise - 0.757344334585125\n","# environment_space - 0.7561675872395017\n","# environment_cleaness - 0.7550698481200497\n","# dish_portion - 0.7148292378148312\n","# dish_taste - 0.7255811088347399\n","# dish_look - 0.5724142935955678\n","# dish_recommendation - 0.7318913467926711\n","# others_overall_experience - 0.591108715315249\n","# others_willing_to_consume_again - 0.7091599551686478\n","# Eval loss 4.50429, f1 0.70399\n","# current result -0.7039947601064905, previous best result -0.705303808284073\n","# Epoch 18  global step 48020 loss 5.14210 batch 226/3281 lr 0.0001 accuracy 94.77734 wps 14830.41 step time 0.49s\n","# Epoch 18  global step 48040 loss 5.05607 batch 246/3281 lr 0.0001 accuracy 95.11328 wps 14252.20 step time 0.43s\n","# Epoch 18  global step 48060 loss 5.02069 batch 266/3281 lr 0.0001 accuracy 95.25000 wps 14151.13 step time 0.44s\n","# Epoch 18  global step 48080 loss 5.21944 batch 286/3281 lr 0.0001 accuracy 94.85547 wps 14838.44 step time 0.49s\n","# Epoch 18  global step 48100 loss 5.05504 batch 306/3281 lr 0.0001 accuracy 95.19141 wps 13944.01 step time 0.43s\n","# Epoch 18  global step 48120 loss 5.17200 batch 326/3281 lr 0.0001 accuracy 94.72656 wps 13737.59 step time 0.61s\n","# Epoch 18  global step 48140 loss 5.13982 batch 346/3281 lr 0.0001 accuracy 94.95313 wps 14556.12 step time 0.47s\n","# Epoch 18  global step 48160 loss 5.20307 batch 366/3281 lr 0.0001 accuracy 94.65625 wps 14922.74 step time 0.52s\n","# Epoch 18  global step 48180 loss 5.20460 batch 386/3281 lr 0.0001 accuracy 94.87891 wps 15297.31 step time 0.55s\n","# Epoch 18  global step 48200 loss 5.09444 batch 406/3281 lr 0.0001 accuracy 95.12109 wps 14158.43 step time 0.43s\n","# Epoch 18  global step 48220 loss 5.23868 batch 426/3281 lr 0.0001 accuracy 94.71484 wps 13967.55 step time 0.63s\n","# Epoch 18  global step 48240 loss 5.06920 batch 446/3281 lr 0.0001 accuracy 95.17578 wps 14673.91 step time 0.54s\n","# Epoch 18  global step 48260 loss 5.01138 batch 466/3281 lr 0.0001 accuracy 95.32031 wps 14190.40 step time 0.43s\n","# Epoch 18  global step 48280 loss 5.09365 batch 486/3281 lr 0.0001 accuracy 94.95313 wps 14467.44 step time 0.46s\n","# Epoch 18  global step 48300 loss 5.06940 batch 506/3281 lr 0.0001 accuracy 95.01563 wps 14161.05 step time 0.43s\n","# Epoch 18  global step 48320 loss 5.03398 batch 526/3281 lr 0.0001 accuracy 95.14062 wps 14335.30 step time 0.45s\n","# Epoch 18  global step 48340 loss 5.14410 batch 546/3281 lr 0.0001 accuracy 94.82031 wps 14268.19 step time 0.52s\n","# Epoch 18  global step 48360 loss 5.09650 batch 566/3281 lr 0.0001 accuracy 95.19141 wps 13462.44 step time 0.57s\n","# Epoch 18  global step 48380 loss 5.19105 batch 586/3281 lr 0.0001 accuracy 94.88672 wps 14991.85 step time 0.56s\n","# Epoch 18  global step 48400 loss 5.27726 batch 606/3281 lr 0.0001 accuracy 94.64844 wps 14621.79 step time 0.53s\n","# Epoch 18  global step 48420 loss 5.21861 batch 626/3281 lr 0.0001 accuracy 94.53906 wps 14457.58 step time 0.58s\n","# Epoch 18  global step 48440 loss 5.14392 batch 646/3281 lr 0.0001 accuracy 95.04688 wps 14589.62 step time 0.47s\n","# Epoch 18  global step 48460 loss 4.96885 batch 666/3281 lr 0.0001 accuracy 95.46094 wps 14083.94 step time 0.44s\n","# Epoch 18  global step 48480 loss 5.23138 batch 686/3281 lr 0.0001 accuracy 94.59766 wps 14270.90 step time 0.65s\n","# Epoch 18  global step 48500 loss 5.15231 batch 706/3281 lr 0.0001 accuracy 94.96875 wps 14602.11 step time 0.51s\n","# Epoch 18  global step 48520 loss 5.06390 batch 726/3281 lr 0.0001 accuracy 95.21094 wps 13705.35 step time 0.42s\n","# Epoch 18  global step 48540 loss 5.12681 batch 746/3281 lr 0.0001 accuracy 94.94922 wps 14241.74 step time 0.47s\n","# Epoch 18  global step 48560 loss 5.09781 batch 766/3281 lr 0.0001 accuracy 94.95703 wps 13434.21 step time 0.56s\n","# Epoch 18  global step 48580 loss 5.09847 batch 786/3281 lr 0.0001 accuracy 94.98828 wps 14049.37 step time 0.44s\n","# Epoch 18  global step 48600 loss 5.23906 batch 806/3281 lr 0.0001 accuracy 94.64062 wps 14993.51 step time 0.54s\n","# Epoch 18  global step 48620 loss 5.19378 batch 826/3281 lr 0.0001 accuracy 94.60938 wps 14671.40 step time 0.50s\n","# Epoch 18  global step 48640 loss 5.12989 batch 846/3281 lr 0.0001 accuracy 94.98438 wps 14472.10 step time 0.48s\n","# Epoch 18  global step 48660 loss 5.13517 batch 866/3281 lr 0.0001 accuracy 94.82812 wps 14799.80 step time 0.52s\n","# Epoch 18  global step 48680 loss 5.24168 batch 886/3281 lr 0.0001 accuracy 94.42578 wps 14276.13 step time 0.59s\n","# Epoch 18  global step 48700 loss 5.16457 batch 906/3281 lr 0.0001 accuracy 94.90625 wps 14390.08 step time 0.49s\n","# Epoch 18  global step 48720 loss 5.08959 batch 926/3281 lr 0.0001 accuracy 94.86328 wps 14067.02 step time 0.57s\n","# Epoch 18  global step 48740 loss 5.02500 batch 946/3281 lr 0.0001 accuracy 95.26562 wps 14625.70 step time 0.49s\n","# Epoch 18  global step 48760 loss 5.20688 batch 966/3281 lr 0.0001 accuracy 94.65234 wps 14430.59 step time 0.58s\n","# Epoch 18  global step 48780 loss 5.05526 batch 986/3281 lr 0.0001 accuracy 95.03906 wps 14053.37 step time 0.54s\n","# Epoch 18  global step 48800 loss 5.01675 batch 1006/3281 lr 0.0001 accuracy 95.18359 wps 14005.74 step time 0.44s\n","# Epoch 18  global step 48820 loss 5.17159 batch 1026/3281 lr 0.0001 accuracy 94.68750 wps 15178.94 step time 0.57s\n","# Epoch 18  global step 48840 loss 5.06742 batch 1046/3281 lr 0.0001 accuracy 95.15234 wps 14399.52 step time 0.47s\n","# Epoch 18  global step 48860 loss 5.14749 batch 1066/3281 lr 0.0001 accuracy 94.78906 wps 14573.60 step time 0.49s\n","# Epoch 18  global step 48880 loss 5.19236 batch 1086/3281 lr 0.0001 accuracy 94.85938 wps 14471.51 step time 0.48s\n","# Epoch 18  global step 48900 loss 5.08133 batch 1106/3281 lr 0.0001 accuracy 94.94531 wps 12637.48 step time 0.49s\n","# Epoch 18  global step 48920 loss 5.09589 batch 1126/3281 lr 0.0001 accuracy 95.17187 wps 11565.02 step time 0.64s\n","# Epoch 18  global step 48940 loss 5.16126 batch 1146/3281 lr 0.0001 accuracy 94.97656 wps 12911.04 step time 0.61s\n","# Epoch 18  global step 48960 loss 5.19151 batch 1166/3281 lr 0.0001 accuracy 94.85547 wps 11647.33 step time 0.67s\n","# Epoch 18  global step 48980 loss 5.21967 batch 1186/3281 lr 0.0001 accuracy 94.64453 wps 11881.98 step time 0.73s\n","# Epoch 18  global step 49000 loss 5.21478 batch 1206/3281 lr 0.0001 accuracy 94.76563 wps 12838.31 step time 0.52s\n","# Epoch 18  global step 49020 loss 4.96759 batch 1226/3281 lr 0.0001 accuracy 95.35547 wps 12590.29 step time 0.46s\n","# Epoch 18  global step 49040 loss 5.13918 batch 1246/3281 lr 0.0001 accuracy 94.80078 wps 12322.91 step time 0.61s\n","# Epoch 18  global step 49060 loss 5.00274 batch 1266/3281 lr 0.0001 accuracy 95.18750 wps 12375.83 step time 0.50s\n","# Epoch 18  global step 49080 loss 5.05945 batch 1286/3281 lr 0.0001 accuracy 95.26172 wps 12044.77 step time 0.58s\n","# Epoch 18  global step 49100 loss 5.12981 batch 1306/3281 lr 0.0001 accuracy 94.96094 wps 12529.02 step time 0.55s\n","# Epoch 18  global step 49120 loss 5.21927 batch 1326/3281 lr 0.0001 accuracy 94.65234 wps 11579.91 step time 0.72s\n","# Epoch 18  global step 49140 loss 5.13218 batch 1346/3281 lr 0.0001 accuracy 94.91016 wps 12108.02 step time 0.57s\n","# Epoch 18  global step 49160 loss 4.94717 batch 1366/3281 lr 0.0001 accuracy 95.48828 wps 11775.51 step time 0.62s\n","# Epoch 18  global step 49180 loss 5.12984 batch 1386/3281 lr 0.0001 accuracy 95.01562 wps 13812.59 step time 0.53s\n","# Epoch 18  global step 49200 loss 5.16114 batch 1406/3281 lr 0.0001 accuracy 95.01172 wps 12508.13 step time 0.53s\n","# Epoch 18  global step 49220 loss 5.11153 batch 1426/3281 lr 0.0001 accuracy 94.96484 wps 12602.10 step time 0.64s\n","# Epoch 18  global step 49240 loss 5.20000 batch 1446/3281 lr 0.0001 accuracy 94.81250 wps 12666.76 step time 0.65s\n","# Epoch 18  global step 49260 loss 5.07479 batch 1466/3281 lr 0.0001 accuracy 95.11328 wps 12669.61 step time 0.47s\n","# Epoch 18  global step 49280 loss 5.05190 batch 1486/3281 lr 0.0001 accuracy 95.06641 wps 11400.58 step time 0.72s\n","# Epoch 18  global step 49300 loss 5.17419 batch 1506/3281 lr 0.0001 accuracy 94.80859 wps 12269.21 step time 0.66s\n","# Epoch 18  global step 49320 loss 5.17734 batch 1526/3281 lr 0.0001 accuracy 94.87500 wps 12515.12 step time 0.63s\n","# Epoch 18  global step 49340 loss 5.24151 batch 1546/3281 lr 0.0001 accuracy 94.80078 wps 12216.90 step time 0.68s\n","# Epoch 18  global step 49360 loss 5.11458 batch 1566/3281 lr 0.0001 accuracy 95.06641 wps 12115.49 step time 0.56s\n","# Epoch 18  global step 49380 loss 5.09595 batch 1586/3281 lr 0.0001 accuracy 95.02734 wps 11839.37 step time 0.56s\n","# Epoch 18  global step 49400 loss 5.05598 batch 1606/3281 lr 0.0001 accuracy 95.11328 wps 12482.62 step time 0.52s\n","# Epoch 18  global step 49420 loss 5.18958 batch 1626/3281 lr 0.0001 accuracy 94.71484 wps 13284.91 step time 0.56s\n","# Epoch 18  global step 49440 loss 5.00023 batch 1646/3281 lr 0.0001 accuracy 95.27344 wps 12636.10 step time 0.53s\n","# Epoch 18  global step 49460 loss 5.25165 batch 1666/3281 lr 0.0001 accuracy 94.61719 wps 12672.67 step time 0.69s\n","# Epoch 18  global step 49480 loss 5.07996 batch 1686/3281 lr 0.0001 accuracy 95.11719 wps 12911.01 step time 0.52s\n","# Epoch 18  global step 49500 loss 5.19486 batch 1706/3281 lr 0.0001 accuracy 95.04688 wps 12796.54 step time 0.56s\n","# Epoch 18  global step 49520 loss 5.02055 batch 1726/3281 lr 0.0001 accuracy 95.28125 wps 12211.07 step time 0.51s\n","# Epoch 18  global step 49540 loss 5.15594 batch 1746/3281 lr 0.0001 accuracy 94.80859 wps 13143.31 step time 0.57s\n","# Epoch 18  global step 49560 loss 5.15863 batch 1766/3281 lr 0.0001 accuracy 94.71875 wps 12699.22 step time 0.61s\n","# Epoch 18  global step 49580 loss 5.14630 batch 1786/3281 lr 0.0001 accuracy 94.92969 wps 12321.54 step time 0.63s\n","# Epoch 18  global step 49600 loss 5.06219 batch 1806/3281 lr 0.0001 accuracy 95.06641 wps 13468.28 step time 0.53s\n","# Epoch 18  global step 49620 loss 5.04168 batch 1826/3281 lr 0.0001 accuracy 95.00781 wps 12934.44 step time 0.50s\n","# Epoch 18  global step 49640 loss 5.14314 batch 1846/3281 lr 0.0001 accuracy 94.97266 wps 12708.07 step time 0.59s\n","# Epoch 18  global step 49660 loss 5.14054 batch 1866/3281 lr 0.0001 accuracy 94.83984 wps 12293.44 step time 0.52s\n","# Epoch 18  global step 49680 loss 5.05342 batch 1886/3281 lr 0.0001 accuracy 95.18750 wps 12438.86 step time 0.47s\n","# Epoch 18  global step 49700 loss 5.17800 batch 1906/3281 lr 0.0001 accuracy 94.87500 wps 12265.47 step time 0.68s\n","# Epoch 18  global step 49720 loss 5.06619 batch 1926/3281 lr 0.0001 accuracy 95.19922 wps 12299.04 step time 0.51s\n","# Epoch 18  global step 49740 loss 5.04999 batch 1946/3281 lr 0.0001 accuracy 95.15625 wps 12120.65 step time 0.55s\n","# Epoch 18  global step 49760 loss 5.12480 batch 1966/3281 lr 0.0001 accuracy 95.16016 wps 12492.60 step time 0.53s\n","# Epoch 18  global step 49780 loss 5.04021 batch 1986/3281 lr 0.0001 accuracy 95.13672 wps 12583.50 step time 0.61s\n","# Epoch 18  global step 49800 loss 5.08650 batch 2006/3281 lr 0.0001 accuracy 95.01953 wps 12367.00 step time 0.58s\n","# Epoch 18  global step 49820 loss 5.21674 batch 2026/3281 lr 0.0001 accuracy 94.71875 wps 11898.05 step time 0.66s\n","# Epoch 18  global step 49840 loss 5.19135 batch 2046/3281 lr 0.0001 accuracy 94.81250 wps 12283.59 step time 0.64s\n","# Epoch 18  global step 49860 loss 5.16209 batch 2066/3281 lr 0.0001 accuracy 94.80469 wps 12445.28 step time 0.63s\n","# Epoch 18  global step 49880 loss 5.06248 batch 2086/3281 lr 0.0001 accuracy 95.21875 wps 12748.32 step time 0.47s\n","# Epoch 18  global step 49900 loss 5.13470 batch 2106/3281 lr 0.0001 accuracy 94.87500 wps 11963.63 step time 0.76s\n","# Epoch 18  global step 49920 loss 5.27293 batch 2126/3281 lr 0.0001 accuracy 94.61719 wps 13186.15 step time 0.59s\n","# Epoch 18  global step 49940 loss 5.10985 batch 2146/3281 lr 0.0001 accuracy 94.99219 wps 12729.48 step time 0.49s\n","# Epoch 18  global step 49960 loss 5.16500 batch 2166/3281 lr 0.0001 accuracy 94.83203 wps 11239.72 step time 0.83s\n","# Epoch 18  global step 49980 loss 5.08575 batch 2186/3281 lr 0.0001 accuracy 95.14844 wps 12852.92 step time 0.52s\n","# Epoch 18  global step 50000 loss 5.06315 batch 2206/3281 lr 0.0001 accuracy 95.26953 wps 12820.50 step time 0.46s\n","# global step 50000, eval model at Sat May 23 16:02:13 2020\n","2020-05-23 16:02:15.680351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 16:02:15.680840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-23 16:02:15.680982: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 16:02:15.681014: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 16:02:15.681043: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 16:02:15.681071: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 16:02:15.681097: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 16:02:15.681124: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 16:02:15.681152: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 16:02:15.681254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 16:02:15.681556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 16:02:15.682002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 16:02:15.682370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 16:02:15.682392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 16:02:15.682403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 16:02:15.682599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 16:02:15.682897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 16:02:15.683150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.659099827472279\n","# location_distance_from_business_district - 0.5593876910592271\n","# location_easy_to_find - 0.7045858388194387\n","# service_wait_time - 0.6658303643972239\n","# service_waiters_attitude - 0.7973112867905228\n","# service_parking_convenience - 0.7431675616005043\n","# service_serving_speed - 0.7543429328750941\n","# price_level - 0.7748515193231527\n","# price_cost_effective - 0.7080711456019666\n","# price_discount - 0.6701615730445869\n","# environment_decoration - 0.7140924340046857\n","# environment_noise - 0.7562379272810048\n","# environment_space - 0.7564743160394777\n","# environment_cleaness - 0.7541475709101078\n","# dish_portion - 0.7144715492825372\n","# dish_taste - 0.7259152013983516\n","# dish_look - 0.5739394931020293\n","# dish_recommendation - 0.7327110014475855\n","# others_overall_experience - 0.5900580192700444\n","# others_willing_to_consume_again - 0.7084036877181155\n","# Eval loss 4.51729, f1 0.70316\n","# current result -0.7031630470718968, previous best result -0.705303808284073\n","# No loss decrease, restore previous best model and set learning rate to half of previous one\n","# Epoch 18  global step 42020 loss 4.79834 batch 2226/3281 lr 1e-05 accuracy 95.74609 wps 12382.37 step time 0.62s\n","# Epoch 18  global step 42040 loss 4.79583 batch 2246/3281 lr 1e-05 accuracy 95.81641 wps 12750.43 step time 0.56s\n","# Epoch 18  global step 42060 loss 4.84115 batch 2266/3281 lr 1e-05 accuracy 95.75391 wps 13099.31 step time 0.53s\n","# Epoch 18  global step 42080 loss 4.76372 batch 2286/3281 lr 1e-05 accuracy 96.18750 wps 12877.13 step time 0.54s\n","# Epoch 18  global step 42100 loss 4.77707 batch 2306/3281 lr 1e-05 accuracy 95.92188 wps 12816.91 step time 0.54s\n","# Epoch 18  global step 42120 loss 4.78372 batch 2326/3281 lr 1e-05 accuracy 95.95313 wps 12162.07 step time 0.65s\n","# Epoch 18  global step 42140 loss 4.79763 batch 2346/3281 lr 1e-05 accuracy 95.96875 wps 12683.89 step time 0.52s\n","# Epoch 18  global step 42160 loss 4.83592 batch 2366/3281 lr 1e-05 accuracy 95.72266 wps 11862.81 step time 0.65s\n","# Epoch 18  global step 42180 loss 4.81858 batch 2386/3281 lr 1e-05 accuracy 95.80859 wps 12926.81 step time 0.59s\n","# Epoch 18  global step 42200 loss 4.76739 batch 2406/3281 lr 1e-05 accuracy 95.96484 wps 12994.27 step time 0.51s\n","# Epoch 18  global step 42220 loss 4.79238 batch 2426/3281 lr 1e-05 accuracy 95.87891 wps 11258.46 step time 0.69s\n","# Epoch 18  global step 42240 loss 4.89037 batch 2446/3281 lr 1e-05 accuracy 95.61328 wps 12435.67 step time 0.62s\n","# Epoch 18  global step 42260 loss 4.76397 batch 2466/3281 lr 1e-05 accuracy 95.85547 wps 11994.73 step time 0.66s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hlZzdUOjimmU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvoqbuR1imWE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"POVsaFWmiawn","colab_type":"code","outputId":"133f03b3-b8d6-4258-d778-7028beee96f6","executionInfo":{"status":"ok","timestamp":1590254892412,"user_tz":-480,"elapsed":256787,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!bash bash/elmo_inference.sh"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:142: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","inference data file ['scripts/data/testa.json']\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:33: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from scripts/data/testa.json ...\n","# Got 15000 data items with 50 batches\n","loading hparams from scripts/data/elmo_ema_focal_smooth/hparams\n","WARNING:tensorflow:From main.py:82: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:187: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","2020-05-23 17:24:04.166187: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2020-05-23 17:24:04.172118: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n","2020-05-23 17:24:04.301794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 17:24:04.302652: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x176cbc0 executing computations on platform CUDA. Devices:\n","2020-05-23 17:24:04.302695: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n","2020-05-23 17:24:04.305709: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n","2020-05-23 17:24:04.305949: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x176c840 executing computations on platform Host. Devices:\n","2020-05-23 17:24:04.305984: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n","2020-05-23 17:24:04.306159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 17:24:04.306844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n","pciBusID: 0000:00:04.0\n","2020-05-23 17:24:04.321500: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 17:24:04.504572: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-23 17:24:04.608393: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-23 17:24:04.628457: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-23 17:24:04.871470: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-23 17:24:05.016657: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-23 17:24:05.540676: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 17:24:05.540859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 17:24:05.541642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 17:24:05.542299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-23 17:24:05.542456: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-23 17:24:05.544516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 17:24:05.544551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-23 17:24:05.544577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-23 17:24:05.544752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 17:24:05.545492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 17:24:05.546225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10869 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:72: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:89: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:134: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","build elmo encoder\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:40: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","seq_dim is deprecated, use seq_axis instead\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","batch_dim is deprecated, use batch_axis instead\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f5920f7deb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f5920f7deb8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f5920fa1940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f5920fa1940>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f59562890b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f59562890b8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f5920f7deb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f5920f7deb8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f5920f2bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f5920f2bf98>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f5920f2bf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f5920f2bf60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:64: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920db2940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920db2940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:242: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920de2a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920de2a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920de2a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920de2a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py:2078: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f5920d2bd68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f592ebf1a58>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f592043a390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5920261908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:41: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","restored model\n","Scalars: [-0.7699267   0.01275063  0.95965606  0.5965034 ]\n","Weight: 0.11208157\n","2020-05-23 17:24:16.707778: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","# process 100.00%# Write result to file ...\n","# Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LFSn6LJJZZR_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-XDF8YeZZei","colab_type":"code","outputId":"3a712a29-b8d1-4506-95de-c9682751442b","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 修改了数据\n","!bash bash/elmo_train.sh"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:142: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:33: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from scripts/data/train.json ...\n","# Got 105000 data items with 3281 batches\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from scripts/data/validation.json ...\n","# Got 15000 data items with 93 batches\n","  saving hparams to scripts/data/elmo_ema_data_modified/hparams\n","mode=train,data_files=['scripts/data/train.json'],eval_files=['scripts/data/validation.json'],label_file=scripts/data/labels.txt,vocab_file=scripts/data/vocab.txt,embed_file=scripts/data/embedding.txt,out_file=None,split_word=True,max_len=1200,batch_size=32,reverse=False,prob=False,num_layers=3,decay_schema=hand,encoder=elmo,decay_steps=10000,learning_rate=0.001,focal_loss=0.0,embedding_dropout=0.1,max_gradient_norm=5.0,dropout_keep_prob=0.8,weight_keep_drop=0.8,l2_loss_ratio=0.0,rnn_cell_name=lstm,embedding_size=300,num_units=300,double_decoder=False,variational_dropout=True,target_label_num=4,feature_num=20,need_early_stop=True,patient=5,debug=False,num_train_epoch=50,steps_per_stats=20,steps_per_summary=50,steps_per_eval=2000,checkpoint_dir=scripts/data/elmo_ema_data_modified,vocab_size=50000\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:72: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:89: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:134: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 43898 300\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:114: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:106: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","build elmo encoder\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:40: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","seq_dim is deprecated, use seq_axis instead\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","batch_dim is deprecated, use batch_axis instead\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f39f5940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f39f5940>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f39f5860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f39f5860>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa50ab22f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa50ab22f28>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f3a77710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f3a77710>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa546f6a7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa546f6a7f0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa50a0a5f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa50a0a5f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:264: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/utils.py:64: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa509607278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa509607278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:242: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa50732da90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa50732da90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa50732da90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa50732da90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1372: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py:2078: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f3a514e0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa509da1d30>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa508516898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5094cd240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:284: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:291: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:291: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n","\n","variable <tf.Variable 'embedding/embedding:0' shape=(50000, 300) dtype=float32_ref> with parameter number 15000000\n","variable <tf.Variable 'embedding/feature_embedding:0' shape=(20, 300) dtype=float32_ref> with parameter number 6000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/fw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/kernel:0' shape=(600, 1200) dtype=float32_ref> with parameter number 720000\n","variable <tf.Variable 'elmo_encoder/bw_0/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_1/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/fw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/kernel:0' shape=(900, 1200) dtype=float32_ref> with parameter number 1080000\n","variable <tf.Variable 'elmo_encoder/bw_2/lstm_fused_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'elmo_encoder/scalar:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","variable <tf.Variable 'elmo_encoder/weight:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classification/attention_semantic/memory_layer/kernel:0' shape=(600, 300) dtype=float32_ref> with parameter number 180000\n","variable <tf.Variable 'classification/attention_semantic/dense/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classification/attention_semantic/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classification/attention_semantic/dense_1/kernel:0' shape=(1800, 300) dtype=float32_ref> with parameter number 540000\n","variable <tf.Variable 'classification/attention_semantic/dense_1/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classification/attention_semantic/attention_wrapper/lstm_cell/kernel:0' shape=(1200, 1200) dtype=float32_ref> with parameter number 1440000\n","variable <tf.Variable 'classification/attention_semantic/attention_wrapper/lstm_cell/bias:0' shape=(1200,) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classification/attention_semantic/attention_wrapper/luong_attention/attention_g:0' shape=() dtype=float32_ref> with parameter number 1\n","variable <tf.Variable 'classification/predict_clf/dense/kernel:0' shape=(900, 300) dtype=float32_ref> with parameter number 270000\n","variable <tf.Variable 'classification/predict_clf/dense/bias:0' shape=(300,) dtype=float32_ref> with parameter number 300\n","variable <tf.Variable 'classification/predict_clf/dense_1/kernel:0' shape=(300, 4) dtype=float32_ref> with parameter number 1200\n","variable <tf.Variable 'classification/predict_clf/dense_1/bias:0' shape=(4,) dtype=float32_ref> with parameter number 4\n","# total parameter number 23746510\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:326: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:295: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:297: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:302: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py:41: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","loading hparams from scripts/data/elmo_ema_data_modified/hparams\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 43898 300\n","build elmo encoder\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa507956f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa507956f60>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f3dc86d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f3dc86d8>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f3dc8d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f3dc8d68>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f3a388d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f3a388d0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f02a1438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f02a1438>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f02af7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7fa4f02af7f0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa4f3dc8438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa4f3dc8438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa4f029b828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa4f029b828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa4f029b828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa4f029b828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fa4f029b5c0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa507941fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa507941c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa503cb6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","2020-05-24 11:17:16.815854: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2020-05-24 11:17:16.824141: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 11:17:17.006652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:17:17.007222: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x249f9c0 executing computations on platform CUDA. Devices:\n","2020-05-24 11:17:17.007254: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1\n","2020-05-24 11:17:17.013499: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n","2020-05-24 11:17:17.013767: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x249ebc0 executing computations on platform Host. Devices:\n","2020-05-24 11:17:17.013815: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n","2020-05-24 11:17:17.014271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:17:17.014698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-24 11:17:17.042016: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 11:17:17.241161: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-24 11:17:17.330743: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-24 11:17:17.354683: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-24 11:17:17.593132: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-24 11:17:17.729959: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-24 11:17:18.232117: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 11:17:18.232334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:17:18.232934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:17:18.233331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-24 11:17:18.236634: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 11:17:18.238077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 11:17:18.238114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-24 11:17:18.238127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-24 11:17:18.252771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:17:18.253231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:17:18.253798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","2020-05-24 11:17:18.377306: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\n","  /job:localhost/replica:0/task:0/device:CPU:0].\n","See below for details of this colocation group:\n","Colocation Debug Info:\n","Colocation group had the following types and supported devices: \n","Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\n","AssignSub: GPU CPU \n","Merge: GPU CPU XLA_CPU XLA_GPU \n","Switch: GPU CPU XLA_CPU XLA_GPU \n","L2Loss: GPU CPU XLA_CPU XLA_GPU \n","Add: GPU CPU XLA_CPU XLA_GPU \n","Assign: GPU CPU \n","Identity: GPU CPU XLA_CPU XLA_GPU \n","VariableV2: GPU CPU \n","Const: GPU CPU XLA_CPU XLA_GPU \n","Fill: GPU CPU XLA_CPU XLA_GPU \n","Sub: GPU CPU XLA_CPU XLA_GPU \n","UnsortedSegmentSum: GPU CPU XLA_CPU XLA_GPU \n","Reshape: GPU CPU XLA_CPU XLA_GPU \n","RefSwitch: GPU CPU \n","GatherV2: GPU CPU XLA_CPU XLA_GPU \n","ExpandDims: GPU CPU XLA_CPU XLA_GPU \n","RandomUniform: GPU CPU XLA_CPU XLA_GPU \n","Cast: GPU CPU XLA_CPU XLA_GPU \n","Mul: GPU CPU XLA_CPU XLA_GPU \n","Unique: GPU CPU \n","SparseApplyRMSProp: CPU \n","StridedSlice: GPU CPU XLA_CPU XLA_GPU \n","ConcatV2: GPU CPU XLA_CPU XLA_GPU \n","Shape: GPU CPU XLA_CPU XLA_GPU \n","IsVariableInitialized: GPU CPU \n","\n","Colocation members, user-requested devices, and framework assigned devices, if any:\n","  embedding/feature_embedding/Initializer/random_uniform/shape (Const) \n","  embedding/feature_embedding/Initializer/random_uniform/min (Const) \n","  embedding/feature_embedding/Initializer/random_uniform/max (Const) \n","  embedding/feature_embedding/Initializer/random_uniform/RandomUniform (RandomUniform) \n","  embedding/feature_embedding/Initializer/random_uniform/sub (Sub) \n","  embedding/feature_embedding/Initializer/random_uniform/mul (Mul) \n","  embedding/feature_embedding/Initializer/random_uniform (Add) \n","  embedding/feature_embedding (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/read (Identity) /device:GPU:0\n","  embedding_lookup_1/axis (Const) /device:GPU:0\n","  embedding_lookup_1 (GatherV2) /device:GPU:0\n","  global_norm/L2Loss_1 (L2Loss) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Shape (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Cast (Cast) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Size (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/ExpandDims/dim (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/ExpandDims (ExpandDims) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice/stack (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice/stack_1 (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice/stack_2 (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/strided_slice (StridedSlice) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/concat/axis (Const) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/concat (ConcatV2) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Reshape (Reshape) /device:GPU:0\n","  gradients/embedding_lookup_1_grad/Reshape_1 (Reshape) /device:GPU:0\n","  global_norm_1/L2Loss_1 (L2Loss) /device:GPU:0\n","  clip_by_global_norm/mul_2 (Mul) /device:GPU:0\n","  clip_by_global_norm/clip_by_global_norm/_1 (Identity) /device:GPU:0\n","  global_norm_2/L2Loss_1 (L2Loss) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Initializer/ones/shape_as_tensor (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Initializer/ones/Const (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Initializer/ones (Fill) /device:GPU:0\n","  embedding/feature_embedding/RMSProp (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/RMSProp/read (Identity) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Initializer/zeros/Const (Const) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Initializer/zeros (Fill) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1 (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/RMSProp_1/read (Identity) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/Unique (Unique) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/Shape (Shape) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice/stack (Const) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice/stack_1 (Const) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice/stack_2 (Const) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/strided_slice (StridedSlice) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/UnsortedSegmentSum (UnsortedSegmentSum) /device:GPU:0\n","  RMSProp/update_embedding/feature_embedding/SparseApplyRMSProp (SparseApplyRMSProp) /device:GPU:0\n","  IsVariableInitialized_1 (IsVariableInitialized) /device:GPU:0\n","  cond_1/read/Switch (RefSwitch) /device:GPU:0\n","  cond_1/Switch_1 (Switch) \n","  embedding/feature_embedding/ExponentialMovingAverage (VariableV2) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/IsVariableInitialized (IsVariableInitialized) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/Switch (Switch) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/switch_t (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/switch_f (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/pred_id (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/read/Switch (RefSwitch) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/read (Identity) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/cond/Switch_1 (Switch) \n","  embedding/feature_embedding/ExponentialMovingAverage/cond/Merge (Merge) /device:GPU:0\n","  cond_1/read/Switch_embedding/feature_embedding/ExponentialMovingAverage (Switch) /device:GPU:0\n","  cond_1/read_embedding/feature_embedding/ExponentialMovingAverage (Identity) /device:GPU:0\n","  cond_1/Merge_embedding/feature_embedding/ExponentialMovingAverage (Merge) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/Assign (Assign) /device:GPU:0\n","  embedding/feature_embedding/ExponentialMovingAverage/read (Identity) /device:GPU:0\n","  ExponentialMovingAverage/AssignMovingAvg_1 (AssignSub) /device:GPU:0\n","  save/Assign_111 (Assign) /device:GPU:0\n","  save/Assign_112 (Assign) /device:GPU:0\n","  save/Assign_113 (Assign) /device:GPU:0\n","  save/Assign_114 (Assign) /device:GPU:0\n","\n","2020-05-24 11:17:18.441097: W tensorflow/core/framework/allocator.cc:107] Allocation of 60000000 exceeds 10% of system memory.\n","2020-05-24 11:17:18.456048: W tensorflow/core/framework/allocator.cc:107] Allocation of 60000000 exceeds 10% of system memory.\n","2020-05-24 11:17:18.470005: W tensorflow/core/framework/allocator.cc:107] Allocation of 60000000 exceeds 10% of system memory.\n","2020-05-24 11:17:18.501212: W tensorflow/core/framework/allocator.cc:107] Allocation of 60000000 exceeds 10% of system memory.\n","2020-05-24 11:17:18.528252: W tensorflow/core/framework/allocator.cc:107] Allocation of 60000000 exceeds 10% of system memory.\n","2020-05-24 11:17:19.318736: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n","unable to restore model, train from scratch\n","# Start to train with learning rate 0.001, Sun May 24 11:17:26 2020\n","# Global step 0\n","2020-05-24 11:17:29.646968: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","# Epoch 1  global step 20 loss 1.39417 batch 20/3281 lr 0.001 accuracy 57.82031 wps 10585.97 step time 0.81s\n","# Epoch 1  global step 40 loss 1.37119 batch 40/3281 lr 0.001 accuracy 73.83984 wps 14615.58 step time 0.45s\n","# Epoch 1  global step 60 loss 1.32012 batch 60/3281 lr 0.001 accuracy 81.07031 wps 14955.60 step time 0.49s\n","# Epoch 1  global step 80 loss 1.19955 batch 80/3281 lr 0.001 accuracy 81.62109 wps 14765.97 step time 0.47s\n","# Epoch 1  global step 100 loss 1.00491 batch 100/3281 lr 0.001 accuracy 81.66406 wps 14713.35 step time 0.47s\n","# Epoch 1  global step 120 loss 0.95782 batch 120/3281 lr 0.001 accuracy 81.38672 wps 14643.38 step time 0.46s\n","# Epoch 1  global step 140 loss 0.93558 batch 140/3281 lr 0.001 accuracy 82.12891 wps 13490.79 step time 0.47s\n","# Epoch 1  global step 160 loss 0.94904 batch 160/3281 lr 0.001 accuracy 82.03125 wps 14561.21 step time 0.44s\n","# Epoch 1  global step 180 loss 0.91529 batch 180/3281 lr 0.001 accuracy 83.49609 wps 14968.02 step time 0.51s\n","# Epoch 1  global step 200 loss 0.87223 batch 200/3281 lr 0.001 accuracy 83.58203 wps 14902.13 step time 0.49s\n","# Epoch 1  global step 220 loss 0.85117 batch 220/3281 lr 0.001 accuracy 83.67578 wps 14674.18 step time 0.59s\n","# Epoch 1  global step 240 loss 0.83283 batch 240/3281 lr 0.001 accuracy 83.96484 wps 14502.12 step time 0.47s\n","# Epoch 1  global step 260 loss 0.86894 batch 260/3281 lr 0.001 accuracy 83.72656 wps 13986.98 step time 0.53s\n","# Epoch 1  global step 280 loss 0.83223 batch 280/3281 lr 0.001 accuracy 84.00781 wps 14978.09 step time 0.51s\n","# Epoch 1  global step 300 loss 0.81400 batch 300/3281 lr 0.001 accuracy 84.26562 wps 14268.01 step time 0.44s\n","# Epoch 1  global step 320 loss 0.84517 batch 320/3281 lr 0.001 accuracy 83.67578 wps 15298.55 step time 0.53s\n","# Epoch 1  global step 340 loss 0.80184 batch 340/3281 lr 0.001 accuracy 84.76563 wps 14130.68 step time 0.52s\n","# Epoch 1  global step 360 loss 0.82145 batch 360/3281 lr 0.001 accuracy 84.19141 wps 14399.70 step time 0.54s\n","# Epoch 1  global step 380 loss 0.79261 batch 380/3281 lr 0.001 accuracy 85.07031 wps 14302.15 step time 0.44s\n","# Epoch 1  global step 400 loss 0.79892 batch 400/3281 lr 0.001 accuracy 84.64453 wps 14670.91 step time 0.46s\n","# Epoch 1  global step 420 loss 0.78921 batch 420/3281 lr 0.001 accuracy 84.83203 wps 14293.39 step time 0.44s\n","# Epoch 1  global step 440 loss 0.79662 batch 440/3281 lr 0.001 accuracy 84.59766 wps 14507.64 step time 0.46s\n","# Epoch 1  global step 460 loss 0.77199 batch 460/3281 lr 0.001 accuracy 85.01172 wps 14236.97 step time 0.44s\n","# Epoch 1  global step 480 loss 0.80593 batch 480/3281 lr 0.001 accuracy 84.44531 wps 13497.90 step time 0.49s\n","# Epoch 1  global step 500 loss 0.79614 batch 500/3281 lr 0.001 accuracy 84.28906 wps 13533.85 step time 0.59s\n","# Epoch 1  global step 520 loss 0.80300 batch 520/3281 lr 0.001 accuracy 84.40625 wps 14338.74 step time 0.45s\n","# Epoch 1  global step 540 loss 0.78730 batch 540/3281 lr 0.001 accuracy 84.42187 wps 13972.13 step time 0.63s\n","# Epoch 1  global step 560 loss 0.79307 batch 560/3281 lr 0.001 accuracy 84.45703 wps 13897.47 step time 0.52s\n","# Epoch 1  global step 580 loss 0.74820 batch 580/3281 lr 0.001 accuracy 85.40625 wps 14273.12 step time 0.44s\n","# Epoch 1  global step 600 loss 0.76837 batch 600/3281 lr 0.001 accuracy 84.68359 wps 14039.98 step time 0.51s\n","# Epoch 1  global step 620 loss 0.79038 batch 620/3281 lr 0.001 accuracy 84.34375 wps 14521.36 step time 0.59s\n","# Epoch 1  global step 640 loss 0.78874 batch 640/3281 lr 0.001 accuracy 84.29688 wps 14307.94 step time 0.55s\n","# Epoch 1  global step 660 loss 0.77461 batch 660/3281 lr 0.001 accuracy 84.54688 wps 13668.10 step time 0.52s\n","# Epoch 1  global step 680 loss 0.77298 batch 680/3281 lr 0.001 accuracy 85.01172 wps 14377.08 step time 0.46s\n","# Epoch 1  global step 700 loss 0.79542 batch 700/3281 lr 0.001 accuracy 83.66797 wps 15167.31 step time 0.58s\n","# Epoch 1  global step 720 loss 0.77426 batch 720/3281 lr 0.001 accuracy 84.68750 wps 14933.35 step time 0.51s\n","# Epoch 1  global step 740 loss 0.78444 batch 740/3281 lr 0.001 accuracy 84.62109 wps 14339.72 step time 0.56s\n","# Epoch 1  global step 760 loss 0.74475 batch 760/3281 lr 0.001 accuracy 85.59375 wps 14387.49 step time 0.46s\n","# Epoch 1  global step 780 loss 0.73737 batch 780/3281 lr 0.001 accuracy 85.74609 wps 14046.37 step time 0.43s\n","# Epoch 1  global step 800 loss 0.78146 batch 800/3281 lr 0.001 accuracy 84.80469 wps 14302.12 step time 0.56s\n","# Epoch 1  global step 820 loss 0.76734 batch 820/3281 lr 0.001 accuracy 84.81641 wps 14063.65 step time 0.65s\n","# Epoch 1  global step 840 loss 0.81032 batch 840/3281 lr 0.001 accuracy 83.64063 wps 14214.99 step time 0.70s\n","# Epoch 1  global step 860 loss 0.72624 batch 860/3281 lr 0.001 accuracy 86.18359 wps 13864.69 step time 0.42s\n","# Epoch 1  global step 880 loss 0.77531 batch 880/3281 lr 0.001 accuracy 84.65234 wps 13989.66 step time 0.61s\n","# Epoch 1  global step 900 loss 0.77791 batch 900/3281 lr 0.001 accuracy 85.01563 wps 14650.60 step time 0.48s\n","# Epoch 1  global step 920 loss 0.77841 batch 920/3281 lr 0.001 accuracy 84.63281 wps 14458.87 step time 0.53s\n","# Epoch 1  global step 940 loss 0.72162 batch 940/3281 lr 0.001 accuracy 86.30078 wps 13913.19 step time 0.41s\n","# Epoch 1  global step 960 loss 0.73279 batch 960/3281 lr 0.001 accuracy 85.76953 wps 14988.77 step time 0.51s\n","# Epoch 1  global step 980 loss 0.71758 batch 980/3281 lr 0.001 accuracy 86.32422 wps 14757.72 step time 0.48s\n","# Epoch 1  global step 1000 loss 0.70927 batch 1000/3281 lr 0.001 accuracy 86.40625 wps 14507.33 step time 0.46s\n","# Epoch 1  global step 1020 loss 0.68993 batch 1020/3281 lr 0.001 accuracy 86.69922 wps 13816.60 step time 0.50s\n","# Epoch 1  global step 1040 loss 0.70221 batch 1040/3281 lr 0.001 accuracy 86.55859 wps 14275.31 step time 0.50s\n","# Epoch 1  global step 1060 loss 0.72275 batch 1060/3281 lr 0.001 accuracy 86.03516 wps 15083.16 step time 0.54s\n","# Epoch 1  global step 1080 loss 0.66060 batch 1080/3281 lr 0.001 accuracy 87.60547 wps 13753.73 step time 0.41s\n","# Epoch 1  global step 1100 loss 0.68648 batch 1100/3281 lr 0.001 accuracy 86.96875 wps 14811.03 step time 0.50s\n","# Epoch 1  global step 1120 loss 0.68832 batch 1120/3281 lr 0.001 accuracy 86.99609 wps 13890.26 step time 0.54s\n","# Epoch 1  global step 1140 loss 0.69672 batch 1140/3281 lr 0.001 accuracy 86.69141 wps 14469.09 step time 0.61s\n","# Epoch 1  global step 1160 loss 0.66710 batch 1160/3281 lr 0.001 accuracy 87.05078 wps 14418.09 step time 0.45s\n","# Epoch 1  global step 1180 loss 0.66826 batch 1180/3281 lr 0.001 accuracy 86.95703 wps 14774.81 step time 0.59s\n","# Epoch 1  global step 1200 loss 0.73650 batch 1200/3281 lr 0.001 accuracy 85.79297 wps 14867.20 step time 0.65s\n","# Epoch 1  global step 1220 loss 0.65962 batch 1220/3281 lr 0.001 accuracy 87.49609 wps 14479.14 step time 0.46s\n","# Epoch 1  global step 1240 loss 0.65468 batch 1240/3281 lr 0.001 accuracy 87.74219 wps 14412.58 step time 0.46s\n","# Epoch 1  global step 1260 loss 0.66307 batch 1260/3281 lr 0.001 accuracy 87.51953 wps 14633.67 step time 0.49s\n","# Epoch 1  global step 1280 loss 0.63496 batch 1280/3281 lr 0.001 accuracy 88.02734 wps 14131.42 step time 0.54s\n","# Epoch 1  global step 1300 loss 0.62272 batch 1300/3281 lr 0.001 accuracy 88.34766 wps 14707.80 step time 0.49s\n","# Epoch 1  global step 1320 loss 0.65807 batch 1320/3281 lr 0.001 accuracy 87.70703 wps 14818.86 step time 0.51s\n","# Epoch 1  global step 1340 loss 0.62379 batch 1340/3281 lr 0.001 accuracy 88.13281 wps 14676.29 step time 0.49s\n","# Epoch 1  global step 1360 loss 0.63151 batch 1360/3281 lr 0.001 accuracy 88.10156 wps 14492.44 step time 0.57s\n","# Epoch 1  global step 1380 loss 0.60653 batch 1380/3281 lr 0.001 accuracy 88.58594 wps 14412.19 step time 0.47s\n","# Epoch 1  global step 1400 loss 0.56794 batch 1400/3281 lr 0.001 accuracy 89.69922 wps 14178.80 step time 0.44s\n","# Epoch 1  global step 1420 loss 0.61528 batch 1420/3281 lr 0.001 accuracy 88.50391 wps 15073.42 step time 0.54s\n","# Epoch 1  global step 1440 loss 0.61259 batch 1440/3281 lr 0.001 accuracy 88.54297 wps 14517.39 step time 0.60s\n","# Epoch 1  global step 1460 loss 0.60367 batch 1460/3281 lr 0.001 accuracy 88.80078 wps 15039.81 step time 0.55s\n","# Epoch 1  global step 1480 loss 0.56043 batch 1480/3281 lr 0.001 accuracy 89.73828 wps 14227.77 step time 0.47s\n","# Epoch 1  global step 1500 loss 0.57777 batch 1500/3281 lr 0.001 accuracy 89.45312 wps 14777.51 step time 0.50s\n","# Epoch 1  global step 1520 loss 0.55761 batch 1520/3281 lr 0.001 accuracy 89.56250 wps 14661.26 step time 0.47s\n","# Epoch 1  global step 1540 loss 0.55823 batch 1540/3281 lr 0.001 accuracy 89.66406 wps 14860.61 step time 0.49s\n","# Epoch 1  global step 1560 loss 0.52388 batch 1560/3281 lr 0.001 accuracy 90.61719 wps 14404.28 step time 0.44s\n","# Epoch 1  global step 1580 loss 0.54387 batch 1580/3281 lr 0.001 accuracy 90.05469 wps 13821.84 step time 0.61s\n","# Epoch 1  global step 1600 loss 0.53397 batch 1600/3281 lr 0.001 accuracy 90.26172 wps 14282.17 step time 0.53s\n","# Epoch 1  global step 1620 loss 0.54458 batch 1620/3281 lr 0.001 accuracy 90.08203 wps 14444.18 step time 0.45s\n","# Epoch 1  global step 1640 loss 0.53677 batch 1640/3281 lr 0.001 accuracy 90.24219 wps 14796.66 step time 0.47s\n","# Epoch 1  global step 1660 loss 0.51006 batch 1660/3281 lr 0.001 accuracy 90.59375 wps 14699.06 step time 0.47s\n","# Epoch 1  global step 1680 loss 0.53169 batch 1680/3281 lr 0.001 accuracy 90.18359 wps 15336.41 step time 0.56s\n","# Epoch 1  global step 1700 loss 0.53674 batch 1700/3281 lr 0.001 accuracy 90.12109 wps 14295.51 step time 0.54s\n","# Epoch 1  global step 1720 loss 0.52694 batch 1720/3281 lr 0.001 accuracy 90.27734 wps 14205.69 step time 0.54s\n","# Epoch 1  global step 1740 loss 0.51926 batch 1740/3281 lr 0.001 accuracy 90.64062 wps 14722.48 step time 0.48s\n","# Epoch 1  global step 1760 loss 0.53640 batch 1760/3281 lr 0.001 accuracy 90.06641 wps 12877.64 step time 0.75s\n","# Epoch 1  global step 1780 loss 0.53259 batch 1780/3281 lr 0.001 accuracy 90.21484 wps 13162.75 step time 0.59s\n","# Epoch 1  global step 1800 loss 0.51727 batch 1800/3281 lr 0.001 accuracy 90.43750 wps 12810.60 step time 0.62s\n","# Epoch 1  global step 1820 loss 0.48599 batch 1820/3281 lr 0.001 accuracy 91.39062 wps 12556.43 step time 0.48s\n","# Epoch 1  global step 1840 loss 0.48204 batch 1840/3281 lr 0.001 accuracy 91.26172 wps 12317.32 step time 0.51s\n","# Epoch 1  global step 1860 loss 0.48523 batch 1860/3281 lr 0.001 accuracy 91.21875 wps 12269.25 step time 0.60s\n","# Epoch 1  global step 1880 loss 0.48672 batch 1880/3281 lr 0.001 accuracy 91.10547 wps 13057.31 step time 0.49s\n","# Epoch 1  global step 1900 loss 0.51438 batch 1900/3281 lr 0.001 accuracy 90.89062 wps 13108.77 step time 0.55s\n","# Epoch 1  global step 1920 loss 0.45854 batch 1920/3281 lr 0.001 accuracy 91.70703 wps 12824.74 step time 0.47s\n","# Epoch 1  global step 1940 loss 0.48264 batch 1940/3281 lr 0.001 accuracy 91.15625 wps 13158.94 step time 0.51s\n","# Epoch 1  global step 1960 loss 0.48350 batch 1960/3281 lr 0.001 accuracy 91.03516 wps 12190.67 step time 0.60s\n","# Epoch 1  global step 1980 loss 0.48865 batch 1980/3281 lr 0.001 accuracy 91.03516 wps 13343.48 step time 0.60s\n","# Epoch 1  global step 2000 loss 0.47534 batch 2000/3281 lr 0.001 accuracy 91.28125 wps 12754.72 step time 0.58s\n","# global step 2000, eval model at Sun May 24 11:34:44 2020\n","2020-05-24 11:34:46.761990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:34:46.762310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-24 11:34:46.762423: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 11:34:46.762455: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-24 11:34:46.762474: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-24 11:34:46.762494: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-24 11:34:46.762519: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-24 11:34:46.762540: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-24 11:34:46.762561: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 11:34:46.762652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:34:46.763011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:34:46.763224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-24 11:34:46.763311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 11:34:46.763327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-24 11:34:46.763337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-24 11:34:46.763465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:34:46.763736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:34:46.763950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","# location_traffic_convenience - 0.21969951788317077\n","# location_distance_from_business_district - 0.22255105060668837\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14272103658536586\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.1850196944737621\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.20089730807577266\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 0.98348, f1 0.20162\n","# current result -0.20162052780075151, previous best result 1000000000\n","# Epoch 1  global step 2020 loss 0.47540 batch 2020/3281 lr 0.001 accuracy 91.35938 wps 12835.07 step time 0.50s\n","# Epoch 1  global step 2040 loss 0.50922 batch 2040/3281 lr 0.001 accuracy 90.70703 wps 12882.15 step time 0.61s\n","# Epoch 1  global step 2060 loss 0.46092 batch 2060/3281 lr 0.001 accuracy 91.79297 wps 13109.15 step time 0.54s\n","# Epoch 1  global step 2080 loss 0.48939 batch 2080/3281 lr 0.001 accuracy 91.23828 wps 12139.84 step time 0.65s\n","# Epoch 1  global step 2100 loss 0.49354 batch 2100/3281 lr 0.001 accuracy 90.88281 wps 12044.85 step time 0.73s\n","# Epoch 1  global step 2120 loss 0.47375 batch 2120/3281 lr 0.001 accuracy 91.51172 wps 12875.91 step time 0.51s\n","# Epoch 1  global step 2140 loss 0.46997 batch 2140/3281 lr 0.001 accuracy 91.43750 wps 12650.89 step time 0.57s\n","# Epoch 1  global step 2160 loss 0.47748 batch 2160/3281 lr 0.001 accuracy 91.39844 wps 12193.42 step time 0.67s\n","# Epoch 1  global step 2180 loss 0.46528 batch 2180/3281 lr 0.001 accuracy 91.58984 wps 13345.23 step time 0.53s\n","# Epoch 1  global step 2200 loss 0.46714 batch 2200/3281 lr 0.001 accuracy 91.53516 wps 12136.72 step time 0.60s\n","# Epoch 1  global step 2220 loss 0.45527 batch 2220/3281 lr 0.001 accuracy 91.77734 wps 13418.54 step time 0.52s\n","# Epoch 1  global step 2240 loss 0.43721 batch 2240/3281 lr 0.001 accuracy 92.14453 wps 13026.58 step time 0.48s\n","# Epoch 1  global step 2260 loss 0.47223 batch 2260/3281 lr 0.001 accuracy 91.65234 wps 12417.54 step time 0.60s\n","# Epoch 1  global step 2280 loss 0.47300 batch 2280/3281 lr 0.001 accuracy 91.52344 wps 13890.69 step time 0.54s\n","# Epoch 1  global step 2300 loss 0.44891 batch 2300/3281 lr 0.001 accuracy 91.93359 wps 12551.79 step time 0.58s\n","# Epoch 1  global step 2320 loss 0.45852 batch 2320/3281 lr 0.001 accuracy 91.66406 wps 12355.78 step time 0.67s\n","# Epoch 1  global step 2340 loss 0.45264 batch 2340/3281 lr 0.001 accuracy 91.88281 wps 12886.97 step time 0.58s\n","# Epoch 1  global step 2360 loss 0.45796 batch 2360/3281 lr 0.001 accuracy 91.94141 wps 12986.92 step time 0.56s\n","# Epoch 1  global step 2380 loss 0.46384 batch 2380/3281 lr 0.001 accuracy 91.60547 wps 12672.04 step time 0.66s\n","# Epoch 1  global step 2400 loss 0.47084 batch 2400/3281 lr 0.001 accuracy 91.48047 wps 12832.63 step time 0.61s\n","# Epoch 1  global step 2420 loss 0.44994 batch 2420/3281 lr 0.001 accuracy 91.88672 wps 13075.30 step time 0.52s\n","# Epoch 1  global step 2440 loss 0.43708 batch 2440/3281 lr 0.001 accuracy 92.29688 wps 12857.79 step time 0.51s\n","# Epoch 1  global step 2460 loss 0.44720 batch 2460/3281 lr 0.001 accuracy 92.02734 wps 12051.76 step time 0.65s\n","# Epoch 1  global step 2480 loss 0.43071 batch 2480/3281 lr 0.001 accuracy 92.20703 wps 12936.47 step time 0.48s\n","# Epoch 1  global step 2500 loss 0.44783 batch 2500/3281 lr 0.001 accuracy 91.96094 wps 12735.80 step time 0.58s\n","# Epoch 1  global step 2520 loss 0.45066 batch 2520/3281 lr 0.001 accuracy 91.90625 wps 12231.98 step time 0.58s\n","# Epoch 1  global step 2540 loss 0.43104 batch 2540/3281 lr 0.001 accuracy 92.26562 wps 12846.44 step time 0.48s\n","# Epoch 1  global step 2560 loss 0.44257 batch 2560/3281 lr 0.001 accuracy 92.06641 wps 12906.28 step time 0.51s\n","# Epoch 1  global step 2580 loss 0.42971 batch 2580/3281 lr 0.001 accuracy 92.37500 wps 12963.96 step time 0.50s\n","# Epoch 1  global step 2600 loss 0.45654 batch 2600/3281 lr 0.001 accuracy 91.81250 wps 13072.43 step time 0.53s\n","# Epoch 1  global step 2620 loss 0.46612 batch 2620/3281 lr 0.001 accuracy 91.65625 wps 12706.01 step time 0.61s\n","# Epoch 1  global step 2640 loss 0.46436 batch 2640/3281 lr 0.001 accuracy 91.39844 wps 12780.87 step time 0.64s\n","# Epoch 1  global step 2660 loss 0.44480 batch 2660/3281 lr 0.001 accuracy 91.97656 wps 12996.42 step time 0.52s\n","# Epoch 1  global step 2680 loss 0.43386 batch 2680/3281 lr 0.001 accuracy 92.19141 wps 13150.58 step time 0.47s\n","# Epoch 1  global step 2700 loss 0.44362 batch 2700/3281 lr 0.001 accuracy 91.90234 wps 12196.36 step time 0.64s\n","# Epoch 1  global step 2720 loss 0.42322 batch 2720/3281 lr 0.001 accuracy 92.51953 wps 12921.15 step time 0.52s\n","# Epoch 1  global step 2740 loss 0.43917 batch 2740/3281 lr 0.001 accuracy 92.05859 wps 12657.21 step time 0.50s\n","# Epoch 1  global step 2760 loss 0.43348 batch 2760/3281 lr 0.001 accuracy 92.19922 wps 12109.51 step time 0.58s\n","# Epoch 1  global step 2780 loss 0.43346 batch 2780/3281 lr 0.001 accuracy 92.04688 wps 12688.05 step time 0.61s\n","# Epoch 1  global step 2800 loss 0.43189 batch 2800/3281 lr 0.001 accuracy 92.06641 wps 12838.52 step time 0.52s\n","# Epoch 1  global step 2820 loss 0.42396 batch 2820/3281 lr 0.001 accuracy 92.24219 wps 11877.53 step time 0.61s\n","# Epoch 1  global step 2840 loss 0.44121 batch 2840/3281 lr 0.001 accuracy 92.12891 wps 13107.42 step time 0.60s\n","# Epoch 1  global step 2860 loss 0.44899 batch 2860/3281 lr 0.001 accuracy 92.03906 wps 11831.60 step time 0.70s\n","# Epoch 1  global step 2880 loss 0.43451 batch 2880/3281 lr 0.001 accuracy 92.19922 wps 12930.45 step time 0.52s\n","# Epoch 1  global step 2900 loss 0.42345 batch 2900/3281 lr 0.001 accuracy 92.25000 wps 11805.50 step time 0.65s\n","# Epoch 1  global step 2920 loss 0.41834 batch 2920/3281 lr 0.001 accuracy 92.50781 wps 12949.89 step time 0.52s\n","# Epoch 1  global step 2940 loss 0.43539 batch 2940/3281 lr 0.001 accuracy 92.28516 wps 13292.79 step time 0.57s\n","# Epoch 1  global step 2960 loss 0.44935 batch 2960/3281 lr 0.001 accuracy 92.03125 wps 12314.27 step time 0.55s\n","# Epoch 1  global step 2980 loss 0.43135 batch 2980/3281 lr 0.001 accuracy 92.33203 wps 11534.22 step time 0.67s\n","# Epoch 1  global step 3000 loss 0.40572 batch 3000/3281 lr 0.001 accuracy 92.63672 wps 13018.46 step time 0.48s\n","# Epoch 1  global step 3020 loss 0.42217 batch 3020/3281 lr 0.001 accuracy 92.56250 wps 12648.01 step time 0.61s\n","# Epoch 1  global step 3040 loss 0.45098 batch 3040/3281 lr 0.001 accuracy 91.82031 wps 12590.14 step time 0.65s\n","# Epoch 1  global step 3060 loss 0.43799 batch 3060/3281 lr 0.001 accuracy 92.19922 wps 13062.35 step time 0.56s\n","# Epoch 1  global step 3080 loss 0.42978 batch 3080/3281 lr 0.001 accuracy 92.10156 wps 12575.93 step time 0.63s\n","# Epoch 1  global step 3100 loss 0.43646 batch 3100/3281 lr 0.001 accuracy 92.25391 wps 11457.71 step time 0.78s\n","# Epoch 1  global step 3120 loss 0.42141 batch 3120/3281 lr 0.001 accuracy 92.36719 wps 13069.51 step time 0.52s\n","# Epoch 1  global step 3140 loss 0.43493 batch 3140/3281 lr 0.001 accuracy 92.18359 wps 12626.68 step time 0.55s\n","# Epoch 1  global step 3160 loss 0.43557 batch 3160/3281 lr 0.001 accuracy 92.05078 wps 12249.44 step time 0.71s\n","# Epoch 1  global step 3180 loss 0.43392 batch 3180/3281 lr 0.001 accuracy 92.26953 wps 12239.39 step time 0.51s\n","# Epoch 1  global step 3200 loss 0.42626 batch 3200/3281 lr 0.001 accuracy 92.18359 wps 12303.85 step time 0.62s\n","# Epoch 1  global step 3220 loss 0.41365 batch 3220/3281 lr 0.001 accuracy 92.61328 wps 12082.07 step time 0.62s\n","# Epoch 1  global step 3240 loss 0.44188 batch 3240/3281 lr 0.001 accuracy 91.87500 wps 13353.07 step time 0.67s\n","# Epoch 1  global step 3260 loss 0.41717 batch 3260/3281 lr 0.001 accuracy 92.30078 wps 13049.79 step time 0.49s\n","# Epoch 1  global step 3280 loss 0.44013 batch 3280/3281 lr 0.001 accuracy 92.04297 wps 13422.13 step time 0.64s\n","# Finsh epoch 1, global step 3282\n","# Epoch 2  global step 3300 loss 0.39499 batch 18/3281 lr 0.001 accuracy 82.74609 wps 14637.51 step time 0.53s\n","# Epoch 2  global step 3320 loss 0.41757 batch 38/3281 lr 0.001 accuracy 92.39453 wps 14608.15 step time 0.47s\n","# Epoch 2  global step 3340 loss 0.41435 batch 58/3281 lr 0.001 accuracy 92.31250 wps 14397.79 step time 0.45s\n","# Epoch 2  global step 3360 loss 0.41419 batch 78/3281 lr 0.001 accuracy 92.55078 wps 14292.85 step time 0.54s\n","# Epoch 2  global step 3380 loss 0.42482 batch 98/3281 lr 0.001 accuracy 92.13672 wps 14887.25 step time 0.50s\n","# Epoch 2  global step 3400 loss 0.43111 batch 118/3281 lr 0.001 accuracy 92.10156 wps 14655.35 step time 0.59s\n","# Epoch 2  global step 3420 loss 0.42788 batch 138/3281 lr 0.001 accuracy 92.35547 wps 15048.57 step time 0.51s\n","# Epoch 2  global step 3440 loss 0.39193 batch 158/3281 lr 0.001 accuracy 92.91016 wps 14059.19 step time 0.42s\n","# Epoch 2  global step 3460 loss 0.41048 batch 178/3281 lr 0.001 accuracy 92.57422 wps 14316.52 step time 0.43s\n","# Epoch 2  global step 3480 loss 0.43658 batch 198/3281 lr 0.001 accuracy 92.03906 wps 15406.51 step time 0.56s\n","# Epoch 2  global step 3500 loss 0.42249 batch 218/3281 lr 0.001 accuracy 92.38672 wps 14553.44 step time 0.47s\n","# Epoch 2  global step 3520 loss 0.38648 batch 238/3281 lr 0.001 accuracy 93.01562 wps 13667.48 step time 0.49s\n","# Epoch 2  global step 3540 loss 0.44038 batch 258/3281 lr 0.001 accuracy 92.08203 wps 15005.84 step time 0.50s\n","# Epoch 2  global step 3560 loss 0.43705 batch 278/3281 lr 0.001 accuracy 92.03125 wps 13908.44 step time 0.67s\n","# Epoch 2  global step 3580 loss 0.41710 batch 298/3281 lr 0.001 accuracy 92.38672 wps 14698.08 step time 0.47s\n","# Epoch 2  global step 3600 loss 0.40076 batch 318/3281 lr 0.001 accuracy 92.79687 wps 13654.87 step time 0.50s\n","# Epoch 2  global step 3620 loss 0.42561 batch 338/3281 lr 0.001 accuracy 92.43359 wps 14846.28 step time 0.48s\n","# Epoch 2  global step 3640 loss 0.40991 batch 358/3281 lr 0.001 accuracy 92.43750 wps 14411.45 step time 0.55s\n","# Epoch 2  global step 3660 loss 0.40741 batch 378/3281 lr 0.001 accuracy 92.67969 wps 14660.41 step time 0.46s\n","# Epoch 2  global step 3680 loss 0.41000 batch 398/3281 lr 0.001 accuracy 92.55078 wps 14325.97 step time 0.44s\n","# Epoch 2  global step 3700 loss 0.41942 batch 418/3281 lr 0.001 accuracy 92.27344 wps 15036.81 step time 0.52s\n","# Epoch 2  global step 3720 loss 0.42803 batch 438/3281 lr 0.001 accuracy 92.27734 wps 14711.39 step time 0.47s\n","# Epoch 2  global step 3740 loss 0.38398 batch 458/3281 lr 0.001 accuracy 93.10938 wps 14510.18 step time 0.46s\n","# Epoch 2  global step 3760 loss 0.42747 batch 478/3281 lr 0.001 accuracy 92.35156 wps 14925.33 step time 0.50s\n","# Epoch 2  global step 3780 loss 0.39586 batch 498/3281 lr 0.001 accuracy 92.95703 wps 14494.33 step time 0.45s\n","# Epoch 2  global step 3800 loss 0.41353 batch 518/3281 lr 0.001 accuracy 92.51562 wps 14360.53 step time 0.56s\n","# Epoch 2  global step 3820 loss 0.41568 batch 538/3281 lr 0.001 accuracy 92.29687 wps 14659.79 step time 0.47s\n","# Epoch 2  global step 3840 loss 0.42681 batch 558/3281 lr 0.001 accuracy 92.21484 wps 14716.18 step time 0.57s\n","# Epoch 2  global step 3860 loss 0.43681 batch 578/3281 lr 0.001 accuracy 92.07812 wps 14649.46 step time 0.57s\n","# Epoch 2  global step 3880 loss 0.39807 batch 598/3281 lr 0.001 accuracy 92.95703 wps 14545.89 step time 0.45s\n","# Epoch 2  global step 3900 loss 0.40814 batch 618/3281 lr 0.001 accuracy 92.60156 wps 14120.89 step time 0.52s\n","# Epoch 2  global step 3920 loss 0.41069 batch 638/3281 lr 0.001 accuracy 92.46875 wps 14361.49 step time 0.44s\n","# Epoch 2  global step 3940 loss 0.41889 batch 658/3281 lr 0.001 accuracy 92.48047 wps 14599.67 step time 0.55s\n","# Epoch 2  global step 3960 loss 0.40677 batch 678/3281 lr 0.001 accuracy 92.34375 wps 14699.54 step time 0.48s\n","# Epoch 2  global step 3980 loss 0.39343 batch 698/3281 lr 0.001 accuracy 92.85156 wps 14118.12 step time 0.52s\n","# Epoch 2  global step 4000 loss 0.40920 batch 718/3281 lr 0.001 accuracy 92.33594 wps 14695.21 step time 0.47s\n","# global step 4000, eval model at Sun May 24 11:55:23 2020\n","2020-05-24 11:55:25.062114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:55:25.062437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-24 11:55:25.062549: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 11:55:25.062576: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-24 11:55:25.062598: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-24 11:55:25.062616: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-24 11:55:25.062636: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-24 11:55:25.062655: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-24 11:55:25.062675: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 11:55:25.062790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:55:25.063058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:55:25.063265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-24 11:55:25.063312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 11:55:25.063326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-24 11:55:25.063339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-24 11:55:25.063474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:55:25.063744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 11:55:25.063957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.22041519336695417\n","# location_distance_from_business_district - 0.22255105060668837\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14272103658536586\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.21018369811880586\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.21354338332234674\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 0.85975, f1 0.20355\n","# current result -0.20354681551952156, previous best result -0.20162052780075151\n","# Epoch 2  global step 4020 loss 0.40854 batch 738/3281 lr 0.001 accuracy 92.74219 wps 14444.10 step time 0.46s\n","# Epoch 2  global step 4040 loss 0.40589 batch 758/3281 lr 0.001 accuracy 92.55469 wps 14572.03 step time 0.45s\n","# Epoch 2  global step 4060 loss 0.41784 batch 778/3281 lr 0.001 accuracy 92.39062 wps 14684.83 step time 0.47s\n","# Epoch 2  global step 4080 loss 0.40313 batch 798/3281 lr 0.001 accuracy 92.83594 wps 14814.75 step time 0.49s\n","# Epoch 2  global step 4100 loss 0.39632 batch 818/3281 lr 0.001 accuracy 92.88281 wps 14351.23 step time 0.42s\n","# Epoch 2  global step 4120 loss 0.42860 batch 838/3281 lr 0.001 accuracy 92.08203 wps 14677.76 step time 0.60s\n","# Epoch 2  global step 4140 loss 0.41016 batch 858/3281 lr 0.001 accuracy 92.53906 wps 14941.75 step time 0.51s\n","# Epoch 2  global step 4160 loss 0.42944 batch 878/3281 lr 0.001 accuracy 92.32031 wps 14587.05 step time 0.57s\n","# Epoch 2  global step 4180 loss 0.40655 batch 898/3281 lr 0.001 accuracy 92.72266 wps 14969.31 step time 0.50s\n","# Epoch 2  global step 4200 loss 0.39453 batch 918/3281 lr 0.001 accuracy 92.83203 wps 14508.68 step time 0.46s\n","# Epoch 2  global step 4220 loss 0.41821 batch 938/3281 lr 0.001 accuracy 92.44922 wps 15047.88 step time 0.49s\n","# Epoch 2  global step 4240 loss 0.39615 batch 958/3281 lr 0.001 accuracy 92.83594 wps 14619.90 step time 0.45s\n","# Epoch 2  global step 4260 loss 0.43368 batch 978/3281 lr 0.001 accuracy 92.03516 wps 15065.91 step time 0.52s\n","# Epoch 2  global step 4280 loss 0.38972 batch 998/3281 lr 0.001 accuracy 92.91016 wps 14686.76 step time 0.45s\n","# Epoch 2  global step 4300 loss 0.39389 batch 1018/3281 lr 0.001 accuracy 92.93359 wps 14333.65 step time 0.53s\n","# Epoch 2  global step 4320 loss 0.39801 batch 1038/3281 lr 0.001 accuracy 92.99219 wps 14646.24 step time 0.45s\n","# Epoch 2  global step 4340 loss 0.42268 batch 1058/3281 lr 0.001 accuracy 92.48047 wps 14700.56 step time 0.59s\n","# Epoch 2  global step 4360 loss 0.39962 batch 1078/3281 lr 0.001 accuracy 92.87891 wps 14069.91 step time 0.51s\n","# Epoch 2  global step 4380 loss 0.39082 batch 1098/3281 lr 0.001 accuracy 93.11719 wps 14904.03 step time 0.48s\n","# Epoch 2  global step 4400 loss 0.43594 batch 1118/3281 lr 0.001 accuracy 91.94531 wps 14470.19 step time 0.70s\n","# Epoch 2  global step 4420 loss 0.41703 batch 1138/3281 lr 0.001 accuracy 92.47656 wps 14791.80 step time 0.48s\n","# Epoch 2  global step 4440 loss 0.38669 batch 1158/3281 lr 0.001 accuracy 92.83594 wps 14550.24 step time 0.45s\n","# Epoch 2  global step 4460 loss 0.39400 batch 1178/3281 lr 0.001 accuracy 92.87109 wps 14308.39 step time 0.44s\n","# Epoch 2  global step 4480 loss 0.40606 batch 1198/3281 lr 0.001 accuracy 92.71484 wps 14441.21 step time 0.44s\n","# Epoch 2  global step 4500 loss 0.41822 batch 1218/3281 lr 0.001 accuracy 92.31641 wps 14731.37 step time 0.58s\n","# Epoch 2  global step 4520 loss 0.40167 batch 1238/3281 lr 0.001 accuracy 92.80078 wps 13773.81 step time 0.57s\n","# Epoch 2  global step 4540 loss 0.40842 batch 1258/3281 lr 0.001 accuracy 92.46484 wps 13279.45 step time 0.66s\n","# Epoch 2  global step 4560 loss 0.37561 batch 1278/3281 lr 0.001 accuracy 93.21484 wps 13698.83 step time 0.50s\n","# Epoch 2  global step 4580 loss 0.42752 batch 1298/3281 lr 0.001 accuracy 92.30078 wps 14976.20 step time 0.49s\n","# Epoch 2  global step 4600 loss 0.42437 batch 1318/3281 lr 0.001 accuracy 92.22656 wps 15208.59 step time 0.52s\n","# Epoch 2  global step 4620 loss 0.40609 batch 1338/3281 lr 0.001 accuracy 92.55078 wps 14270.08 step time 0.54s\n","# Epoch 2  global step 4640 loss 0.38428 batch 1358/3281 lr 0.001 accuracy 92.82812 wps 14322.17 step time 0.43s\n","# Epoch 2  global step 4660 loss 0.42804 batch 1378/3281 lr 0.001 accuracy 92.01953 wps 14268.46 step time 0.55s\n","# Epoch 2  global step 4680 loss 0.40749 batch 1398/3281 lr 0.001 accuracy 92.44141 wps 14988.27 step time 0.50s\n","# Epoch 2  global step 4700 loss 0.40090 batch 1418/3281 lr 0.001 accuracy 92.76953 wps 14654.66 step time 0.46s\n","# Epoch 2  global step 4720 loss 0.41077 batch 1438/3281 lr 0.001 accuracy 92.63281 wps 14306.71 step time 0.54s\n","# Epoch 2  global step 4740 loss 0.39693 batch 1458/3281 lr 0.001 accuracy 92.79687 wps 14117.30 step time 0.51s\n","# Epoch 2  global step 4760 loss 0.41834 batch 1478/3281 lr 0.001 accuracy 92.34766 wps 14855.14 step time 0.49s\n","# Epoch 2  global step 4780 loss 0.40902 batch 1498/3281 lr 0.001 accuracy 92.55859 wps 14611.06 step time 0.55s\n","# Epoch 2  global step 4800 loss 0.40155 batch 1518/3281 lr 0.001 accuracy 92.67969 wps 14914.44 step time 0.49s\n","# Epoch 2  global step 4820 loss 0.41443 batch 1538/3281 lr 0.001 accuracy 92.51953 wps 14763.12 step time 0.59s\n","# Epoch 2  global step 4840 loss 0.40158 batch 1558/3281 lr 0.001 accuracy 92.72656 wps 14472.81 step time 0.43s\n","# Epoch 2  global step 4860 loss 0.38796 batch 1578/3281 lr 0.001 accuracy 92.92578 wps 14506.35 step time 0.45s\n","# Epoch 2  global step 4880 loss 0.42906 batch 1598/3281 lr 0.001 accuracy 92.09375 wps 15445.74 step time 0.55s\n","# Epoch 2  global step 4900 loss 0.40765 batch 1618/3281 lr 0.001 accuracy 92.50000 wps 15275.83 step time 0.55s\n","# Epoch 2  global step 4920 loss 0.40970 batch 1638/3281 lr 0.001 accuracy 92.51953 wps 15117.75 step time 0.51s\n","# Epoch 2  global step 4940 loss 0.37943 batch 1658/3281 lr 0.001 accuracy 93.19531 wps 14551.38 step time 0.46s\n","# Epoch 2  global step 4960 loss 0.43558 batch 1678/3281 lr 0.001 accuracy 92.20312 wps 14782.59 step time 0.58s\n","# Epoch 2  global step 4980 loss 0.43584 batch 1698/3281 lr 0.001 accuracy 91.98047 wps 14567.43 step time 0.59s\n","# Epoch 2  global step 5000 loss 0.39262 batch 1718/3281 lr 0.001 accuracy 92.96094 wps 14366.34 step time 0.42s\n","# Epoch 2  global step 5020 loss 0.41175 batch 1738/3281 lr 0.001 accuracy 92.39844 wps 15050.08 step time 0.50s\n","# Epoch 2  global step 5040 loss 0.40309 batch 1758/3281 lr 0.001 accuracy 92.60938 wps 14962.52 step time 0.50s\n","# Epoch 2  global step 5060 loss 0.40183 batch 1778/3281 lr 0.001 accuracy 92.75391 wps 15124.99 step time 0.51s\n","# Epoch 2  global step 5080 loss 0.38493 batch 1798/3281 lr 0.001 accuracy 92.97656 wps 14342.26 step time 0.43s\n","# Epoch 2  global step 5100 loss 0.41332 batch 1818/3281 lr 0.001 accuracy 92.60156 wps 14303.96 step time 0.52s\n","# Epoch 2  global step 5120 loss 0.41349 batch 1838/3281 lr 0.001 accuracy 92.55469 wps 15027.92 step time 0.50s\n","# Epoch 2  global step 5140 loss 0.38446 batch 1858/3281 lr 0.001 accuracy 92.92188 wps 14794.72 step time 0.46s\n","# Epoch 2  global step 5160 loss 0.38977 batch 1878/3281 lr 0.001 accuracy 92.91016 wps 14521.33 step time 0.45s\n","# Epoch 2  global step 5180 loss 0.40677 batch 1898/3281 lr 0.001 accuracy 92.36719 wps 14652.54 step time 0.56s\n","# Epoch 2  global step 5200 loss 0.40158 batch 1918/3281 lr 0.001 accuracy 92.63672 wps 14541.46 step time 0.44s\n","# Epoch 2  global step 5220 loss 0.37558 batch 1938/3281 lr 0.001 accuracy 93.25391 wps 13817.05 step time 0.40s\n","# Epoch 2  global step 5240 loss 0.41065 batch 1958/3281 lr 0.001 accuracy 92.50000 wps 15151.41 step time 0.52s\n","# Epoch 2  global step 5260 loss 0.38071 batch 1978/3281 lr 0.001 accuracy 93.11719 wps 14658.20 step time 0.45s\n","# Epoch 2  global step 5280 loss 0.38198 batch 1998/3281 lr 0.001 accuracy 93.16016 wps 14639.35 step time 0.46s\n","# Epoch 2  global step 5300 loss 0.40538 batch 2018/3281 lr 0.001 accuracy 92.69141 wps 14672.21 step time 0.48s\n","# Epoch 2  global step 5320 loss 0.42221 batch 2038/3281 lr 0.001 accuracy 92.25000 wps 14346.46 step time 0.53s\n","# Epoch 2  global step 5340 loss 0.38876 batch 2058/3281 lr 0.001 accuracy 92.83984 wps 14981.60 step time 0.52s\n","# Epoch 2  global step 5360 loss 0.40342 batch 2078/3281 lr 0.001 accuracy 92.57422 wps 13863.35 step time 0.51s\n","# Epoch 2  global step 5380 loss 0.43406 batch 2098/3281 lr 0.001 accuracy 91.98047 wps 14521.24 step time 0.71s\n","# Epoch 2  global step 5400 loss 0.40690 batch 2118/3281 lr 0.001 accuracy 92.47266 wps 14747.42 step time 0.49s\n","# Epoch 2  global step 5420 loss 0.37171 batch 2138/3281 lr 0.001 accuracy 93.16797 wps 14107.46 step time 0.51s\n","# Epoch 2  global step 5440 loss 0.42270 batch 2158/3281 lr 0.001 accuracy 92.26172 wps 14776.72 step time 0.49s\n","# Epoch 2  global step 5460 loss 0.39719 batch 2178/3281 lr 0.001 accuracy 92.71094 wps 14874.68 step time 0.48s\n","# Epoch 2  global step 5480 loss 0.39658 batch 2198/3281 lr 0.001 accuracy 92.71094 wps 15150.43 step time 0.53s\n","# Epoch 2  global step 5500 loss 0.40389 batch 2218/3281 lr 0.001 accuracy 92.62500 wps 14365.40 step time 0.50s\n","# Epoch 2  global step 5520 loss 0.39663 batch 2238/3281 lr 0.001 accuracy 92.83594 wps 12644.23 step time 0.50s\n","# Epoch 2  global step 5540 loss 0.41591 batch 2258/3281 lr 0.001 accuracy 92.39844 wps 12907.12 step time 0.63s\n","# Epoch 2  global step 5560 loss 0.40025 batch 2278/3281 lr 0.001 accuracy 92.79297 wps 12415.96 step time 0.67s\n","# Epoch 2  global step 5580 loss 0.40191 batch 2298/3281 lr 0.001 accuracy 92.69141 wps 12558.86 step time 0.61s\n","# Epoch 2  global step 5600 loss 0.40788 batch 2318/3281 lr 0.001 accuracy 92.55859 wps 12814.29 step time 0.52s\n","# Epoch 2  global step 5620 loss 0.35792 batch 2338/3281 lr 0.001 accuracy 93.53906 wps 12938.07 step time 0.42s\n","# Epoch 2  global step 5640 loss 0.41766 batch 2358/3281 lr 0.001 accuracy 92.48047 wps 13200.45 step time 0.55s\n","# Epoch 2  global step 5660 loss 0.39913 batch 2378/3281 lr 0.001 accuracy 92.76562 wps 12353.87 step time 0.61s\n","# Epoch 2  global step 5680 loss 0.40801 batch 2398/3281 lr 0.001 accuracy 92.58984 wps 13115.73 step time 0.53s\n","# Epoch 2  global step 5700 loss 0.39156 batch 2418/3281 lr 0.001 accuracy 92.95703 wps 11936.93 step time 0.62s\n","# Epoch 2  global step 5720 loss 0.38527 batch 2438/3281 lr 0.001 accuracy 93.07031 wps 13361.04 step time 0.49s\n","# Epoch 2  global step 5740 loss 0.39502 batch 2458/3281 lr 0.001 accuracy 92.86328 wps 13041.93 step time 0.51s\n","# Epoch 2  global step 5760 loss 0.40901 batch 2478/3281 lr 0.001 accuracy 92.56641 wps 12870.59 step time 0.58s\n","# Epoch 2  global step 5780 loss 0.38432 batch 2498/3281 lr 0.001 accuracy 93.05078 wps 12548.74 step time 0.52s\n","# Epoch 2  global step 5800 loss 0.40055 batch 2518/3281 lr 0.001 accuracy 92.92969 wps 11780.83 step time 0.71s\n","# Epoch 2  global step 5820 loss 0.38144 batch 2538/3281 lr 0.001 accuracy 92.98828 wps 12910.76 step time 0.50s\n","# Epoch 2  global step 5840 loss 0.40168 batch 2558/3281 lr 0.001 accuracy 92.58203 wps 12747.01 step time 0.61s\n","# Epoch 2  global step 5860 loss 0.40074 batch 2578/3281 lr 0.001 accuracy 92.62500 wps 13530.15 step time 0.52s\n","# Epoch 2  global step 5880 loss 0.39509 batch 2598/3281 lr 0.001 accuracy 92.97656 wps 12776.65 step time 0.47s\n","# Epoch 2  global step 5900 loss 0.39341 batch 2618/3281 lr 0.001 accuracy 92.86328 wps 12327.81 step time 0.62s\n","# Epoch 2  global step 5920 loss 0.39293 batch 2638/3281 lr 0.001 accuracy 92.91797 wps 13096.90 step time 0.50s\n","# Epoch 2  global step 5940 loss 0.39242 batch 2658/3281 lr 0.001 accuracy 92.89844 wps 12204.45 step time 0.68s\n","# Epoch 2  global step 5960 loss 0.39456 batch 2678/3281 lr 0.001 accuracy 92.76172 wps 12635.77 step time 0.57s\n","# Epoch 2  global step 5980 loss 0.39576 batch 2698/3281 lr 0.001 accuracy 92.73438 wps 12240.29 step time 0.60s\n","# Epoch 2  global step 6000 loss 0.40049 batch 2718/3281 lr 0.001 accuracy 92.67578 wps 12415.80 step time 0.57s\n","# global step 6000, eval model at Sun May 24 12:14:56 2020\n","2020-05-24 12:14:58.469057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:14:58.469512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-24 12:14:58.469677: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 12:14:58.469699: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-24 12:14:58.469745: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-24 12:14:58.469771: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-24 12:14:58.469793: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-24 12:14:58.469816: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-24 12:14:58.469847: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 12:14:58.469955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:14:58.470237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:14:58.470441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-24 12:14:58.470556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 12:14:58.470574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-24 12:14:58.470585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-24 12:14:58.470690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:14:58.470985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:14:58.471247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.22167075227754426\n","# location_distance_from_business_district - 0.22255105060668837\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.14272103658536586\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.17106705846234815\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.30375217722003195\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.4231631759831238\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 0.74944, f1 0.21877\n","# current result -0.21876900705315117, previous best result -0.20354681551952156\n","# Epoch 2  global step 6020 loss 0.39476 batch 2738/3281 lr 0.001 accuracy 93.04688 wps 12516.95 step time 0.54s\n","# Epoch 2  global step 6040 loss 0.38738 batch 2758/3281 lr 0.001 accuracy 92.90234 wps 12998.76 step time 0.52s\n","# Epoch 2  global step 6060 loss 0.38542 batch 2778/3281 lr 0.001 accuracy 92.95703 wps 12488.70 step time 0.56s\n","# Epoch 2  global step 6080 loss 0.38440 batch 2798/3281 lr 0.001 accuracy 93.03906 wps 13269.06 step time 0.51s\n","# Epoch 2  global step 6100 loss 0.39267 batch 2818/3281 lr 0.001 accuracy 92.83594 wps 12525.05 step time 0.66s\n","# Epoch 2  global step 6120 loss 0.39514 batch 2838/3281 lr 0.001 accuracy 92.91797 wps 12874.77 step time 0.58s\n","# Epoch 2  global step 6140 loss 0.39753 batch 2858/3281 lr 0.001 accuracy 92.76172 wps 12923.42 step time 0.63s\n","# Epoch 2  global step 6160 loss 0.41126 batch 2878/3281 lr 0.001 accuracy 92.40234 wps 12516.69 step time 0.72s\n","# Epoch 2  global step 6180 loss 0.37186 batch 2898/3281 lr 0.001 accuracy 93.23828 wps 13387.61 step time 0.48s\n","# Epoch 2  global step 6200 loss 0.39967 batch 2918/3281 lr 0.001 accuracy 92.80078 wps 12554.12 step time 0.60s\n","# Epoch 2  global step 6220 loss 0.37940 batch 2938/3281 lr 0.001 accuracy 93.04297 wps 11939.35 step time 0.60s\n","# Epoch 2  global step 6240 loss 0.40422 batch 2958/3281 lr 0.001 accuracy 92.78125 wps 13111.86 step time 0.56s\n","# Epoch 2  global step 6260 loss 0.41291 batch 2978/3281 lr 0.001 accuracy 92.37109 wps 13475.23 step time 0.62s\n","# Epoch 2  global step 6280 loss 0.40013 batch 2998/3281 lr 0.001 accuracy 92.96484 wps 13225.33 step time 0.51s\n","# Epoch 2  global step 6300 loss 0.38928 batch 3018/3281 lr 0.001 accuracy 92.87891 wps 12583.38 step time 0.57s\n","# Epoch 2  global step 6320 loss 0.39512 batch 3038/3281 lr 0.001 accuracy 92.94922 wps 12831.09 step time 0.53s\n","# Epoch 2  global step 6340 loss 0.41398 batch 3058/3281 lr 0.001 accuracy 92.45312 wps 11998.76 step time 0.67s\n","# Epoch 2  global step 6360 loss 0.39274 batch 3078/3281 lr 0.001 accuracy 92.88672 wps 12915.92 step time 0.54s\n","# Epoch 2  global step 6380 loss 0.37527 batch 3098/3281 lr 0.001 accuracy 93.19531 wps 12970.59 step time 0.50s\n","# Epoch 2  global step 6400 loss 0.40859 batch 3118/3281 lr 0.001 accuracy 92.61328 wps 12354.40 step time 0.63s\n","# Epoch 2  global step 6420 loss 0.41435 batch 3138/3281 lr 0.001 accuracy 92.57031 wps 13035.18 step time 0.62s\n","# Epoch 2  global step 6440 loss 0.41689 batch 3158/3281 lr 0.001 accuracy 92.39844 wps 13180.83 step time 0.58s\n","# Epoch 2  global step 6460 loss 0.38265 batch 3178/3281 lr 0.001 accuracy 93.18750 wps 11994.49 step time 0.63s\n","# Epoch 2  global step 6480 loss 0.38620 batch 3198/3281 lr 0.001 accuracy 93.16016 wps 13032.58 step time 0.50s\n","# Epoch 2  global step 6500 loss 0.39704 batch 3218/3281 lr 0.001 accuracy 92.69922 wps 12120.58 step time 0.65s\n","# Epoch 2  global step 6520 loss 0.40018 batch 3238/3281 lr 0.001 accuracy 92.68359 wps 12711.12 step time 0.56s\n","# Epoch 2  global step 6540 loss 0.40378 batch 3258/3281 lr 0.001 accuracy 92.69531 wps 12709.37 step time 0.66s\n","# Epoch 2  global step 6560 loss 0.37346 batch 3278/3281 lr 0.001 accuracy 93.21875 wps 12582.25 step time 0.53s\n","# Finsh epoch 2, global step 6564\n","# Epoch 3  global step 6580 loss 0.32535 batch 16/3281 lr 0.001 accuracy 74.19141 wps 14482.49 step time 0.48s\n","# Epoch 3  global step 6600 loss 0.35886 batch 36/3281 lr 0.001 accuracy 93.63281 wps 14142.34 step time 0.41s\n","# Epoch 3  global step 6620 loss 0.39326 batch 56/3281 lr 0.001 accuracy 92.76563 wps 14213.34 step time 0.56s\n","# Epoch 3  global step 6640 loss 0.39202 batch 76/3281 lr 0.001 accuracy 92.80078 wps 14756.64 step time 0.47s\n","# Epoch 3  global step 6660 loss 0.37702 batch 96/3281 lr 0.001 accuracy 93.13281 wps 14411.05 step time 0.43s\n","# Epoch 3  global step 6680 loss 0.35817 batch 116/3281 lr 0.001 accuracy 93.55469 wps 14077.39 step time 0.40s\n","# Epoch 3  global step 6700 loss 0.38320 batch 136/3281 lr 0.001 accuracy 92.91016 wps 14620.21 step time 0.58s\n","# Epoch 3  global step 6720 loss 0.37530 batch 156/3281 lr 0.001 accuracy 93.08984 wps 14056.33 step time 0.52s\n","# Epoch 3  global step 6740 loss 0.39518 batch 176/3281 lr 0.001 accuracy 92.74609 wps 15051.10 step time 0.49s\n","# Epoch 3  global step 6760 loss 0.35480 batch 196/3281 lr 0.001 accuracy 93.53125 wps 14053.05 step time 0.41s\n","# Epoch 3  global step 6780 loss 0.40133 batch 216/3281 lr 0.001 accuracy 92.87500 wps 14591.43 step time 0.55s\n","# Epoch 3  global step 6800 loss 0.38336 batch 236/3281 lr 0.001 accuracy 93.14063 wps 14767.74 step time 0.47s\n","# Epoch 3  global step 6820 loss 0.37062 batch 256/3281 lr 0.001 accuracy 93.19141 wps 14506.66 step time 0.43s\n","# Epoch 3  global step 6840 loss 0.38618 batch 276/3281 lr 0.001 accuracy 92.91016 wps 14967.43 step time 0.49s\n","# Epoch 3  global step 6860 loss 0.39963 batch 296/3281 lr 0.001 accuracy 92.69922 wps 15202.69 step time 0.52s\n","# Epoch 3  global step 6880 loss 0.38702 batch 316/3281 lr 0.001 accuracy 92.95312 wps 14869.86 step time 0.48s\n","# Epoch 3  global step 6900 loss 0.39170 batch 336/3281 lr 0.001 accuracy 92.80078 wps 14370.75 step time 0.52s\n","# Epoch 3  global step 6920 loss 0.39559 batch 356/3281 lr 0.001 accuracy 92.80078 wps 14635.98 step time 0.54s\n","# Epoch 3  global step 6940 loss 0.40781 batch 376/3281 lr 0.001 accuracy 92.58203 wps 15311.16 step time 0.53s\n","# Epoch 3  global step 6960 loss 0.36666 batch 396/3281 lr 0.001 accuracy 93.21875 wps 14640.63 step time 0.47s\n","# Epoch 3  global step 6980 loss 0.36304 batch 416/3281 lr 0.001 accuracy 93.54297 wps 14512.32 step time 0.46s\n","# Epoch 3  global step 7000 loss 0.38506 batch 436/3281 lr 0.001 accuracy 92.82422 wps 14893.34 step time 0.48s\n","# Epoch 3  global step 7020 loss 0.37952 batch 456/3281 lr 0.001 accuracy 93.12891 wps 15284.36 step time 0.53s\n","# Epoch 3  global step 7040 loss 0.39732 batch 476/3281 lr 0.001 accuracy 92.57422 wps 14590.15 step time 0.61s\n","# Epoch 3  global step 7060 loss 0.38870 batch 496/3281 lr 0.001 accuracy 93.06641 wps 14930.34 step time 0.48s\n","# Epoch 3  global step 7080 loss 0.36104 batch 516/3281 lr 0.001 accuracy 93.43750 wps 14528.54 step time 0.45s\n","# Epoch 3  global step 7100 loss 0.36650 batch 536/3281 lr 0.001 accuracy 93.25781 wps 14503.61 step time 0.44s\n","# Epoch 3  global step 7120 loss 0.39407 batch 556/3281 lr 0.001 accuracy 92.73828 wps 14970.32 step time 0.48s\n","# Epoch 3  global step 7140 loss 0.38425 batch 576/3281 lr 0.001 accuracy 92.94922 wps 14903.09 step time 0.48s\n","# Epoch 3  global step 7160 loss 0.39506 batch 596/3281 lr 0.001 accuracy 92.64453 wps 14100.11 step time 0.65s\n","# Epoch 3  global step 7180 loss 0.36668 batch 616/3281 lr 0.001 accuracy 93.45313 wps 13779.73 step time 0.49s\n","# Epoch 3  global step 7200 loss 0.40042 batch 636/3281 lr 0.001 accuracy 92.71094 wps 14626.04 step time 0.46s\n","# Epoch 3  global step 7220 loss 0.38694 batch 656/3281 lr 0.001 accuracy 92.78516 wps 14932.44 step time 0.47s\n","# Epoch 3  global step 7240 loss 0.36957 batch 676/3281 lr 0.001 accuracy 93.38281 wps 15014.87 step time 0.48s\n","# Epoch 3  global step 7260 loss 0.39201 batch 696/3281 lr 0.001 accuracy 92.90625 wps 14687.93 step time 0.46s\n","# Epoch 3  global step 7280 loss 0.38418 batch 716/3281 lr 0.001 accuracy 92.87500 wps 14577.97 step time 0.56s\n","# Epoch 3  global step 7300 loss 0.39159 batch 736/3281 lr 0.001 accuracy 92.87500 wps 14723.17 step time 0.45s\n","# Epoch 3  global step 7320 loss 0.38608 batch 756/3281 lr 0.001 accuracy 93.07031 wps 14601.61 step time 0.55s\n","# Epoch 3  global step 7340 loss 0.35815 batch 776/3281 lr 0.001 accuracy 93.48437 wps 14146.90 step time 0.41s\n","# Epoch 3  global step 7360 loss 0.40453 batch 796/3281 lr 0.001 accuracy 92.54297 wps 14239.78 step time 0.62s\n","# Epoch 3  global step 7380 loss 0.39271 batch 816/3281 lr 0.001 accuracy 92.81250 wps 14829.08 step time 0.48s\n","# Epoch 3  global step 7400 loss 0.36740 batch 836/3281 lr 0.001 accuracy 93.38281 wps 14925.00 step time 0.47s\n","# Epoch 3  global step 7420 loss 0.37555 batch 856/3281 lr 0.001 accuracy 93.19531 wps 15019.30 step time 0.50s\n","# Epoch 3  global step 7440 loss 0.39099 batch 876/3281 lr 0.001 accuracy 92.97266 wps 14476.86 step time 0.57s\n","# Epoch 3  global step 7460 loss 0.40461 batch 896/3281 lr 0.001 accuracy 92.78906 wps 15451.65 step time 0.53s\n","# Epoch 3  global step 7480 loss 0.38113 batch 916/3281 lr 0.001 accuracy 93.29297 wps 14707.87 step time 0.45s\n","# Epoch 3  global step 7500 loss 0.36685 batch 936/3281 lr 0.001 accuracy 93.39062 wps 14626.92 step time 0.45s\n","# Epoch 3  global step 7520 loss 0.36026 batch 956/3281 lr 0.001 accuracy 93.23828 wps 14680.48 step time 0.45s\n","# Epoch 3  global step 7540 loss 0.38009 batch 976/3281 lr 0.001 accuracy 93.10938 wps 14057.73 step time 0.51s\n","# Epoch 3  global step 7560 loss 0.37661 batch 996/3281 lr 0.001 accuracy 93.17188 wps 14827.46 step time 0.49s\n","# Epoch 3  global step 7580 loss 0.36331 batch 1016/3281 lr 0.001 accuracy 93.35156 wps 14928.45 step time 0.48s\n","# Epoch 3  global step 7600 loss 0.36497 batch 1036/3281 lr 0.001 accuracy 93.46484 wps 13659.14 step time 0.49s\n","# Epoch 3  global step 7620 loss 0.38417 batch 1056/3281 lr 0.001 accuracy 92.94141 wps 14818.48 step time 0.48s\n","# Epoch 3  global step 7640 loss 0.38017 batch 1076/3281 lr 0.001 accuracy 93.06250 wps 14977.84 step time 0.49s\n","# Epoch 3  global step 7660 loss 0.41487 batch 1096/3281 lr 0.001 accuracy 92.30859 wps 14831.48 step time 0.61s\n","# Epoch 3  global step 7680 loss 0.37766 batch 1116/3281 lr 0.001 accuracy 93.03516 wps 14768.10 step time 0.46s\n","# Epoch 3  global step 7700 loss 0.42555 batch 1136/3281 lr 0.001 accuracy 92.32031 wps 14725.67 step time 0.71s\n","# Epoch 3  global step 7720 loss 0.36033 batch 1156/3281 lr 0.001 accuracy 93.30859 wps 14529.38 step time 0.44s\n","# Epoch 3  global step 7740 loss 0.37064 batch 1176/3281 lr 0.001 accuracy 93.28906 wps 14736.38 step time 0.46s\n","# Epoch 3  global step 7760 loss 0.39492 batch 1196/3281 lr 0.001 accuracy 92.72266 wps 14438.89 step time 0.61s\n","# Epoch 3  global step 7780 loss 0.39075 batch 1216/3281 lr 0.001 accuracy 92.92188 wps 15047.98 step time 0.50s\n","# Epoch 3  global step 7800 loss 0.39159 batch 1236/3281 lr 0.001 accuracy 93.03125 wps 14575.61 step time 0.57s\n","# Epoch 3  global step 7820 loss 0.40361 batch 1256/3281 lr 0.001 accuracy 92.71094 wps 14765.88 step time 0.60s\n","# Epoch 3  global step 7840 loss 0.37466 batch 1276/3281 lr 0.001 accuracy 93.02344 wps 14359.34 step time 0.55s\n","# Epoch 3  global step 7860 loss 0.37200 batch 1296/3281 lr 0.001 accuracy 93.19531 wps 14724.21 step time 0.45s\n","# Epoch 3  global step 7880 loss 0.40788 batch 1316/3281 lr 0.001 accuracy 92.42969 wps 15027.65 step time 0.51s\n","# Epoch 3  global step 7900 loss 0.37390 batch 1336/3281 lr 0.001 accuracy 93.21094 wps 13606.90 step time 0.50s\n","# Epoch 3  global step 7920 loss 0.38876 batch 1356/3281 lr 0.001 accuracy 93.10937 wps 14895.43 step time 0.47s\n","# Epoch 3  global step 7940 loss 0.38413 batch 1376/3281 lr 0.001 accuracy 92.92187 wps 13945.27 step time 0.59s\n","# Epoch 3  global step 7960 loss 0.38495 batch 1396/3281 lr 0.001 accuracy 93.11719 wps 14407.77 step time 0.44s\n","# Epoch 3  global step 7980 loss 0.37640 batch 1416/3281 lr 0.001 accuracy 93.15625 wps 14776.26 step time 0.48s\n","# Epoch 3  global step 8000 loss 0.38009 batch 1436/3281 lr 0.001 accuracy 93.20703 wps 14336.64 step time 0.53s\n","# global step 8000, eval model at Sun May 24 12:34:39 2020\n","2020-05-24 12:34:41.587441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:34:41.587877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-24 12:34:41.587997: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 12:34:41.588028: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-24 12:34:41.588052: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-24 12:34:41.588076: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-24 12:34:41.588097: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-24 12:34:41.588119: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-24 12:34:41.588143: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 12:34:41.588234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:34:41.588499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:34:41.588731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-24 12:34:41.588809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 12:34:41.588824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-24 12:34:41.588834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-24 12:34:41.588993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:34:41.589258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:34:41.589474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.23316235471863497\n","# location_distance_from_business_district - 0.22255105060668837\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.15699596885737233\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16662221629550608\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.19087461874536316\n","# environment_decoration - 0.1716460838477769\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19507236949097415\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.35758050596035484\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.5204417650231392\n","# others_willing_to_consume_again - 0.19204237496920423\n","# Eval loss 0.66531, f1 0.22764\n","# current result -0.22764163094709441, previous best result -0.21876900705315117\n","# Epoch 3  global step 8020 loss 0.36379 batch 1456/3281 lr 0.001 accuracy 93.20313 wps 14215.44 step time 0.52s\n","# Epoch 3  global step 8040 loss 0.37043 batch 1476/3281 lr 0.001 accuracy 93.22656 wps 14998.34 step time 0.50s\n","# Epoch 3  global step 8060 loss 0.38471 batch 1496/3281 lr 0.001 accuracy 93.19141 wps 15146.22 step time 0.51s\n","# Epoch 3  global step 8080 loss 0.37438 batch 1516/3281 lr 0.001 accuracy 93.18750 wps 14709.49 step time 0.46s\n","# Epoch 3  global step 8100 loss 0.38355 batch 1536/3281 lr 0.001 accuracy 93.08203 wps 14366.24 step time 0.43s\n","# Epoch 3  global step 8120 loss 0.37631 batch 1556/3281 lr 0.001 accuracy 93.24609 wps 14951.54 step time 0.49s\n","# Epoch 3  global step 8140 loss 0.36310 batch 1576/3281 lr 0.001 accuracy 93.37891 wps 14682.75 step time 0.46s\n","# Epoch 3  global step 8160 loss 0.37344 batch 1596/3281 lr 0.001 accuracy 93.40625 wps 14725.73 step time 0.46s\n","# Epoch 3  global step 8180 loss 0.39949 batch 1616/3281 lr 0.001 accuracy 92.63672 wps 14916.38 step time 0.50s\n","# Epoch 3  global step 8200 loss 0.38576 batch 1636/3281 lr 0.001 accuracy 92.96484 wps 14355.08 step time 0.56s\n","# Epoch 3  global step 8220 loss 0.37846 batch 1656/3281 lr 0.001 accuracy 93.16797 wps 14827.77 step time 0.47s\n","# Epoch 3  global step 8240 loss 0.38764 batch 1676/3281 lr 0.001 accuracy 93.01953 wps 14549.29 step time 0.53s\n","# Epoch 3  global step 8260 loss 0.38477 batch 1696/3281 lr 0.001 accuracy 92.92578 wps 15072.13 step time 0.51s\n","# Epoch 3  global step 8280 loss 0.37154 batch 1716/3281 lr 0.001 accuracy 93.01953 wps 14968.42 step time 0.50s\n","# Epoch 3  global step 8300 loss 0.35666 batch 1736/3281 lr 0.001 accuracy 93.53125 wps 14160.05 step time 0.53s\n","# Epoch 3  global step 8320 loss 0.35384 batch 1756/3281 lr 0.001 accuracy 93.57031 wps 14035.58 step time 0.41s\n","# Epoch 3  global step 8340 loss 0.37820 batch 1776/3281 lr 0.001 accuracy 93.25781 wps 14375.27 step time 0.55s\n","# Epoch 3  global step 8360 loss 0.38347 batch 1796/3281 lr 0.001 accuracy 93.00000 wps 15058.58 step time 0.49s\n","# Epoch 3  global step 8380 loss 0.37844 batch 1816/3281 lr 0.001 accuracy 93.17969 wps 14544.66 step time 0.55s\n","# Epoch 3  global step 8400 loss 0.37517 batch 1836/3281 lr 0.001 accuracy 93.17969 wps 14678.40 step time 0.47s\n","# Epoch 3  global step 8420 loss 0.38939 batch 1856/3281 lr 0.001 accuracy 93.14453 wps 15140.50 step time 0.52s\n","# Epoch 3  global step 8440 loss 0.39118 batch 1876/3281 lr 0.001 accuracy 92.95703 wps 14473.31 step time 0.55s\n","# Epoch 3  global step 8460 loss 0.37278 batch 1896/3281 lr 0.001 accuracy 93.23828 wps 14825.62 step time 0.47s\n","# Epoch 3  global step 8480 loss 0.36549 batch 1916/3281 lr 0.001 accuracy 93.25000 wps 14616.86 step time 0.44s\n","# Epoch 3  global step 8500 loss 0.39239 batch 1936/3281 lr 0.001 accuracy 92.89062 wps 14597.59 step time 0.56s\n","# Epoch 3  global step 8520 loss 0.39267 batch 1956/3281 lr 0.001 accuracy 92.71094 wps 14723.89 step time 0.59s\n","# Epoch 3  global step 8540 loss 0.38252 batch 1976/3281 lr 0.001 accuracy 93.16406 wps 14264.48 step time 0.49s\n","# Epoch 3  global step 8560 loss 0.37756 batch 1996/3281 lr 0.001 accuracy 93.15625 wps 14949.01 step time 0.47s\n","# Epoch 3  global step 8580 loss 0.37544 batch 2016/3281 lr 0.001 accuracy 93.16016 wps 14691.19 step time 0.46s\n","# Epoch 3  global step 8600 loss 0.38520 batch 2036/3281 lr 0.001 accuracy 92.91797 wps 14874.53 step time 0.48s\n","# Epoch 3  global step 8620 loss 0.37238 batch 2056/3281 lr 0.001 accuracy 93.18750 wps 14609.42 step time 0.44s\n","# Epoch 3  global step 8640 loss 0.38655 batch 2076/3281 lr 0.001 accuracy 92.98437 wps 15381.64 step time 0.56s\n","# Epoch 3  global step 8660 loss 0.35491 batch 2096/3281 lr 0.001 accuracy 93.57031 wps 13883.93 step time 0.40s\n","# Epoch 3  global step 8680 loss 0.38066 batch 2116/3281 lr 0.001 accuracy 92.91406 wps 14519.09 step time 0.54s\n","# Epoch 3  global step 8700 loss 0.37964 batch 2136/3281 lr 0.001 accuracy 93.08984 wps 13682.76 step time 0.60s\n","# Epoch 3  global step 8720 loss 0.40286 batch 2156/3281 lr 0.001 accuracy 92.62109 wps 15178.83 step time 0.51s\n","# Epoch 3  global step 8740 loss 0.36572 batch 2176/3281 lr 0.001 accuracy 93.48437 wps 14390.39 step time 0.54s\n","# Epoch 3  global step 8760 loss 0.36938 batch 2196/3281 lr 0.001 accuracy 93.16016 wps 14783.84 step time 0.46s\n","# Epoch 3  global step 8780 loss 0.37641 batch 2216/3281 lr 0.001 accuracy 93.11719 wps 14696.79 step time 0.46s\n","# Epoch 3  global step 8800 loss 0.37725 batch 2236/3281 lr 0.001 accuracy 93.21094 wps 14872.20 step time 0.46s\n","# Epoch 3  global step 8820 loss 0.36356 batch 2256/3281 lr 0.001 accuracy 93.39844 wps 14170.06 step time 0.42s\n","# Epoch 3  global step 8840 loss 0.37430 batch 2276/3281 lr 0.001 accuracy 93.13281 wps 14923.32 step time 0.47s\n","# Epoch 3  global step 8860 loss 0.37438 batch 2296/3281 lr 0.001 accuracy 93.27734 wps 13898.56 step time 0.48s\n","# Epoch 3  global step 8880 loss 0.36593 batch 2316/3281 lr 0.001 accuracy 93.46875 wps 14592.31 step time 0.46s\n","# Epoch 3  global step 8900 loss 0.38775 batch 2336/3281 lr 0.001 accuracy 92.92578 wps 13887.45 step time 0.49s\n","# Epoch 3  global step 8920 loss 0.39105 batch 2356/3281 lr 0.001 accuracy 92.99219 wps 14826.94 step time 0.59s\n","# Epoch 3  global step 8940 loss 0.36538 batch 2376/3281 lr 0.001 accuracy 93.44141 wps 14049.73 step time 0.42s\n","# Epoch 3  global step 8960 loss 0.39422 batch 2396/3281 lr 0.001 accuracy 92.85156 wps 14216.83 step time 0.61s\n","# Epoch 3  global step 8980 loss 0.39228 batch 2416/3281 lr 0.001 accuracy 92.79297 wps 15163.48 step time 0.51s\n","# Epoch 3  global step 9000 loss 0.36235 batch 2436/3281 lr 0.001 accuracy 93.53906 wps 14628.87 step time 0.47s\n","# Epoch 3  global step 9020 loss 0.39286 batch 2456/3281 lr 0.001 accuracy 92.98437 wps 15187.58 step time 0.51s\n","# Epoch 3  global step 9040 loss 0.38796 batch 2476/3281 lr 0.001 accuracy 92.75000 wps 14600.71 step time 0.60s\n","# Epoch 3  global step 9060 loss 0.37203 batch 2496/3281 lr 0.001 accuracy 93.27734 wps 14436.17 step time 0.45s\n","# Epoch 3  global step 9080 loss 0.39202 batch 2516/3281 lr 0.001 accuracy 92.86719 wps 14366.68 step time 0.55s\n","# Epoch 3  global step 9100 loss 0.35587 batch 2536/3281 lr 0.001 accuracy 93.54688 wps 14625.17 step time 0.44s\n","# Epoch 3  global step 9120 loss 0.36605 batch 2556/3281 lr 0.001 accuracy 93.48047 wps 14188.89 step time 0.54s\n","# Epoch 3  global step 9140 loss 0.38370 batch 2576/3281 lr 0.001 accuracy 93.00391 wps 14664.86 step time 0.57s\n","# Epoch 3  global step 9160 loss 0.38126 batch 2596/3281 lr 0.001 accuracy 93.08984 wps 14820.41 step time 0.46s\n","# Epoch 3  global step 9180 loss 0.37764 batch 2616/3281 lr 0.001 accuracy 92.96875 wps 14836.36 step time 0.48s\n","# Epoch 3  global step 9200 loss 0.37538 batch 2636/3281 lr 0.001 accuracy 93.13281 wps 14899.14 step time 0.47s\n","# Epoch 3  global step 9220 loss 0.39143 batch 2656/3281 lr 0.001 accuracy 92.88281 wps 15002.24 step time 0.50s\n","# Epoch 3  global step 9240 loss 0.37902 batch 2676/3281 lr 0.001 accuracy 93.25391 wps 14789.14 step time 0.46s\n","# Epoch 3  global step 9260 loss 0.35272 batch 2696/3281 lr 0.001 accuracy 93.46484 wps 14512.71 step time 0.44s\n","# Epoch 3  global step 9280 loss 0.39107 batch 2716/3281 lr 0.001 accuracy 92.57812 wps 14874.47 step time 0.58s\n","# Epoch 3  global step 9300 loss 0.42165 batch 2736/3281 lr 0.001 accuracy 92.13281 wps 14230.84 step time 0.74s\n","# Epoch 3  global step 9320 loss 0.35948 batch 2756/3281 lr 0.001 accuracy 93.30078 wps 14529.30 step time 0.46s\n","# Epoch 3  global step 9340 loss 0.38444 batch 2776/3281 lr 0.001 accuracy 92.98047 wps 14602.83 step time 0.45s\n","# Epoch 3  global step 9360 loss 0.38638 batch 2796/3281 lr 0.001 accuracy 92.87109 wps 14445.10 step time 0.55s\n","# Epoch 3  global step 9380 loss 0.37148 batch 2816/3281 lr 0.001 accuracy 93.30078 wps 14229.53 step time 0.51s\n","# Epoch 3  global step 9400 loss 0.38359 batch 2836/3281 lr 0.001 accuracy 93.07422 wps 14098.16 step time 0.64s\n","# Epoch 3  global step 9420 loss 0.37399 batch 2856/3281 lr 0.001 accuracy 93.30078 wps 14058.45 step time 0.51s\n","# Epoch 3  global step 9440 loss 0.36332 batch 2876/3281 lr 0.001 accuracy 93.43750 wps 14424.39 step time 0.44s\n","# Epoch 3  global step 9460 loss 0.37639 batch 2896/3281 lr 0.001 accuracy 93.31250 wps 14886.23 step time 0.48s\n","# Epoch 3  global step 9480 loss 0.38860 batch 2916/3281 lr 0.001 accuracy 93.00391 wps 14648.92 step time 0.45s\n","# Epoch 3  global step 9500 loss 0.39826 batch 2936/3281 lr 0.001 accuracy 92.49219 wps 14974.12 step time 0.58s\n","# Epoch 3  global step 9520 loss 0.38483 batch 2956/3281 lr 0.001 accuracy 92.95312 wps 13502.57 step time 0.53s\n","# Epoch 3  global step 9540 loss 0.37297 batch 2976/3281 lr 0.001 accuracy 93.26172 wps 12922.08 step time 0.53s\n","# Epoch 3  global step 9560 loss 0.38341 batch 2996/3281 lr 0.001 accuracy 92.98438 wps 11847.39 step time 0.62s\n","# Epoch 3  global step 9580 loss 0.38371 batch 3016/3281 lr 0.001 accuracy 92.97656 wps 11822.99 step time 0.73s\n","# Epoch 3  global step 9600 loss 0.36437 batch 3036/3281 lr 0.001 accuracy 93.33203 wps 12282.30 step time 0.63s\n","# Epoch 3  global step 9620 loss 0.38581 batch 3056/3281 lr 0.001 accuracy 92.98437 wps 12728.17 step time 0.53s\n","# Epoch 3  global step 9640 loss 0.35189 batch 3076/3281 lr 0.001 accuracy 93.47266 wps 13411.62 step time 0.46s\n","# Epoch 3  global step 9660 loss 0.38792 batch 3096/3281 lr 0.001 accuracy 92.92578 wps 13009.94 step time 0.57s\n","# Epoch 3  global step 9680 loss 0.36840 batch 3116/3281 lr 0.001 accuracy 93.40625 wps 13205.51 step time 0.49s\n","# Epoch 3  global step 9700 loss 0.37637 batch 3136/3281 lr 0.001 accuracy 93.36328 wps 12520.38 step time 0.54s\n","# Epoch 3  global step 9720 loss 0.38427 batch 3156/3281 lr 0.001 accuracy 93.03125 wps 13116.32 step time 0.53s\n","# Epoch 3  global step 9740 loss 0.36800 batch 3176/3281 lr 0.001 accuracy 93.29687 wps 12798.51 step time 0.55s\n","# Epoch 3  global step 9760 loss 0.37181 batch 3196/3281 lr 0.001 accuracy 93.39062 wps 13468.54 step time 0.53s\n","# Epoch 3  global step 9780 loss 0.38122 batch 3216/3281 lr 0.001 accuracy 92.95312 wps 13125.04 step time 0.55s\n","# Epoch 3  global step 9800 loss 0.37550 batch 3236/3281 lr 0.001 accuracy 92.96094 wps 12422.63 step time 0.62s\n","# Epoch 3  global step 9820 loss 0.35320 batch 3256/3281 lr 0.001 accuracy 93.53125 wps 12861.36 step time 0.50s\n","# Epoch 3  global step 9840 loss 0.39374 batch 3276/3281 lr 0.001 accuracy 92.80078 wps 11984.33 step time 0.62s\n","# Finsh epoch 3, global step 9846\n","# Epoch 4  global step 9860 loss 0.25194 batch 14/3281 lr 0.001 accuracy 65.28516 wps 14055.41 step time 0.38s\n","# Epoch 4  global step 9880 loss 0.35003 batch 34/3281 lr 0.001 accuracy 93.51172 wps 14680.74 step time 0.46s\n","# Epoch 4  global step 9900 loss 0.36093 batch 54/3281 lr 0.001 accuracy 93.43359 wps 14409.34 step time 0.45s\n","# Epoch 4  global step 9920 loss 0.37244 batch 74/3281 lr 0.001 accuracy 93.09766 wps 14848.00 step time 0.58s\n","# Epoch 4  global step 9940 loss 0.34112 batch 94/3281 lr 0.001 accuracy 93.76953 wps 14589.14 step time 0.44s\n","# Epoch 4  global step 9960 loss 0.35737 batch 114/3281 lr 0.001 accuracy 93.56641 wps 14948.94 step time 0.48s\n","# Epoch 4  global step 9980 loss 0.34447 batch 134/3281 lr 0.001 accuracy 93.79687 wps 14480.71 step time 0.43s\n","# Epoch 4  global step 10000 loss 0.37038 batch 154/3281 lr 0.001 accuracy 93.17969 wps 15270.10 step time 0.52s\n","# global step 10000, eval model at Sun May 24 12:53:57 2020\n","2020-05-24 12:53:59.366806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:53:59.367333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-24 12:53:59.367461: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 12:53:59.367488: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-24 12:53:59.367511: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-24 12:53:59.367537: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-24 12:53:59.367563: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-24 12:53:59.367587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-24 12:53:59.367612: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 12:53:59.367722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:53:59.368003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:53:59.368201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-24 12:53:59.368312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 12:53:59.368328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-24 12:53:59.368338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-24 12:53:59.368442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:53:59.368699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 12:53:59.368951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.33041483038386654\n","# location_distance_from_business_district - 0.2227379190121503\n","# location_easy_to_find - 0.21715190828179212\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.2111134782655603\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22901687321602773\n","# price_level - 0.16706316709014404\n","# price_cost_effective - 0.21621008021795066\n","# price_discount - 0.1954820819727633\n","# environment_decoration - 0.17701016987984955\n","# environment_noise - 0.20612436816739158\n","# environment_space - 0.19376505655138623\n","# environment_cleaness - 0.19520069576941787\n","# dish_portion - 0.17549325025960538\n","# dish_taste - 0.4360110933265817\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.5448959060621099\n","# others_willing_to_consume_again - 0.19886173117720984\n","# Eval loss 0.59757, f1 0.24123\n","# current result -0.24123171916832647, previous best result -0.22764163094709441\n","# Epoch 4  global step 10020 loss 0.36842 batch 174/3281 lr 0.001 accuracy 93.33203 wps 13796.96 step time 0.59s\n","# Epoch 4  global step 10040 loss 0.37678 batch 194/3281 lr 0.001 accuracy 93.37500 wps 14952.44 step time 0.51s\n","# Epoch 4  global step 10060 loss 0.35192 batch 214/3281 lr 0.001 accuracy 93.60156 wps 14578.64 step time 0.55s\n","# Epoch 4  global step 10080 loss 0.37100 batch 234/3281 lr 0.001 accuracy 93.23828 wps 15120.15 step time 0.51s\n","# Epoch 4  global step 10100 loss 0.36990 batch 254/3281 lr 0.001 accuracy 93.28906 wps 15042.32 step time 0.51s\n","# Epoch 4  global step 10120 loss 0.37632 batch 274/3281 lr 0.001 accuracy 93.09766 wps 14221.18 step time 0.52s\n","# Epoch 4  global step 10140 loss 0.37483 batch 294/3281 lr 0.001 accuracy 93.14453 wps 14654.60 step time 0.46s\n","# Epoch 4  global step 10160 loss 0.36457 batch 314/3281 lr 0.001 accuracy 93.29688 wps 13761.80 step time 0.66s\n","# Epoch 4  global step 10180 loss 0.35750 batch 334/3281 lr 0.001 accuracy 93.43750 wps 14004.55 step time 0.51s\n","# Epoch 4  global step 10200 loss 0.36457 batch 354/3281 lr 0.001 accuracy 93.43750 wps 14232.14 step time 0.41s\n","# Epoch 4  global step 10220 loss 0.38369 batch 374/3281 lr 0.001 accuracy 93.07031 wps 14430.48 step time 0.64s\n","# Epoch 4  global step 10240 loss 0.38483 batch 394/3281 lr 0.001 accuracy 92.76172 wps 15055.14 step time 0.51s\n","# Epoch 4  global step 10260 loss 0.35488 batch 414/3281 lr 0.001 accuracy 93.67188 wps 14371.38 step time 0.42s\n","# Epoch 4  global step 10280 loss 0.36554 batch 434/3281 lr 0.001 accuracy 93.33984 wps 14707.24 step time 0.47s\n","# Epoch 4  global step 10300 loss 0.34722 batch 454/3281 lr 0.001 accuracy 93.57031 wps 14808.50 step time 0.48s\n","# Epoch 4  global step 10320 loss 0.36471 batch 474/3281 lr 0.001 accuracy 93.35938 wps 15000.53 step time 0.50s\n","# Epoch 4  global step 10340 loss 0.36142 batch 494/3281 lr 0.001 accuracy 93.35156 wps 14568.02 step time 0.44s\n","# Epoch 4  global step 10360 loss 0.35766 batch 514/3281 lr 0.001 accuracy 93.49219 wps 14818.66 step time 0.47s\n","# Epoch 4  global step 10380 loss 0.35024 batch 534/3281 lr 0.001 accuracy 93.67578 wps 13981.02 step time 0.51s\n","# Epoch 4  global step 10400 loss 0.36354 batch 554/3281 lr 0.001 accuracy 93.54297 wps 15038.03 step time 0.52s\n","# Epoch 4  global step 10420 loss 0.38014 batch 574/3281 lr 0.001 accuracy 93.01953 wps 13810.67 step time 0.58s\n","# Epoch 4  global step 10440 loss 0.35079 batch 594/3281 lr 0.001 accuracy 93.74219 wps 14289.12 step time 0.56s\n","# Epoch 4  global step 10460 loss 0.36688 batch 614/3281 lr 0.001 accuracy 93.24609 wps 14502.56 step time 0.46s\n","# Epoch 4  global step 10480 loss 0.36648 batch 634/3281 lr 0.001 accuracy 93.37500 wps 14779.12 step time 0.49s\n","# Epoch 4  global step 10500 loss 0.37503 batch 654/3281 lr 0.001 accuracy 93.14062 wps 15153.37 step time 0.50s\n","# Epoch 4  global step 10520 loss 0.37900 batch 674/3281 lr 0.001 accuracy 93.17187 wps 15273.94 step time 0.51s\n","# Epoch 4  global step 10540 loss 0.35108 batch 694/3281 lr 0.001 accuracy 93.67578 wps 14666.28 step time 0.45s\n","# Epoch 4  global step 10560 loss 0.36401 batch 714/3281 lr 0.001 accuracy 93.38672 wps 14933.12 step time 0.48s\n","# Epoch 4  global step 10580 loss 0.35645 batch 734/3281 lr 0.001 accuracy 93.68750 wps 14794.54 step time 0.46s\n","# Epoch 4  global step 10600 loss 0.36737 batch 754/3281 lr 0.001 accuracy 93.32422 wps 14936.69 step time 0.49s\n","# Epoch 4  global step 10620 loss 0.36359 batch 774/3281 lr 0.001 accuracy 93.42578 wps 14417.20 step time 0.43s\n","# Epoch 4  global step 10640 loss 0.36501 batch 794/3281 lr 0.001 accuracy 93.33594 wps 13837.34 step time 0.52s\n","# Epoch 4  global step 10660 loss 0.38850 batch 814/3281 lr 0.001 accuracy 92.85547 wps 13674.92 step time 0.61s\n","# Epoch 4  global step 10680 loss 0.34652 batch 834/3281 lr 0.001 accuracy 93.69141 wps 14098.15 step time 0.51s\n","# Epoch 4  global step 10700 loss 0.35985 batch 854/3281 lr 0.001 accuracy 93.57812 wps 14315.89 step time 0.42s\n","# Epoch 4  global step 10720 loss 0.36093 batch 874/3281 lr 0.001 accuracy 93.53125 wps 14793.23 step time 0.47s\n","# Epoch 4  global step 10740 loss 0.37236 batch 894/3281 lr 0.001 accuracy 93.18359 wps 13970.60 step time 0.60s\n","# Epoch 4  global step 10760 loss 0.36725 batch 914/3281 lr 0.001 accuracy 93.45313 wps 15010.57 step time 0.49s\n","# Epoch 4  global step 10780 loss 0.38262 batch 934/3281 lr 0.001 accuracy 92.99219 wps 15316.27 step time 0.52s\n","# Epoch 4  global step 10800 loss 0.36021 batch 954/3281 lr 0.001 accuracy 93.51562 wps 14313.25 step time 0.42s\n","# Epoch 4  global step 10820 loss 0.37577 batch 974/3281 lr 0.001 accuracy 93.17578 wps 14428.91 step time 0.54s\n","# Epoch 4  global step 10840 loss 0.35751 batch 994/3281 lr 0.001 accuracy 93.41016 wps 14249.70 step time 0.50s\n","# Epoch 4  global step 10860 loss 0.38354 batch 1014/3281 lr 0.001 accuracy 93.09375 wps 14717.52 step time 0.59s\n","# Epoch 4  global step 10880 loss 0.35984 batch 1034/3281 lr 0.001 accuracy 93.44922 wps 15275.64 step time 0.52s\n","# Epoch 4  global step 10900 loss 0.39142 batch 1054/3281 lr 0.001 accuracy 92.79687 wps 15552.45 step time 0.55s\n","# Epoch 4  global step 10920 loss 0.36260 batch 1074/3281 lr 0.001 accuracy 93.33203 wps 14320.90 step time 0.52s\n","# Epoch 4  global step 10940 loss 0.37257 batch 1094/3281 lr 0.001 accuracy 93.26562 wps 14939.69 step time 0.48s\n","# Epoch 4  global step 10960 loss 0.37274 batch 1114/3281 lr 0.001 accuracy 93.22266 wps 14648.63 step time 0.48s\n","# Epoch 4  global step 10980 loss 0.37991 batch 1134/3281 lr 0.001 accuracy 92.99219 wps 14384.50 step time 0.54s\n","# Epoch 4  global step 11000 loss 0.37067 batch 1154/3281 lr 0.001 accuracy 93.18750 wps 14318.90 step time 0.52s\n","# Epoch 4  global step 11020 loss 0.35697 batch 1174/3281 lr 0.001 accuracy 93.61719 wps 14410.36 step time 0.42s\n","# Epoch 4  global step 11040 loss 0.37196 batch 1194/3281 lr 0.001 accuracy 93.19531 wps 14770.11 step time 0.47s\n","# Epoch 4  global step 11060 loss 0.36950 batch 1214/3281 lr 0.001 accuracy 93.34375 wps 15084.28 step time 0.51s\n","# Epoch 4  global step 11080 loss 0.37573 batch 1234/3281 lr 0.001 accuracy 93.07422 wps 14041.69 step time 0.56s\n","# Epoch 4  global step 11100 loss 0.34984 batch 1254/3281 lr 0.001 accuracy 93.62109 wps 14053.53 step time 0.52s\n","# Epoch 4  global step 11120 loss 0.35888 batch 1274/3281 lr 0.001 accuracy 93.55078 wps 14653.87 step time 0.46s\n","# Epoch 4  global step 11140 loss 0.37259 batch 1294/3281 lr 0.001 accuracy 93.10547 wps 14915.57 step time 0.48s\n","# Epoch 4  global step 11160 loss 0.36561 batch 1314/3281 lr 0.001 accuracy 93.40625 wps 14794.66 step time 0.47s\n","# Epoch 4  global step 11180 loss 0.36834 batch 1334/3281 lr 0.001 accuracy 93.16406 wps 14734.59 step time 0.46s\n","# Epoch 4  global step 11200 loss 0.37154 batch 1354/3281 lr 0.001 accuracy 93.24609 wps 14945.89 step time 0.48s\n","# Epoch 4  global step 11220 loss 0.36120 batch 1374/3281 lr 0.001 accuracy 93.32812 wps 13934.25 step time 0.57s\n","# Epoch 4  global step 11240 loss 0.34649 batch 1394/3281 lr 0.001 accuracy 93.71484 wps 14617.95 step time 0.46s\n","# Epoch 4  global step 11260 loss 0.38249 batch 1414/3281 lr 0.001 accuracy 92.81641 wps 15248.24 step time 0.52s\n","# Epoch 4  global step 11280 loss 0.36903 batch 1434/3281 lr 0.001 accuracy 93.15625 wps 14427.44 step time 0.55s\n","# Epoch 4  global step 11300 loss 0.38314 batch 1454/3281 lr 0.001 accuracy 92.92188 wps 15337.63 step time 0.58s\n","# Epoch 4  global step 11320 loss 0.36566 batch 1474/3281 lr 0.001 accuracy 93.36719 wps 14502.21 step time 0.44s\n","# Epoch 4  global step 11340 loss 0.37000 batch 1494/3281 lr 0.001 accuracy 93.39453 wps 14612.53 step time 0.45s\n","# Epoch 4  global step 11360 loss 0.36836 batch 1514/3281 lr 0.001 accuracy 93.42578 wps 13663.45 step time 0.59s\n","# Epoch 4  global step 11380 loss 0.35315 batch 1534/3281 lr 0.001 accuracy 93.60938 wps 14370.45 step time 0.65s\n","# Epoch 4  global step 11400 loss 0.35785 batch 1554/3281 lr 0.001 accuracy 93.70703 wps 14769.05 step time 0.47s\n","# Epoch 4  global step 11420 loss 0.33928 batch 1574/3281 lr 0.001 accuracy 93.84766 wps 14020.53 step time 0.42s\n","# Epoch 4  global step 11440 loss 0.35241 batch 1594/3281 lr 0.001 accuracy 93.62500 wps 14643.09 step time 0.46s\n","# Epoch 4  global step 11460 loss 0.38718 batch 1614/3281 lr 0.001 accuracy 92.96484 wps 14648.79 step time 0.57s\n","# Epoch 4  global step 11480 loss 0.35679 batch 1634/3281 lr 0.001 accuracy 93.45703 wps 13781.12 step time 0.51s\n","# Epoch 4  global step 11500 loss 0.32936 batch 1654/3281 lr 0.001 accuracy 94.07031 wps 14292.34 step time 0.42s\n","# Epoch 4  global step 11520 loss 0.37117 batch 1674/3281 lr 0.001 accuracy 93.27734 wps 14786.75 step time 0.47s\n","# Epoch 4  global step 11540 loss 0.35787 batch 1694/3281 lr 0.001 accuracy 93.33594 wps 14718.75 step time 0.47s\n","# Epoch 4  global step 11560 loss 0.37254 batch 1714/3281 lr 0.001 accuracy 93.05469 wps 14659.05 step time 0.45s\n","# Epoch 4  global step 11580 loss 0.35624 batch 1734/3281 lr 0.001 accuracy 93.49219 wps 14917.41 step time 0.47s\n","# Epoch 4  global step 11600 loss 0.35876 batch 1754/3281 lr 0.001 accuracy 93.47266 wps 14859.19 step time 0.49s\n","# Epoch 4  global step 11620 loss 0.35426 batch 1774/3281 lr 0.001 accuracy 93.79297 wps 14609.90 step time 0.44s\n","# Epoch 4  global step 11640 loss 0.36267 batch 1794/3281 lr 0.001 accuracy 93.42578 wps 14424.19 step time 0.45s\n","# Epoch 4  global step 11660 loss 0.35950 batch 1814/3281 lr 0.001 accuracy 93.41797 wps 14269.98 step time 0.43s\n","# Epoch 4  global step 11680 loss 0.35770 batch 1834/3281 lr 0.001 accuracy 93.55078 wps 14677.32 step time 0.46s\n","# Epoch 4  global step 11700 loss 0.36176 batch 1854/3281 lr 0.001 accuracy 93.59375 wps 14690.90 step time 0.46s\n","# Epoch 4  global step 11720 loss 0.37317 batch 1874/3281 lr 0.001 accuracy 93.13281 wps 14994.97 step time 0.49s\n","# Epoch 4  global step 11740 loss 0.38550 batch 1894/3281 lr 0.001 accuracy 92.95703 wps 14027.46 step time 0.61s\n","# Epoch 4  global step 11760 loss 0.35708 batch 1914/3281 lr 0.001 accuracy 93.48437 wps 14011.14 step time 0.51s\n","# Epoch 4  global step 11780 loss 0.35136 batch 1934/3281 lr 0.001 accuracy 93.66797 wps 14569.80 step time 0.44s\n","# Epoch 4  global step 11800 loss 0.38889 batch 1954/3281 lr 0.001 accuracy 92.77734 wps 15501.91 step time 0.54s\n","# Epoch 4  global step 11820 loss 0.35882 batch 1974/3281 lr 0.001 accuracy 93.50000 wps 14937.53 step time 0.47s\n","# Epoch 4  global step 11840 loss 0.37041 batch 1994/3281 lr 0.001 accuracy 93.28516 wps 15117.93 step time 0.50s\n","# Epoch 4  global step 11860 loss 0.37773 batch 2014/3281 lr 0.001 accuracy 93.09375 wps 15125.53 step time 0.51s\n","# Epoch 4  global step 11880 loss 0.37823 batch 2034/3281 lr 0.001 accuracy 92.92969 wps 14412.74 step time 0.55s\n","# Epoch 4  global step 11900 loss 0.38936 batch 2054/3281 lr 0.001 accuracy 92.85547 wps 13994.29 step time 0.64s\n","# Epoch 4  global step 11920 loss 0.38649 batch 2074/3281 lr 0.001 accuracy 92.87109 wps 15005.62 step time 0.50s\n","# Epoch 4  global step 11940 loss 0.37804 batch 2094/3281 lr 0.001 accuracy 92.94141 wps 14812.13 step time 0.46s\n","# Epoch 4  global step 11960 loss 0.37174 batch 2114/3281 lr 0.001 accuracy 93.16016 wps 14966.50 step time 0.48s\n","# Epoch 4  global step 11980 loss 0.35092 batch 2134/3281 lr 0.001 accuracy 93.65234 wps 14873.49 step time 0.47s\n","# Epoch 4  global step 12000 loss 0.34967 batch 2154/3281 lr 0.001 accuracy 93.52344 wps 14634.11 step time 0.44s\n","# global step 12000, eval model at Sun May 24 13:12:53 2020\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","2020-05-24 13:12:56.011001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:12:56.011396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-24 13:12:56.011503: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 13:12:56.011535: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-24 13:12:56.011558: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-24 13:12:56.011586: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-24 13:12:56.011612: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-24 13:12:56.011632: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-24 13:12:56.011654: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 13:12:56.011771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:12:56.012055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:12:56.012278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-24 13:12:56.012395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 13:12:56.012429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-24 13:12:56.012452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-24 13:12:56.012613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:12:56.012885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:12:56.013117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.503787985292891\n","# location_distance_from_business_district - 0.2726438629282544\n","# location_easy_to_find - 0.29316913766324826\n","# service_wait_time - 0.2343910472075645\n","# service_waiters_attitude - 0.3017840152032568\n","# service_parking_convenience - 0.241788886593679\n","# service_serving_speed - 0.22964549553733613\n","# price_level - 0.17677458818117994\n","# price_cost_effective - 0.2164014118095172\n","# price_discount - 0.29670192769501075\n","# environment_decoration - 0.26134230854590496\n","# environment_noise - 0.2076888648602012\n","# environment_space - 0.22337036412500227\n","# environment_cleaness - 0.21193537593164438\n","# dish_portion - 0.17689601396760762\n","# dish_taste - 0.5253994185975415\n","# dish_look - 0.2088283251805264\n","# dish_recommendation - 0.22307351475095077\n","# others_overall_experience - 0.5559067851876991\n","# others_willing_to_consume_again - 0.3315586933757049\n","# Eval loss 0.50764, f1 0.28465\n","# current result -0.284654401131736, previous best result -0.24123171916832647\n","# Epoch 4  global step 12020 loss 0.37076 batch 2174/3281 lr 0.001 accuracy 93.47266 wps 14021.10 step time 0.51s\n","# Epoch 4  global step 12040 loss 0.36768 batch 2194/3281 lr 0.001 accuracy 93.28516 wps 14803.03 step time 0.49s\n","# Epoch 4  global step 12060 loss 0.35666 batch 2214/3281 lr 0.001 accuracy 93.55859 wps 14380.31 step time 0.44s\n","# Epoch 4  global step 12080 loss 0.40170 batch 2234/3281 lr 0.001 accuracy 92.60938 wps 13773.67 step time 0.73s\n","# Epoch 4  global step 12100 loss 0.34568 batch 2254/3281 lr 0.001 accuracy 93.96875 wps 14239.77 step time 0.42s\n","# Epoch 4  global step 12120 loss 0.37037 batch 2274/3281 lr 0.001 accuracy 93.31641 wps 15246.16 step time 0.55s\n","# Epoch 4  global step 12140 loss 0.38048 batch 2294/3281 lr 0.001 accuracy 93.21094 wps 14491.82 step time 0.44s\n","# Epoch 4  global step 12160 loss 0.36948 batch 2314/3281 lr 0.001 accuracy 93.33203 wps 13336.28 step time 0.66s\n","# Epoch 4  global step 12180 loss 0.37251 batch 2334/3281 lr 0.001 accuracy 93.28125 wps 14910.95 step time 0.50s\n","# Epoch 4  global step 12200 loss 0.35967 batch 2354/3281 lr 0.001 accuracy 93.49609 wps 14564.64 step time 0.45s\n","# Epoch 4  global step 12220 loss 0.38643 batch 2374/3281 lr 0.001 accuracy 92.96094 wps 14707.69 step time 0.60s\n","# Epoch 4  global step 12240 loss 0.34439 batch 2394/3281 lr 0.001 accuracy 93.71875 wps 14140.75 step time 0.42s\n","# Epoch 4  global step 12260 loss 0.35708 batch 2414/3281 lr 0.001 accuracy 93.50391 wps 14175.61 step time 0.42s\n","# Epoch 4  global step 12280 loss 0.36639 batch 2434/3281 lr 0.001 accuracy 93.48828 wps 14637.82 step time 0.47s\n","# Epoch 4  global step 12300 loss 0.37162 batch 2454/3281 lr 0.001 accuracy 93.27734 wps 15123.22 step time 0.52s\n","# Epoch 4  global step 12320 loss 0.36275 batch 2474/3281 lr 0.001 accuracy 93.33203 wps 14996.18 step time 0.50s\n","# Epoch 4  global step 12340 loss 0.36941 batch 2494/3281 lr 0.001 accuracy 93.16016 wps 14791.95 step time 0.47s\n","# Epoch 4  global step 12360 loss 0.38432 batch 2514/3281 lr 0.001 accuracy 93.01563 wps 14651.61 step time 0.54s\n","# Epoch 4  global step 12380 loss 0.38737 batch 2534/3281 lr 0.001 accuracy 92.80469 wps 15056.80 step time 0.61s\n","# Epoch 4  global step 12400 loss 0.36419 batch 2554/3281 lr 0.001 accuracy 93.46484 wps 14553.56 step time 0.45s\n","# Epoch 4  global step 12420 loss 0.35149 batch 2574/3281 lr 0.001 accuracy 93.57812 wps 14816.99 step time 0.48s\n","# Epoch 4  global step 12440 loss 0.38000 batch 2594/3281 lr 0.001 accuracy 93.15625 wps 15080.21 step time 0.50s\n","# Epoch 4  global step 12460 loss 0.36029 batch 2614/3281 lr 0.001 accuracy 93.53125 wps 14763.89 step time 0.46s\n","# Epoch 4  global step 12480 loss 0.35806 batch 2634/3281 lr 0.001 accuracy 93.42187 wps 14680.97 step time 0.46s\n","# Epoch 4  global step 12500 loss 0.38076 batch 2654/3281 lr 0.001 accuracy 93.02734 wps 15098.78 step time 0.53s\n","# Epoch 4  global step 12520 loss 0.36033 batch 2674/3281 lr 0.001 accuracy 93.39063 wps 14992.01 step time 0.48s\n","# Epoch 4  global step 12540 loss 0.37373 batch 2694/3281 lr 0.001 accuracy 92.96875 wps 15013.88 step time 0.50s\n","# Epoch 4  global step 12560 loss 0.36243 batch 2714/3281 lr 0.001 accuracy 93.37891 wps 14661.77 step time 0.45s\n","# Epoch 4  global step 12580 loss 0.38962 batch 2734/3281 lr 0.001 accuracy 92.83594 wps 14030.31 step time 0.61s\n","# Epoch 4  global step 12600 loss 0.37579 batch 2754/3281 lr 0.001 accuracy 92.92578 wps 13963.45 step time 0.59s\n","# Epoch 4  global step 12620 loss 0.37088 batch 2774/3281 lr 0.001 accuracy 93.17187 wps 14616.52 step time 0.56s\n","# Epoch 4  global step 12640 loss 0.36030 batch 2794/3281 lr 0.001 accuracy 93.20313 wps 14878.54 step time 0.49s\n","# Epoch 4  global step 12660 loss 0.36265 batch 2814/3281 lr 0.001 accuracy 93.37500 wps 14974.73 step time 0.52s\n","# Epoch 4  global step 12680 loss 0.35708 batch 2834/3281 lr 0.001 accuracy 93.48047 wps 14537.69 step time 0.45s\n","# Epoch 4  global step 12700 loss 0.36360 batch 2854/3281 lr 0.001 accuracy 93.44531 wps 14719.77 step time 0.48s\n","# Epoch 4  global step 12720 loss 0.37280 batch 2874/3281 lr 0.001 accuracy 93.11719 wps 14418.96 step time 0.55s\n","# Epoch 4  global step 12740 loss 0.39414 batch 2894/3281 lr 0.001 accuracy 92.57422 wps 14586.25 step time 0.68s\n","# Epoch 4  global step 12760 loss 0.36693 batch 2914/3281 lr 0.001 accuracy 93.22656 wps 15012.28 step time 0.50s\n","# Epoch 4  global step 12780 loss 0.37993 batch 2934/3281 lr 0.001 accuracy 92.98438 wps 14426.50 step time 0.55s\n","# Epoch 4  global step 12800 loss 0.36301 batch 2954/3281 lr 0.001 accuracy 93.44531 wps 14671.74 step time 0.46s\n","# Epoch 4  global step 12820 loss 0.36069 batch 2974/3281 lr 0.001 accuracy 93.49609 wps 14810.40 step time 0.47s\n","# Epoch 4  global step 12840 loss 0.34470 batch 2994/3281 lr 0.001 accuracy 93.66797 wps 14687.78 step time 0.46s\n","# Epoch 4  global step 12860 loss 0.35751 batch 3014/3281 lr 0.001 accuracy 93.37500 wps 14523.55 step time 0.44s\n","# Epoch 4  global step 12880 loss 0.35776 batch 3034/3281 lr 0.001 accuracy 93.58984 wps 14859.87 step time 0.47s\n","# Epoch 4  global step 12900 loss 0.39532 batch 3054/3281 lr 0.001 accuracy 92.55469 wps 15105.43 step time 0.63s\n","# Epoch 4  global step 12920 loss 0.35177 batch 3074/3281 lr 0.001 accuracy 93.56641 wps 14445.24 step time 0.44s\n","# Epoch 4  global step 12940 loss 0.37621 batch 3094/3281 lr 0.001 accuracy 93.12109 wps 14835.65 step time 0.50s\n","# Epoch 4  global step 12960 loss 0.34051 batch 3114/3281 lr 0.001 accuracy 93.66406 wps 14557.04 step time 0.44s\n","# Epoch 4  global step 12980 loss 0.37973 batch 3134/3281 lr 0.001 accuracy 93.06250 wps 15197.26 step time 0.51s\n","# Epoch 4  global step 13000 loss 0.38299 batch 3154/3281 lr 0.001 accuracy 92.91406 wps 15289.86 step time 0.55s\n","# Epoch 4  global step 13020 loss 0.36604 batch 3174/3281 lr 0.001 accuracy 93.20312 wps 14740.82 step time 0.48s\n","# Epoch 4  global step 13040 loss 0.37137 batch 3194/3281 lr 0.001 accuracy 93.29297 wps 14458.67 step time 0.43s\n","# Epoch 4  global step 13060 loss 0.35304 batch 3214/3281 lr 0.001 accuracy 93.59375 wps 14588.78 step time 0.46s\n","# Epoch 4  global step 13080 loss 0.34993 batch 3234/3281 lr 0.001 accuracy 93.53125 wps 14539.06 step time 0.45s\n","# Epoch 4  global step 13100 loss 0.36034 batch 3254/3281 lr 0.001 accuracy 93.36328 wps 14790.30 step time 0.47s\n","# Epoch 4  global step 13120 loss 0.37254 batch 3274/3281 lr 0.001 accuracy 93.30469 wps 13964.61 step time 0.51s\n","# Finsh epoch 4, global step 13128\n","# Epoch 5  global step 13140 loss 0.20265 batch 12/3281 lr 0.001 accuracy 56.34375 wps 13971.06 step time 0.35s\n","# Epoch 5  global step 13160 loss 0.37952 batch 32/3281 lr 0.001 accuracy 93.08203 wps 14362.67 step time 0.57s\n","# Epoch 5  global step 13180 loss 0.36146 batch 52/3281 lr 0.001 accuracy 93.25000 wps 14602.97 step time 0.55s\n","# Epoch 5  global step 13200 loss 0.35826 batch 72/3281 lr 0.001 accuracy 93.37109 wps 14658.04 step time 0.45s\n","# Epoch 5  global step 13220 loss 0.36052 batch 92/3281 lr 0.001 accuracy 93.42187 wps 14609.06 step time 0.59s\n","# Epoch 5  global step 13240 loss 0.39526 batch 112/3281 lr 0.001 accuracy 92.77344 wps 14952.33 step time 0.62s\n","# Epoch 5  global step 13260 loss 0.36665 batch 132/3281 lr 0.001 accuracy 93.28516 wps 14465.60 step time 0.56s\n","# Epoch 5  global step 13280 loss 0.35631 batch 152/3281 lr 0.001 accuracy 93.48047 wps 14949.75 step time 0.51s\n","# Epoch 5  global step 13300 loss 0.33931 batch 172/3281 lr 0.001 accuracy 93.71484 wps 14769.08 step time 0.48s\n","# Epoch 5  global step 13320 loss 0.33157 batch 192/3281 lr 0.001 accuracy 94.09375 wps 14748.93 step time 0.46s\n","# Epoch 5  global step 13340 loss 0.37311 batch 212/3281 lr 0.001 accuracy 93.29687 wps 14430.25 step time 0.57s\n","# Epoch 5  global step 13360 loss 0.35291 batch 232/3281 lr 0.001 accuracy 93.60937 wps 14542.52 step time 0.45s\n","# Epoch 5  global step 13380 loss 0.34130 batch 252/3281 lr 0.001 accuracy 93.74219 wps 14432.02 step time 0.43s\n","# Epoch 5  global step 13400 loss 0.35878 batch 272/3281 lr 0.001 accuracy 93.27734 wps 15071.15 step time 0.50s\n","# Epoch 5  global step 13420 loss 0.35667 batch 292/3281 lr 0.001 accuracy 93.55469 wps 14816.13 step time 0.48s\n","# Epoch 5  global step 13440 loss 0.37008 batch 312/3281 lr 0.001 accuracy 93.23828 wps 15000.23 step time 0.49s\n","# Epoch 5  global step 13460 loss 0.36189 batch 332/3281 lr 0.001 accuracy 93.31641 wps 15023.44 step time 0.49s\n","# Epoch 5  global step 13480 loss 0.35976 batch 352/3281 lr 0.001 accuracy 93.44922 wps 15116.85 step time 0.51s\n","# Epoch 5  global step 13500 loss 0.35254 batch 372/3281 lr 0.001 accuracy 93.44141 wps 14488.63 step time 0.43s\n","# Epoch 5  global step 13520 loss 0.35888 batch 392/3281 lr 0.001 accuracy 93.36719 wps 14259.36 step time 0.55s\n","# Epoch 5  global step 13540 loss 0.34926 batch 412/3281 lr 0.001 accuracy 93.81641 wps 13949.57 step time 0.51s\n","# Epoch 5  global step 13560 loss 0.36942 batch 432/3281 lr 0.001 accuracy 93.25000 wps 14701.63 step time 0.61s\n","# Epoch 5  global step 13580 loss 0.34445 batch 452/3281 lr 0.001 accuracy 93.60547 wps 15031.38 step time 0.50s\n","# Epoch 5  global step 13600 loss 0.33792 batch 472/3281 lr 0.001 accuracy 93.96094 wps 13888.57 step time 0.41s\n","# Epoch 5  global step 13620 loss 0.36058 batch 492/3281 lr 0.001 accuracy 93.55469 wps 14884.61 step time 0.47s\n","# Epoch 5  global step 13640 loss 0.34278 batch 512/3281 lr 0.001 accuracy 93.83594 wps 14096.15 step time 0.41s\n","# Epoch 5  global step 13660 loss 0.33860 batch 532/3281 lr 0.001 accuracy 93.90625 wps 14494.91 step time 0.45s\n","# Epoch 5  global step 13680 loss 0.34976 batch 552/3281 lr 0.001 accuracy 93.56250 wps 14878.05 step time 0.48s\n","# Epoch 5  global step 13700 loss 0.37400 batch 572/3281 lr 0.001 accuracy 93.14063 wps 15265.57 step time 0.55s\n","# Epoch 5  global step 13720 loss 0.36236 batch 592/3281 lr 0.001 accuracy 93.36719 wps 14898.16 step time 0.56s\n","# Epoch 5  global step 13740 loss 0.35575 batch 612/3281 lr 0.001 accuracy 93.39453 wps 14764.63 step time 0.47s\n","# Epoch 5  global step 13760 loss 0.34079 batch 632/3281 lr 0.001 accuracy 93.83984 wps 14719.09 step time 0.47s\n","# Epoch 5  global step 13780 loss 0.36967 batch 652/3281 lr 0.001 accuracy 93.30469 wps 14750.28 step time 0.53s\n","# Epoch 5  global step 13800 loss 0.35782 batch 672/3281 lr 0.001 accuracy 93.44141 wps 14836.28 step time 0.50s\n","# Epoch 5  global step 13820 loss 0.35204 batch 692/3281 lr 0.001 accuracy 93.70703 wps 14691.97 step time 0.45s\n","# Epoch 5  global step 13840 loss 0.37002 batch 712/3281 lr 0.001 accuracy 93.14453 wps 15019.98 step time 0.53s\n","# Epoch 5  global step 13860 loss 0.34954 batch 732/3281 lr 0.001 accuracy 93.62109 wps 14790.31 step time 0.46s\n","# Epoch 5  global step 13880 loss 0.39080 batch 752/3281 lr 0.001 accuracy 92.80469 wps 14393.56 step time 0.64s\n","# Epoch 5  global step 13900 loss 0.35944 batch 772/3281 lr 0.001 accuracy 93.56641 wps 13906.17 step time 0.56s\n","# Epoch 5  global step 13920 loss 0.35921 batch 792/3281 lr 0.001 accuracy 93.30078 wps 14592.85 step time 0.60s\n","# Epoch 5  global step 13940 loss 0.35612 batch 812/3281 lr 0.001 accuracy 93.55469 wps 14914.62 step time 0.49s\n","# Epoch 5  global step 13960 loss 0.33746 batch 832/3281 lr 0.001 accuracy 94.01953 wps 14344.58 step time 0.43s\n","# Epoch 5  global step 13980 loss 0.35170 batch 852/3281 lr 0.001 accuracy 93.62109 wps 15117.48 step time 0.51s\n","# Epoch 5  global step 14000 loss 0.35137 batch 872/3281 lr 0.001 accuracy 93.57031 wps 14770.39 step time 0.47s\n","# global step 14000, eval model at Sun May 24 13:32:00 2020\n","2020-05-24 13:32:02.258928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:32:02.259380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-24 13:32:02.262399: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 13:32:02.262507: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-24 13:32:02.262538: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-24 13:32:02.262565: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-24 13:32:02.262586: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-24 13:32:02.262608: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-24 13:32:02.262631: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 13:32:02.262743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:32:02.263022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:32:02.263216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-24 13:32:02.263331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 13:32:02.263346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-24 13:32:02.263356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-24 13:32:02.263453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:32:02.263706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:32:02.263992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.5798141849711635\n","# location_distance_from_business_district - 0.37011352024627164\n","# location_easy_to_find - 0.5630546836278489\n","# service_wait_time - 0.3134801235085121\n","# service_waiters_attitude - 0.5814376012417909\n","# service_parking_convenience - 0.40615280280293287\n","# service_serving_speed - 0.40645804721552947\n","# price_level - 0.39091653149288264\n","# price_cost_effective - 0.3889800536487774\n","# price_discount - 0.44378639753516635\n","# environment_decoration - 0.46768354440822324\n","# environment_noise - 0.38058976078653495\n","# environment_space - 0.47975690460899534\n","# environment_cleaness - 0.4647369035243347\n","# dish_portion - 0.27328552576683873\n","# dish_taste - 0.6216698256804214\n","# dish_look - 0.21944143807494007\n","# dish_recommendation - 0.22955744776581533\n","# others_overall_experience - 0.5668136080241848\n","# others_willing_to_consume_again - 0.5195348909676627\n","# Eval loss 0.41544, f1 0.43336\n","# current result -0.4333631897949413, previous best result -0.284654401131736\n","# Epoch 5  global step 14020 loss 0.37642 batch 892/3281 lr 0.001 accuracy 92.94531 wps 15051.41 step time 0.54s\n","# Epoch 5  global step 14040 loss 0.34960 batch 912/3281 lr 0.001 accuracy 93.83203 wps 14243.59 step time 0.52s\n","# Epoch 5  global step 14060 loss 0.35631 batch 932/3281 lr 0.001 accuracy 93.33984 wps 14502.79 step time 0.55s\n","# Epoch 5  global step 14080 loss 0.34490 batch 952/3281 lr 0.001 accuracy 93.73438 wps 14141.07 step time 0.41s\n","# Epoch 5  global step 14100 loss 0.36480 batch 972/3281 lr 0.001 accuracy 93.48438 wps 14617.63 step time 0.45s\n","# Epoch 5  global step 14120 loss 0.35894 batch 992/3281 lr 0.001 accuracy 93.55078 wps 13957.94 step time 0.52s\n","# Epoch 5  global step 14140 loss 0.36007 batch 1012/3281 lr 0.001 accuracy 93.35156 wps 14404.47 step time 0.61s\n","# Epoch 5  global step 14160 loss 0.38053 batch 1032/3281 lr 0.001 accuracy 92.98047 wps 14864.79 step time 0.62s\n","# Epoch 5  global step 14180 loss 0.33853 batch 1052/3281 lr 0.001 accuracy 93.82422 wps 14258.51 step time 0.52s\n","# Epoch 5  global step 14200 loss 0.34799 batch 1072/3281 lr 0.001 accuracy 93.63672 wps 13923.26 step time 0.50s\n","# Epoch 5  global step 14220 loss 0.36347 batch 1092/3281 lr 0.001 accuracy 93.39844 wps 14266.89 step time 0.67s\n","# Epoch 5  global step 14240 loss 0.34540 batch 1112/3281 lr 0.001 accuracy 93.75000 wps 14273.09 step time 0.43s\n","# Epoch 5  global step 14260 loss 0.34166 batch 1132/3281 lr 0.001 accuracy 93.68750 wps 14666.00 step time 0.47s\n","# Epoch 5  global step 14280 loss 0.34931 batch 1152/3281 lr 0.001 accuracy 93.58984 wps 13808.38 step time 0.51s\n","# Epoch 5  global step 14300 loss 0.34696 batch 1172/3281 lr 0.001 accuracy 93.81641 wps 14909.15 step time 0.48s\n","# Epoch 5  global step 14320 loss 0.35826 batch 1192/3281 lr 0.001 accuracy 93.57031 wps 14585.91 step time 0.45s\n","# Epoch 5  global step 14340 loss 0.35637 batch 1212/3281 lr 0.001 accuracy 93.55469 wps 15239.12 step time 0.51s\n","# Epoch 5  global step 14360 loss 0.37441 batch 1232/3281 lr 0.001 accuracy 93.18359 wps 14530.91 step time 0.59s\n","# Epoch 5  global step 14380 loss 0.34467 batch 1252/3281 lr 0.001 accuracy 93.56250 wps 14586.88 step time 0.47s\n","# Epoch 5  global step 14400 loss 0.34230 batch 1272/3281 lr 0.001 accuracy 93.74219 wps 14734.62 step time 0.47s\n","# Epoch 5  global step 14420 loss 0.32741 batch 1292/3281 lr 0.001 accuracy 94.17187 wps 14358.62 step time 0.43s\n","# Epoch 5  global step 14440 loss 0.36836 batch 1312/3281 lr 0.001 accuracy 93.29297 wps 12740.55 step time 0.54s\n","# Epoch 5  global step 14460 loss 0.35353 batch 1332/3281 lr 0.001 accuracy 93.32031 wps 12627.51 step time 0.58s\n","# Epoch 5  global step 14480 loss 0.36685 batch 1352/3281 lr 0.001 accuracy 93.33984 wps 13106.83 step time 0.52s\n","# Epoch 5  global step 14500 loss 0.35226 batch 1372/3281 lr 0.001 accuracy 93.52734 wps 12925.37 step time 0.50s\n","# Epoch 5  global step 14520 loss 0.34418 batch 1392/3281 lr 0.001 accuracy 93.69922 wps 12558.48 step time 0.52s\n","# Epoch 5  global step 14540 loss 0.34843 batch 1412/3281 lr 0.001 accuracy 93.54687 wps 13250.47 step time 0.61s\n","# Epoch 5  global step 14560 loss 0.37476 batch 1432/3281 lr 0.001 accuracy 93.11328 wps 11617.22 step time 0.73s\n","# Epoch 5  global step 14580 loss 0.35836 batch 1452/3281 lr 0.001 accuracy 93.24609 wps 12097.66 step time 0.63s\n","# Epoch 5  global step 14600 loss 0.36734 batch 1472/3281 lr 0.001 accuracy 93.35156 wps 12707.11 step time 0.63s\n","# Epoch 5  global step 14620 loss 0.35064 batch 1492/3281 lr 0.001 accuracy 93.47656 wps 12768.13 step time 0.53s\n","# Epoch 5  global step 14640 loss 0.35297 batch 1512/3281 lr 0.001 accuracy 93.75391 wps 12809.81 step time 0.48s\n","# Epoch 5  global step 14660 loss 0.35661 batch 1532/3281 lr 0.001 accuracy 93.45703 wps 13034.52 step time 0.54s\n","# Epoch 5  global step 14680 loss 0.34202 batch 1552/3281 lr 0.001 accuracy 93.63281 wps 12546.11 step time 0.59s\n","# Epoch 5  global step 14700 loss 0.38117 batch 1572/3281 lr 0.001 accuracy 93.19141 wps 13544.44 step time 0.59s\n","# Epoch 5  global step 14720 loss 0.34505 batch 1592/3281 lr 0.001 accuracy 93.78906 wps 12641.82 step time 0.48s\n","# Epoch 5  global step 14740 loss 0.34956 batch 1612/3281 lr 0.001 accuracy 93.54688 wps 12533.93 step time 0.57s\n","# Epoch 5  global step 14760 loss 0.36937 batch 1632/3281 lr 0.001 accuracy 93.13281 wps 13002.41 step time 0.58s\n","# Epoch 5  global step 14780 loss 0.36672 batch 1652/3281 lr 0.001 accuracy 93.40234 wps 12328.73 step time 0.59s\n","# Epoch 5  global step 14800 loss 0.33970 batch 1672/3281 lr 0.001 accuracy 93.92578 wps 13006.66 step time 0.48s\n","# Epoch 5  global step 14820 loss 0.35392 batch 1692/3281 lr 0.001 accuracy 93.50781 wps 13292.78 step time 0.55s\n","# Epoch 5  global step 14840 loss 0.35103 batch 1712/3281 lr 0.001 accuracy 93.64844 wps 13274.62 step time 0.49s\n","# Epoch 5  global step 14860 loss 0.34423 batch 1732/3281 lr 0.001 accuracy 93.81641 wps 12967.76 step time 0.46s\n","# Epoch 5  global step 14880 loss 0.36238 batch 1752/3281 lr 0.001 accuracy 93.35156 wps 12595.85 step time 0.60s\n","# Epoch 5  global step 14900 loss 0.36706 batch 1772/3281 lr 0.001 accuracy 93.23047 wps 11869.57 step time 0.69s\n","# Epoch 5  global step 14920 loss 0.36755 batch 1792/3281 lr 0.001 accuracy 93.28125 wps 13682.04 step time 0.52s\n","# Epoch 5  global step 14940 loss 0.34678 batch 1812/3281 lr 0.001 accuracy 93.73047 wps 12765.30 step time 0.55s\n","# Epoch 5  global step 14960 loss 0.34577 batch 1832/3281 lr 0.001 accuracy 93.95313 wps 12233.85 step time 0.55s\n","# Epoch 5  global step 14980 loss 0.37024 batch 1852/3281 lr 0.001 accuracy 93.33203 wps 12649.91 step time 0.68s\n","# Epoch 5  global step 15000 loss 0.37126 batch 1872/3281 lr 0.001 accuracy 92.95313 wps 12796.37 step time 0.74s\n","# Epoch 5  global step 15020 loss 0.33915 batch 1892/3281 lr 0.001 accuracy 93.89453 wps 11822.23 step time 0.64s\n","# Epoch 5  global step 15040 loss 0.34608 batch 1912/3281 lr 0.001 accuracy 93.67578 wps 12974.73 step time 0.51s\n","# Epoch 5  global step 15060 loss 0.35302 batch 1932/3281 lr 0.001 accuracy 93.50781 wps 13433.09 step time 0.50s\n","# Epoch 5  global step 15080 loss 0.34018 batch 1952/3281 lr 0.001 accuracy 93.76172 wps 13175.63 step time 0.50s\n","# Epoch 5  global step 15100 loss 0.37228 batch 1972/3281 lr 0.001 accuracy 93.29687 wps 12922.58 step time 0.57s\n","# Epoch 5  global step 15120 loss 0.36665 batch 1992/3281 lr 0.001 accuracy 93.39062 wps 13295.23 step time 0.56s\n","# Epoch 5  global step 15140 loss 0.33686 batch 2012/3281 lr 0.001 accuracy 93.78125 wps 13103.72 step time 0.47s\n","# Epoch 5  global step 15160 loss 0.37243 batch 2032/3281 lr 0.001 accuracy 93.13281 wps 12485.74 step time 0.61s\n","# Epoch 5  global step 15180 loss 0.35367 batch 2052/3281 lr 0.001 accuracy 93.51953 wps 11587.56 step time 0.71s\n","# Epoch 5  global step 15200 loss 0.34942 batch 2072/3281 lr 0.001 accuracy 93.64453 wps 12160.01 step time 0.63s\n","# Epoch 5  global step 15220 loss 0.35639 batch 2092/3281 lr 0.001 accuracy 93.52734 wps 12864.67 step time 0.54s\n","# Epoch 5  global step 15240 loss 0.34678 batch 2112/3281 lr 0.001 accuracy 93.75781 wps 12884.97 step time 0.57s\n","# Epoch 5  global step 15260 loss 0.36908 batch 2132/3281 lr 0.001 accuracy 93.29297 wps 12912.81 step time 0.60s\n","# Epoch 5  global step 15280 loss 0.38197 batch 2152/3281 lr 0.001 accuracy 92.98828 wps 13035.15 step time 0.59s\n","# Epoch 5  global step 15300 loss 0.35455 batch 2172/3281 lr 0.001 accuracy 93.37500 wps 13525.03 step time 0.50s\n","# Epoch 5  global step 15320 loss 0.35832 batch 2192/3281 lr 0.001 accuracy 93.37891 wps 13318.42 step time 0.52s\n","# Epoch 5  global step 15340 loss 0.36564 batch 2212/3281 lr 0.001 accuracy 93.20703 wps 13016.95 step time 0.61s\n","# Epoch 5  global step 15360 loss 0.36936 batch 2232/3281 lr 0.001 accuracy 93.37891 wps 13006.26 step time 0.59s\n","# Epoch 5  global step 15380 loss 0.34661 batch 2252/3281 lr 0.001 accuracy 93.78125 wps 13164.88 step time 0.51s\n","# Epoch 5  global step 15400 loss 0.33946 batch 2272/3281 lr 0.001 accuracy 93.82812 wps 12973.97 step time 0.43s\n","# Epoch 5  global step 15420 loss 0.34188 batch 2292/3281 lr 0.001 accuracy 93.79297 wps 12902.31 step time 0.56s\n","# Epoch 5  global step 15440 loss 0.36612 batch 2312/3281 lr 0.001 accuracy 93.17187 wps 12599.03 step time 0.57s\n","# Epoch 5  global step 15460 loss 0.35545 batch 2332/3281 lr 0.001 accuracy 93.68359 wps 13137.20 step time 0.59s\n","# Epoch 5  global step 15480 loss 0.36184 batch 2352/3281 lr 0.001 accuracy 93.36328 wps 12783.62 step time 0.59s\n","# Epoch 5  global step 15500 loss 0.36879 batch 2372/3281 lr 0.001 accuracy 93.26172 wps 12809.46 step time 0.58s\n","# Epoch 5  global step 15520 loss 0.34506 batch 2392/3281 lr 0.001 accuracy 93.78906 wps 13087.15 step time 0.46s\n","# Epoch 5  global step 15540 loss 0.36338 batch 2412/3281 lr 0.001 accuracy 93.41406 wps 12436.62 step time 0.60s\n","# Epoch 5  global step 15560 loss 0.36426 batch 2432/3281 lr 0.001 accuracy 93.29297 wps 11940.14 step time 0.70s\n","# Epoch 5  global step 15580 loss 0.36374 batch 2452/3281 lr 0.001 accuracy 93.23438 wps 13153.65 step time 0.53s\n","# Epoch 5  global step 15600 loss 0.33465 batch 2472/3281 lr 0.001 accuracy 93.97656 wps 12545.58 step time 0.52s\n","# Epoch 5  global step 15620 loss 0.34125 batch 2492/3281 lr 0.001 accuracy 93.75391 wps 12899.17 step time 0.51s\n","# Epoch 5  global step 15640 loss 0.34780 batch 2512/3281 lr 0.001 accuracy 93.59375 wps 13006.64 step time 0.51s\n","# Epoch 5  global step 15660 loss 0.36085 batch 2532/3281 lr 0.001 accuracy 93.18359 wps 13125.48 step time 0.58s\n","# Epoch 5  global step 15680 loss 0.36265 batch 2552/3281 lr 0.001 accuracy 93.42187 wps 11763.19 step time 0.80s\n","# Epoch 5  global step 15700 loss 0.32900 batch 2572/3281 lr 0.001 accuracy 94.01953 wps 12499.77 step time 0.50s\n","# Epoch 5  global step 15720 loss 0.35603 batch 2592/3281 lr 0.001 accuracy 93.55859 wps 13165.91 step time 0.49s\n","# Epoch 5  global step 15740 loss 0.34795 batch 2612/3281 lr 0.001 accuracy 93.62891 wps 12876.14 step time 0.53s\n","# Epoch 5  global step 15760 loss 0.34388 batch 2632/3281 lr 0.001 accuracy 93.75781 wps 12770.26 step time 0.54s\n","# Epoch 5  global step 15780 loss 0.36781 batch 2652/3281 lr 0.001 accuracy 93.38281 wps 13248.41 step time 0.53s\n","# Epoch 5  global step 15800 loss 0.36446 batch 2672/3281 lr 0.001 accuracy 93.51953 wps 12303.40 step time 0.64s\n","# Epoch 5  global step 15820 loss 0.35874 batch 2692/3281 lr 0.001 accuracy 93.44922 wps 12027.19 step time 0.61s\n","# Epoch 5  global step 15840 loss 0.36124 batch 2712/3281 lr 0.001 accuracy 93.41016 wps 13138.98 step time 0.48s\n","# Epoch 5  global step 15860 loss 0.35316 batch 2732/3281 lr 0.001 accuracy 93.67188 wps 12569.72 step time 0.53s\n","# Epoch 5  global step 15880 loss 0.33590 batch 2752/3281 lr 0.001 accuracy 93.83594 wps 12930.64 step time 0.47s\n","# Epoch 5  global step 15900 loss 0.35759 batch 2772/3281 lr 0.001 accuracy 93.58594 wps 13113.03 step time 0.51s\n","# Epoch 5  global step 15920 loss 0.33499 batch 2792/3281 lr 0.001 accuracy 93.95313 wps 12889.42 step time 0.48s\n","# Epoch 5  global step 15940 loss 0.35089 batch 2812/3281 lr 0.001 accuracy 93.67969 wps 12887.06 step time 0.58s\n","# Epoch 5  global step 15960 loss 0.35228 batch 2832/3281 lr 0.001 accuracy 93.41797 wps 12854.82 step time 0.61s\n","# Epoch 5  global step 15980 loss 0.38599 batch 2852/3281 lr 0.001 accuracy 93.09375 wps 12185.53 step time 0.84s\n","# Epoch 5  global step 16000 loss 0.34997 batch 2872/3281 lr 0.001 accuracy 93.65625 wps 12382.33 step time 0.61s\n","# global step 16000, eval model at Sun May 24 13:52:43 2020\n","2020-05-24 13:52:45.814422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:52:45.814787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-24 13:52:45.814889: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 13:52:45.814914: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-24 13:52:45.814934: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-24 13:52:45.814953: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-24 13:52:45.814972: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-24 13:52:45.814990: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-24 13:52:45.815015: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 13:52:45.815106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:52:45.815362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:52:45.815582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-24 13:52:45.815660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 13:52:45.815675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-24 13:52:45.815685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-24 13:52:45.815886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:52:45.816203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 13:52:45.816439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6095509043752434\n","# location_distance_from_business_district - 0.3980694878916493\n","# location_easy_to_find - 0.6450727728895124\n","# service_wait_time - 0.4322843193769462\n","# service_waiters_attitude - 0.7152663885681314\n","# service_parking_convenience - 0.6701972629657773\n","# service_serving_speed - 0.6039214313426609\n","# price_level - 0.6104847815258312\n","# price_cost_effective - 0.5998218266322604\n","# price_discount - 0.5396735327310714\n","# environment_decoration - 0.5971614514916019\n","# environment_noise - 0.6498201661684345\n","# environment_space - 0.6768014151596946\n","# environment_cleaness - 0.6455781910169894\n","# dish_portion - 0.5043753340299724\n","# dish_taste - 0.6728891653723804\n","# dish_look - 0.3096294687482245\n","# dish_recommendation - 0.3909385981477601\n","# others_overall_experience - 0.5718961793215729\n","# others_willing_to_consume_again - 0.6227358205174345\n","# Eval loss 0.34977, f1 0.57331\n","# current result -0.5733084249136576, previous best result -0.4333631897949413\n","# Epoch 5  global step 16020 loss 0.38423 batch 2892/3281 lr 0.001 accuracy 93.05859 wps 12160.37 step time 0.77s\n","# Epoch 5  global step 16040 loss 0.34506 batch 2912/3281 lr 0.001 accuracy 93.71875 wps 11723.34 step time 0.62s\n","# Epoch 5  global step 16060 loss 0.35067 batch 2932/3281 lr 0.001 accuracy 93.60938 wps 12812.09 step time 0.51s\n","# Epoch 5  global step 16080 loss 0.38112 batch 2952/3281 lr 0.001 accuracy 92.97656 wps 12676.72 step time 0.71s\n","# Epoch 5  global step 16100 loss 0.34437 batch 2972/3281 lr 0.001 accuracy 93.67969 wps 13015.12 step time 0.45s\n","# Epoch 5  global step 16120 loss 0.34201 batch 2992/3281 lr 0.001 accuracy 93.67578 wps 11824.08 step time 0.66s\n","# Epoch 5  global step 16140 loss 0.36515 batch 3012/3281 lr 0.001 accuracy 93.29688 wps 13526.29 step time 0.56s\n","# Epoch 5  global step 16160 loss 0.35815 batch 3032/3281 lr 0.001 accuracy 93.60938 wps 13033.76 step time 0.53s\n","# Epoch 5  global step 16180 loss 0.36859 batch 3052/3281 lr 0.001 accuracy 93.22266 wps 12364.42 step time 0.67s\n","# Epoch 5  global step 16200 loss 0.36729 batch 3072/3281 lr 0.001 accuracy 93.64063 wps 12128.24 step time 0.61s\n","# Epoch 5  global step 16220 loss 0.34580 batch 3092/3281 lr 0.001 accuracy 93.62500 wps 13060.23 step time 0.49s\n","# Epoch 5  global step 16240 loss 0.38325 batch 3112/3281 lr 0.001 accuracy 93.05859 wps 12254.27 step time 0.72s\n","# Epoch 5  global step 16260 loss 0.35411 batch 3132/3281 lr 0.001 accuracy 93.70703 wps 13049.65 step time 0.50s\n","# Epoch 5  global step 16280 loss 0.36533 batch 3152/3281 lr 0.001 accuracy 93.32422 wps 13143.04 step time 0.54s\n","# Epoch 5  global step 16300 loss 0.35647 batch 3172/3281 lr 0.001 accuracy 93.48828 wps 12980.77 step time 0.50s\n","# Epoch 5  global step 16320 loss 0.36450 batch 3192/3281 lr 0.001 accuracy 93.23828 wps 11775.98 step time 0.75s\n","# Epoch 5  global step 16340 loss 0.35549 batch 3212/3281 lr 0.001 accuracy 93.42969 wps 12928.08 step time 0.49s\n","# Epoch 5  global step 16360 loss 0.33905 batch 3232/3281 lr 0.001 accuracy 93.76953 wps 12805.73 step time 0.53s\n","# Epoch 5  global step 16380 loss 0.36120 batch 3252/3281 lr 0.001 accuracy 93.38281 wps 12091.24 step time 0.64s\n","# Epoch 5  global step 16400 loss 0.38277 batch 3272/3281 lr 0.001 accuracy 93.06641 wps 12654.18 step time 0.69s\n","# Finsh epoch 5, global step 16410\n","# Epoch 6  global step 16420 loss 0.17691 batch 10/3281 lr 0.001 accuracy 46.80469 wps 13707.80 step time 0.29s\n","# Epoch 6  global step 16440 loss 0.34458 batch 30/3281 lr 0.001 accuracy 93.92969 wps 14618.15 step time 0.56s\n","# Epoch 6  global step 16460 loss 0.35094 batch 50/3281 lr 0.001 accuracy 93.63281 wps 15141.32 step time 0.50s\n","# Epoch 6  global step 16480 loss 0.35793 batch 70/3281 lr 0.001 accuracy 93.37500 wps 14790.04 step time 0.46s\n","# Epoch 6  global step 16500 loss 0.32959 batch 90/3281 lr 0.001 accuracy 94.03125 wps 14010.41 step time 0.40s\n","# Epoch 6  global step 16520 loss 0.34847 batch 110/3281 lr 0.001 accuracy 93.62891 wps 14417.18 step time 0.45s\n","# Epoch 6  global step 16540 loss 0.33068 batch 130/3281 lr 0.001 accuracy 93.96875 wps 13938.82 step time 0.51s\n","# Epoch 6  global step 16560 loss 0.35274 batch 150/3281 lr 0.001 accuracy 93.68359 wps 14571.98 step time 0.45s\n","# Epoch 6  global step 16580 loss 0.34260 batch 170/3281 lr 0.001 accuracy 93.69531 wps 14938.13 step time 0.50s\n","# Epoch 6  global step 16600 loss 0.31401 batch 190/3281 lr 0.001 accuracy 94.15625 wps 14587.03 step time 0.44s\n","# Epoch 6  global step 16620 loss 0.35865 batch 210/3281 lr 0.001 accuracy 93.45703 wps 13942.79 step time 0.62s\n","# Epoch 6  global step 16640 loss 0.34286 batch 230/3281 lr 0.001 accuracy 93.80078 wps 14963.83 step time 0.48s\n","# Epoch 6  global step 16660 loss 0.33489 batch 250/3281 lr 0.001 accuracy 93.97266 wps 14435.97 step time 0.43s\n","# Epoch 6  global step 16680 loss 0.32570 batch 270/3281 lr 0.001 accuracy 94.05859 wps 14455.36 step time 0.44s\n","# Epoch 6  global step 16700 loss 0.36993 batch 290/3281 lr 0.001 accuracy 93.11328 wps 14068.96 step time 0.62s\n","# Epoch 6  global step 16720 loss 0.31969 batch 310/3281 lr 0.001 accuracy 94.31641 wps 14249.55 step time 0.41s\n","# Epoch 6  global step 16740 loss 0.34231 batch 330/3281 lr 0.001 accuracy 93.71484 wps 14847.14 step time 0.46s\n","# Epoch 6  global step 16760 loss 0.32644 batch 350/3281 lr 0.001 accuracy 94.01172 wps 14792.85 step time 0.46s\n","# Epoch 6  global step 16780 loss 0.35604 batch 370/3281 lr 0.001 accuracy 93.39062 wps 14385.94 step time 0.56s\n","# Epoch 6  global step 16800 loss 0.33575 batch 390/3281 lr 0.001 accuracy 93.82031 wps 13987.55 step time 0.59s\n","# Epoch 6  global step 16820 loss 0.32565 batch 410/3281 lr 0.001 accuracy 94.01953 wps 14442.29 step time 0.44s\n","# Epoch 6  global step 16840 loss 0.34686 batch 430/3281 lr 0.001 accuracy 93.59375 wps 14943.95 step time 0.50s\n","# Epoch 6  global step 16860 loss 0.33791 batch 450/3281 lr 0.001 accuracy 93.88672 wps 15010.48 step time 0.49s\n","# Epoch 6  global step 16880 loss 0.34854 batch 470/3281 lr 0.001 accuracy 93.57422 wps 14986.06 step time 0.50s\n","# Epoch 6  global step 16900 loss 0.33517 batch 490/3281 lr 0.001 accuracy 93.81641 wps 14628.69 step time 0.45s\n","# Epoch 6  global step 16920 loss 0.34975 batch 510/3281 lr 0.001 accuracy 93.64062 wps 14899.78 step time 0.48s\n","# Epoch 6  global step 16940 loss 0.35903 batch 530/3281 lr 0.001 accuracy 93.44922 wps 15180.85 step time 0.53s\n","# Epoch 6  global step 16960 loss 0.33584 batch 550/3281 lr 0.001 accuracy 93.68359 wps 14926.86 step time 0.48s\n","# Epoch 6  global step 16980 loss 0.35153 batch 570/3281 lr 0.001 accuracy 93.56250 wps 14861.40 step time 0.49s\n","# Epoch 6  global step 17000 loss 0.34574 batch 590/3281 lr 0.001 accuracy 93.68359 wps 14058.72 step time 0.53s\n","# Epoch 6  global step 17020 loss 0.36966 batch 610/3281 lr 0.001 accuracy 93.08984 wps 14358.34 step time 0.56s\n","# Epoch 6  global step 17040 loss 0.36463 batch 630/3281 lr 0.001 accuracy 93.49609 wps 15037.62 step time 0.50s\n","# Epoch 6  global step 17060 loss 0.35508 batch 650/3281 lr 0.001 accuracy 93.39844 wps 14668.14 step time 0.59s\n","# Epoch 6  global step 17080 loss 0.36858 batch 670/3281 lr 0.001 accuracy 93.21484 wps 14704.91 step time 0.69s\n","# Epoch 6  global step 17100 loss 0.35386 batch 690/3281 lr 0.001 accuracy 93.42578 wps 14643.44 step time 0.57s\n","# Epoch 6  global step 17120 loss 0.32013 batch 710/3281 lr 0.001 accuracy 94.25781 wps 14174.92 step time 0.52s\n","# Epoch 6  global step 17140 loss 0.35202 batch 730/3281 lr 0.001 accuracy 93.56250 wps 15092.61 step time 0.53s\n","# Epoch 6  global step 17160 loss 0.34295 batch 750/3281 lr 0.001 accuracy 93.90625 wps 14333.43 step time 0.52s\n","# Epoch 6  global step 17180 loss 0.34101 batch 770/3281 lr 0.001 accuracy 93.82422 wps 14254.12 step time 0.42s\n","# Epoch 6  global step 17200 loss 0.34688 batch 790/3281 lr 0.001 accuracy 93.61328 wps 14255.41 step time 0.54s\n","# Epoch 6  global step 17220 loss 0.34360 batch 810/3281 lr 0.001 accuracy 93.66406 wps 14826.96 step time 0.47s\n","# Epoch 6  global step 17240 loss 0.35962 batch 830/3281 lr 0.001 accuracy 93.33594 wps 15028.82 step time 0.48s\n","# Epoch 6  global step 17260 loss 0.35075 batch 850/3281 lr 0.001 accuracy 93.58984 wps 14505.47 step time 0.46s\n","# Epoch 6  global step 17280 loss 0.36239 batch 870/3281 lr 0.001 accuracy 93.47266 wps 14752.29 step time 0.46s\n","# Epoch 6  global step 17300 loss 0.35680 batch 890/3281 lr 0.001 accuracy 93.41406 wps 14661.02 step time 0.56s\n","# Epoch 6  global step 17320 loss 0.35261 batch 910/3281 lr 0.001 accuracy 93.53906 wps 14374.66 step time 0.55s\n","# Epoch 6  global step 17340 loss 0.35859 batch 930/3281 lr 0.001 accuracy 93.35937 wps 14496.28 step time 0.56s\n","# Epoch 6  global step 17360 loss 0.35160 batch 950/3281 lr 0.001 accuracy 93.57422 wps 14565.00 step time 0.55s\n","# Epoch 6  global step 17380 loss 0.35718 batch 970/3281 lr 0.001 accuracy 93.55078 wps 15127.98 step time 0.50s\n","# Epoch 6  global step 17400 loss 0.35265 batch 990/3281 lr 0.001 accuracy 93.70703 wps 13815.89 step time 0.58s\n","# Epoch 6  global step 17420 loss 0.32052 batch 1010/3281 lr 0.001 accuracy 94.12891 wps 14242.94 step time 0.43s\n","# Epoch 6  global step 17440 loss 0.33794 batch 1030/3281 lr 0.001 accuracy 93.89844 wps 14894.29 step time 0.47s\n","# Epoch 6  global step 17460 loss 0.34986 batch 1050/3281 lr 0.001 accuracy 93.60547 wps 15301.79 step time 0.54s\n","# Epoch 6  global step 17480 loss 0.34277 batch 1070/3281 lr 0.001 accuracy 93.82031 wps 15022.71 step time 0.50s\n","# Epoch 6  global step 17500 loss 0.36375 batch 1090/3281 lr 0.001 accuracy 93.29687 wps 15262.51 step time 0.54s\n","# Epoch 6  global step 17520 loss 0.32908 batch 1110/3281 lr 0.001 accuracy 94.12891 wps 14528.13 step time 0.44s\n","# Epoch 6  global step 17540 loss 0.34995 batch 1130/3281 lr 0.001 accuracy 93.59766 wps 15287.12 step time 0.51s\n","# Epoch 6  global step 17560 loss 0.33682 batch 1150/3281 lr 0.001 accuracy 93.89844 wps 14599.12 step time 0.45s\n","# Epoch 6  global step 17580 loss 0.33202 batch 1170/3281 lr 0.001 accuracy 93.85547 wps 14722.88 step time 0.46s\n","# Epoch 6  global step 17600 loss 0.34437 batch 1190/3281 lr 0.001 accuracy 93.86328 wps 14917.64 step time 0.48s\n","# Epoch 6  global step 17620 loss 0.33709 batch 1210/3281 lr 0.001 accuracy 93.89063 wps 13934.20 step time 0.48s\n","# Epoch 6  global step 17640 loss 0.33498 batch 1230/3281 lr 0.001 accuracy 93.95312 wps 14817.23 step time 0.48s\n","# Epoch 6  global step 17660 loss 0.33528 batch 1250/3281 lr 0.001 accuracy 93.95703 wps 14559.26 step time 0.46s\n","# Epoch 6  global step 17680 loss 0.34289 batch 1270/3281 lr 0.001 accuracy 93.71094 wps 14968.55 step time 0.48s\n","# Epoch 6  global step 17700 loss 0.37536 batch 1290/3281 lr 0.001 accuracy 93.15625 wps 15406.37 step time 0.53s\n","# Epoch 6  global step 17720 loss 0.35927 batch 1310/3281 lr 0.001 accuracy 93.42187 wps 14780.97 step time 0.47s\n","# Epoch 6  global step 17740 loss 0.36552 batch 1330/3281 lr 0.001 accuracy 93.27344 wps 15053.76 step time 0.52s\n","# Epoch 6  global step 17760 loss 0.35964 batch 1350/3281 lr 0.001 accuracy 93.50391 wps 15237.79 step time 0.52s\n","# Epoch 6  global step 17780 loss 0.35738 batch 1370/3281 lr 0.001 accuracy 93.38281 wps 14328.03 step time 0.56s\n","# Epoch 6  global step 17800 loss 0.33217 batch 1390/3281 lr 0.001 accuracy 94.09766 wps 14621.25 step time 0.45s\n","# Epoch 6  global step 17820 loss 0.35843 batch 1410/3281 lr 0.001 accuracy 93.36328 wps 15076.07 step time 0.49s\n","# Epoch 6  global step 17840 loss 0.35897 batch 1430/3281 lr 0.001 accuracy 93.40625 wps 13925.32 step time 0.59s\n","# Epoch 6  global step 17860 loss 0.34872 batch 1450/3281 lr 0.001 accuracy 93.62109 wps 14695.92 step time 0.57s\n","# Epoch 6  global step 17880 loss 0.33332 batch 1470/3281 lr 0.001 accuracy 93.83594 wps 14406.05 step time 0.56s\n","# Epoch 6  global step 17900 loss 0.34118 batch 1490/3281 lr 0.001 accuracy 93.73047 wps 14891.47 step time 0.47s\n","# Epoch 6  global step 17920 loss 0.33745 batch 1510/3281 lr 0.001 accuracy 93.81250 wps 14881.15 step time 0.48s\n","# Epoch 6  global step 17940 loss 0.38830 batch 1530/3281 lr 0.001 accuracy 92.91016 wps 15469.88 step time 0.55s\n","# Epoch 6  global step 17960 loss 0.34708 batch 1550/3281 lr 0.001 accuracy 93.51953 wps 14526.32 step time 0.55s\n","# Epoch 6  global step 17980 loss 0.34920 batch 1570/3281 lr 0.001 accuracy 93.73828 wps 13964.14 step time 0.52s\n","# Epoch 6  global step 18000 loss 0.33559 batch 1590/3281 lr 0.001 accuracy 93.96875 wps 14559.79 step time 0.44s\n","# global step 18000, eval model at Sun May 24 14:12:23 2020\n","2020-05-24 14:12:25.826026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 14:12:25.826470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n","pciBusID: 0000:00:04.0\n","2020-05-24 14:12:25.826582: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2020-05-24 14:12:25.826608: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2020-05-24 14:12:25.826636: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2020-05-24 14:12:25.826657: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2020-05-24 14:12:25.826676: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2020-05-24 14:12:25.826694: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2020-05-24 14:12:25.826735: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 14:12:25.826837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 14:12:25.827106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 14:12:25.827315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2020-05-24 14:12:25.827432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 14:12:25.827448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2020-05-24 14:12:25.827457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2020-05-24 14:12:25.827566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 14:12:25.827858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 14:12:25.828097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7231 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n","# location_traffic_convenience - 0.6326875220442122\n","# location_distance_from_business_district - 0.42398675691531396\n","# location_easy_to_find - 0.6656453930645473\n","# service_wait_time - 0.5424384914022825\n","# service_waiters_attitude - 0.7603260984155171\n","# service_parking_convenience - 0.7092832640654312\n","# service_serving_speed - 0.6805782523560798\n","# price_level - 0.7036073670667184\n","# price_cost_effective - 0.6658181009696927\n","# price_discount - 0.5939449669433978\n","# environment_decoration - 0.6694207317412277\n","# environment_noise - 0.7290193152205736\n","# environment_space - 0.7390278654869665\n","# environment_cleaness - 0.7106000061652904\n","# dish_portion - 0.6250445997098828\n","# dish_taste - 0.7041160933378519\n","# dish_look - 0.40933776755467344\n","# dish_recommendation - 0.6455919962957866\n","# others_overall_experience - 0.5781594385846489\n","# others_willing_to_consume_again - 0.6629250956571239\n","# Eval loss 0.31081, f1 0.64258\n","# current result -0.6425779561498609, previous best result -0.5733084249136576\n","# Epoch 6  global step 18020 loss 0.34532 batch 1610/3281 lr 0.001 accuracy 93.58203 wps 14865.85 step time 0.50s\n","# Epoch 6  global step 18040 loss 0.35715 batch 1630/3281 lr 0.001 accuracy 93.43750 wps 14195.78 step time 0.63s\n","# Epoch 6  global step 18060 loss 0.34925 batch 1650/3281 lr 0.001 accuracy 93.78906 wps 14227.78 step time 0.51s\n","# Epoch 6  global step 18080 loss 0.37849 batch 1670/3281 lr 0.001 accuracy 92.90625 wps 14715.39 step time 0.61s\n","# Epoch 6  global step 18100 loss 0.34028 batch 1690/3281 lr 0.001 accuracy 93.74609 wps 14514.01 step time 0.45s\n","# Epoch 6  global step 18120 loss 0.35377 batch 1710/3281 lr 0.001 accuracy 93.75781 wps 14581.07 step time 0.45s\n","# Epoch 6  global step 18140 loss 0.32039 batch 1730/3281 lr 0.001 accuracy 94.22656 wps 14233.56 step time 0.41s\n","# Epoch 6  global step 18160 loss 0.35101 batch 1750/3281 lr 0.001 accuracy 93.47266 wps 14872.02 step time 0.47s\n","# Epoch 6  global step 18180 loss 0.33552 batch 1770/3281 lr 0.001 accuracy 93.98437 wps 14346.23 step time 0.42s\n","# Epoch 6  global step 18200 loss 0.34201 batch 1790/3281 lr 0.001 accuracy 93.67188 wps 14885.53 step time 0.47s\n","# Epoch 6  global step 18220 loss 0.33838 batch 1810/3281 lr 0.001 accuracy 93.80469 wps 14343.49 step time 0.43s\n","# Epoch 6  global step 18240 loss 0.31722 batch 1830/3281 lr 0.001 accuracy 94.32031 wps 14184.53 step time 0.41s\n","# Epoch 6  global step 18260 loss 0.33113 batch 1850/3281 lr 0.001 accuracy 94.03516 wps 14530.64 step time 0.46s\n","# Epoch 6  global step 18280 loss 0.33465 batch 1870/3281 lr 0.001 accuracy 94.10156 wps 14544.70 step time 0.45s\n","# Epoch 6  global step 18300 loss 0.35272 batch 1890/3281 lr 0.001 accuracy 93.46875 wps 14319.35 step time 0.53s\n","# Epoch 6  global step 18320 loss 0.34552 batch 1910/3281 lr 0.001 accuracy 93.61328 wps 14830.79 step time 0.47s\n","# Epoch 6  global step 18340 loss 0.32970 batch 1930/3281 lr 0.001 accuracy 93.95703 wps 13925.11 step time 0.51s\n","# Epoch 6  global step 18360 loss 0.34138 batch 1950/3281 lr 0.001 accuracy 93.81250 wps 14589.89 step time 0.44s\n","# Epoch 6  global step 18380 loss 0.35901 batch 1970/3281 lr 0.001 accuracy 93.27344 wps 14948.06 step time 0.61s\n","# Epoch 6  global step 18400 loss 0.35306 batch 1990/3281 lr 0.001 accuracy 93.63281 wps 15184.64 step time 0.51s\n","# Epoch 6  global step 18420 loss 0.33662 batch 2010/3281 lr 0.001 accuracy 93.82031 wps 14157.95 step time 0.41s\n","# Epoch 6  global step 18440 loss 0.36457 batch 2030/3281 lr 0.001 accuracy 93.14453 wps 15116.74 step time 0.54s\n","# Epoch 6  global step 18460 loss 0.36292 batch 2050/3281 lr 0.001 accuracy 93.23047 wps 15176.96 step time 0.53s\n","# Epoch 6  global step 18480 loss 0.32649 batch 2070/3281 lr 0.001 accuracy 93.95703 wps 13235.24 step time 0.47s\n","# Epoch 6  global step 18500 loss 0.36010 batch 2090/3281 lr 0.001 accuracy 93.46875 wps 13342.71 step time 0.60s\n","# Epoch 6  global step 18520 loss 0.33184 batch 2110/3281 lr 0.001 accuracy 93.86719 wps 12897.83 step time 0.53s\n","# Epoch 6  global step 18540 loss 0.34453 batch 2130/3281 lr 0.001 accuracy 93.73438 wps 13700.48 step time 0.51s\n","# Epoch 6  global step 18560 loss 0.35366 batch 2150/3281 lr 0.001 accuracy 93.52734 wps 12801.72 step time 0.49s\n","# Epoch 6  global step 18580 loss 0.32654 batch 2170/3281 lr 0.001 accuracy 93.99609 wps 12149.26 step time 0.58s\n","# Epoch 6  global step 18600 loss 0.34559 batch 2190/3281 lr 0.001 accuracy 93.47266 wps 11624.19 step time 0.72s\n","# Epoch 6  global step 18620 loss 0.35732 batch 2210/3281 lr 0.001 accuracy 93.61328 wps 12379.20 step time 0.61s\n","# Epoch 6  global step 18640 loss 0.35287 batch 2230/3281 lr 0.001 accuracy 93.49609 wps 12766.09 step time 0.59s\n","# Epoch 6  global step 18660 loss 0.35604 batch 2250/3281 lr 0.001 accuracy 93.43359 wps 12694.22 step time 0.66s\n","# Epoch 6  global step 18680 loss 0.35224 batch 2270/3281 lr 0.001 accuracy 93.58594 wps 13427.01 step time 0.55s\n","# Epoch 6  global step 18700 loss 0.34504 batch 2290/3281 lr 0.001 accuracy 93.62891 wps 12431.94 step time 0.61s\n","# Epoch 6  global step 18720 loss 0.35341 batch 2310/3281 lr 0.001 accuracy 93.57422 wps 13336.86 step time 0.46s\n","# Epoch 6  global step 18740 loss 0.37822 batch 2330/3281 lr 0.001 accuracy 93.03516 wps 13326.24 step time 0.71s\n","# Epoch 6  global step 18760 loss 0.36374 batch 2350/3281 lr 0.001 accuracy 93.56641 wps 12072.41 step time 0.65s\n","# Epoch 6  global step 18780 loss 0.34539 batch 2370/3281 lr 0.001 accuracy 93.68750 wps 12287.45 step time 0.61s\n","# Epoch 6  global step 18800 loss 0.33360 batch 2390/3281 lr 0.001 accuracy 94.04688 wps 12957.99 step time 0.49s\n","# Epoch 6  global step 18820 loss 0.33967 batch 2410/3281 lr 0.001 accuracy 93.83984 wps 13225.40 step time 0.54s\n","# Epoch 6  global step 18840 loss 0.35403 batch 2430/3281 lr 0.001 accuracy 93.42188 wps 13161.76 step time 0.53s\n","# Epoch 6  global step 18860 loss 0.35916 batch 2450/3281 lr 0.001 accuracy 93.54297 wps 12962.72 step time 0.56s\n","# Epoch 6  global step 18880 loss 0.33711 batch 2470/3281 lr 0.001 accuracy 93.80859 wps 12743.08 step time 0.47s\n","# Epoch 6  global step 18900 loss 0.35268 batch 2490/3281 lr 0.001 accuracy 93.57812 wps 12832.11 step time 0.51s\n","# Epoch 6  global step 18920 loss 0.35539 batch 2510/3281 lr 0.001 accuracy 93.39063 wps 12832.49 step time 0.65s\n","# Epoch 6  global step 18940 loss 0.36624 batch 2530/3281 lr 0.001 accuracy 93.37500 wps 12895.34 step time 0.57s\n","# Epoch 6  global step 18960 loss 0.33838 batch 2550/3281 lr 0.001 accuracy 93.89453 wps 12920.85 step time 0.53s\n","# Epoch 6  global step 18980 loss 0.35431 batch 2570/3281 lr 0.001 accuracy 93.60156 wps 13139.31 step time 0.51s\n","# Epoch 6  global step 19000 loss 0.34984 batch 2590/3281 lr 0.001 accuracy 93.67188 wps 13036.42 step time 0.55s\n","# Epoch 6  global step 19020 loss 0.35501 batch 2610/3281 lr 0.001 accuracy 93.45313 wps 12486.94 step time 0.74s\n","# Epoch 6  global step 19040 loss 0.33396 batch 2630/3281 lr 0.001 accuracy 94.03516 wps 12229.36 step time 0.54s\n","# Epoch 6  global step 19060 loss 0.35919 batch 2650/3281 lr 0.001 accuracy 93.46094 wps 12620.41 step time 0.59s\n","# Epoch 6  global step 19080 loss 0.36086 batch 2670/3281 lr 0.001 accuracy 93.39844 wps 12883.45 step time 0.52s\n","# Epoch 6  global step 19100 loss 0.37251 batch 2690/3281 lr 0.001 accuracy 93.12891 wps 12927.93 step time 0.60s\n","# Epoch 6  global step 19120 loss 0.37165 batch 2710/3281 lr 0.001 accuracy 93.10156 wps 13483.31 step time 0.59s\n","# Epoch 6  global step 19140 loss 0.33300 batch 2730/3281 lr 0.001 accuracy 94.08984 wps 12942.72 step time 0.45s\n","# Epoch 6  global step 19160 loss 0.36720 batch 2750/3281 lr 0.001 accuracy 93.39453 wps 13806.01 step time 0.54s\n","# Epoch 6  global step 19180 loss 0.34976 batch 2770/3281 lr 0.001 accuracy 93.78516 wps 11739.99 step time 0.55s\n","# Epoch 6  global step 19200 loss 0.35308 batch 2790/3281 lr 0.001 accuracy 93.57031 wps 12727.46 step time 0.58s\n","# Epoch 6  global step 19220 loss 0.34409 batch 2810/3281 lr 0.001 accuracy 93.62109 wps 12732.61 step time 0.60s\n","# Epoch 6  global step 19240 loss 0.33843 batch 2830/3281 lr 0.001 accuracy 93.61328 wps 12166.31 step time 0.70s\n","# Epoch 6  global step 19260 loss 0.35793 batch 2850/3281 lr 0.001 accuracy 93.39062 wps 12683.18 step time 0.61s\n","# Epoch 6  global step 19280 loss 0.35328 batch 2870/3281 lr 0.001 accuracy 93.65234 wps 12499.03 step time 0.56s\n","# Epoch 6  global step 19300 loss 0.37205 batch 2890/3281 lr 0.001 accuracy 93.06250 wps 11592.53 step time 0.85s\n","# Epoch 6  global step 19320 loss 0.34979 batch 2910/3281 lr 0.001 accuracy 93.51953 wps 13156.20 step time 0.47s\n","# Epoch 6  global step 19340 loss 0.34320 batch 2930/3281 lr 0.001 accuracy 93.85156 wps 12253.12 step time 0.58s\n","# Epoch 6  global step 19360 loss 0.33664 batch 2950/3281 lr 0.001 accuracy 93.80859 wps 13021.02 step time 0.54s\n","# Epoch 6  global step 19380 loss 0.34131 batch 2970/3281 lr 0.001 accuracy 93.79297 wps 13032.48 step time 0.54s\n","# Epoch 6  global step 19400 loss 0.35005 batch 2990/3281 lr 0.001 accuracy 93.66797 wps 13212.21 step time 0.51s\n","# Epoch 6  global step 19420 loss 0.33394 batch 3010/3281 lr 0.001 accuracy 94.08203 wps 13398.20 step time 0.45s\n","# Epoch 6  global step 19440 loss 0.36164 batch 3030/3281 lr 0.001 accuracy 93.47656 wps 11638.33 step time 0.75s\n","# Epoch 6  global step 19460 loss 0.33998 batch 3050/3281 lr 0.001 accuracy 93.75000 wps 12698.07 step time 0.62s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7QXcEruuL-Fi","colab_type":"code","outputId":"03c968a9-7296-4d53-9fc2-b1fab9a3e3ff","executionInfo":{"status":"ok","timestamp":1590484317082,"user_tz":-480,"elapsed":87822,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 修改了损失函数\n","!bash bash/elmo_train.sh"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/thrid_utils.py:142: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from scripts/data/train.json ...\n","# Got 105000 data items with 3281 batches\n","# vocab size:  50000\n","# vocab size:  20\n","# Start to preprocessing data...\n","# load data from scripts/data/validation.json ...\n","# Got 15000 data items with 93 batches\n","  saving hparams to scripts/data/elmo_ema_loss_modified/hparams\n","mode=train,data_files=['scripts/data/train.json'],eval_files=['scripts/data/validation.json'],label_file=scripts/data/labels.txt,vocab_file=scripts/data/vocab.txt,embed_file=scripts/data/embedding.txt,out_file=None,split_word=True,max_len=1200,batch_size=32,reverse=False,prob=False,num_layers=3,decay_schema=hand,encoder=elmo,decay_steps=10000,learning_rate=0.001,focal_loss=0.0,embedding_dropout=0.1,max_gradient_norm=5.0,dropout_keep_prob=0.8,weight_keep_drop=0.8,l2_loss_ratio=0.0,rnn_cell_name=lstm,embedding_size=300,num_units=300,double_decoder=False,variational_dropout=True,target_label_num=4,feature_num=20,need_early_stop=True,patient=5,debug=False,num_train_epoch=50,steps_per_stats=20,steps_per_summary=50,steps_per_eval=2000,checkpoint_dir=scripts/data/elmo_ema_loss_modified,vocab_size=50000\n","# Start to load pretrained embedding...\n","# vocab size:  50000\n","# pretrained embedding size 43898 300\n","build elmo encoder\n","Traceback (most recent call last):\n","  File \"main.py\", line 279, in <module>\n","    train_clf(flags)\n","  File \"main.py\", line 185, in train_clf\n","    train_model.build()\n","  File \"/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py\", line 37, in build\n","    self.setup_loss()\n","  File \"/content/gdrive/My Drive/Colab Notebooks/nlp_task/fine_gained/fsauor2018/model.py\", line 292, in setup_loss\n","    for i in range(self.feature_num):\n","AttributeError: 'Model' object has no attribute 'feature_num'\n"],"name":"stdout"}]}]}
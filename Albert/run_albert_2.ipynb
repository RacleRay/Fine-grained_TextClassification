{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_albert_large.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO/nKISvey++/8TUndb1my2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"73419adc744e40eeb89124f847c1d64b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dc4b468a1c3341b7b496e2a14b80eabd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f4e66e8723af4ccca4a98dadab7e7679","IPY_MODEL_1162540c3b044cc29fe0dd94906084c9"]}},"dc4b468a1c3341b7b496e2a14b80eabd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f4e66e8723af4ccca4a98dadab7e7679":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4357307200fd4d179412382b2c107163","_dom_classes":[],"description":"Epoch:   0%","_model_name":"FloatProgressModel","bar_style":"","max":10,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6753d3928dde4d89a0d28d7bb2c574c8"}},"1162540c3b044cc29fe0dd94906084c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9969732cddc3431a9f3b8a215a354b40","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/10 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_21917be9f7cf46019477414549f94b78"}},"4357307200fd4d179412382b2c107163":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6753d3928dde4d89a0d28d7bb2c574c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9969732cddc3431a9f3b8a215a354b40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"21917be9f7cf46019477414549f94b78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b566c18f0824525a9561f76e656d9d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0e3c1dd2b9374cba8d19ccd057c82eb2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dd88e0a71e264dd38705a2cda10f3d98","IPY_MODEL_310541f447d440b1885f8d6269d1090b"]}},"0e3c1dd2b9374cba8d19ccd057c82eb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dd88e0a71e264dd38705a2cda10f3d98":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f0f536feaa30486ab3e006fe9de5b119","_dom_classes":[],"description":"Current iteration:   0%","_model_name":"FloatProgressModel","bar_style":"","max":26250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_885ec2e730ac4d5b9368410ed58bc50a"}},"310541f447d440b1885f8d6269d1090b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b8ce92f0346349c4a9bf01b6ecdb31ab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4/26250 [00:05&lt;10:03:01,  1.38s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_49f53e66160843ca90622e4deb660b1e"}},"f0f536feaa30486ab3e006fe9de5b119":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"885ec2e730ac4d5b9368410ed58bc50a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8ce92f0346349c4a9bf01b6ecdb31ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"49f53e66160843ca90622e4deb660b1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6fb4947024d745698fd18b64db044d09":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_73b8f704e537436eae0abb8ad13af484","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_854c974b8aee4592952f9c52d1aafba0","IPY_MODEL_74e676ed00da4e53a26215a2dfad4808"]}},"d43be0dc3b734d9b95cabc0d6107806b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8dcaeb14be2b4731ab8223e48cce377d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3bcedf94ed62480a8f3a9c2079e3ed70","IPY_MODEL_20a9207282a54c84bbd14ecffa96da99"]}},"8dcaeb14be2b4731ab8223e48cce377d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3bcedf94ed62480a8f3a9c2079e3ed70":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c402f35719654dd796435d017ee14d02","_dom_classes":[],"description":"Epoch: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":10,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b108491410db43ff821e80d78ab091ee"}},"20a9207282a54c84bbd14ecffa96da99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4168a809532a40aeb8392141ab62cf6b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10/10 [49:45&lt;00:00, 298.55s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ad14617861944755afef2acd6bf08b2e"}},"c402f35719654dd796435d017ee14d02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b108491410db43ff821e80d78ab091ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4168a809532a40aeb8392141ab62cf6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ad14617861944755afef2acd6bf08b2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"56784ec6e0f84a87be9be438714db01d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cb6a04e4f10b48a1ac066017f35929bb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6ed625e2bc654f20b89cbbd2b9be5081","IPY_MODEL_863d84756a29484b8bf7042ccfa6db65"]}},"cb6a04e4f10b48a1ac066017f35929bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6ed625e2bc654f20b89cbbd2b9be5081":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2909509ef5c445cd8573c2a0dc409e0e","_dom_classes":[],"description":"Current iteration: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":13125,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":13125,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b36fe46338c14f3891266a6f88c904bb"}},"863d84756a29484b8bf7042ccfa6db65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6e2a23a5380c4bf8b42c2dbdfb4c5c8e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 13125/13125 [14:53&lt;00:00, 14.69it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d8c3c47e5d54bc5817f27e72a4530c1"}},"2909509ef5c445cd8573c2a0dc409e0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b36fe46338c14f3891266a6f88c904bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6e2a23a5380c4bf8b42c2dbdfb4c5c8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8d8c3c47e5d54bc5817f27e72a4530c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a253b926749a4913afda1843bea393a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8a48236a488242dfbdaabbbfa1db4983","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_decdb966601449b99e088ce82b36c8cb","IPY_MODEL_f8259b4d944345838410227b6d53e4e2"]}},"8a48236a488242dfbdaabbbfa1db4983":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"decdb966601449b99e088ce82b36c8cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f432d75277f04d5fb78e92c0c865990a","_dom_classes":[],"description":"Current iteration: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":13125,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":13125,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5fcea34570c84ed69f45544cb2a123ca"}},"f8259b4d944345838410227b6d53e4e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6c226ae13c9344bc876c9331bb26b6e3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 13125/13125 [20:05&lt;00:00, 10.88it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_25983166e4834416b44a709a70bca8b5"}},"f432d75277f04d5fb78e92c0c865990a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5fcea34570c84ed69f45544cb2a123ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c226ae13c9344bc876c9331bb26b6e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"25983166e4834416b44a709a70bca8b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d50e8d2d82274a84a185fadf012d8c72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3a0490291d5b4c8d84ef516e293bcf3e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_718fbcf1d43948ddbc9d66b7db3adb3a","IPY_MODEL_3ef8709a8123448bb2c078aa3b8b4744"]}},"3a0490291d5b4c8d84ef516e293bcf3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"718fbcf1d43948ddbc9d66b7db3adb3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d0ae7fc656344dcaaa8e7f8b7f2a7c22","_dom_classes":[],"description":"Current iteration: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":13125,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":13125,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5cbccabd002e42b1b0e845a06c26d1e7"}},"3ef8709a8123448bb2c078aa3b8b4744":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bdf2437125f34c8f8b0fbf59b7439ed0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 13125/13125 [16:35&lt;00:00, 13.18it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d54a74c1eaf4a98b5c0229714e5b87c"}},"d0ae7fc656344dcaaa8e7f8b7f2a7c22":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5cbccabd002e42b1b0e845a06c26d1e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bdf2437125f34c8f8b0fbf59b7439ed0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8d54a74c1eaf4a98b5c0229714e5b87c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4603feedccce4105b03df354b92d2371":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a5b2aae5ec644bd08e4e82f5b55f542f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b2d3f8d8c8bb444f8d4c950593b0ab3e","IPY_MODEL_4a34c535255d49a2856438b762219a4d"]}},"a5b2aae5ec644bd08e4e82f5b55f542f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b2d3f8d8c8bb444f8d4c950593b0ab3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_72fb607d8d374acb9127d965f829e232","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1875,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1875,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1654a4fca92c47ddbfeb1e8a666a0bfe"}},"4a34c535255d49a2856438b762219a4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a936d2c288f1474ca64ee00792db3dff","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1875/1875 [01:01&lt;00:00, 30.45it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e1af80ed60fd4d88ab4c593e6e645ad7"}},"72fb607d8d374acb9127d965f829e232":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1654a4fca92c47ddbfeb1e8a666a0bfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a936d2c288f1474ca64ee00792db3dff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e1af80ed60fd4d88ab4c593e6e645ad7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"68782099c22a4702ba62ca0b9822a8d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_80de9f7a7286446e9be88cfe77185b19","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_00bc04ea1cb94372b500e8301f663c6d","IPY_MODEL_d262067fe18748758fbec5efd17865e5"]}},"80de9f7a7286446e9be88cfe77185b19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"00bc04ea1cb94372b500e8301f663c6d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_da21235454f443fa971b6162d0f7ead3","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1875,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1875,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_291345ca98194e06a7142d7752e95a0a"}},"d262067fe18748758fbec5efd17865e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0d9a1d4d2a434e5381fd07c37cc836a4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1875/1875 [05:57&lt;00:00,  5.24it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c841b293d16b495596d03da29183c65d"}},"da21235454f443fa971b6162d0f7ead3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"291345ca98194e06a7142d7752e95a0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0d9a1d4d2a434e5381fd07c37cc836a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c841b293d16b495596d03da29183c65d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"JKdi1gPSi4M7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1593608837859,"user_tz":-480,"elapsed":28923,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"f1422bcb-36c8-4196-8858-dbf5cc774d36"},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RyhkSJ4xi-KB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"ok","timestamp":1593615972289,"user_tz":-480,"elapsed":33,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"feac7335-0cca-4901-d57f-86a4414bf605"},"source":["% cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/nlp_task/bert_cla\n","! pwd\n","! ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/nlp_task/bert_cla\n","/content/gdrive/My Drive/Colab Notebooks/nlp_task/bert_cla\n","albert_large\t  main.py\t\t  run_multi_label_linear.py\n","albert_tiny\t  models\t\t  runs\n","apex\t\t  outputs\t\t  train.tsv\n","base_runner.py\t  outputs_tiny\t\t  val.tsv\n","base_utils.py\t  __pycache__\t\t  wandb\n","cache_dir\t  run_albert_large.ipynb  xbert_cnn_runner.py\n","global_config.py  run_albert_tiny.ipynb   xbert_multi_label_linear_runner.py\n","__init__.py\t  run_multi_class_cnn.py  xbert_rnn_runner.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tN_yGlKijAeR","colab_type":"code","colab":{}},"source":["!pip install wandb tensorboardX transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWDFH0-wjOcC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593610006551,"user_tz":-480,"elapsed":602516,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"b5d969ea-ef5a-4975-81fc-84f606857890"},"source":["# !git clone https://github.com/NVIDIA/apex\n","%cd apex\n","!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n","%cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/nlp_task/bert_cla/apex\n","/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n","  cmdoptions.check_install_build_global(options)\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-_6uxp271\n","Created temporary directory: /tmp/pip-req-tracker-8zf30mlr\n","Created requirements tracker '/tmp/pip-req-tracker-8zf30mlr'\n","Created temporary directory: /tmp/pip-install-33er889z\n","Processing /content/gdrive/My Drive/Colab Notebooks/nlp_task/bert_cla/apex\n","  Created temporary directory: /tmp/pip-req-build-e6v_9n4s\n","  Added file:///content/gdrive/My%20Drive/Colab%20Notebooks/nlp_task/bert_cla/apex to build tracker '/tmp/pip-req-tracker-8zf30mlr'\n","    Running setup.py (path:/tmp/pip-req-build-e6v_9n4s/setup.py) egg_info for package from file:///content/gdrive/My%20Drive/Colab%20Notebooks/nlp_task/bert_cla/apex\n","    Running command python setup.py egg_info\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    running egg_info\n","    creating /tmp/pip-req-build-e6v_9n4s/pip-egg-info/apex.egg-info\n","    writing /tmp/pip-req-build-e6v_9n4s/pip-egg-info/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-req-build-e6v_9n4s/pip-egg-info/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-req-build-e6v_9n4s/pip-egg-info/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-req-build-e6v_9n4s/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    writing manifest file '/tmp/pip-req-build-e6v_9n4s/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-e6v_9n4s/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-e6v_9n4s has version 0.1, which satisfies requirement apex==0.1 from file:///content/gdrive/My%20Drive/Colab%20Notebooks/nlp_task/bert_cla/apex\n","  Removed apex==0.1 from file:///content/gdrive/My%20Drive/Colab%20Notebooks/nlp_task/bert_cla/apex from build tracker '/tmp/pip-req-tracker-8zf30mlr'\n","Skipping wheel build for apex, due to binaries being disabled for it.\n","Installing collected packages: apex\n","  Created temporary directory: /tmp/pip-record-vzvfqawt\n","    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-e6v_9n4s/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-e6v_9n4s/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-vzvfqawt/install-record.txt --single-version-externally-managed --compile\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    /tmp/pip-req-build-e6v_9n4s/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","\n","    Compiling cuda extensions with\n","    nvcc: NVIDIA (R) Cuda compiler driver\n","    Copyright (c) 2005-2019 NVIDIA Corporation\n","    Built on Sun_Jul_28_19:07:16_PDT_2019\n","    Cuda compilation tools, release 10.1, V10.1.243\n","    from /usr/local/cuda/bin\n","\n","    running install\n","    running build\n","    running build_py\n","    creating build\n","    creating build/lib.linux-x86_64-3.6\n","    creating build/lib.linux-x86_64-3.6/apex\n","    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n","    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    creating build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof\n","    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n","    creating build/lib.linux-x86_64-3.6/apex/contrib\n","    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n","    creating build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    creating build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    creating build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    creating build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    creating build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    running build_ext\n","    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:305: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","      warnings.warn(msg.format('we could not find ninja.'))\n","    building 'apex_C' extension\n","    creating build/temp.linux-x86_64-3.6\n","    creating build/temp.linux-x86_64-3.6/csrc\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from csrc/flatten_unflatten.cpp:2:0:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         return tensors[0].type();\n","                                ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/flatten_unflatten.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'amp_C' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'syncbn' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n","    building 'fused_layer_norm_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n","    building 'mlp_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n","                                                                                 ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                        ^\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < inputs.size(); i++) {\n","                       ~~^~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n","    running install_lib\n","    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    creating /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n","    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n","    running install_egg_info\n","    running egg_info\n","    creating apex.egg-info\n","    writing apex.egg-info/PKG-INFO\n","    writing dependency_links to apex.egg-info/dependency_links.txt\n","    writing top-level names to apex.egg-info/top_level.txt\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n","    running install_scripts\n","    writing list of installed files to '/tmp/pip-record-vzvfqawt/install-record.txt'\n","    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n","  Removing source in /tmp/pip-req-build-e6v_9n4s\n","Successfully installed apex-0.1\n","Cleaning up...\n","Removed build tracker '/tmp/pip-req-tracker-8zf30mlr'\n","/content/gdrive/My Drive/Colab Notebooks/nlp_task/bert_cla\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g8jXq1dKrdBv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"ok","timestamp":1593616145679,"user_tz":-480,"elapsed":9825,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"ac22cbef-3315-40c2-fe21-2e3f7d8cf737"},"source":["!/opt/bin/nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Wed Jul  1 15:08:59 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   49C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cW0NdkPirh7C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593616364132,"user_tz":-480,"elapsed":4455,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"faaa6eb8-1568-45bb-b84e-5950306d5b2e"},"source":["import pandas as pd\n","import numpy as np\n","import sklearn\n","from sklearn.preprocessing import OneHotEncoder\n","from xbert_multi_label_linear_runner import MultiBinaryClaRunner\n","\n","\n","def process_tsv(file):\n","    \"根据数据的储存方式修改\"\n","    data = pd.read_table(file, names=['label', 'text'], encoding='utf-8')\n","    label = data.label.apply(lambda x: x.split('@'))\n","    label = np.array(label.to_list())\n","    enc = OneHotEncoder()\n","    # '0_-1', '0_-2', '0_0', '0_1' => 1000, 0100, 0010, 0001\n","    label_ont_hot = enc.fit_transform(label)\n","    for i, row_label in enumerate(label_ont_hot.toarray()):\n","        data.iloc[i]['label'] = row_label\n","    return data\n","\n","\n","def main():\n","    # fine-tuning 阶段 分类器和LM一起训练\n","    train_df = process_tsv('train.tsv')\n","    eval_df = process_tsv('val.tsv')\n","    model = MultiBinaryClaRunner('albert',\n","                                 './albert_large/',\n","                                 num_labels=80,\n","                                 use_cuda=True,\n","                                 freez_pretrained=False,\n","                                 args={\n","                                     \"max_seq_length\": 512,\n","                                     \"reprocess_input_data\": False,\n","                                     \"overwrite_output_dir\": False,\n","                                     \"num_train_epochs\": 10,\n","                                     \"evaluate_during_training\": False,\n","                                     \"use_cached_eval_features\": True,\n","                                 })\n","    \n","    model.train_model(train_df)\n","\n","    result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n","    print(result)\n","    print(model_outputs)\n","\n","    return result, model_outputs, wrong_predictions"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"uguizfcUulBD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["73419adc744e40eeb89124f847c1d64b","dc4b468a1c3341b7b496e2a14b80eabd","f4e66e8723af4ccca4a98dadab7e7679","1162540c3b044cc29fe0dd94906084c9","4357307200fd4d179412382b2c107163","6753d3928dde4d89a0d28d7bb2c574c8","9969732cddc3431a9f3b8a215a354b40","21917be9f7cf46019477414549f94b78","7b566c18f0824525a9561f76e656d9d5","0e3c1dd2b9374cba8d19ccd057c82eb2","dd88e0a71e264dd38705a2cda10f3d98","310541f447d440b1885f8d6269d1090b","f0f536feaa30486ab3e006fe9de5b119","885ec2e730ac4d5b9368410ed58bc50a","b8ce92f0346349c4a9bf01b6ecdb31ab","49f53e66160843ca90622e4deb660b1e"]},"outputId":"de02f37b-0bde-4575-dd82-7e87db08971c"},"source":["result, model_outputs, wrong_predictions = main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:transformers.configuration_utils:loading configuration file ./albert_large/config.json\n","INFO:transformers.configuration_utils:Model config AlbertConfig {\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\",\n","    \"28\": \"LABEL_28\",\n","    \"29\": \"LABEL_29\",\n","    \"30\": \"LABEL_30\",\n","    \"31\": \"LABEL_31\",\n","    \"32\": \"LABEL_32\",\n","    \"33\": \"LABEL_33\",\n","    \"34\": \"LABEL_34\",\n","    \"35\": \"LABEL_35\",\n","    \"36\": \"LABEL_36\",\n","    \"37\": \"LABEL_37\",\n","    \"38\": \"LABEL_38\",\n","    \"39\": \"LABEL_39\",\n","    \"40\": \"LABEL_40\",\n","    \"41\": \"LABEL_41\",\n","    \"42\": \"LABEL_42\",\n","    \"43\": \"LABEL_43\",\n","    \"44\": \"LABEL_44\",\n","    \"45\": \"LABEL_45\",\n","    \"46\": \"LABEL_46\",\n","    \"47\": \"LABEL_47\",\n","    \"48\": \"LABEL_48\",\n","    \"49\": \"LABEL_49\",\n","    \"50\": \"LABEL_50\",\n","    \"51\": \"LABEL_51\",\n","    \"52\": \"LABEL_52\",\n","    \"53\": \"LABEL_53\",\n","    \"54\": \"LABEL_54\",\n","    \"55\": \"LABEL_55\",\n","    \"56\": \"LABEL_56\",\n","    \"57\": \"LABEL_57\",\n","    \"58\": \"LABEL_58\",\n","    \"59\": \"LABEL_59\",\n","    \"60\": \"LABEL_60\",\n","    \"61\": \"LABEL_61\",\n","    \"62\": \"LABEL_62\",\n","    \"63\": \"LABEL_63\",\n","    \"64\": \"LABEL_64\",\n","    \"65\": \"LABEL_65\",\n","    \"66\": \"LABEL_66\",\n","    \"67\": \"LABEL_67\",\n","    \"68\": \"LABEL_68\",\n","    \"69\": \"LABEL_69\",\n","    \"70\": \"LABEL_70\",\n","    \"71\": \"LABEL_71\",\n","    \"72\": \"LABEL_72\",\n","    \"73\": \"LABEL_73\",\n","    \"74\": \"LABEL_74\",\n","    \"75\": \"LABEL_75\",\n","    \"76\": \"LABEL_76\",\n","    \"77\": \"LABEL_77\",\n","    \"78\": \"LABEL_78\",\n","    \"79\": \"LABEL_79\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_28\": 28,\n","    \"LABEL_29\": 29,\n","    \"LABEL_3\": 3,\n","    \"LABEL_30\": 30,\n","    \"LABEL_31\": 31,\n","    \"LABEL_32\": 32,\n","    \"LABEL_33\": 33,\n","    \"LABEL_34\": 34,\n","    \"LABEL_35\": 35,\n","    \"LABEL_36\": 36,\n","    \"LABEL_37\": 37,\n","    \"LABEL_38\": 38,\n","    \"LABEL_39\": 39,\n","    \"LABEL_4\": 4,\n","    \"LABEL_40\": 40,\n","    \"LABEL_41\": 41,\n","    \"LABEL_42\": 42,\n","    \"LABEL_43\": 43,\n","    \"LABEL_44\": 44,\n","    \"LABEL_45\": 45,\n","    \"LABEL_46\": 46,\n","    \"LABEL_47\": 47,\n","    \"LABEL_48\": 48,\n","    \"LABEL_49\": 49,\n","    \"LABEL_5\": 5,\n","    \"LABEL_50\": 50,\n","    \"LABEL_51\": 51,\n","    \"LABEL_52\": 52,\n","    \"LABEL_53\": 53,\n","    \"LABEL_54\": 54,\n","    \"LABEL_55\": 55,\n","    \"LABEL_56\": 56,\n","    \"LABEL_57\": 57,\n","    \"LABEL_58\": 58,\n","    \"LABEL_59\": 59,\n","    \"LABEL_6\": 6,\n","    \"LABEL_60\": 60,\n","    \"LABEL_61\": 61,\n","    \"LABEL_62\": 62,\n","    \"LABEL_63\": 63,\n","    \"LABEL_64\": 64,\n","    \"LABEL_65\": 65,\n","    \"LABEL_66\": 66,\n","    \"LABEL_67\": 67,\n","    \"LABEL_68\": 68,\n","    \"LABEL_69\": 69,\n","    \"LABEL_7\": 7,\n","    \"LABEL_70\": 70,\n","    \"LABEL_71\": 71,\n","    \"LABEL_72\": 72,\n","    \"LABEL_73\": 73,\n","    \"LABEL_74\": 74,\n","    \"LABEL_75\": 75,\n","    \"LABEL_76\": 76,\n","    \"LABEL_77\": 77,\n","    \"LABEL_78\": 78,\n","    \"LABEL_79\": 79,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"ln_type\": \"postln\",\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"share_type\": \"all\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 21128\n","}\n","\n","INFO:transformers.modeling_utils:loading weights file ./albert_large/pytorch_model.bin\n","WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./albert_large/ were not used when initializing AlbertForMultiBinaryLabelSeqClassification: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.word_embeddings_2.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer_shared.attention.self.query.weight', 'bert.encoder.layer_shared.attention.self.query.bias', 'bert.encoder.layer_shared.attention.self.key.weight', 'bert.encoder.layer_shared.attention.self.key.bias', 'bert.encoder.layer_shared.attention.self.value.weight', 'bert.encoder.layer_shared.attention.self.value.bias', 'bert.encoder.layer_shared.attention.output.dense.weight', 'bert.encoder.layer_shared.attention.output.dense.bias', 'bert.encoder.layer_shared.attention.output.LayerNorm.weight', 'bert.encoder.layer_shared.attention.output.LayerNorm.bias', 'bert.encoder.layer_shared.intermediate.dense.weight', 'bert.encoder.layer_shared.intermediate.dense.bias', 'bert.encoder.layer_shared.output.dense.weight', 'bert.encoder.layer_shared.output.dense.bias', 'bert.encoder.layer_shared.output.LayerNorm.weight', 'bert.encoder.layer_shared.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.project_layer.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing AlbertForMultiBinaryLabelSeqClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing AlbertForMultiBinaryLabelSeqClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","WARNING:transformers.modeling_utils:Some weights of AlbertForMultiBinaryLabelSeqClassification were not initialized from the model checkpoint at ./albert_large/ and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.embedding_hidden_mapping_in.weight', 'encoder.embedding_hidden_mapping_in.bias', 'encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'encoder.albert_layer_groups.0.albert_layers.0.ffn.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'pooler.weight', 'pooler.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","INFO:transformers.tokenization_utils_base:Model name './albert_large/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './albert_large/' is a path, a model identifier, or url to a directory containing tokenizer files.\n","INFO:transformers.tokenization_utils_base:Didn't find file ./albert_large/added_tokens.json. We won't load it.\n","INFO:transformers.tokenization_utils_base:Didn't find file ./albert_large/special_tokens_map.json. We won't load it.\n","INFO:transformers.tokenization_utils_base:Didn't find file ./albert_large/tokenizer_config.json. We won't load it.\n","INFO:transformers.tokenization_utils_base:Didn't find file ./albert_large/tokenizer.json. We won't load it.\n","INFO:transformers.tokenization_utils_base:loading file ./albert_large/vocab.txt\n","INFO:transformers.tokenization_utils_base:loading file None\n","INFO:transformers.tokenization_utils_base:loading file None\n","INFO:transformers.tokenization_utils_base:loading file None\n","INFO:transformers.tokenization_utils_base:loading file None\n","INFO:base_runner:>>> 加载缓存的特征文件 cache_dir/cached_train_albert_512_80_105000\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73419adc744e40eeb89124f847c1d64b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:base_runner:>>> 开始 fine-tuning.\n"],"name":"stderr"},{"output_type":"stream","text":["Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b566c18f0824525a9561f76e656d9d5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=26250.0, style=ProgressStyle(desc…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rRunning loss: 0.691955"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eejHJ00d2mzK","colab_type":"code","colab":{}},"source":["def main_2():\n","    # 只训练分类器部分\n","    train_df = process_tsv('train.tsv')\n","    eval_df = process_tsv('val.tsv')\n","    model = MultiBinaryClaRunner('albert',\n","                                 './outputs/checkpoint-4000',\n","                                 num_labels=80,\n","                                 use_cuda=True,\n","                                 freez_pretrained=True,\n","                                 args={\n","                                     \"output_dir\": './outputs/',\n","                                     \"max_seq_length\": 512,\n","                                     \"reprocess_input_data\": False,\n","                                     \"overwrite_output_dir\": True,\n","                                     \"num_train_epochs\": 10,\n","                                     \"evaluate_during_training\": True,\n","                                     \"evaluate_during_training_steps\": 1000,\n","                                     \"use_cached_eval_features\": True,\n","                                     \"save_steps\": 1000,\n","                                 })\n","    \n","    model.train_model(train_df, eval_df=eval_df)\n","\n","    result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n","    print(result)\n","    print(model_outputs)\n","\n","    return result, model_outputs, wrong_predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LybF7emKO93I","colab_type":"code","colab":{}},"source":["result, model_outputs, wrong_predictions = main_2()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XUNXxMD34GXm","colab_type":"text"},"source":["albert_large训练太慢，前6000步和tiny效果差别不大"]},{"cell_type":"code","metadata":{"id":"KDY4GGL93nZZ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMJGf_1o3nyY","colab_type":"code","colab":{}},"source":["def main_tiny():\n","    train_df = process_tsv('train.tsv')\n","    eval_df = process_tsv('val.tsv')\n","    model = MultiBinaryClaRunner('albert',\n","                                 './outputs_tiny/checkpoint-16000',\n","                                 num_labels=80,\n","                                 use_cuda=True,\n","                                 freez_pretrained=False,\n","                                 args={\n","                                     \"train_batch_size\": 8,\n","                                     \"output_dir\": './outputs_tiny',\n","                                     \"max_seq_length\": 512,\n","                                     \"reprocess_input_data\": False,\n","                                     \"overwrite_output_dir\": True,\n","                                     \"num_train_epochs\": 10,\n","                                     \"evaluate_during_training\": True,\n","                                     \"evaluate_during_training_steps\": 500,\n","                                     \"use_cached_eval_features\": True,\n","                                     \"save_steps\": 1000,\n","                                 })\n","    \n","    model.train_model(train_df, eval_df=eval_df)\n","\n","    result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n","    print(result)\n","    print(model_outputs)\n","\n","    return result, model_outputs, wrong_predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C20Q3Di439jT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["6fb4947024d745698fd18b64db044d09","73b8f704e537436eae0abb8ad13af484","854c974b8aee4592952f9c52d1aafba0","74e676ed00da4e53a26215a2dfad4808","079e3f3f5515461cab228abadfbc16df","4a16f82056754bffa2130d9159725a97","e60845b3ca1d43748b328b465a430a4f","ca7e2e4e82f44438b4461ec2549a9448","f8cf9ab6b0bd4e869bea0afbdd8c3cf9"]},"executionInfo":{"status":"ok","timestamp":1593615787010,"user_tz":-480,"elapsed":45575,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"cdf317f7-686e-4012-ffe6-d52fb4f92d17"},"source":["result, model_outputs, wrong_predictions = main_tiny()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["全局设置： {'sliding_window': False, 'tie_value': -1, 'stride': 0.8, 'regression': False, 'threshold': 0.5, 'best_model_dir': 'outputs/best_model', 'output_dir': './outputs_tiny', 'overwrite_output_dir': True, 'tensorboard_dir': None, 'cache_dir': 'cache_dir/', 'config': {}, 'do_lower_case': False, 'encoding': None, 'train_batch_size': 8, 'num_train_epochs': 10, 'learning_rate': 4e-05, 'adam_epsilon': 1e-06, 'weight_decay': 0, 'warmup_ratio': 0, 'warmup_steps': 1000, 'max_grad_norm': 1.0, 'max_seq_length': 512, 'n_gpu': 1, 'fp16': True, 'fp16_opt_level': 'O1', 'gradient_accumulation_steps': 3, 'eval_batch_size': 8, 'evaluate_during_training': True, 'evaluate_during_training_steps': 500, 'evaluate_during_training_verbose': False, 'use_early_stopping': True, 'early_stopping_consider_epochs': False, 'early_stopping_delta': 0, 'early_stopping_metric': 'eval_loss', 'early_stopping_metric_minimize': True, 'early_stopping_patience': 3, 'manual_seed': 1234, 'save_best_model': True, 'save_eval_checkpoints': True, 'save_model_every_epoch': True, 'save_steps': 1000, 'silent': False, 'logging_steps': 50, 'no_cache': False, 'no_save': False, 'use_cached_eval_features': True, 'use_multiprocessing': True, 'process_count': 1, 'multiprocessing_chunksize': 500, 'reprocess_input_data': False, 'wandb_project': None, 'wandb_kwargs': {}, 'model_file': './outputs_tiny/checkpoint-16000', 'model_type': 'albert'}\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:loading configuration file ./outputs_tiny/checkpoint-16000/config.json\n","INFO:transformers.configuration_utils:Model config AlbertConfig {\n","  \"architectures\": [\n","    \"AlbertForMultiBinaryLabelSeqClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 312,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\",\n","    \"28\": \"LABEL_28\",\n","    \"29\": \"LABEL_29\",\n","    \"30\": \"LABEL_30\",\n","    \"31\": \"LABEL_31\",\n","    \"32\": \"LABEL_32\",\n","    \"33\": \"LABEL_33\",\n","    \"34\": \"LABEL_34\",\n","    \"35\": \"LABEL_35\",\n","    \"36\": \"LABEL_36\",\n","    \"37\": \"LABEL_37\",\n","    \"38\": \"LABEL_38\",\n","    \"39\": \"LABEL_39\",\n","    \"40\": \"LABEL_40\",\n","    \"41\": \"LABEL_41\",\n","    \"42\": \"LABEL_42\",\n","    \"43\": \"LABEL_43\",\n","    \"44\": \"LABEL_44\",\n","    \"45\": \"LABEL_45\",\n","    \"46\": \"LABEL_46\",\n","    \"47\": \"LABEL_47\",\n","    \"48\": \"LABEL_48\",\n","    \"49\": \"LABEL_49\",\n","    \"50\": \"LABEL_50\",\n","    \"51\": \"LABEL_51\",\n","    \"52\": \"LABEL_52\",\n","    \"53\": \"LABEL_53\",\n","    \"54\": \"LABEL_54\",\n","    \"55\": \"LABEL_55\",\n","    \"56\": \"LABEL_56\",\n","    \"57\": \"LABEL_57\",\n","    \"58\": \"LABEL_58\",\n","    \"59\": \"LABEL_59\",\n","    \"60\": \"LABEL_60\",\n","    \"61\": \"LABEL_61\",\n","    \"62\": \"LABEL_62\",\n","    \"63\": \"LABEL_63\",\n","    \"64\": \"LABEL_64\",\n","    \"65\": \"LABEL_65\",\n","    \"66\": \"LABEL_66\",\n","    \"67\": \"LABEL_67\",\n","    \"68\": \"LABEL_68\",\n","    \"69\": \"LABEL_69\",\n","    \"70\": \"LABEL_70\",\n","    \"71\": \"LABEL_71\",\n","    \"72\": \"LABEL_72\",\n","    \"73\": \"LABEL_73\",\n","    \"74\": \"LABEL_74\",\n","    \"75\": \"LABEL_75\",\n","    \"76\": \"LABEL_76\",\n","    \"77\": \"LABEL_77\",\n","    \"78\": \"LABEL_78\",\n","    \"79\": \"LABEL_79\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 1248,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_28\": 28,\n","    \"LABEL_29\": 29,\n","    \"LABEL_3\": 3,\n","    \"LABEL_30\": 30,\n","    \"LABEL_31\": 31,\n","    \"LABEL_32\": 32,\n","    \"LABEL_33\": 33,\n","    \"LABEL_34\": 34,\n","    \"LABEL_35\": 35,\n","    \"LABEL_36\": 36,\n","    \"LABEL_37\": 37,\n","    \"LABEL_38\": 38,\n","    \"LABEL_39\": 39,\n","    \"LABEL_4\": 4,\n","    \"LABEL_40\": 40,\n","    \"LABEL_41\": 41,\n","    \"LABEL_42\": 42,\n","    \"LABEL_43\": 43,\n","    \"LABEL_44\": 44,\n","    \"LABEL_45\": 45,\n","    \"LABEL_46\": 46,\n","    \"LABEL_47\": 47,\n","    \"LABEL_48\": 48,\n","    \"LABEL_49\": 49,\n","    \"LABEL_5\": 5,\n","    \"LABEL_50\": 50,\n","    \"LABEL_51\": 51,\n","    \"LABEL_52\": 52,\n","    \"LABEL_53\": 53,\n","    \"LABEL_54\": 54,\n","    \"LABEL_55\": 55,\n","    \"LABEL_56\": 56,\n","    \"LABEL_57\": 57,\n","    \"LABEL_58\": 58,\n","    \"LABEL_59\": 59,\n","    \"LABEL_6\": 6,\n","    \"LABEL_60\": 60,\n","    \"LABEL_61\": 61,\n","    \"LABEL_62\": 62,\n","    \"LABEL_63\": 63,\n","    \"LABEL_64\": 64,\n","    \"LABEL_65\": 65,\n","    \"LABEL_66\": 66,\n","    \"LABEL_67\": 67,\n","    \"LABEL_68\": 68,\n","    \"LABEL_69\": 69,\n","    \"LABEL_7\": 7,\n","    \"LABEL_70\": 70,\n","    \"LABEL_71\": 71,\n","    \"LABEL_72\": 72,\n","    \"LABEL_73\": 73,\n","    \"LABEL_74\": 74,\n","    \"LABEL_75\": 75,\n","    \"LABEL_76\": 76,\n","    \"LABEL_77\": 77,\n","    \"LABEL_78\": 78,\n","    \"LABEL_79\": 79,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"ln_type\": \"postln\",\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"share_type\": \"all\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 21128\n","}\n","\n","INFO:transformers.modeling_utils:loading weights file ./outputs_tiny/checkpoint-16000/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["模型设置： AlbertConfig {\n","  \"architectures\": [\n","    \"AlbertForMultiBinaryLabelSeqClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 312,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\",\n","    \"28\": \"LABEL_28\",\n","    \"29\": \"LABEL_29\",\n","    \"30\": \"LABEL_30\",\n","    \"31\": \"LABEL_31\",\n","    \"32\": \"LABEL_32\",\n","    \"33\": \"LABEL_33\",\n","    \"34\": \"LABEL_34\",\n","    \"35\": \"LABEL_35\",\n","    \"36\": \"LABEL_36\",\n","    \"37\": \"LABEL_37\",\n","    \"38\": \"LABEL_38\",\n","    \"39\": \"LABEL_39\",\n","    \"40\": \"LABEL_40\",\n","    \"41\": \"LABEL_41\",\n","    \"42\": \"LABEL_42\",\n","    \"43\": \"LABEL_43\",\n","    \"44\": \"LABEL_44\",\n","    \"45\": \"LABEL_45\",\n","    \"46\": \"LABEL_46\",\n","    \"47\": \"LABEL_47\",\n","    \"48\": \"LABEL_48\",\n","    \"49\": \"LABEL_49\",\n","    \"50\": \"LABEL_50\",\n","    \"51\": \"LABEL_51\",\n","    \"52\": \"LABEL_52\",\n","    \"53\": \"LABEL_53\",\n","    \"54\": \"LABEL_54\",\n","    \"55\": \"LABEL_55\",\n","    \"56\": \"LABEL_56\",\n","    \"57\": \"LABEL_57\",\n","    \"58\": \"LABEL_58\",\n","    \"59\": \"LABEL_59\",\n","    \"60\": \"LABEL_60\",\n","    \"61\": \"LABEL_61\",\n","    \"62\": \"LABEL_62\",\n","    \"63\": \"LABEL_63\",\n","    \"64\": \"LABEL_64\",\n","    \"65\": \"LABEL_65\",\n","    \"66\": \"LABEL_66\",\n","    \"67\": \"LABEL_67\",\n","    \"68\": \"LABEL_68\",\n","    \"69\": \"LABEL_69\",\n","    \"70\": \"LABEL_70\",\n","    \"71\": \"LABEL_71\",\n","    \"72\": \"LABEL_72\",\n","    \"73\": \"LABEL_73\",\n","    \"74\": \"LABEL_74\",\n","    \"75\": \"LABEL_75\",\n","    \"76\": \"LABEL_76\",\n","    \"77\": \"LABEL_77\",\n","    \"78\": \"LABEL_78\",\n","    \"79\": \"LABEL_79\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 1248,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_28\": 28,\n","    \"LABEL_29\": 29,\n","    \"LABEL_3\": 3,\n","    \"LABEL_30\": 30,\n","    \"LABEL_31\": 31,\n","    \"LABEL_32\": 32,\n","    \"LABEL_33\": 33,\n","    \"LABEL_34\": 34,\n","    \"LABEL_35\": 35,\n","    \"LABEL_36\": 36,\n","    \"LABEL_37\": 37,\n","    \"LABEL_38\": 38,\n","    \"LABEL_39\": 39,\n","    \"LABEL_4\": 4,\n","    \"LABEL_40\": 40,\n","    \"LABEL_41\": 41,\n","    \"LABEL_42\": 42,\n","    \"LABEL_43\": 43,\n","    \"LABEL_44\": 44,\n","    \"LABEL_45\": 45,\n","    \"LABEL_46\": 46,\n","    \"LABEL_47\": 47,\n","    \"LABEL_48\": 48,\n","    \"LABEL_49\": 49,\n","    \"LABEL_5\": 5,\n","    \"LABEL_50\": 50,\n","    \"LABEL_51\": 51,\n","    \"LABEL_52\": 52,\n","    \"LABEL_53\": 53,\n","    \"LABEL_54\": 54,\n","    \"LABEL_55\": 55,\n","    \"LABEL_56\": 56,\n","    \"LABEL_57\": 57,\n","    \"LABEL_58\": 58,\n","    \"LABEL_59\": 59,\n","    \"LABEL_6\": 6,\n","    \"LABEL_60\": 60,\n","    \"LABEL_61\": 61,\n","    \"LABEL_62\": 62,\n","    \"LABEL_63\": 63,\n","    \"LABEL_64\": 64,\n","    \"LABEL_65\": 65,\n","    \"LABEL_66\": 66,\n","    \"LABEL_67\": 67,\n","    \"LABEL_68\": 68,\n","    \"LABEL_69\": 69,\n","    \"LABEL_7\": 7,\n","    \"LABEL_70\": 70,\n","    \"LABEL_71\": 71,\n","    \"LABEL_72\": 72,\n","    \"LABEL_73\": 73,\n","    \"LABEL_74\": 74,\n","    \"LABEL_75\": 75,\n","    \"LABEL_76\": 76,\n","    \"LABEL_77\": 77,\n","    \"LABEL_78\": 78,\n","    \"LABEL_79\": 79,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"ln_type\": \"postln\",\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"share_type\": \"all\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 21128\n","}\n","\n","Freeze参数： False\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing AlbertForMultiBinaryLabelSeqClassification.\n","\n","INFO:transformers.modeling_utils:All the weights of AlbertForMultiBinaryLabelSeqClassification were initialized from the model checkpoint at ./outputs_tiny/checkpoint-16000.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use AlbertForMultiBinaryLabelSeqClassification for predictions without further training.\n","INFO:transformers.tokenization_utils_base:Model name './outputs_tiny/checkpoint-16000' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './outputs_tiny/checkpoint-16000' is a path, a model identifier, or url to a directory containing tokenizer files.\n","INFO:transformers.tokenization_utils_base:Didn't find file ./outputs_tiny/checkpoint-16000/added_tokens.json. We won't load it.\n","INFO:transformers.tokenization_utils_base:Didn't find file ./outputs_tiny/checkpoint-16000/tokenizer.json. We won't load it.\n","INFO:transformers.tokenization_utils_base:loading file ./outputs_tiny/checkpoint-16000/vocab.txt\n","INFO:transformers.tokenization_utils_base:loading file None\n","INFO:transformers.tokenization_utils_base:loading file ./outputs_tiny/checkpoint-16000/special_tokens_map.json\n","INFO:transformers.tokenization_utils_base:loading file ./outputs_tiny/checkpoint-16000/tokenizer_config.json\n","INFO:transformers.tokenization_utils_base:loading file None\n","INFO:base_runner:>>> 加载缓存的特征文件 cache_dir/cached_train_albert_512_80_105000\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6fb4947024d745698fd18b64db044d09","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:base_runner:>>> 加载checkpoint，更新global_step.\n","INFO:base_runner:>>> 已训练 3 轮.\n","INFO:base_runner:>>> 当前global_step： 16000.\n","INFO:base_runner:>>> 跳过当前epoch 2875 steps in.\n"],"name":"stderr"},{"output_type":"stream","text":["Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"079e3f3f5515461cab228abadfbc16df","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=13125.0, style=ProgressStyle(desc…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.280779"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.323375"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-16500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-16500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.280225"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.278429"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-17000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-17000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-17000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-17000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 2\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.269067"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.225281"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-17500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-17500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 3\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.257169"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.305454"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-18000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-18000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-18000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-18000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.216124"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.226189"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-18500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-18500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.228854"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.254468"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-19000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-19000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-19000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-19000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.187341"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.255121"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-19417-epoch-1/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-19417-epoch-1/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a16f82056754bffa2130d9159725a97","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=13125.0, style=ProgressStyle(desc…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rRunning loss: 0.238642"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.315969"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-19500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-19500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.299668"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.294703"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-20000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-20000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-20000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-20000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 2\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.256248"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.275191"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-20500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-20500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.215015"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.245671Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.263894"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-21000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-21000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-21000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-21000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.198689"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.237353Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.289384"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-21500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-21500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.279236"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.236082"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-22000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-22000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-22000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-22000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.325377Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.269550"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-22500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-22500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.244749"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.246023Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.293573"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-23000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-23000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-23000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-23000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.218795"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.215914Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.240500"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-23500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-23500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.195539"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.240655"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-23792-epoch-2/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-23792-epoch-2/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e60845b3ca1d43748b328b465a430a4f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=13125.0, style=ProgressStyle(desc…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rRunning loss: 0.239408"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.230838"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-24000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-24000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-24000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-24000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.225782"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.285993Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.226351"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-24500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-24500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.256432"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.204239Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.197340"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-25000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-25000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-25000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-25000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.283786"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.208157Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.219945"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-25500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-25500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.208555"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.337383"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-26000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-26000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-26000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-26000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.247462"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.295424Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.290104"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-26500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-26500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.295701"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.208245Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.233067"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-27000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-27000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-27000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-27000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.248161"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.223050Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.177869"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-27500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-27500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.238909"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.187026"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-28000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-28000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-28000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-28000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in outputs/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.256837"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.193696Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.193376"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-28167-epoch-3/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-28167-epoch-3/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca7e2e4e82f44438b4461ec2549a9448","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=13125.0, style=ProgressStyle(desc…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rRunning loss: 0.265034"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.330743"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-28500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-28500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.173133"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.243027Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.288029"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-29000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-29000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-29000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-29000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 2\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.207295"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.213045"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-29500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-29500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 3\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.219279Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n","Running loss: 0.197920"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.248191"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-30000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-30000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-30000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-30000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: 达到Early stop最大容忍轮次.\n","INFO:base_runner: 停止训练@.\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/pytorch_model.bin\n","INFO:base_runner: 训练 albert 结束. Saved to ./outputs_tiny.\n","INFO:base_runner: 当前global step： 30000. 运行平局loss： 0.11377298623360693.\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8cf9ab6b0bd4e869bea0afbdd8c3cf9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1875.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","{'LRAP': 0.8801484972131866, 'eval_loss': 0.2407681870142619}\n","[[0.0026207  0.99121094 0.00252151 ... 0.7211914  0.01634216 0.23742676]\n"," [0.01908875 0.79003906 0.01428223 ... 0.25561523 0.01634216 0.7109375 ]\n"," [0.00482941 0.9916992  0.00370789 ... 0.25854492 0.09484863 0.34716797]\n"," ...\n"," [0.00666809 0.88427734 0.00793457 ... 0.5576172  0.00462723 0.515625  ]\n"," [0.00206757 0.9921875  0.00294685 ... 0.76660156 0.01267242 0.26782227]\n"," [0.02377319 0.03622437 0.03521729 ... 0.17700195 0.01177979 0.7832031 ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0LJYJL4_4Czq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593616146837,"user_tz":-480,"elapsed":1914,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}}},"source":[""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJZS8-hR9Pn4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593618394014,"user_tz":-480,"elapsed":1955,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}}},"source":["def main_tiny():\n","    # fine tuning with fixed albert\n","    train_df = process_tsv('train.tsv')\n","    eval_df = process_tsv('val.tsv')\n","    model = MultiBinaryClaRunner('albert',\n","                                 './outputs_tiny/checkpoint-32500',\n","                                 num_labels=80,\n","                                 use_cuda=True,\n","                                 freez_pretrained=True,\n","                                 args={\n","                                     \"train_batch_size\": 8,\n","                                     \"learning_rate\": 0.0001,\n","                                     \"output_dir\": './outputs_tiny',\n","                                     \"best_model_dir\": './outputs_tiny/best_model',\n","                                     \"max_seq_length\": 512,\n","                                     \"reprocess_input_data\": False,\n","                                     \"overwrite_output_dir\": True,\n","                                     \"num_train_epochs\": 10,\n","                                     \"evaluate_during_training\": True,\n","                                     \"evaluate_during_training_steps\": 500,\n","                                     \"use_cached_eval_features\": True,\n","                                     \"save_steps\": 500,\n","                                 })\n","    \n","    model.train_model(train_df, eval_df=eval_df)\n","\n","    result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n","    print(result)\n","    print(model_outputs)\n","\n","    return result, model_outputs, wrong_predictions"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqZ1C3oY9W51","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d43be0dc3b734d9b95cabc0d6107806b","8dcaeb14be2b4731ab8223e48cce377d","3bcedf94ed62480a8f3a9c2079e3ed70","20a9207282a54c84bbd14ecffa96da99","c402f35719654dd796435d017ee14d02","b108491410db43ff821e80d78ab091ee","4168a809532a40aeb8392141ab62cf6b","ad14617861944755afef2acd6bf08b2e","56784ec6e0f84a87be9be438714db01d","cb6a04e4f10b48a1ac066017f35929bb","6ed625e2bc654f20b89cbbd2b9be5081","863d84756a29484b8bf7042ccfa6db65","2909509ef5c445cd8573c2a0dc409e0e","b36fe46338c14f3891266a6f88c904bb","6e2a23a5380c4bf8b42c2dbdfb4c5c8e","8d8c3c47e5d54bc5817f27e72a4530c1","a253b926749a4913afda1843bea393a5","8a48236a488242dfbdaabbbfa1db4983","decdb966601449b99e088ce82b36c8cb","f8259b4d944345838410227b6d53e4e2","f432d75277f04d5fb78e92c0c865990a","5fcea34570c84ed69f45544cb2a123ca","6c226ae13c9344bc876c9331bb26b6e3","25983166e4834416b44a709a70bca8b5","d50e8d2d82274a84a185fadf012d8c72","3a0490291d5b4c8d84ef516e293bcf3e","718fbcf1d43948ddbc9d66b7db3adb3a","3ef8709a8123448bb2c078aa3b8b4744","d0ae7fc656344dcaaa8e7f8b7f2a7c22","5cbccabd002e42b1b0e845a06c26d1e7","bdf2437125f34c8f8b0fbf59b7439ed0","8d54a74c1eaf4a98b5c0229714e5b87c","4603feedccce4105b03df354b92d2371","a5b2aae5ec644bd08e4e82f5b55f542f","b2d3f8d8c8bb444f8d4c950593b0ab3e","4a34c535255d49a2856438b762219a4d","72fb607d8d374acb9127d965f829e232","1654a4fca92c47ddbfeb1e8a666a0bfe","a936d2c288f1474ca64ee00792db3dff","e1af80ed60fd4d88ab4c593e6e645ad7"]},"executionInfo":{"status":"ok","timestamp":1593621474171,"user_tz":-480,"elapsed":3077028,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"e52af4d5-6784-41c3-8770-598aca21330e"},"source":["result, model_outputs, wrong_predictions = main_tiny()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["INFO:transformers.configuration_utils:loading configuration file ./outputs_tiny/checkpoint-32500/config.json\n","INFO:transformers.configuration_utils:Model config AlbertConfig {\n","  \"architectures\": [\n","    \"AlbertForMultiBinaryLabelSeqClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 312,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\",\n","    \"28\": \"LABEL_28\",\n","    \"29\": \"LABEL_29\",\n","    \"30\": \"LABEL_30\",\n","    \"31\": \"LABEL_31\",\n","    \"32\": \"LABEL_32\",\n","    \"33\": \"LABEL_33\",\n","    \"34\": \"LABEL_34\",\n","    \"35\": \"LABEL_35\",\n","    \"36\": \"LABEL_36\",\n","    \"37\": \"LABEL_37\",\n","    \"38\": \"LABEL_38\",\n","    \"39\": \"LABEL_39\",\n","    \"40\": \"LABEL_40\",\n","    \"41\": \"LABEL_41\",\n","    \"42\": \"LABEL_42\",\n","    \"43\": \"LABEL_43\",\n","    \"44\": \"LABEL_44\",\n","    \"45\": \"LABEL_45\",\n","    \"46\": \"LABEL_46\",\n","    \"47\": \"LABEL_47\",\n","    \"48\": \"LABEL_48\",\n","    \"49\": \"LABEL_49\",\n","    \"50\": \"LABEL_50\",\n","    \"51\": \"LABEL_51\",\n","    \"52\": \"LABEL_52\",\n","    \"53\": \"LABEL_53\",\n","    \"54\": \"LABEL_54\",\n","    \"55\": \"LABEL_55\",\n","    \"56\": \"LABEL_56\",\n","    \"57\": \"LABEL_57\",\n","    \"58\": \"LABEL_58\",\n","    \"59\": \"LABEL_59\",\n","    \"60\": \"LABEL_60\",\n","    \"61\": \"LABEL_61\",\n","    \"62\": \"LABEL_62\",\n","    \"63\": \"LABEL_63\",\n","    \"64\": \"LABEL_64\",\n","    \"65\": \"LABEL_65\",\n","    \"66\": \"LABEL_66\",\n","    \"67\": \"LABEL_67\",\n","    \"68\": \"LABEL_68\",\n","    \"69\": \"LABEL_69\",\n","    \"70\": \"LABEL_70\",\n","    \"71\": \"LABEL_71\",\n","    \"72\": \"LABEL_72\",\n","    \"73\": \"LABEL_73\",\n","    \"74\": \"LABEL_74\",\n","    \"75\": \"LABEL_75\",\n","    \"76\": \"LABEL_76\",\n","    \"77\": \"LABEL_77\",\n","    \"78\": \"LABEL_78\",\n","    \"79\": \"LABEL_79\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 1248,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_28\": 28,\n","    \"LABEL_29\": 29,\n","    \"LABEL_3\": 3,\n","    \"LABEL_30\": 30,\n","    \"LABEL_31\": 31,\n","    \"LABEL_32\": 32,\n","    \"LABEL_33\": 33,\n","    \"LABEL_34\": 34,\n","    \"LABEL_35\": 35,\n","    \"LABEL_36\": 36,\n","    \"LABEL_37\": 37,\n","    \"LABEL_38\": 38,\n","    \"LABEL_39\": 39,\n","    \"LABEL_4\": 4,\n","    \"LABEL_40\": 40,\n","    \"LABEL_41\": 41,\n","    \"LABEL_42\": 42,\n","    \"LABEL_43\": 43,\n","    \"LABEL_44\": 44,\n","    \"LABEL_45\": 45,\n","    \"LABEL_46\": 46,\n","    \"LABEL_47\": 47,\n","    \"LABEL_48\": 48,\n","    \"LABEL_49\": 49,\n","    \"LABEL_5\": 5,\n","    \"LABEL_50\": 50,\n","    \"LABEL_51\": 51,\n","    \"LABEL_52\": 52,\n","    \"LABEL_53\": 53,\n","    \"LABEL_54\": 54,\n","    \"LABEL_55\": 55,\n","    \"LABEL_56\": 56,\n","    \"LABEL_57\": 57,\n","    \"LABEL_58\": 58,\n","    \"LABEL_59\": 59,\n","    \"LABEL_6\": 6,\n","    \"LABEL_60\": 60,\n","    \"LABEL_61\": 61,\n","    \"LABEL_62\": 62,\n","    \"LABEL_63\": 63,\n","    \"LABEL_64\": 64,\n","    \"LABEL_65\": 65,\n","    \"LABEL_66\": 66,\n","    \"LABEL_67\": 67,\n","    \"LABEL_68\": 68,\n","    \"LABEL_69\": 69,\n","    \"LABEL_7\": 7,\n","    \"LABEL_70\": 70,\n","    \"LABEL_71\": 71,\n","    \"LABEL_72\": 72,\n","    \"LABEL_73\": 73,\n","    \"LABEL_74\": 74,\n","    \"LABEL_75\": 75,\n","    \"LABEL_76\": 76,\n","    \"LABEL_77\": 77,\n","    \"LABEL_78\": 78,\n","    \"LABEL_79\": 79,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"ln_type\": \"postln\",\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"share_type\": \"all\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 21128\n","}\n","\n","INFO:transformers.modeling_utils:loading weights file ./outputs_tiny/checkpoint-32500/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["全局设置： {'sliding_window': False, 'tie_value': -1, 'stride': 0.8, 'regression': False, 'threshold': 0.5, 'best_model_dir': './outputs_tiny/best_model', 'output_dir': './outputs_tiny', 'overwrite_output_dir': True, 'tensorboard_dir': None, 'cache_dir': 'cache_dir/', 'config': {}, 'do_lower_case': False, 'encoding': None, 'train_batch_size': 8, 'num_train_epochs': 10, 'learning_rate': 0.0001, 'adam_epsilon': 1e-06, 'weight_decay': 0, 'warmup_ratio': 0, 'warmup_steps': 1000, 'max_grad_norm': 1.0, 'max_seq_length': 512, 'n_gpu': 1, 'fp16': True, 'fp16_opt_level': 'O1', 'gradient_accumulation_steps': 3, 'eval_batch_size': 8, 'evaluate_during_training': True, 'evaluate_during_training_steps': 500, 'evaluate_during_training_verbose': False, 'use_early_stopping': True, 'early_stopping_consider_epochs': False, 'early_stopping_delta': 0, 'early_stopping_metric': 'eval_loss', 'early_stopping_metric_minimize': True, 'early_stopping_patience': 3, 'manual_seed': 1234, 'save_best_model': True, 'save_eval_checkpoints': True, 'save_model_every_epoch': True, 'save_steps': 500, 'silent': False, 'logging_steps': 50, 'no_cache': False, 'no_save': False, 'use_cached_eval_features': True, 'use_multiprocessing': True, 'process_count': 1, 'multiprocessing_chunksize': 500, 'reprocess_input_data': False, 'wandb_project': None, 'wandb_kwargs': {}, 'model_file': './outputs_tiny/checkpoint-32500', 'model_type': 'albert'}\n","模型设置： AlbertConfig {\n","  \"architectures\": [\n","    \"AlbertForMultiBinaryLabelSeqClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 312,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\",\n","    \"28\": \"LABEL_28\",\n","    \"29\": \"LABEL_29\",\n","    \"30\": \"LABEL_30\",\n","    \"31\": \"LABEL_31\",\n","    \"32\": \"LABEL_32\",\n","    \"33\": \"LABEL_33\",\n","    \"34\": \"LABEL_34\",\n","    \"35\": \"LABEL_35\",\n","    \"36\": \"LABEL_36\",\n","    \"37\": \"LABEL_37\",\n","    \"38\": \"LABEL_38\",\n","    \"39\": \"LABEL_39\",\n","    \"40\": \"LABEL_40\",\n","    \"41\": \"LABEL_41\",\n","    \"42\": \"LABEL_42\",\n","    \"43\": \"LABEL_43\",\n","    \"44\": \"LABEL_44\",\n","    \"45\": \"LABEL_45\",\n","    \"46\": \"LABEL_46\",\n","    \"47\": \"LABEL_47\",\n","    \"48\": \"LABEL_48\",\n","    \"49\": \"LABEL_49\",\n","    \"50\": \"LABEL_50\",\n","    \"51\": \"LABEL_51\",\n","    \"52\": \"LABEL_52\",\n","    \"53\": \"LABEL_53\",\n","    \"54\": \"LABEL_54\",\n","    \"55\": \"LABEL_55\",\n","    \"56\": \"LABEL_56\",\n","    \"57\": \"LABEL_57\",\n","    \"58\": \"LABEL_58\",\n","    \"59\": \"LABEL_59\",\n","    \"60\": \"LABEL_60\",\n","    \"61\": \"LABEL_61\",\n","    \"62\": \"LABEL_62\",\n","    \"63\": \"LABEL_63\",\n","    \"64\": \"LABEL_64\",\n","    \"65\": \"LABEL_65\",\n","    \"66\": \"LABEL_66\",\n","    \"67\": \"LABEL_67\",\n","    \"68\": \"LABEL_68\",\n","    \"69\": \"LABEL_69\",\n","    \"70\": \"LABEL_70\",\n","    \"71\": \"LABEL_71\",\n","    \"72\": \"LABEL_72\",\n","    \"73\": \"LABEL_73\",\n","    \"74\": \"LABEL_74\",\n","    \"75\": \"LABEL_75\",\n","    \"76\": \"LABEL_76\",\n","    \"77\": \"LABEL_77\",\n","    \"78\": \"LABEL_78\",\n","    \"79\": \"LABEL_79\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 1248,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_28\": 28,\n","    \"LABEL_29\": 29,\n","    \"LABEL_3\": 3,\n","    \"LABEL_30\": 30,\n","    \"LABEL_31\": 31,\n","    \"LABEL_32\": 32,\n","    \"LABEL_33\": 33,\n","    \"LABEL_34\": 34,\n","    \"LABEL_35\": 35,\n","    \"LABEL_36\": 36,\n","    \"LABEL_37\": 37,\n","    \"LABEL_38\": 38,\n","    \"LABEL_39\": 39,\n","    \"LABEL_4\": 4,\n","    \"LABEL_40\": 40,\n","    \"LABEL_41\": 41,\n","    \"LABEL_42\": 42,\n","    \"LABEL_43\": 43,\n","    \"LABEL_44\": 44,\n","    \"LABEL_45\": 45,\n","    \"LABEL_46\": 46,\n","    \"LABEL_47\": 47,\n","    \"LABEL_48\": 48,\n","    \"LABEL_49\": 49,\n","    \"LABEL_5\": 5,\n","    \"LABEL_50\": 50,\n","    \"LABEL_51\": 51,\n","    \"LABEL_52\": 52,\n","    \"LABEL_53\": 53,\n","    \"LABEL_54\": 54,\n","    \"LABEL_55\": 55,\n","    \"LABEL_56\": 56,\n","    \"LABEL_57\": 57,\n","    \"LABEL_58\": 58,\n","    \"LABEL_59\": 59,\n","    \"LABEL_6\": 6,\n","    \"LABEL_60\": 60,\n","    \"LABEL_61\": 61,\n","    \"LABEL_62\": 62,\n","    \"LABEL_63\": 63,\n","    \"LABEL_64\": 64,\n","    \"LABEL_65\": 65,\n","    \"LABEL_66\": 66,\n","    \"LABEL_67\": 67,\n","    \"LABEL_68\": 68,\n","    \"LABEL_69\": 69,\n","    \"LABEL_7\": 7,\n","    \"LABEL_70\": 70,\n","    \"LABEL_71\": 71,\n","    \"LABEL_72\": 72,\n","    \"LABEL_73\": 73,\n","    \"LABEL_74\": 74,\n","    \"LABEL_75\": 75,\n","    \"LABEL_76\": 76,\n","    \"LABEL_77\": 77,\n","    \"LABEL_78\": 78,\n","    \"LABEL_79\": 79,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"ln_type\": \"postln\",\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"share_type\": \"all\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 21128\n","}\n","\n","Freeze参数： True\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing AlbertForMultiBinaryLabelSeqClassification.\n","\n","INFO:transformers.modeling_utils:All the weights of AlbertForMultiBinaryLabelSeqClassification were initialized from the model checkpoint at ./outputs_tiny/checkpoint-32500.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use AlbertForMultiBinaryLabelSeqClassification for predictions without further training.\n","INFO:transformers.tokenization_utils_base:Model name './outputs_tiny/checkpoint-32500' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './outputs_tiny/checkpoint-32500' is a path, a model identifier, or url to a directory containing tokenizer files.\n","INFO:transformers.tokenization_utils_base:Didn't find file ./outputs_tiny/checkpoint-32500/added_tokens.json. We won't load it.\n","INFO:transformers.tokenization_utils_base:Didn't find file ./outputs_tiny/checkpoint-32500/tokenizer.json. We won't load it.\n","INFO:transformers.tokenization_utils_base:loading file ./outputs_tiny/checkpoint-32500/vocab.txt\n","INFO:transformers.tokenization_utils_base:loading file None\n","INFO:transformers.tokenization_utils_base:loading file ./outputs_tiny/checkpoint-32500/special_tokens_map.json\n","INFO:transformers.tokenization_utils_base:loading file ./outputs_tiny/checkpoint-32500/tokenizer_config.json\n","INFO:transformers.tokenization_utils_base:loading file None\n","INFO:base_runner:>>> 加载缓存的特征文件 cache_dir/cached_train_albert_512_80_105000\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d43be0dc3b734d9b95cabc0d6107806b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:base_runner:>>> 加载checkpoint，更新global_step.\n","INFO:base_runner:>>> 已训练 7 轮.\n","INFO:base_runner:>>> 当前global_step： 32500.\n","INFO:base_runner:>>> 跳过当前epoch 1875 steps in.\n"],"name":"stderr"},{"output_type":"stream","text":["Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56784ec6e0f84a87be9be438714db01d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=13125.0, style=ProgressStyle(desc…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.226807"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.240977"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-33000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-33000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-33000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-33000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.246680"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.258229"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-33500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-33500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-33500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-33500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 2\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.279990"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.207837"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-34000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-34000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-34000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-34000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.257744"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.213402"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-34500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-34500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-34500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-34500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.290923"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.196456"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-35000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-35000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-35000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-35000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.209229"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.243464"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-35500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-35500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-35500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-35500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.234690"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.265116"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-36000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-36000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-36000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-36000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.269945"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.232943"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-36250-epoch-1/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-36250-epoch-1/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a253b926749a4913afda1843bea393a5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=13125.0, style=ProgressStyle(desc…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Running loss: 0.174249"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.267286"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-36500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-36500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-36500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-36500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.226679"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.176862"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-37000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-37000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-37000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-37000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.239826"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.213056"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-37500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-37500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-37500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-37500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.188991"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.256176"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-38000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-38000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-38000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-38000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.229409"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.213904"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-38500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-38500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-38500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-38500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.201331"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.242138"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-39000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-39000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-39000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-39000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.252540"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.228774"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-39500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-39500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-39500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-39500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.230188"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.280730"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-40000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-40000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-40000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-40000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.235837"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.217638"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-40500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-40500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-40500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-40500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.272072"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.227960"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-40625-epoch-2/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-40625-epoch-2/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d50e8d2d82274a84a185fadf012d8c72","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=13125.0, style=ProgressStyle(desc…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rRunning loss: 0.262026"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.231763"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-41000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-41000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-41000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-41000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.192454"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.187628"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-41500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-41500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-41500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-41500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 2\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.292448"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.261699"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-42000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-42000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-42000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-42000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.192020"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.240159"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-42500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-42500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-42500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-42500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.162113"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.208099"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-43000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-43000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-43000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-43000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.201723"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.244109"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-43500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-43500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-43500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-43500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:base_runner: Early stop指标 eval_loss 没有提升.\n","INFO:base_runner: 当前已执行early stop count: 1\n","INFO:base_runner: Early stop最大容忍轮次: 3\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.149112"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.214761"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-44000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-44000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-44000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-44000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.180936"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.253868"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-44500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-44500/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-44500/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-44500/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\rRunning loss: 0.203847"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"],"name":"stderr"},{"output_type":"stream","text":["Running loss: 0.247618"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-45000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-45000/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-45000/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-45000/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/best_model/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/best_model/pytorch_model.bin\n","INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/checkpoint-45000-epoch-3/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/checkpoint-45000-epoch-3/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.configuration_utils:Configuration saved in ./outputs_tiny/config.json\n","INFO:transformers.modeling_utils:Model weights saved in ./outputs_tiny/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:base_runner: 训练 albert 结束. Saved to ./outputs_tiny.\n","INFO:base_runner: 当前global step： 45000. 运行平局loss： 0.06435975685798459.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4603feedccce4105b03df354b92d2371","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1875.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","{'LRAP': 0.8821297770655848, 'eval_loss': 0.23851567214330038}\n","[[0.00300407 0.9946289  0.00234985 ... 0.77001953 0.02546692 0.15515137]\n"," [0.01652527 0.8105469  0.01040649 ... 0.22033691 0.01869202 0.71875   ]\n"," [0.00344276 0.9970703  0.00202751 ... 0.27783203 0.13757324 0.23400879]\n"," ...\n"," [0.00542831 0.9482422  0.00593567 ... 0.3935547  0.00566483 0.6254883 ]\n"," [0.0017004  0.9951172  0.00203514 ... 0.6191406  0.01428223 0.3701172 ]\n"," [0.0236969  0.01864624 0.0292511  ... 0.05706787 0.01205444 0.9223633 ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n_Yf__RL9bMa","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TUkPwOWB7KZ3","colab_type":"text"},"source":["## 测试"]},{"cell_type":"code","metadata":{"id":"zKt3N1ZI7LhC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["68782099c22a4702ba62ca0b9822a8d3","80de9f7a7286446e9be88cfe77185b19","00bc04ea1cb94372b500e8301f663c6d","d262067fe18748758fbec5efd17865e5","da21235454f443fa971b6162d0f7ead3","291345ca98194e06a7142d7752e95a0a","0d9a1d4d2a434e5381fd07c37cc836a4","c841b293d16b495596d03da29183c65d"]},"executionInfo":{"status":"ok","timestamp":1593616452372,"user_tz":-480,"elapsed":78655,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"48a53c93-44d0-4106-c58d-0efc4d39396f"},"source":["eval_df = process_tsv('val.tsv')\n","model = MultiBinaryClaRunner('albert',\n","                                './outputs/best_model',\n","                                num_labels=80,\n","                                use_cuda=True,\n","                                freez_pretrained=False,\n","                                args={\n","                                    \"output_dir\": './outputs_tiny',\n","                                    \"max_seq_length\": 512,\n","                                    \"reprocess_input_data\": False,\n","                                    \"overwrite_output_dir\": True,\n","                                    \"use_cached_eval_features\": True,\n","                                })\n","\n","result, model_outputs, wrong_predictions = model.eval_model(eval_df)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["INFO:transformers.configuration_utils:loading configuration file ./outputs/best_model/config.json\n","INFO:transformers.configuration_utils:Model config AlbertConfig {\n","  \"architectures\": [\n","    \"AlbertForMultiBinaryLabelSeqClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 312,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\",\n","    \"28\": \"LABEL_28\",\n","    \"29\": \"LABEL_29\",\n","    \"30\": \"LABEL_30\",\n","    \"31\": \"LABEL_31\",\n","    \"32\": \"LABEL_32\",\n","    \"33\": \"LABEL_33\",\n","    \"34\": \"LABEL_34\",\n","    \"35\": \"LABEL_35\",\n","    \"36\": \"LABEL_36\",\n","    \"37\": \"LABEL_37\",\n","    \"38\": \"LABEL_38\",\n","    \"39\": \"LABEL_39\",\n","    \"40\": \"LABEL_40\",\n","    \"41\": \"LABEL_41\",\n","    \"42\": \"LABEL_42\",\n","    \"43\": \"LABEL_43\",\n","    \"44\": \"LABEL_44\",\n","    \"45\": \"LABEL_45\",\n","    \"46\": \"LABEL_46\",\n","    \"47\": \"LABEL_47\",\n","    \"48\": \"LABEL_48\",\n","    \"49\": \"LABEL_49\",\n","    \"50\": \"LABEL_50\",\n","    \"51\": \"LABEL_51\",\n","    \"52\": \"LABEL_52\",\n","    \"53\": \"LABEL_53\",\n","    \"54\": \"LABEL_54\",\n","    \"55\": \"LABEL_55\",\n","    \"56\": \"LABEL_56\",\n","    \"57\": \"LABEL_57\",\n","    \"58\": \"LABEL_58\",\n","    \"59\": \"LABEL_59\",\n","    \"60\": \"LABEL_60\",\n","    \"61\": \"LABEL_61\",\n","    \"62\": \"LABEL_62\",\n","    \"63\": \"LABEL_63\",\n","    \"64\": \"LABEL_64\",\n","    \"65\": \"LABEL_65\",\n","    \"66\": \"LABEL_66\",\n","    \"67\": \"LABEL_67\",\n","    \"68\": \"LABEL_68\",\n","    \"69\": \"LABEL_69\",\n","    \"70\": \"LABEL_70\",\n","    \"71\": \"LABEL_71\",\n","    \"72\": \"LABEL_72\",\n","    \"73\": \"LABEL_73\",\n","    \"74\": \"LABEL_74\",\n","    \"75\": \"LABEL_75\",\n","    \"76\": \"LABEL_76\",\n","    \"77\": \"LABEL_77\",\n","    \"78\": \"LABEL_78\",\n","    \"79\": \"LABEL_79\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 1248,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_28\": 28,\n","    \"LABEL_29\": 29,\n","    \"LABEL_3\": 3,\n","    \"LABEL_30\": 30,\n","    \"LABEL_31\": 31,\n","    \"LABEL_32\": 32,\n","    \"LABEL_33\": 33,\n","    \"LABEL_34\": 34,\n","    \"LABEL_35\": 35,\n","    \"LABEL_36\": 36,\n","    \"LABEL_37\": 37,\n","    \"LABEL_38\": 38,\n","    \"LABEL_39\": 39,\n","    \"LABEL_4\": 4,\n","    \"LABEL_40\": 40,\n","    \"LABEL_41\": 41,\n","    \"LABEL_42\": 42,\n","    \"LABEL_43\": 43,\n","    \"LABEL_44\": 44,\n","    \"LABEL_45\": 45,\n","    \"LABEL_46\": 46,\n","    \"LABEL_47\": 47,\n","    \"LABEL_48\": 48,\n","    \"LABEL_49\": 49,\n","    \"LABEL_5\": 5,\n","    \"LABEL_50\": 50,\n","    \"LABEL_51\": 51,\n","    \"LABEL_52\": 52,\n","    \"LABEL_53\": 53,\n","    \"LABEL_54\": 54,\n","    \"LABEL_55\": 55,\n","    \"LABEL_56\": 56,\n","    \"LABEL_57\": 57,\n","    \"LABEL_58\": 58,\n","    \"LABEL_59\": 59,\n","    \"LABEL_6\": 6,\n","    \"LABEL_60\": 60,\n","    \"LABEL_61\": 61,\n","    \"LABEL_62\": 62,\n","    \"LABEL_63\": 63,\n","    \"LABEL_64\": 64,\n","    \"LABEL_65\": 65,\n","    \"LABEL_66\": 66,\n","    \"LABEL_67\": 67,\n","    \"LABEL_68\": 68,\n","    \"LABEL_69\": 69,\n","    \"LABEL_7\": 7,\n","    \"LABEL_70\": 70,\n","    \"LABEL_71\": 71,\n","    \"LABEL_72\": 72,\n","    \"LABEL_73\": 73,\n","    \"LABEL_74\": 74,\n","    \"LABEL_75\": 75,\n","    \"LABEL_76\": 76,\n","    \"LABEL_77\": 77,\n","    \"LABEL_78\": 78,\n","    \"LABEL_79\": 79,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"ln_type\": \"postln\",\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"share_type\": \"all\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 21128\n","}\n","\n","INFO:transformers.modeling_utils:loading weights file ./outputs/best_model/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["全局设置： {'sliding_window': False, 'tie_value': -1, 'stride': 0.8, 'regression': False, 'threshold': 0.5, 'best_model_dir': 'outputs/best_model', 'output_dir': './outputs_tiny', 'overwrite_output_dir': True, 'tensorboard_dir': None, 'cache_dir': 'cache_dir/', 'config': {}, 'do_lower_case': False, 'encoding': None, 'train_batch_size': 8, 'num_train_epochs': 10, 'learning_rate': 4e-05, 'adam_epsilon': 1e-06, 'weight_decay': 0, 'warmup_ratio': 0, 'warmup_steps': 1000, 'max_grad_norm': 1.0, 'max_seq_length': 512, 'n_gpu': 1, 'fp16': True, 'fp16_opt_level': 'O1', 'gradient_accumulation_steps': 3, 'eval_batch_size': 8, 'evaluate_during_training': True, 'evaluate_during_training_steps': 500, 'evaluate_during_training_verbose': False, 'use_early_stopping': True, 'early_stopping_consider_epochs': False, 'early_stopping_delta': 0, 'early_stopping_metric': 'eval_loss', 'early_stopping_metric_minimize': True, 'early_stopping_patience': 3, 'manual_seed': 1234, 'save_best_model': True, 'save_eval_checkpoints': True, 'save_model_every_epoch': True, 'save_steps': 1000, 'silent': False, 'logging_steps': 50, 'no_cache': False, 'no_save': False, 'use_cached_eval_features': True, 'use_multiprocessing': True, 'process_count': 1, 'multiprocessing_chunksize': 500, 'reprocess_input_data': False, 'wandb_project': None, 'wandb_kwargs': {}, 'model_file': './outputs/best_model', 'model_type': 'albert'}\n","模型设置： AlbertConfig {\n","  \"architectures\": [\n","    \"AlbertForMultiBinaryLabelSeqClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 312,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\",\n","    \"28\": \"LABEL_28\",\n","    \"29\": \"LABEL_29\",\n","    \"30\": \"LABEL_30\",\n","    \"31\": \"LABEL_31\",\n","    \"32\": \"LABEL_32\",\n","    \"33\": \"LABEL_33\",\n","    \"34\": \"LABEL_34\",\n","    \"35\": \"LABEL_35\",\n","    \"36\": \"LABEL_36\",\n","    \"37\": \"LABEL_37\",\n","    \"38\": \"LABEL_38\",\n","    \"39\": \"LABEL_39\",\n","    \"40\": \"LABEL_40\",\n","    \"41\": \"LABEL_41\",\n","    \"42\": \"LABEL_42\",\n","    \"43\": \"LABEL_43\",\n","    \"44\": \"LABEL_44\",\n","    \"45\": \"LABEL_45\",\n","    \"46\": \"LABEL_46\",\n","    \"47\": \"LABEL_47\",\n","    \"48\": \"LABEL_48\",\n","    \"49\": \"LABEL_49\",\n","    \"50\": \"LABEL_50\",\n","    \"51\": \"LABEL_51\",\n","    \"52\": \"LABEL_52\",\n","    \"53\": \"LABEL_53\",\n","    \"54\": \"LABEL_54\",\n","    \"55\": \"LABEL_55\",\n","    \"56\": \"LABEL_56\",\n","    \"57\": \"LABEL_57\",\n","    \"58\": \"LABEL_58\",\n","    \"59\": \"LABEL_59\",\n","    \"60\": \"LABEL_60\",\n","    \"61\": \"LABEL_61\",\n","    \"62\": \"LABEL_62\",\n","    \"63\": \"LABEL_63\",\n","    \"64\": \"LABEL_64\",\n","    \"65\": \"LABEL_65\",\n","    \"66\": \"LABEL_66\",\n","    \"67\": \"LABEL_67\",\n","    \"68\": \"LABEL_68\",\n","    \"69\": \"LABEL_69\",\n","    \"70\": \"LABEL_70\",\n","    \"71\": \"LABEL_71\",\n","    \"72\": \"LABEL_72\",\n","    \"73\": \"LABEL_73\",\n","    \"74\": \"LABEL_74\",\n","    \"75\": \"LABEL_75\",\n","    \"76\": \"LABEL_76\",\n","    \"77\": \"LABEL_77\",\n","    \"78\": \"LABEL_78\",\n","    \"79\": \"LABEL_79\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 1248,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_28\": 28,\n","    \"LABEL_29\": 29,\n","    \"LABEL_3\": 3,\n","    \"LABEL_30\": 30,\n","    \"LABEL_31\": 31,\n","    \"LABEL_32\": 32,\n","    \"LABEL_33\": 33,\n","    \"LABEL_34\": 34,\n","    \"LABEL_35\": 35,\n","    \"LABEL_36\": 36,\n","    \"LABEL_37\": 37,\n","    \"LABEL_38\": 38,\n","    \"LABEL_39\": 39,\n","    \"LABEL_4\": 4,\n","    \"LABEL_40\": 40,\n","    \"LABEL_41\": 41,\n","    \"LABEL_42\": 42,\n","    \"LABEL_43\": 43,\n","    \"LABEL_44\": 44,\n","    \"LABEL_45\": 45,\n","    \"LABEL_46\": 46,\n","    \"LABEL_47\": 47,\n","    \"LABEL_48\": 48,\n","    \"LABEL_49\": 49,\n","    \"LABEL_5\": 5,\n","    \"LABEL_50\": 50,\n","    \"LABEL_51\": 51,\n","    \"LABEL_52\": 52,\n","    \"LABEL_53\": 53,\n","    \"LABEL_54\": 54,\n","    \"LABEL_55\": 55,\n","    \"LABEL_56\": 56,\n","    \"LABEL_57\": 57,\n","    \"LABEL_58\": 58,\n","    \"LABEL_59\": 59,\n","    \"LABEL_6\": 6,\n","    \"LABEL_60\": 60,\n","    \"LABEL_61\": 61,\n","    \"LABEL_62\": 62,\n","    \"LABEL_63\": 63,\n","    \"LABEL_64\": 64,\n","    \"LABEL_65\": 65,\n","    \"LABEL_66\": 66,\n","    \"LABEL_67\": 67,\n","    \"LABEL_68\": 68,\n","    \"LABEL_69\": 69,\n","    \"LABEL_7\": 7,\n","    \"LABEL_70\": 70,\n","    \"LABEL_71\": 71,\n","    \"LABEL_72\": 72,\n","    \"LABEL_73\": 73,\n","    \"LABEL_74\": 74,\n","    \"LABEL_75\": 75,\n","    \"LABEL_76\": 76,\n","    \"LABEL_77\": 77,\n","    \"LABEL_78\": 78,\n","    \"LABEL_79\": 79,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"ln_type\": \"postln\",\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"share_type\": \"all\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 21128\n","}\n","\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing AlbertForMultiBinaryLabelSeqClassification.\n","\n","INFO:transformers.modeling_utils:All the weights of AlbertForMultiBinaryLabelSeqClassification were initialized from the model checkpoint at ./outputs/best_model.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use AlbertForMultiBinaryLabelSeqClassification for predictions without further training.\n","INFO:transformers.tokenization_utils_base:Model name './outputs/best_model' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './outputs/best_model' is a path, a model identifier, or url to a directory containing tokenizer files.\n","INFO:transformers.tokenization_utils_base:Didn't find file ./outputs/best_model/added_tokens.json. We won't load it.\n","INFO:transformers.tokenization_utils_base:Didn't find file ./outputs/best_model/tokenizer.json. We won't load it.\n","INFO:transformers.tokenization_utils_base:loading file ./outputs/best_model/vocab.txt\n","INFO:transformers.tokenization_utils_base:loading file None\n","INFO:transformers.tokenization_utils_base:loading file ./outputs/best_model/special_tokens_map.json\n","INFO:transformers.tokenization_utils_base:loading file ./outputs/best_model/tokenizer_config.json\n","INFO:transformers.tokenization_utils_base:loading file None\n"],"name":"stderr"},{"output_type":"stream","text":["Freeze参数： False\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68782099c22a4702ba62ca0b9822a8d3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1875.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JgzpgIcf8LUw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593621659980,"user_tz":-480,"elapsed":2167,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"8e940c8c-aa4e-4b54-a069-f977a0bdb959"},"source":["model_outputs.shape"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15000, 80)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"i8f-sORQ8MX1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593621660674,"user_tz":-480,"elapsed":859,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"552b0e9f-ddd6-488c-b57f-f81389d91460"},"source":["m = np.hsplit(model_outputs, 20)  # 每种label的预测\n","print(m[0].shape)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["(15000, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e8U446xX8bsd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1593621663082,"user_tz":-480,"elapsed":2246,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"4df79ea2-1ec4-49a6-9ed8-df235033ae90"},"source":["y_pre = []\n","for lab in m:\n","    lab_pre = []\n","    for item in lab:\n","        i = np.argmax(item)\n","        new = np.zeros((4))\n","        new[i] = 1\n","        lab_pre.append(new)\n","    y_pre.append(lab_pre)\n","\n","print(len(y_pre))\n","print(len(y_pre[0]))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["20\n","15000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4WyT_z1L8fk1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1593621665741,"user_tz":-480,"elapsed":2527,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"8c9f0e96-a496-4705-966e-c5cad7b98fa0"},"source":["y = eval_df.label.to_numpy()\n","y_split = [np.split(ar, 20) for ar in y]\n","y_zip = list(zip(*y_split))\n","\n","print(y_split[0])"],"execution_count":23,"outputs":[{"output_type":"stream","text":["[array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 0., 1.]), array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 1., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 0., 1.]), array([0., 0., 1., 0.])]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o8vy3mnH8jgL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593621666365,"user_tz":-480,"elapsed":1848,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}}},"source":["def cal_f1(label_num,predicted,truth):\n","    results = []\n","    for i in range(label_num):\n","        results.append({\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0})\n","    \n","    for i, p in enumerate(predicted):\n","        t = truth[i]\n","        for j in range(label_num):\n","            if p[j] == 1:\n","                if t[j] == 1:\n","                    results[j]['TP'] += 1\n","                else:\n","                    results[j]['FP'] += 1\n","            else:\n","                if t[j] == 1:\n","                    results[j]['FN'] += 1\n","                else:\n","                    results[j]['TN'] += 1\n","    \n","    precision = [0.0] * label_num\n","    recall = [0.0] * label_num\n","    f1 = [0.0] * label_num\n","    for i in range(label_num):\n","        if results[i]['TP'] == 0:\n","            if results[i]['FP']==0 and results[i]['FN']==0:\n","                precision[i] = 1.0\n","                recall[i] = 1.0\n","                f1[i] = 1.0\n","            else:\n","                precision[i] = 0.0\n","                recall[i] = 0.0\n","                f1[i] = 0.0\n","        else:\n","            precision[i] = results[i]['TP'] / (results[i]['TP'] + results[i]['FP'])\n","            recall[i] = results[i]['TP'] / (results[i]['TP'] + results[i]['FN'])\n","            f1[i] =  2 * precision[i] * recall[i] / (precision[i] + recall[i])\n","    \n","    # for i in range(label_num):\n","    #     print(i,results[i], precision[i], recall[i], f1[i])\n","    return sum(f1)/label_num, sum(precision)/label_num, sum(recall)/label_num"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"dbmT_pXV8qKL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"status":"ok","timestamp":1593621668733,"user_tz":-480,"elapsed":2455,"user":{"displayName":"Racle He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNRi7DUXsEvpMRqVqHTUF_Oen4KW7kU7MKQekk=s64","userId":"10673173760458122172"}},"outputId":"2c04c348-5e36-4f5b-e220-320af52e0085"},"source":["results = {}\n","total_f1 = 0\n","for i in range(20):\n","    # print(\"# Get f1 score for\",label_name)\n","    f1,precision,recall = cal_f1(4, y_pre[i], y_zip[i])\n","    results[i] = f1\n","    total_f1 += f1\n","    print(\"# {0} - {1}\".format(i,f1))\n","\n","final_f1 = total_f1 / len(results)\n","    \n","print(final_f1)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["# 0 - 0.44064020977293755\n","# 1 - 0.3696688644733156\n","# 2 - 0.3982878840786615\n","# 3 - 0.34883744707704983\n","# 4 - 0.6016700385214818\n","# 5 - 0.45781193386366087\n","# 6 - 0.42781576926606496\n","# 7 - 0.524714241799282\n","# 8 - 0.4495948166820826\n","# 9 - 0.5343606252608031\n","# 10 - 0.4363348338622312\n","# 11 - 0.5021951116568629\n","# 12 - 0.3748723510664493\n","# 13 - 0.49197442669459646\n","# 14 - 0.4101902428777168\n","# 15 - 0.506711133024883\n","# 16 - 0.3371772537035236\n","# 17 - 0.3672668029606441\n","# 18 - 0.5164810910196235\n","# 19 - 0.5158296444530948\n","0.4506217361057483\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GwdWVJkh8vAy","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}